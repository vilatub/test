# –£—á–µ–±–Ω—ã–µ –ø—Ä–æ–µ–∫—Ç—ã –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–∞–Ω–Ω—ã—Ö –∏ –º–∞—à–∏–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é

–†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∞–Ω–∞–ª–∏–∑–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Python, –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö.

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è

```
.
‚îú‚îÄ‚îÄ notebooks/              # Jupyter notebooks —Å –∞–Ω–∞–ª–∏–∑–æ–º
‚îÇ   ‚îú‚îÄ‚îÄ titanic/           # –ê–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¢–∏—Ç–∞–Ω–∏–∫
‚îÇ   ‚îî‚îÄ‚îÄ game_of_thrones/   # –ê–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ Game of Thrones
‚îú‚îÄ‚îÄ datasets/              # –î–∞—Ç–∞—Å–µ—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
‚îú‚îÄ‚îÄ models/                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
‚îú‚îÄ‚îÄ requirements.txt       # –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ Python
‚îî‚îÄ‚îÄ README.md             # –≠—Ç–æ—Ç —Ñ–∞–π–ª
```

## üéØ –¶–µ–ª—å –ø—Ä–æ–µ–∫—Ç–∞

–≠—Ç–æ—Ç —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π —Å–æ–∑–¥–∞–Ω –¥–ª—è **–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Ü–µ–ª–µ–π** –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç:
- –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª Data Science –ø—Ä–æ–µ–∫—Ç–∞
- –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö (EDA)
- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö
- –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É –∏ Feature Engineering
- –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
- –û—Ü–µ–Ω–∫—É –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –º–æ–¥–µ–ª–µ–π

## üìä –î–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–æ–µ–∫—Ç—ã

### 1. –ê–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¢–∏—Ç–∞–Ω–∏–∫
**–ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ:** `notebooks/titanic/titanic_analysis.ipynb`

–ü–æ–¥—Ä–æ–±–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–Ω–∞–º–µ–Ω–∏—Ç–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø–∞—Å—Å–∞–∂–∏—Ä–æ–≤ –¢–∏—Ç–∞–Ω–∏–∫–∞.

**–ó–∞–¥–∞—á–∞:** –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –≤—ã–∂–∏–≤–∞–µ–º–æ—Å—Ç—å –ø–∞—Å—Å–∞–∂–∏—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫

**–ß—Ç–æ –≤–∫–ª—é—á–µ–Ω–æ:**
- –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö (EDA)
- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
- Feature Engineering (—Å–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
- 6 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
- –û—Ü–µ–Ω–∫–∞ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
- –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

**–õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:** Random Forest —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é ~82-85%

---

### 2. –ê–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ Game of Thrones
**–ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ:** `notebooks/game_of_thrones/got_analysis.ipynb`

–ê–Ω–∞–ª–∏–∑ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –≤—Å–µ–ª–µ–Ω–Ω–æ–π "–ò–≥—Ä—ã –ü—Ä–µ—Å—Ç–æ–ª–æ–≤" –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ A Wiki of Ice and Fire.

**–ó–∞–¥–∞—á–∞:** –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å, –∫—Ç–æ –∏–∑ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π —É–º—Ä–µ—Ç, –∞ –∫—Ç–æ –æ—Å—Ç–∞–Ω–µ—Ç—Å—è –≤ –∂–∏–≤—ã—Ö

**–ü—Ä–∏–∑–Ω–∞–∫–∏:**
- –°–æ—Ü–∏–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å –∏ –∑–Ω–∞—Ç–Ω–æ—Å—Ç—å
- –î–æ–º –∏ –∫—É–ª—å—Ç—É—Ä–∞
- –ü–æ—è–≤–ª–µ–Ω–∏—è –≤ –∫–Ω–∏–≥–∞—Ö
- –í–æ–∑—Ä–∞—Å—Ç –∏ –ø–æ–ª
- –°–µ–º–µ–π–Ω—ã–µ —Å–≤—è–∑–∏ (—Ä–æ–¥–∏—Ç–µ–ª–∏, —Å—É–ø—Ä—É–≥–∏, –Ω–∞—Å–ª–µ–¥–Ω–∏–∫–∏)
- –°—Ç–∞—Ç—É—Å —Ä–æ–¥—Å—Ç–≤–µ–Ω–Ω–∏–∫–æ–≤ (–∂–∏–≤—ã/–º–µ—Ä—Ç–≤—ã)
- –ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å –Ω–∞ –≤–∏–∫–∏

**–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è:** isAlive - –∂–∏–≤ –ª–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂

---

## üõ†Ô∏è –£—Å—Ç–∞–Ω–æ–≤–∫–∞

### –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è
- Python 3.8+
- pip –∏–ª–∏ conda

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```bash
pip install -r requirements.txt
```

–ò–ª–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º conda:

```bash
conda install --file requirements.txt
```

## üñ•Ô∏è –†–∞–±–æ—Ç–∞ –Ω–∞ Windows

**–í–∞–∂–Ω–æ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π Windows:** Jupyter Notebook –Ω–∞ Windows –º–æ–∂–µ—Ç —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ñ–∞–π–ª—ã —Å Windows line endings (CRLF), —á—Ç–æ —Å–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –≤ git.

üëâ **–°–º. –ø–æ–¥—Ä–æ–±–Ω—É—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é:** [WINDOWS_SETUP.md](WINDOWS_SETUP.md)

**–ö—Ä–∞—Ç–∫–∞—è –≤–µ—Ä—Å–∏—è:**
```bash
# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ git (–æ–¥–∏–Ω —Ä–∞–∑)
git config core.autocrlf input

# Pre-commit hook –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø—Ä–∞–≤–∏—Ç line endings
```

–†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π —É–∂–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω —Å `.gitattributes` –∏ pre-commit hook –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã.

---

## üöÄ –ó–∞–ø—É—Å–∫

### –õ–æ–∫–∞–ª—å–Ω–æ —Å Jupyter Notebook

```bash
# –ó–∞–ø—É—Å–∫ Jupyter
jupyter notebook

# –ó–∞—Ç–µ–º –æ—Ç–∫—Ä–æ–π—Ç–µ –Ω—É–∂–Ω—ã–π –Ω–æ—É—Ç–±—É–∫ –∏–∑ –ø–∞–ø–∫–∏ notebooks/
```

### –í Google Colab

1. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –Ω—É–∂–Ω—ã–π `.ipynb` —Ñ–∞–π–ª –Ω–∞ Google Drive
2. –û—Ç–∫—Ä–æ–π—Ç–µ —Å –ø–æ–º–æ—â—å—é Google Colaboratory
3. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –≤—Å–µ —è—á–µ–π–∫–∏ (Runtime ‚Üí Run all)

## üìö –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏

- **pandas** - —Ä–∞–±–æ—Ç–∞ —Å —Ç–∞–±–ª–∏—á–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
- **numpy** - —á–∏—Å–ª–µ–Ω–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è
- **matplotlib** - –±–∞–∑–æ–≤–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
- **seaborn** - —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
- **scikit-learn** - –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
- **jupyter** - –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ notebooks

## üéì –û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è —Ü–µ–Ω–Ω–æ—Å—Ç—å

–ö–∞–∂–¥—ã–π –Ω–æ—É—Ç–±—É–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç:
- ‚úÖ –ü–æ–¥—Ä–æ–±–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
- ‚úÖ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
- ‚úÖ –ü–æ—à–∞–≥–æ–≤—ã–π –ø—Ä–æ—Ü–µ—Å—Å –∞–Ω–∞–ª–∏–∑–∞
- ‚úÖ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤
- ‚úÖ –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

–ò–¥–µ–∞–ª—å–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è:
- üìñ –û–±—É—á–µ–Ω–∏—è Data Science —Å –Ω—É–ª—è
- üíº –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∫ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏—è–º
- üèÜ –£—á–∞—Å—Ç–∏—è –≤ Kaggle —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è—Ö
- üíª –°–æ–∑–¥–∞–Ω–∏—è –ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ –ø—Ä–æ–µ–∫—Ç–æ–≤
- üë®‚Äçüè´ –ü—Ä–µ–ø–æ–¥–∞–≤–∞–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö

## üí° –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

### –î–ª—è –æ–±—É—á–µ–Ω–∏—è
1. –ß–∏—Ç–∞–π—Ç–µ –∫–æ–¥ –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ
2. –ó–∞–ø—É—Å–∫–∞–π—Ç–µ —è—á–µ–π–∫–∏ –∏ –∏–∑—É—á–∞–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
3. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
4. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑ –Ω–∞ –¥—Ä—É–≥–∏—Ö –¥–∞–Ω–Ω—ã—Ö

### –î–ª—è –ø—Ä–∞–∫—Ç–∏–∫–∏
1. –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–π—Ç–µ –∫–æ–¥
2. –°–æ–∑–¥–∞–≤–∞–π—Ç–µ –Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
3. –ü—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏
4. –£–ª—É—á—à–∞–π—Ç–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞

### –ö–∞–∫ —à–∞–±–ª–æ–Ω –¥–ª—è —Å–≤–æ–∏—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤
1. –°–∫–æ–ø–∏—Ä—É–π—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –Ω–æ—É—Ç–±—É–∫–∞
2. –ê–¥–∞–ø—Ç–∏—Ä—É–π—Ç–µ –ø–æ–¥ —Å–≤–æ–∏ –¥–∞–Ω–Ω—ã–µ
3. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–µ –∂–µ —ç—Ç–∞–ø—ã –∞–Ω–∞–ª–∏–∑–∞
4. –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —Å–≤–æ–∏ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è

## üîÆ –ü–ª–∞–Ω–∏—Ä—É–µ–º—ã–µ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è

- [ ] –ê–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ Iris
- [ ] –ê–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ Boston Housing
- [ ] –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã (Time Series)
- [ ] NLP –ø—Ä–æ–µ–∫—Ç—ã
- [ ] Computer Vision –ø—Ä–∏–º–µ—Ä—ã
- [ ] –†–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã

## üìà –û–±—â–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–∞–∂–¥–æ–≥–æ –Ω–æ—É—Ç–±—É–∫–∞

–í—Å–µ –Ω–æ—É—Ç–±—É–∫–∏ —Å–ª–µ–¥—É—é—Ç –µ–¥–∏–Ω–æ–º—É —à–∞–±–ª–æ–Ω—É:

1. **–ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫** - –∑–∞–≥—Ä—É–∑–∫–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
2. **–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö** - –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
3. **–ü–µ—Ä–≤–∏—á–Ω—ã–π –æ—Å–º–æ—Ç—Ä** - –∑–Ω–∞–∫–æ–º—Å—Ç–≤–æ —Å –¥–∞–Ω–Ω—ã–º–∏
4. **–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—Å–∫–æ–≤** - —Ä–∞–±–æ—Ç–∞ —Å missing values
5. **EDA** - –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
6. **–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è** - –≥—Ä–∞—Ñ–∏–∫–∏ –∏ –¥–∏–∞–≥—Ä–∞–º–º—ã
7. **–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞** - –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
8. **Feature Engineering** - —Å–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
9. **–ú–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ** - –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
10. **–°—Ä–∞–≤–Ω–µ–Ω–∏–µ** - –≤—ã–±–æ—Ä –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
11. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è** - —Ç—é–Ω–∏–Ω–≥ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
12. **–í—ã–≤–æ–¥—ã** - –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

## üó∫Ô∏è Roadmap —Ä–∞–∑–≤–∏—Ç–∏—è –ø—Ä–æ–µ–∫—Ç–∞

–ü–æ–¥—Ä–æ–±–Ω—ã–π –ø–ª–∞–Ω —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞ —Å –ø–µ—Ä–µ—Ö–æ–¥–æ–º –æ—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç–æ–¥–æ–≤ ML –∫ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.

### üìä –§–∞–∑–∞ 1: –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–µ ML (–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –í—ã—Å–æ–∫–∏–π)

#### 1.1 –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥
- [ ] **XGBoost** - —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å Random Forest
  - –¢—é–Ω–∏–Ω–≥ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (learning_rate, max_depth, n_estimators)
  - –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
  - Early stopping –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
- [ ] **LightGBM** - –±—ã—Å—Ç—Ä–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ XGBoost
  - –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è
  - –†–∞–±–æ—Ç–∞ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –Ω–∞–ø—Ä—è–º—É—é
  - Histogram-based –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥
- [ ] **CatBoost** - —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
  - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
  - Ordered boosting –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
  - Symmetric trees

#### 1.2 –ê–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã
- [ ] **Stacking** - –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ
  - –°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ –∏–∑ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
  - Cross-validation –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —É—Ç–µ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö
  - –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
- [ ] **Blending** - –ø—Ä–æ—Å—Ç–æ–µ —Å–º–µ—à–∏–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
  - Weighted averaging —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
  - Voting classifier (hard –∏ soft voting)
  - –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π
- [ ] **Bagging** - —É–º–µ–Ω—å—à–µ–Ω–∏–µ –¥–∏—Å–ø–µ—Ä—Å–∏–∏
  - Bootstrap aggregating
  - Out-of-bag –æ—Ü–µ–Ω–∫–∞

#### 1.3 –†–∞–±–æ—Ç–∞ —Å –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
- [ ] **Oversampling**: SMOTE, ADASYN
- [ ] **Undersampling**: RandomUnderSampler, TomekLinks
- [ ] **–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã**: SMOTETomek
- [ ] **Class weights** - –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ loss —Ñ—É–Ω–∫—Ü–∏–∏
- [ ] **Threshold tuning** - –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ—Ä–æ–≥–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏

#### 1.4 –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π Feature Engineering
- [ ] –ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è)
- [ ] Target encoding –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
- [ ] Frequency encoding
- [ ] WoE (Weight of Evidence) encoding
- [ ] –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Ä–µ–¥–∫–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
- [ ] –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ–º–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π
- [ ] Feature selection (RFE, SelectKBest, Feature importance)

---

### üß† –§–∞–∑–∞ 2: –ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ - –û—Å–Ω–æ–≤—ã (–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –í—ã—Å–æ–∫–∏–π)

#### 2.1 –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ (FNN/MLP)
- [ ] **–ë–∞–∑–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞** –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¢–∏—Ç–∞–Ω–∏–∫–∞
  - Multi-Layer Perceptron —Å 2-3 —Å–∫—Ä—ã—Ç—ã–º–∏ —Å–ª–æ—è–º–∏
  - Batch normalization
  - Dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
  - –§—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏: ReLU, LeakyReLU, ELU
- [ ] **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è**
  - Adam, AdamW, RMSprop –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã
  - Learning rate scheduling (ReduceLROnPlateau, CosineAnnealing)
  - Early stopping –∏ model checkpointing
- [ ] **–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è**
  - L1/L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
  - Dropout (—Ä–∞–∑–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è)
  - Weight decay
  - Data augmentation –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

#### 2.2 –°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ (CNN)
- [ ] **1D-CNN –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö**
  - –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫–æ–Ω–≤–æ–ª—é—Ü–∏–π –∫ —É–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º
  - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
  - –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏
- [ ] **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã**
  - –ü—Ä–æ—Å—Ç–∞—è CNN (2-3 —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —Å–ª–æ—è)
  - ResNet-like skip connections –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
  - Inception modules
- [ ] **–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è**
  - Feature maps
  - Activation maps
  - Filter visualization

#### 2.3 –ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã (Autoencoders)
- [ ] **Vanilla Autoencoder** - —Å–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
  - –û–±—É—á–µ–Ω–∏–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
  - –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å PCA
  - –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
- [ ] **Denoising Autoencoder** - –æ–±—É—á–µ–Ω–∏–µ —Ä–æ–±–∞—Å—Ç–Ω—ã–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º
  - –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞ –∫ –≤—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º
  - –û—á–∏—Å—Ç–∫–∞ –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ
- [ ] **Variational Autoencoder (VAE)** - –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å
  - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–∞—Å—Å–∞–∂–∏—Ä–æ–≤
  - –ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ
  - –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö

---

### üìà –§–∞–∑–∞ 3: –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã –∏ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ —Å–µ—Ç–∏ (–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –°—Ä–µ–¥–Ω–∏–π)

#### 3.1 –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- [ ] **–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¢–∏—Ç–∞–Ω–∏–∫–∞ –≤ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ**
  - –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–æ–±—ã—Ç–∏–π (boarding ‚Üí journey ‚Üí outcome)
  - –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è
  - –°–∏–º—É–ª—è—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —ç–≤–∞–∫—É–∞—Ü–∏–∏
- [ ] **–ù–æ–≤—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Ä—è–¥–∞–º–∏**
  - **–ü–∞—Å—Å–∞–∂–∏—Ä–æ–ø–æ—Ç–æ–∫** - –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–∞—Å—Å–∞–∂–∏—Ä–æ–≤
  - **–¶–µ–Ω—ã –Ω–∞ –±–∏–ª–µ—Ç—ã** - –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–æ–∏–º–æ—Å—Ç–∏
  - **–ó–∞–≥—Ä—É–∑–∫–∞ –∫–∞—é—Ç** - –¥–∏–Ω–∞–º–∏–∫–∞ –∑–∞–ø–æ–ª–Ω—è–µ–º–æ—Å—Ç–∏
  - **–ü–æ–≥–æ–¥–Ω—ã–µ —É—Å–ª–æ–≤–∏—è** - –≤–ª–∏—è–Ω–∏–µ –Ω–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å —Ä–µ–π—Å–æ–≤

#### 3.2 –†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ (RNN)
- [ ] **Vanilla RNN** - –±–∞–∑–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
  - –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
  - –ü—Ä–æ–±–ª–µ–º–∞ –∏—Å—á–µ–∑–∞—é—â–µ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞
  - –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø—Ä–æ—Å—Ç—ã—Ö RNN
- [ ] **LSTM (Long Short-Term Memory)**
  - –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å gates (forget, input, output)
  - –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª–∏–Ω–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
  - Bidirectional LSTM
  - Stacked LSTM (–º–Ω–æ–≥–æ—Å–ª–æ–π–Ω–∞—è)
- [ ] **GRU (Gated Recurrent Unit)**
  - –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å LSTM
  - –ú–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –±—ã—Å—Ç—Ä–µ–µ –æ–±—É—á–µ–Ω–∏–µ
  - –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ LSTM vs GRU
- [ ] **Attention –¥–ª—è RNN**
  - –ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –≤ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö —Å–µ—Ç—è—Ö
  - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è attention weights
  - –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –≤–∞–∂–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤

#### 3.3 –ó–∞–¥–∞—á–∏ –Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–∞—Ö
- [ ] **–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ (Forecasting)**
  - Univariate forecasting - –æ–¥–∏–Ω –ø—Ä–∏–∑–Ω–∞–∫
  - Multivariate forecasting - –º–Ω–æ–∂–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
  - Multi-step ahead prediction
- [ ] **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π**
  - Seq-to-label (–≤—Å—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å ‚Üí –º–µ—Ç–∫–∞)
  - Many-to-one –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
- [ ] **Seq2Seq –º–æ–¥–µ–ª–∏**
  - Encoder-Decoder –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
  - Sequence-to-sequence –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
  - Teacher forcing –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏

#### 3.4 –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
- [ ] **ARIMA/SARIMA** - –∞–≤—Ç–æ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
- [ ] **Prophet** (Facebook) - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ
- [ ] **Exponential Smoothing** - —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ
- [ ] **Seasonal decomposition** - –∞–Ω–∞–ª–∏–∑ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç–∏

---

### ü§ñ –§–∞–∑–∞ 4: –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã (–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –°—Ä–µ–¥–Ω–∏–π)

#### 4.1 –ú–µ—Ö–∞–Ω–∏–∑–º Self-Attention
- [ ] **Scaled Dot-Product Attention**
  - –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑–æ–≤–æ–≥–æ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è
  - –í—ã—á–∏—Å–ª–µ–Ω–∏–µ Query, Key, Value
  - Softmax –¥–ª—è –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è
  - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü—ã –≤–Ω–∏–º–∞–Ω–∏—è
- [ ] **Multi-Head Attention**
  - –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –≥–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è
  - –ö–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è –∏ –ø—Ä–æ–µ–∫—Ü–∏—è
  - –†–∞–∑–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
- [ ] **Self-Attention –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö**
  - –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫ –ø—Ä–∏–∑–Ω–∞–∫–∞–º –ø–∞—Å—Å–∞–∂–∏—Ä–æ–≤
  - –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
  - –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è attention weights

#### 4.2 Transformer –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
- [ ] **–ë–∞–∑–æ–≤—ã–π Transformer Encoder**
  - Positional encoding –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
  - Layer normalization
  - Feed-forward —Å–µ—Ç–∏
  - Residual connections
- [ ] **Transformer –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¢–∏—Ç–∞–Ω–∏–∫–∞**
  - –ê–¥–∞–ø—Ç–∞—Ü–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
  - –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å MLP –∏ CNN
  - –ê–Ω–∞–ª–∏–∑ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
- [ ] **TabTransformer**
  - –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è —Ç–∞–±–ª–∏—Ü
  - Embedding –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
  - Self-attention –Ω–∞–¥ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏

#### 4.3 –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—á–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã
- [ ] **FT-Transformer** (Feature Tokenizer)
  - –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ —Ç–æ–∫–µ–Ω—ã
  - –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ Transformer encoder
  - State-of-the-art –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- [ ] **SAINT** (Self-Attention and Intersample Attention)
  - –í–Ω–∏–º–∞–Ω–∏–µ –º–µ–∂–¥—É –æ–±—ä–µ–∫—Ç–∞–º–∏
  - Contrastive learning
- [ ] **TabNet** (Google)
  - Sequential attention
  - Feature selection –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ
  - –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å

#### 4.4 Transfer Learning –∏ Pre-training
- [ ] **–ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏**
  - Fine-tuning –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –¢–∏—Ç–∞–Ω–∏–∫–∞
  - Domain adaptation
- [ ] **Self-supervised learning**
  - Masked feature prediction
  - Contrastive learning –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- [ ] **Few-shot learning**
  - –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –º–∞–ª–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –ø—Ä–∏–º–µ—Ä–æ–≤

---

### üîç –§–∞–∑–∞ 5: –ü–æ–∏—Å–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π (–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –°—Ä–µ–¥–Ω–∏–π)

#### 5.1 –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
- [ ] **Association Rules** (Apriori, FP-Growth)
  - –ü–æ–∏—Å–∫ —á–∞—Å—Ç—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
  - Support, Confidence, Lift –º–µ—Ç—Ä–∏–∫–∏
  - –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –ø—Ä–∞–≤–∏–ª (–Ω–∞–ø—Ä–∏–º–µ—Ä: "1st class + female ‚Üí survived")
- [ ] **Sequence Mining**
  - –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ —Å–æ–±—ã—Ç–∏—è—Ö
  - GSP (Generalized Sequential Pattern) –∞–ª–≥–æ—Ä–∏—Ç–º
- [ ] **Clustering**
  - K-Means, DBSCAN, Hierarchical clustering
  - –ü–æ–∏—Å–∫ –≥—Ä—É–ø–ø –ø–æ—Ö–æ–∂–∏—Ö –ø–∞—Å—Å–∞–∂–∏—Ä–æ–≤
  - –ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤

#### 5.2 –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π (Anomaly Detection)
- [ ] **–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã**
  - Z-score, IQR –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –≤—ã–±—Ä–æ—Å–æ–≤
  - Mahalanobis distance
  - Chi-square test
- [ ] **Isolation Forest**
  - –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ä–µ–¥–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
  - –ê–Ω–æ–º–∞–ª—å–Ω—ã–µ –ø–∞—Å—Å–∞–∂–∏—Ä—ã (–Ω–µ–æ–±—ã—á–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏)
- [ ] **One-Class SVM**
  - –û–±—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –Ω–∞ "–Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö" –¥–∞–Ω–Ω—ã—Ö
  - –í—ã—è–≤–ª–µ–Ω–∏–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π
- [ ] **Autoencoders –¥–ª—è –∞–Ω–æ–º–∞–ª–∏–π**
  - Reconstruction error –∫–∞–∫ –º–µ—Ç—Ä–∏–∫–∞ –∞–Ω–æ–º–∞–ª—å–Ω–æ—Å—Ç–∏
  - Threshold tuning
  - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤
- [ ] **Local Outlier Factor (LOF)**
  - –ü–ª–æ—Ç–Ω–æ—Å—Ç—å –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç–∏
  - –õ–æ–∫–∞–ª—å–Ω—ã–µ –≤—ã–±—Ä–æ—Å—ã

#### 5.3 –í—Ä–µ–º–µ–Ω–Ω—ã–µ –∞–Ω–æ–º–∞–ª–∏–∏
- [ ] **Change Point Detection**
  - –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ä–µ–∑–∫–∏—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π
  - CUSUM, PELT –∞–ª–≥–æ—Ä–∏—Ç–º—ã
- [ ] **LSTM-Autoencoders –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤**
  - –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
  - Multi-variate time series anomaly detection
- [ ] **Prophet –¥–ª—è –∞–Ω–æ–º–∞–ª–∏–π**
  - –í—ã—è–≤–ª–µ–Ω–∏–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π –æ—Ç —Ç—Ä–µ–Ω–¥–∞

#### 5.4 –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
- [ ] **t-SNE, UMAP** - —Å–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
  - –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ 2D/3D –≥—Ä–∞—Ñ–∏–∫–∏
  - –¶–≤–µ—Ç–æ–≤–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ –≤—ã–∂–∏–≤–∞–µ–º–æ—Å—Ç–∏
- [ ] **Parallel Coordinates** - –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
- [ ] **Sankey Diagrams** - –ø–æ—Ç–æ–∫–∏ –º–µ–∂–¥—É –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏
- [ ] **Network Graphs** - —Å–≤—è–∑–∏ –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏

---

### üî¨ –§–∞–∑–∞ 6: –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å –∏ Explainable AI (–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –í—ã—Å–æ–∫–∏–π)

#### 6.1 Model-Agnostic –º–µ—Ç–æ–¥—ã
- [ ] **SHAP (SHapley Additive exPlanations)**
  - TreeSHAP –¥–ª—è –¥—Ä–µ–≤–æ–≤–∏–¥–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
  - KernelSHAP –¥–ª—è –ª—é–±—ã—Ö –º–æ–¥–µ–ª–µ–π
  - DeepSHAP –¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π
  - Waterfall plots, force plots, summary plots
  - Dependence plots –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
- [ ] **LIME (Local Interpretable Model-agnostic Explanations)**
  - –õ–æ–∫–∞–ª—å–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
  - –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤
  - –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å SHAP
- [ ] **Permutation Importance**
  - –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —á–µ—Ä–µ–∑ –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∏
  - –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–π –≤–∞–∂–Ω–æ—Å—Ç—å—é
- [ ] **Partial Dependence Plots (PDP)**
  - –í–ª–∏—è–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
  - Individual Conditional Expectation (ICE) curves
  - 2D PDP –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π

#### 6.2 Model-Specific –º–µ—Ç–æ–¥—ã
- [ ] **Feature Importance** –∏–∑ –¥—Ä–µ–≤–æ–≤–∏–¥–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
  - Gini importance vs Permutation importance
  - Feature importance bias
- [ ] **Decision Tree Visualization**
  - –ì—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª
  - –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –ø—É—Ç–µ–π
- [ ] **Linear Model Coefficients**
  - –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –≤–µ—Å–æ–≤ Logistic Regression
  - –î–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
- [ ] **Attention Visualization** –¥–ª—è Transformers
  - Heatmap –º–∞—Ç—Ä–∏—Ü—ã –≤–Ω–∏–º–∞–Ω–∏—è
  - –ö–∞–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤–∞–∂–Ω—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è

#### 6.3 –ì–ª–æ–±–∞–ª—å–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è
- [ ] **Model Distillation** - —É–ø—Ä–æ—â–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ–π –º–æ–¥–µ–ª–∏
  - –û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö —Å–ª–æ–∂–Ω–æ–π
  - Decision tree –∫–∞–∫ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–π –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ç–æ—Ä
- [ ] **Rule Extraction** - –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª
  - –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ –Ω–∞–±–æ—Ä if-then –ø—Ä–∞–≤–∏–ª
- [ ] **Surrogate Models** - –∑–∞–º–µ–Ω–∞ –Ω–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—É—é –º–æ–¥–µ–ª—å

#### 6.4 Fairness –∏ Bias Analysis
- [ ] **Demographic Parity** - —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç—å –ø–æ –≥—Ä—É–ø–ø–∞–º
  - –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –ø–æ –ø–æ–ª—É, –∫–ª–∞—Å—Å—É
  - Disparate Impact
- [ ] **Calibration Analysis** - —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
- [ ] **Counterfactual Explanations**
  - "–ß—Ç–æ –Ω—É–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å, —á—Ç–æ–±—ã –∏–∑–º–µ–Ω–∏—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ?"
  - DiCE, Alibi –∞–ª–≥–æ—Ä–∏—Ç–º—ã

#### 6.5 –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
- [ ] **Interactive Dashboards** (Plotly Dash, Streamlit)
  - –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ SHAP
  - –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º
  - –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
- [ ] **What-If Tool** - –∞–Ω–∞–ª–∏–∑ —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤
- [ ] **Model Cards** - –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –º–æ–¥–µ–ª–∏

---

### üöÄ –§–∞–∑–∞ 7: Production –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ (–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –°—Ä–µ–¥–Ω–∏–π)

#### 7.1 MLOps –∏ Pipeline
- [ ] **scikit-learn Pipeline**
  - –ü–æ–ª–Ω—ã–π pipeline –æ—Ç —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
  - ColumnTransformer –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
  - Custom transformers
- [ ] **Feature Store**
  - –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
  - Feast, Tecton
- [ ] **Experiment Tracking**
  - MLflow –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
  - Weights & Biases (wandb)
  - TensorBoard
- [ ] **Model Versioning**
  - DVC (Data Version Control)
  - Git –¥–ª—è –∫–æ–¥–∞ + DVC –¥–ª—è –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–µ–ª–µ–π

#### 7.2 Deployment
- [ ] **Model Serialization**
  - Pickle, Joblib –¥–ª—è sklearn
  - ONNX –¥–ª—è –∫—Ä–æ—Å—Å–ø–ª–∞—Ç—Ñ–æ—Ä–º–µ–Ω–Ω–æ—Å—Ç–∏
  - TorchScript –¥–ª—è PyTorch
  - SavedModel –¥–ª—è TensorFlow
- [ ] **REST API** –¥–ª—è –º–æ–¥–µ–ª–∏
  - FastAPI / Flask —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã
  - Request validation —Å Pydantic
  - Async –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤
- [ ] **Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∏–∑–∞—Ü–∏—è**
  - Dockerfile –¥–ª—è –º–æ–¥–µ–ª–∏
  - Multi-stage builds
  - Docker Compose –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
- [ ] **Cloud Deployment**
  - AWS SageMaker
  - Google Cloud AI Platform
  - Azure ML
  - Heroku –¥–ª—è –ø—Ä–æ—Ç–æ—Ç–∏–ø–æ–≤

#### 7.3 –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ
- [ ] **Model Performance Monitoring**
  - –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –≤ production
  - Alerting –ø—Ä–∏ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏
- [ ] **Data Drift Detection**
  - Evidently AI –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –¥—Ä–∏—Ñ—Ç–∞
  - KS-test, PSI (Population Stability Index)
  - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ
- [ ] **A/B Testing**
  - –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –≤ production
  - Statistical significance tests
- [ ] **Logging –∏ Debugging**
  - –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ª–æ–≥–∏
  - Distributed tracing

#### 7.4 –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- [ ] **Model Compression**
  - Pruning (—É–¥–∞–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤)
  - Quantization (—É–º–µ–Ω—å—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏)
  - Knowledge Distillation
- [ ] **Inference Optimization**
  - ONNX Runtime
  - TensorRT (NVIDIA)
  - OpenVINO (Intel)
  - Batch inference
- [ ] **Caching** - –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
- [ ] **Parallel Processing** - –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞

#### 7.5 Testing –∏ CI/CD
- [ ] **Unit Tests** –¥–ª—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
  - pytest –¥–ª—è transformers, models
  - Mocking –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
- [ ] **Integration Tests** - —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ pipeline
- [ ] **Model Tests**
  - Smoke tests (–±–∞–∑–æ–≤–∞—è —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å)
  - Performance tests (—Å–∫–æ—Ä–æ—Å—Ç—å inference)
  - Data validation tests
- [ ] **CI/CD Pipeline**
  - GitHub Actions / GitLab CI
  - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∑–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤
  - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π deployment

---

### üìö –§–∞–∑–∞ 8: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è (–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –ù–∏–∑–∫–∏–π)

#### 8.1 –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏
- [ ] **Regression** - –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –±–∏–ª–µ—Ç–∞
- [ ] **Multi-task Learning** - –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤—ã–∂–∏–≤–∞–Ω–∏—è –∏ –∫–ª–∞—Å—Å–∞
- [ ] **Survival Analysis** - –∞–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–∏ –¥–æ —Å–æ–±—ã—Ç–∏—è
  - Kaplan-Meier estimator
  - Cox Proportional Hazards model
- [ ] **Causal Inference** - –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–≤—è–∑–∏
  - Uplift modeling
  - Propensity score matching

#### 8.2 –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏
- [ ] **Meta-Learning** - –æ–±—É—á–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—é
- [ ] **Neural Architecture Search (NAS)**
  - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
- [ ] **Federated Learning** - —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
- [ ] **Online Learning** - –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –ø–æ—Ç–æ–∫–µ –¥–∞–Ω–Ω—ã—Ö
- [ ] **Active Learning** - –≤—ã–±–æ—Ä –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è —Ä–∞–∑–º–µ—Ç–∫–∏

#### 8.3 –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
- [ ] **Web-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ** (Streamlit/Gradio)
  - –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
  - –ó–∞–≥—Ä—É–∑–∫–∞ —Å–≤–æ–∏—Ö –¥–∞–Ω–Ω—ã—Ö
  - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—ä—è—Å–Ω–µ–Ω–∏–π
- [ ] **Telegram/Discord –±–æ—Ç**
  - –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —á–µ—Ä–µ–∑ –º–µ—Å—Å–µ–Ω–¥–∂–µ—Ä
- [ ] **Mobile App** - –ø—Ä–æ—Å—Ç–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ

#### 8.4 –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ
- [ ] **–ü–æ–¥—Ä–æ–±–Ω—ã–µ —Ç—É—Ç–æ—Ä–∏–∞–ª—ã** –ø–æ –∫–∞–∂–¥–æ–π —Ñ–∞–∑–µ
- [ ] **–í–∏–¥–µ–æ-—É—Ä–æ–∫–∏** (YouTube)
- [ ] **Blog posts** —Å –æ–±—ä—è—Å–Ω–µ–Ω–∏—è–º–∏
- [ ] **Kaggle Kernels** –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
- [ ] **Medium articles** –¥–ª—è –ø–æ–ø—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏

---

### üéØ –ü–æ—Ä—è–¥–æ–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è (–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π)

**–ö–æ—Ä–æ—Ç–∫–∏–π –ø—É—Ç—å (Quick Wins - 2-4 –Ω–µ–¥–µ–ª–∏):**
1. –§–∞–∑–∞ 1.1-1.2: XGBoost, LightGBM, CatBoost + Stacking ‚úÖ
2. –§–∞–∑–∞ 6.1: SHAP –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è ‚úÖ
3. –§–∞–∑–∞ 2.1: –ü—Ä–æ—Å—Ç–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å ‚úÖ
4. –§–∞–∑–∞ 7.1-7.2: –ë–∞–∑–æ–≤—ã–π deployment (API + Docker) ‚úÖ

**–°—Ä–µ–¥–Ω–∏–π –ø—É—Ç—å (Comprehensive - 2-3 –º–µ—Å—è—Ü–∞):**
1. –ü–æ–ª–Ω–∞—è –§–∞–∑–∞ 1: –í—Å–µ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã
2. –ü–æ–ª–Ω–∞—è –§–∞–∑–∞ 6: –í—Å—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è
3. –§–∞–∑–∞ 2: Deep Learning –æ—Å–Ω–æ–≤—ã
4. –§–∞–∑–∞ 5.1-5.2: –ü–∞—Ç—Ç–µ—Ä–Ω—ã –∏ –∞–Ω–æ–º–∞–ª–∏–∏
5. –§–∞–∑–∞ 7: Production-ready solution

**–î–ª–∏–Ω–Ω—ã–π –ø—É—Ç—å (Full Mastery - 6+ –º–µ—Å—è—Ü–µ–≤):**
- –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Ñ–∞–∑ 1-8
- –ì–ª—É–±–æ–∫–æ–µ –∏–∑—É—á–µ–Ω–∏–µ –∫–∞–∂–¥–æ–π —Ç–µ—Ö–Ω–∏–∫–∏
- –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –Ω–æ—É—Ç–±—É–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞
- –ü–æ–ª–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –∏ —Ç—É—Ç–æ—Ä–∏–∞–ª—ã

---

### üõ†Ô∏è –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Å—Ç–µ–∫ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è

**–¢–µ–∫—É—â–∏–π:**
- pandas, numpy, matplotlib, seaborn, scikit-learn

**–î–ª—è –§–∞–∑—ã 1-2:**
- xgboost, lightgbm, catboost
- imbalanced-learn
- tensorflow / pytorch (–¥–ª—è Deep Learning)

**–î–ª—è –§–∞–∑—ã 3:**
- tensorflow / pytorch (LSTM, GRU)
- statsmodels (ARIMA)
- prophet

**–î–ª—è –§–∞–∑—ã 4:**
- pytorch (Transformers)
- huggingface transformers
- pytorch-tabular

**–î–ª—è –§–∞–∑—ã 5:**
- mlxtend (Association Rules)
- pyod (Anomaly Detection)
- umap-learn

**–î–ª—è –§–∞–∑—ã 6:**
- shap, lime
- alibi, dice-ml
- yellowbrick

**–î–ª—è –§–∞–∑—ã 7:**
- mlflow, wandb
- fastapi, flask
- docker
- evidently

## üìÑ –õ–∏—Ü–µ–Ω–∑–∏—è

–≠—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç —Å–æ–∑–¥–∞–Ω –≤ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Ü–µ–ª—è—Ö –∏ —Å–≤–æ–±–æ–¥–µ–Ω –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.

## ‚≠ê –ï—Å–ª–∏ –ø—Ä–æ–µ–∫—Ç –ø–æ–ª–µ–∑–µ–Ω

–ü–æ—Å—Ç–∞–≤—å—Ç–µ –∑–≤–µ–∑–¥—É –ø—Ä–æ–µ–∫—Ç—É, —á—Ç–æ–±—ã –¥—Ä—É–≥–∏–µ —Å—Ç—É–¥–µ–Ω—Ç—ã –º–æ–≥–ª–∏ –µ–≥–æ –Ω–∞–π—Ç–∏!

---

**–°–æ–∑–¥–∞–Ω–æ —Å –ø–æ–º–æ—â—å—é:** Claude Code
**–í–µ—Ä—Å–∏—è:** 2.0
**–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ:** 2025

#!/usr/bin/env python3
"""
–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–π —Ç–µ–æ—Ä–∏–∏ –∏ –ø—Ä–∞–∫—Ç–∏–∫–∏ –≤ MLP notebook
"""

import json

# –ß–∏—Ç–∞–µ–º —Ç–µ–∫—É—â–∏–π –Ω–æ—É—Ç–±—É–∫
notebook_path = '01_mlp_basics.ipynb'
with open(notebook_path, 'r', encoding='utf-8') as f:
    notebook = json.load(f)

new_cells = []

# ============================================================================
# THEORY: ACTIVATION FUNCTIONS
# ============================================================================

new_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 1.2 –§—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏\n",
        "\n",
        "**–ó–∞—á–µ–º –Ω—É–∂–Ω—ã?** –ë–µ–∑ –Ω–µ–ª–∏–Ω–µ–π–Ω–æ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏, –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω–∞—è —Å–µ—Ç—å = –ª–∏–Ω–µ–π–Ω–∞—è –º–æ–¥–µ–ª—å!\n",
        "\n",
        "#### 1.2.1 Sigmoid\n",
        "\n",
        "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
        "\n",
        "**–°–≤–æ–π—Å—Ç–≤–∞:**\n",
        "- –í—ã—Ö–æ–¥: $(0, 1)$\n",
        "- ‚úÖ –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫–∞–∫ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å\n",
        "- ‚ùå **Vanishing gradient** –¥–ª—è –±–æ–ª—å—à–∏—Ö $|x|$\n",
        "- ‚ùå **Not zero-centered** (–≤—Å–µ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã)\n",
        "\n",
        "**–ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è:**\n",
        "\n",
        "$$\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$$\n",
        "\n",
        "---\n",
        "\n",
        "#### 1.2.2 Tanh\n",
        "\n",
        "$$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
        "\n",
        "**–°–≤–æ–π—Å—Ç–≤–∞:**\n",
        "- –í—ã—Ö–æ–¥: $(-1, 1)$\n",
        "- ‚úÖ **Zero-centered**\n",
        "- ‚ùå **Vanishing gradient** –¥–ª—è –±–æ–ª—å—à–∏—Ö $|x|$\n",
        "\n",
        "**–ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è:**\n",
        "\n",
        "$$\\tanh'(x) = 1 - \\tanh^2(x)$$\n",
        "\n",
        "---\n",
        "\n",
        "#### 1.2.3 ReLU (Rectified Linear Unit) ‚≠ê\n",
        "\n",
        "$$\\text{ReLU}(x) = \\max(0, x)$$\n",
        "\n",
        "**–°–≤–æ–π—Å—Ç–≤–∞:**\n",
        "- ‚úÖ **–ü—Ä–æ—Å—Ç–æ—Ç–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π**\n",
        "- ‚úÖ **–ù–µ—Ç vanishing gradient** –¥–ª—è $x > 0$\n",
        "- ‚úÖ **Sparse activations** (–º–Ω–æ–≥–∏–µ –Ω–µ–π—Ä–æ–Ω—ã = 0)\n",
        "- ‚ùå **Dying ReLU** –ø—Ä–æ–±–ª–µ–º–∞ (–Ω–µ–π—Ä–æ–Ω—ã \"—É–º–∏—Ä–∞—é—Ç\" –ø—Ä–∏ $x \\leq 0$)\n",
        "\n",
        "**–ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è:**\n",
        "\n",
        "$$\\text{ReLU}'(x) = \\begin{cases} 1, & \\text{if } x > 0 \\\\ 0, & \\text{if } x \\leq 0 \\end{cases}$$\n",
        "\n",
        "**–ü–æ—á–µ–º—É ReLU —Å—Ç–∞–ª —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–º?**\n",
        "- Faster convergence (–¥–æ 6x –±—ã—Å—Ç—Ä–µ–µ, —á–µ–º sigmoid/tanh)\n",
        "- Simplicity\n",
        "- –ë–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω–æ\n",
        "\n",
        "---\n",
        "\n",
        "#### 1.2.4 Leaky ReLU\n",
        "\n",
        "$$\\text{LeakyReLU}(x) = \\max(0.01x, x)$$\n",
        "\n",
        "**–†–µ—à–∞–µ—Ç dying ReLU:**\n",
        "- –ú–∞–ª—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç (0.01) –¥–ª—è $x < 0$ –≤–º–µ—Å—Ç–æ 0\n",
        "\n",
        "---\n",
        "\n",
        "#### 1.2.5 ELU (Exponential Linear Unit)\n",
        "\n",
        "$$\\text{ELU}(x) = \\begin{cases} x, & \\text{if } x > 0 \\\\ \\alpha(e^x - 1), & \\text{if } x \\leq 0 \\end{cases}$$\n",
        "\n",
        "**–°–≤–æ–π—Å—Ç–≤–∞:**\n",
        "- ‚úÖ Smooth –≤–µ–∑–¥–µ\n",
        "- ‚úÖ Mean activations –±–ª–∏–∂–µ –∫ –Ω—É–ª—é\n",
        "- ‚ùå –ú–µ–¥–ª–µ–Ω–Ω–µ–µ ReLU (—ç–∫—Å–ø–æ–Ω–µ–Ω—Ç–∞)\n",
        "\n",
        "---\n",
        "\n",
        "#### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–π\n",
        "\n",
        "| –ê–∫—Ç–∏–≤–∞—Ü–∏—è | Vanishing Gradient | Dying Units | –°–∫–æ—Ä–æ—Å—Ç—å | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |\n",
        "|-----------|-------------------|-------------|----------|-------------|\n",
        "| **Sigmoid** | ‚ùå –î–∞ | ‚úÖ –ù–µ—Ç | Slow | –¢–æ–ª—å–∫–æ output –¥–ª—è binary |\n",
        "| **Tanh** | ‚ùå –î–∞ | ‚úÖ –ù–µ—Ç | Slow | RNN (–∏–Ω–æ–≥–¥–∞) |\n",
        "| **ReLU** | ‚úÖ –ù–µ—Ç | ‚ùå –î–∞ | ‚ö° Fast | ‚≠ê –°—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è hidden |\n",
        "| **Leaky ReLU** | ‚úÖ –ù–µ—Ç | ‚úÖ –ù–µ—Ç | ‚ö° Fast | –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ ReLU |\n",
        "| **ELU** | ‚úÖ –ù–µ—Ç | ‚úÖ –ù–µ—Ç | Slower | –ö–æ–≥–¥–∞ –Ω—É–∂–Ω–∞ –≥–ª–∞–¥–∫–æ—Å—Ç—å |\n",
        "\n",
        "**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:**\n",
        "1. **Default:** ReLU –¥–ª—è –≤—Å–µ—Ö —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ–µ–≤\n",
        "2. **–ï—Å–ª–∏ dying ReLU –ø—Ä–æ–±–ª–µ–º–∞:** Leaky ReLU –∏–ª–∏ ELU\n",
        "3. **Output layer (binary):** Sigmoid\n",
        "4. **Output layer (multiclass):** Softmax\n",
        "5. **Output layer (regression):** Linear (no activation)\n",
        "\n",
        "---"
    ]
})

# ============================================================================
# THEORY: BACKPROPAGATION
# ============================================================================

new_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 1.3 Backpropagation –∏ Gradient Descent\n",
        "\n",
        "#### Loss Function\n",
        "\n",
        "**Binary Cross-Entropy (–¥–ª—è classification):**\n",
        "\n",
        "$$L = -\\frac{1}{N} \\sum_{i=1}^N \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]$$\n",
        "\n",
        "–≥–¥–µ:\n",
        "- $y_i$ ‚Äî –∏—Å—Ç–∏–Ω–Ω–∞—è –º–µ—Ç–∫–∞ (0 –∏–ª–∏ 1)\n",
        "- $\\hat{y}_i$ ‚Äî –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å\n",
        "\n",
        "**Mean Squared Error (–¥–ª—è regression):**\n",
        "\n",
        "$$L = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2$$\n",
        "\n",
        "---\n",
        "\n",
        "#### Backpropagation Algorithm\n",
        "\n",
        "**–¶–µ–ª—å:** –í—ã—á–∏—Å–ª–∏—Ç—å $\\frac{\\partial L}{\\partial W^{(l)}}$ –¥–ª—è –≤—Å–µ—Ö —Å–ª–æ–µ–≤.\n",
        "\n",
        "**Chain rule:**\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial W^{(l)}} = \\frac{\\partial L}{\\partial a^{(L)}} \\cdot \\frac{\\partial a^{(L)}}{\\partial z^{(L)}} \\cdot \\ldots \\cdot \\frac{\\partial z^{(l)}}{\\partial W^{(l)}}$$\n",
        "\n",
        "**–ê–ª–≥–æ—Ä–∏—Ç–º:**\n",
        "\n",
        "1. **Forward pass:** –í—ã—á–∏—Å–ª–∏—Ç—å $a^{(l)}$ –¥–ª—è –≤—Å–µ—Ö —Å–ª–æ–µ–≤\n",
        "2. **Compute loss:** $L = \\text{loss}(a^{(L)}, y)$\n",
        "3. **Backward pass:** \n",
        "   - –í—ã—á–∏—Å–ª–∏—Ç—å $\\delta^{(L)} = \\frac{\\partial L}{\\partial z^{(L)}}$\n",
        "   - –î–ª—è $l = L-1, L-2, \\ldots, 1$:\n",
        "     $$\\delta^{(l)} = (W^{(l+1)})^T \\delta^{(l+1)} \\odot \\sigma'(z^{(l)})$$\n",
        "   - –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤–µ—Å–æ–≤:\n",
        "     $$\\frac{\\partial L}{\\partial W^{(l)}} = \\delta^{(l)} (a^{(l-1)})^T$$\n",
        "\n",
        "4. **Update weights:**\n",
        "   $$W^{(l)} \\leftarrow W^{(l)} - \\eta \\frac{\\partial L}{\\partial W^{(l)}}$$\n",
        "\n",
        "–≥–¥–µ $\\eta$ ‚Äî learning rate.\n",
        "\n",
        "---\n",
        "\n",
        "#### Gradient Descent –≤–∞—Ä–∏–∞–Ω—Ç—ã\n",
        "\n",
        "**1. Batch Gradient Descent:**\n",
        "- –ò—Å–ø–æ–ª—å–∑—É–µ–º **–≤—Å–µ** –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞\n",
        "- ‚úÖ Stable convergence\n",
        "- ‚ùå Slow –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "\n",
        "**2. Stochastic Gradient Descent (SGD):**\n",
        "- –ò—Å–ø–æ–ª—å–∑—É–µ–º **–æ–¥–∏–Ω** –ø—Ä–∏–º–µ—Ä –∑–∞ —Ä–∞–∑\n",
        "- ‚úÖ Fast updates\n",
        "- ‚ùå Noisy gradient, –º–æ–∂–µ—Ç –Ω–µ —Å–æ–π—Ç–∏—Å—å\n",
        "\n",
        "**3. Mini-Batch Gradient Descent:** ‚≠ê\n",
        "- –ò—Å–ø–æ–ª—å–∑—É–µ–º **batch** –ø—Ä–∏–º–µ—Ä–æ–≤ (–æ–±—ã—á–Ω–æ 32, 64, 128, 256)\n",
        "- ‚úÖ –ë–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
        "- ‚úÖ –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU\n",
        "\n",
        "---"
    ]
})

# ============================================================================
# THEORY: OPTIMIZERS
# ============================================================================

new_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 1.4 –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã\n",
        "\n",
        "#### 1.4.1 SGD (Stochastic Gradient Descent)\n",
        "\n",
        "$$W_{t+1} = W_t - \\eta \\nabla L(W_t)$$\n",
        "\n",
        "**–ü—Ä–æ–±–ª–µ–º—ã:**\n",
        "- ‚ùå –û–¥–∏–Ω–∞–∫–æ–≤—ã–π learning rate –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
        "- ‚ùå –ú–æ–∂–µ—Ç –∑–∞—Å—Ç—Ä—è—Ç—å –≤ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–∞—Ö\n",
        "\n",
        "---\n",
        "\n",
        "#### 1.4.2 SGD with Momentum\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "v_t &= \\beta v_{t-1} + \\nabla L(W_t) \\\\\n",
        "W_{t+1} &= W_t - \\eta v_t\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "–≥–¥–µ $\\beta$ (–æ–±—ã—á–Ω–æ 0.9) ‚Äî momentum coefficient.\n",
        "\n",
        "**–≠—Ñ—Ñ–µ–∫—Ç:**\n",
        "- ‚úÖ –£—Å–∫–æ—Ä–µ–Ω–∏–µ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏\n",
        "- ‚úÖ –ü–æ–¥–∞–≤–ª–µ–Ω–∏–µ –∫–æ–ª–µ–±–∞–Ω–∏–π\n",
        "\n",
        "---\n",
        "\n",
        "#### 1.4.3 RMSprop (Root Mean Square Propagation)\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "E[g^2]_t &= \\beta E[g^2]_{t-1} + (1-\\beta) (\\nabla L)^2 \\\\\n",
        "W_{t+1} &= W_t - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} \\nabla L\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "**–≠—Ñ—Ñ–µ–∫—Ç:**\n",
        "- ‚úÖ –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π learning rate –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞\n",
        "- ‚úÖ –ú–∞–ª—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å –±–æ–ª—å—à–∏–º–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏\n",
        "\n",
        "---\n",
        "\n",
        "#### 1.4.4 Adam (Adaptive Moment Estimation) ‚≠ê\n",
        "\n",
        "**–ö–æ–º–±–∏–Ω–∞—Ü–∏—è Momentum + RMSprop:**\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "m_t &= \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla L \\quad \\text{(1st moment, momentum)} \\\\\n",
        "v_t &= \\beta_2 v_{t-1} + (1 - \\beta_2) (\\nabla L)^2 \\quad \\text{(2nd moment, RMSprop)} \\\\\n",
        "\\hat{m}_t &= \\frac{m_t}{1 - \\beta_1^t} \\quad \\text{(bias correction)} \\\\\n",
        "\\hat{v}_t &= \\frac{v_t}{1 - \\beta_2^t} \\\\\n",
        "W_{t+1} &= W_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "**Default –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:**\n",
        "- $\\beta_1 = 0.9$\n",
        "- $\\beta_2 = 0.999$\n",
        "- $\\epsilon = 10^{-8}$\n",
        "\n",
        "**–ü–æ—á–µ–º—É Adam –ª—É—á—à–∏–π?**\n",
        "- ‚úÖ Adaptive learning rates\n",
        "- ‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç out-of-the-box –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∑–∞–¥–∞—á\n",
        "- ‚úÖ –ú–µ–Ω—å—à–µ tuning, —á–µ–º SGD\n",
        "\n",
        "---\n",
        "\n",
        "#### 1.4.5 AdamW\n",
        "\n",
        "Adam + **decoupled weight decay:**\n",
        "\n",
        "$$W_{t+1} = W_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t - \\eta \\lambda W_t$$\n",
        "\n",
        "–≥–¥–µ $\\lambda$ ‚Äî weight decay coefficient.\n",
        "\n",
        "**–£–ª—É—á—à–µ–Ω–∏–µ:** –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (–≤ Adam –±—ã–ª–∞ –æ—à–∏–±–∫–∞).\n",
        "\n",
        "---\n",
        "\n",
        "#### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤\n",
        "\n",
        "| –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä | –°–∫–æ—Ä–æ—Å—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ | Tuning —Å–ª–æ–∂–Ω–æ—Å—Ç—å | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |\n",
        "|-------------|-------------------|------------------|-------------|\n",
        "| **SGD** | Slow | High (LR, momentum) | –ö–æ–≥–¥–∞ –Ω—É–∂–Ω–∞ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å |\n",
        "| **SGD + Momentum** | Medium | Medium | Computer Vision (–∏–Ω–æ–≥–¥–∞) |\n",
        "| **RMSprop** | Fast | Low | RNN (–∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏) |\n",
        "| **Adam** | ‚ö° Fast | ‚≠ê Very Low | **Default –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∑–∞–¥–∞—á** |\n",
        "| **AdamW** | ‚ö° Fast | ‚≠ê Very Low | Transformers, modern DL |\n",
        "\n",
        "**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:**\n",
        "1. **Start with Adam** (lr=0.001)\n",
        "2. –ï—Å–ª–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ ‚Üí AdamW (weight_decay=0.01)\n",
        "3. –î–ª—è CV –∏–Ω–æ–≥–¥–∞ SGD+Momentum –ª—É—á—à–µ (–Ω–æ –Ω—É–∂–µ–Ω tuning)\n",
        "\n",
        "---"
    ]
})

# ============================================================================
# THEORY: REGULARIZATION
# ============================================================================

new_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 1.5 –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –∏ Batch Normalization\n",
        "\n",
        "#### 1.5.1 Dropout\n",
        "\n",
        "**–ò–¥–µ—è:** –°–ª—É—á–∞–π–Ω–æ \"–≤—ã–∫–ª—é—á–∞—Ç—å\" –Ω–µ–π—Ä–æ–Ω—ã –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è.\n",
        "\n",
        "**–ê–ª–≥–æ—Ä–∏—Ç–º (training):**\n",
        "```python\n",
        "for each neuron:\n",
        "    with probability p:\n",
        "        set activation to 0\n",
        "    else:\n",
        "        keep activation / (1-p)  # scaling\n",
        "```\n",
        "\n",
        "**–ü—Ä–∏ inference:** All neurons active (no dropout).\n",
        "\n",
        "**–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏:**\n",
        "\n",
        "$$a_i^{\\text{train}} = \\frac{1}{1-p} \\cdot a_i \\cdot r_i$$\n",
        "\n",
        "–≥–¥–µ $r_i \\sim \\text{Bernoulli}(1-p)$.\n",
        "\n",
        "**–ü–æ—á–µ–º—É —Ä–∞–±–æ—Ç–∞–µ—Ç?**\n",
        "- ‚úÖ **Ensemble —ç—Ñ—Ñ–µ–∫—Ç:** –û–±—É—á–∞–µ–º –º–Ω–æ–≥–æ \"–ø–æ–¥—Å–µ—Ç–µ–π\"\n",
        "- ‚úÖ **–ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ co-adaptation:** –ù–µ–π—Ä–æ–Ω—ã –Ω–µ –º–æ–≥—É—Ç –ø–æ–ª–∞–≥–∞—Ç—å—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥—Ä—É–≥–∏–µ\n",
        "- ‚úÖ **Regularization:** –£–º–µ–Ω—å—à–∞–µ—Ç overfitting\n",
        "\n",
        "**–¢–∏–ø–∏—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:**\n",
        "- $p = 0.2$ –¥–ª—è –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è\n",
        "- $p = 0.5$ –¥–ª—è —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ–µ–≤\n",
        "\n",
        "---\n",
        "\n",
        "#### 1.5.2 L1/L2 Regularization\n",
        "\n",
        "**L2 (Weight Decay):**\n",
        "\n",
        "$$L_{\\text{total}} = L_{\\text{data}} + \\frac{\\lambda}{2} \\sum_i w_i^2$$\n",
        "\n",
        "**–≠—Ñ—Ñ–µ–∫—Ç:** –ú–∞–ª—ã–µ –≤–µ—Å–∞ ‚Üí –ø—Ä–æ—â–µ –º–æ–¥–µ–ª—å ‚Üí –º–µ–Ω—å—à–µ overfitting.\n",
        "\n",
        "**L1 (Lasso):**\n",
        "\n",
        "$$L_{\\text{total}} = L_{\\text{data}} + \\lambda \\sum_i |w_i|$$\n",
        "\n",
        "**–≠—Ñ—Ñ–µ–∫—Ç:** Sparse weights (–º–Ω–æ–≥–∏–µ = 0) ‚Üí feature selection.\n",
        "\n",
        "---\n",
        "\n",
        "#### 1.5.3 Batch Normalization\n",
        "\n",
        "**–ü—Ä–æ–±–ª–µ–º–∞:** Internal covariate shift ‚Äî —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–π –º–µ–Ω—è–µ—Ç—Å—è –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è.\n",
        "\n",
        "**–†–µ—à–µ–Ω–∏–µ:** –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ mini-batch.\n",
        "\n",
        "**–ê–ª–≥–æ—Ä–∏—Ç–º:**\n",
        "\n",
        "–î–ª—è batch $B = \\{x_1, \\ldots, x_m\\}$:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\mu_B &= \\frac{1}{m} \\sum_{i=1}^m x_i \\quad \\text{(mean)} \\\\\n",
        "\\sigma_B^2 &= \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu_B)^2 \\quad \\text{(variance)} \\\\\n",
        "\\hat{x}_i &= \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\quad \\text{(normalize)} \\\\\n",
        "y_i &= \\gamma \\hat{x}_i + \\beta \\quad \\text{(scale and shift)}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "–≥–¥–µ $\\gamma, \\beta$ ‚Äî learnable parameters.\n",
        "\n",
        "**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**\n",
        "- ‚úÖ **Faster training** (–º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª—å—à–∏–π learning rate)\n",
        "- ‚úÖ **Regularization —ç—Ñ—Ñ–µ–∫—Ç** (–º–µ–Ω—å—à–µ –Ω—É–∂–µ–Ω dropout)\n",
        "- ‚úÖ **–ú–µ–Ω—å—à–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏**\n",
        "\n",
        "**–ì–¥–µ –≤—Å—Ç–∞–≤–ª—è—Ç—å?**\n",
        "\n",
        "```\n",
        "Linear ‚Üí BatchNorm ‚Üí ReLU\n",
        "```\n",
        "\n",
        "(–∏–ª–∏ –∏–Ω–æ–≥–¥–∞: Linear ‚Üí ReLU ‚Üí BatchNorm)\n",
        "\n",
        "---\n",
        "\n",
        "#### 1.5.4 Early Stopping\n",
        "\n",
        "**–ò–¥–µ—è:** –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ, –∫–æ–≥–¥–∞ validation loss –ø–µ—Ä–µ—Å—Ç–∞–µ—Ç —É–ª—É—á—à–∞—Ç—å—Å—è.\n",
        "\n",
        "**–ê–ª–≥–æ—Ä–∏—Ç–º:**\n",
        "```python\n",
        "best_val_loss = infinity\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    train(...)\n",
        "    val_loss = validate(...)\n",
        "    \n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        save_model()\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        \n",
        "    if patience_counter >= patience:\n",
        "        break  # Stop training\n",
        "```\n",
        "\n",
        "**–¢–∏–ø–∏—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:**\n",
        "- `patience = 5-20` epochs\n",
        "\n",
        "---\n",
        "\n",
        "## –¢–µ–æ—Ä–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –ø—Ä–∞–∫—Ç–∏–∫–µ üöÄ\n",
        "\n",
        "---"
    ]
})

# –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —è—á–µ–π–∫–∏
for cell in new_cells:
    notebook['cells'].append(cell)

# –°–æ—Ö—Ä–∞–Ω—è–µ–º
with open(notebook_path, 'w', encoding='utf-8') as f:
    json.dump(notebook, f, ensure_ascii=False, indent=1)

print(f'‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–∞ —Ç–µ–æ—Ä–∏—è: {len(new_cells)} —è—á–µ–µ–∫')
print(f'–í—Å–µ–≥–æ —è—á–µ–µ–∫: {len(notebook["cells"])}')

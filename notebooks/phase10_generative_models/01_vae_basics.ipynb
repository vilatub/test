{
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 10: Generative Models\n",
    "## Часть 1: Variational Autoencoders (VAE)\n",
    "\n",
    "### В этом ноутбуке:\n",
    "\n",
    "1. **Autoencoders** - сжатие и восстановление\n",
    "2. **VAE** - вероятностный подход\n",
    "3. **Reparametrization Trick** - обучение через sampling\n",
    "4. **Latent Space** - исследование пространства\n",
    "5. **Генерация новых образцов**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Синтетические данные\n",
    "\n",
    "Создаём простой датасет изображений 8x8 с паттернами."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def generate_patterns(n_samples=1000):\n",
    "    \"\"\"Генерация простых паттернов 8x8\"\"\"\n",
    "    patterns = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        img = np.zeros((8, 8))\n",
    "        pattern_type = np.random.randint(4)\n",
    "        \n",
    "        if pattern_type == 0:  # Горизонтальная линия\n",
    "            row = np.random.randint(1, 7)\n",
    "            img[row, 1:7] = 1\n",
    "        elif pattern_type == 1:  # Вертикальная линия\n",
    "            col = np.random.randint(1, 7)\n",
    "            img[1:7, col] = 1\n",
    "        elif pattern_type == 2:  # Квадрат\n",
    "            size = np.random.randint(2, 4)\n",
    "            start = np.random.randint(1, 6-size)\n",
    "            img[start:start+size, start:start+size] = 1\n",
    "        else:  # Диагональ\n",
    "            for i in range(6):\n",
    "                img[i+1, i+1] = 1\n",
    "        \n",
    "        # Добавляем шум\n",
    "        img += np.random.normal(0, 0.1, (8, 8))\n",
    "        img = np.clip(img, 0, 1)\n",
    "        patterns.append(img)\n",
    "    \n",
    "    return np.array(patterns)\n",
    "\n",
    "# Генерация данных\n",
    "X = generate_patterns(2000)\n",
    "X = torch.FloatTensor(X).view(-1, 1, 8, 8)\n",
    "\n",
    "# Датасет\n",
    "dataset = TensorDataset(X)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "print(f'Data shape: {X.shape}')\n",
    "\n",
    "# Визуализация\n",
    "fig, axes = plt.subplots(2, 8, figsize=(12, 3))\n",
    "for i in range(16):\n",
    "    ax = axes[i//8, i%8]\n",
    "    ax.imshow(X[i, 0], cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Sample Patterns')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Обычный Autoencoder\n",
    "\n",
    "Для сравнения сначала реализуем простой autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    \"\"\"Простой Autoencoder\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, latent_dim)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon.view(-1, 1, 8, 8)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# Тест\n",
    "ae = Autoencoder(latent_dim=8)\n",
    "test_input = torch.randn(1, 1, 8, 8)\n",
    "output = ae(test_input)\n",
    "print(f'Input: {test_input.shape} -> Output: {output.shape}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Variational Autoencoder (VAE)\n",
    "\n",
    "### Ключевые идеи:\n",
    "\n",
    "1. Encoder выдаёт **распределение** (μ, σ), а не точку\n",
    "2. **Reparametrization trick**: z = μ + σ * ε, где ε ~ N(0, 1)\n",
    "3. **ELBO Loss** = Reconstruction + KL Divergence"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class VAE(nn.Module):\n",
    "    \"\"\"Variational Autoencoder\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=8):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Latent space parameters\n",
    "        self.fc_mu = nn.Linear(16, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(16, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Reparametrization trick\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z).view(-1, 1, 8, 8)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "# Тест\n",
    "vae = VAE(latent_dim=8)\n",
    "test_input = torch.randn(1, 1, 8, 8)\n",
    "recon, mu, logvar = vae(test_input)\n",
    "print(f'Reconstruction: {recon.shape}')\n",
    "print(f'Mu: {mu.shape}, LogVar: {logvar.shape}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. VAE Loss Function\n",
    "\n",
    "**ELBO = Reconstruction Loss + KL Divergence**\n",
    "\n",
    "$$\\mathcal{L} = \\mathbb{E}[\\log p(x|z)] - D_{KL}(q(z|x) || p(z))$$\n",
    "\n",
    "Для Gaussian prior:\n",
    "\n",
    "$$D_{KL} = -\\frac{1}{2} \\sum (1 + \\log\\sigma^2 - \\mu^2 - \\sigma^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def vae_loss(recon_x, x, mu, logvar, beta=1.0):\n",
    "    \"\"\"\n",
    "    VAE Loss = Reconstruction + beta * KL Divergence\n",
    "    \n",
    "    beta-VAE: beta > 1 для лучшего disentanglement\n",
    "    \"\"\"\n",
    "    # Reconstruction loss (Binary Cross Entropy)\n",
    "    recon_loss = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    \n",
    "    # KL Divergence\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    return recon_loss + beta * kl_loss, recon_loss, kl_loss\n",
    "\n",
    "print('VAE Loss function готова')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Обучение VAE"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def train_vae(model, dataloader, epochs=100, lr=1e-3, beta=1.0):\n",
    "    \"\"\"Обучение VAE\"\"\"\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    history = {'total': [], 'recon': [], 'kl': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_recon = 0\n",
    "        total_kl = 0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            x = batch[0].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            recon, mu, logvar = model(x)\n",
    "            \n",
    "            loss, recon_loss, kl_loss = vae_loss(recon, x, mu, logvar, beta)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_recon += recon_loss.item()\n",
    "            total_kl += kl_loss.item()\n",
    "        \n",
    "        n = len(dataloader.dataset)\n",
    "        history['total'].append(total_loss / n)\n",
    "        history['recon'].append(total_recon / n)\n",
    "        history['kl'].append(total_kl / n)\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f'Epoch {epoch+1}, Loss: {total_loss/n:.4f}, '\n",
    "                  f'Recon: {total_recon/n:.4f}, KL: {total_kl/n:.4f}')\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Обучение\n",
    "vae = VAE(latent_dim=8)\n",
    "print('Обучение VAE...\\n')\n",
    "history = train_vae(vae, dataloader, epochs=100, lr=1e-3, beta=1.0)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Визуализация обучения\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 3))\n",
    "\n",
    "axes[0].plot(history['total'])\n",
    "axes[0].set_title('Total Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "\n",
    "axes[1].plot(history['recon'])\n",
    "axes[1].set_title('Reconstruction Loss')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "\n",
    "axes[2].plot(history['kl'])\n",
    "axes[2].set_title('KL Divergence')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Реконструкция"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Тестируем реконструкцию\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    test_samples = X[:8].to(device)\n",
    "    recon, _, _ = vae(test_samples)\n",
    "\n",
    "# Визуализация\n",
    "fig, axes = plt.subplots(2, 8, figsize=(12, 3))\n",
    "\n",
    "for i in range(8):\n",
    "    # Original\n",
    "    axes[0, i].imshow(test_samples[i, 0].cpu(), cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[0, i].set_title('Original')\n",
    "    \n",
    "    # Reconstruction\n",
    "    axes[1, i].imshow(recon[i, 0].cpu(), cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[1, i].set_title('Reconstructed')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Генерация новых образцов"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Генерация из случайного z\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    # Сэмплируем из стандартного нормального распределения\n",
    "    z = torch.randn(16, 8).to(device)\n",
    "    generated = vae.decode(z)\n",
    "\n",
    "# Визуализация\n",
    "fig, axes = plt.subplots(2, 8, figsize=(12, 3))\n",
    "for i in range(16):\n",
    "    ax = axes[i//8, i%8]\n",
    "    ax.imshow(generated[i, 0].cpu(), cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Generated Samples from Random z')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Исследование Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Интерполяция между двумя точками\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    # Берём два образца\n",
    "    x1 = X[0:1].to(device)\n",
    "    x2 = X[100:101].to(device)\n",
    "    \n",
    "    # Кодируем\n",
    "    mu1, _ = vae.encode(x1)\n",
    "    mu2, _ = vae.encode(x2)\n",
    "    \n",
    "    # Интерполяция\n",
    "    n_steps = 8\n",
    "    interpolations = []\n",
    "    for alpha in np.linspace(0, 1, n_steps):\n",
    "        z = (1 - alpha) * mu1 + alpha * mu2\n",
    "        img = vae.decode(z)\n",
    "        interpolations.append(img[0, 0].cpu())\n",
    "\n",
    "# Визуализация\n",
    "fig, axes = plt.subplots(1, n_steps, figsize=(12, 2))\n",
    "for i, img in enumerate(interpolations):\n",
    "    axes[i].imshow(img, cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f'α={i/(n_steps-1):.1f}')\n",
    "\n",
    "plt.suptitle('Latent Space Interpolation')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Визуализация 2D latent space (используя первые 2 компоненты)\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    mu, _ = vae.encode(X[:500].to(device))\n",
    "    mu = mu.cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(mu[:, 0], mu[:, 1], alpha=0.5, s=10)\n",
    "plt.xlabel('z[0]')\n",
    "plt.ylabel('z[1]')\n",
    "plt.title('Latent Space (first 2 dimensions)')\n",
    "plt.colorbar(label='Sample index')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Итоги\n",
    "\n",
    "### Что мы изучили:\n",
    "\n",
    "1. **Autoencoder** - сжатие и восстановление\n",
    "2. **VAE** - вероятностный latent space\n",
    "3. **Reparametrization trick** - градиенты через sampling\n",
    "4. **ELBO Loss** - reconstruction + KL divergence\n",
    "\n",
    "### Ключевые формулы:\n",
    "\n",
    "**Reparametrization:**\n",
    "$$z = \\mu + \\sigma \\cdot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)$$\n",
    "\n",
    "**KL Divergence (Gaussian):**\n",
    "$$D_{KL} = -\\frac{1}{2} \\sum (1 + \\log\\sigma^2 - \\mu^2 - \\sigma^2)$$\n",
    "\n",
    "### Преимущества VAE:\n",
    "\n",
    "- Структурированный latent space\n",
    "- Возможность генерации\n",
    "- Интерполяция между образцами\n",
    "\n",
    "### Следующий шаг:\n",
    "\n",
    "В ноутбуке 02 изучим GAN - Generative Adversarial Networks."
   ]
  }
 ]
}
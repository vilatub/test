#!/usr/bin/env python3
"""
Phase 4 Step 2: TabTransformer for Tabular Data
Part 2: Data Preprocessing, TabTransformer Theory and Implementation
"""

import json

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π notebook
notebook_path = '/home/user/test/notebooks/phase4_transformers/02_tabtransformer.ipynb'
with open(notebook_path, 'r', encoding='utf-8') as f:
    notebook = json.load(f)

cells = notebook['cells']

# ============================================================================
# DATA PREPROCESSING
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "---\n",
        "\n",
        "## üîß –ß–∞—Å—Ç—å 2: Preprocessing\n",
        "\n",
        "### 2.1 Cleaning –∏ Feature Engineering"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–ª—è preprocessing\n",
        "data = df.copy()\n",
        "\n",
        "# Clean target\n",
        "data['income'] = data['income'].str.replace('.', '', regex=False).str.strip()\n",
        "data['income'] = (data['income'] == '>50K').astype(int)\n",
        "\n",
        "# Handle missing values\n",
        "if data.isnull().sum().sum() > 0:\n",
        "    print(\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤...\")\n",
        "    # Categorical: fill with 'Unknown'\n",
        "    for col in categorical_cols:\n",
        "        if data[col].isnull().sum() > 0:\n",
        "            data[col].fillna('Unknown', inplace=True)\n",
        "    \n",
        "    # Numerical: fill with median\n",
        "    for col in numerical_cols:\n",
        "        if data[col].isnull().sum() > 0:\n",
        "            data[col].fillna(data[col].median(), inplace=True)\n",
        "    \n",
        "    print(f\"‚úÖ –ü—Ä–æ–ø—É—Å–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã\")\n",
        "\n",
        "print(f\"\\nFinal dataset shape: {data.shape}\")\n",
        "print(f\"Target distribution: {data['income'].value_counts().to_dict()}\")\n",
        "print(f\"Positive rate: {data['income'].mean():.2%}\")"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Label encoding –¥–ª—è categorical features\n",
        "print(\"Label Encoding categorical features...\")\n",
        "\n",
        "label_encoders = {}\n",
        "categorical_vocabs = {}\n",
        "\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    data[col] = le.fit_transform(data[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "    categorical_vocabs[col] = len(le.classes_)\n",
        "    print(f\"  {col:20s}: {len(le.classes_):3d} categories ‚Üí encoded to [0, {len(le.classes_)-1}]\")\n",
        "\n",
        "print(f\"\\n‚úÖ Label encoding done\")\n",
        "print(f\"\\nVocabulary sizes: {categorical_vocabs}\")"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Train/Test Split\n",
        "print(\"Train/Test Split...\")\n",
        "\n",
        "# Separate features and target\n",
        "X_cat = data[categorical_cols].values  # categorical features\n",
        "X_num = data[numerical_cols].values    # numerical features\n",
        "y = data['income'].values\n",
        "\n",
        "print(f\"X_cat shape: {X_cat.shape}  (categorical)\")\n",
        "print(f\"X_num shape: {X_num.shape}  (numerical)\")\n",
        "print(f\"y shape: {y.shape}\")\n",
        "\n",
        "# Split\n",
        "X_cat_train, X_cat_test, X_num_train, X_num_test, y_train, y_test = train_test_split(\n",
        "    X_cat, X_num, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain set: {len(X_cat_train):,} samples\")\n",
        "print(f\"Test set: {len(X_cat_test):,} samples\")\n",
        "\n",
        "# Standardize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_num_train = scaler.fit_transform(X_num_train)\n",
        "X_num_test = scaler.transform(X_num_test)\n",
        "\n",
        "print(\"\\n‚úÖ Numerical features standardized\")\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_cat_train_tensor = torch.LongTensor(X_cat_train)\n",
        "X_num_train_tensor = torch.FloatTensor(X_num_train)\n",
        "y_train_tensor = torch.LongTensor(y_train)\n",
        "\n",
        "X_cat_test_tensor = torch.LongTensor(X_cat_test)\n",
        "X_num_test_tensor = torch.FloatTensor(X_num_test)\n",
        "y_test_tensor = torch.LongTensor(y_test)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 256\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(\n",
        "    X_cat_train_tensor, X_num_train_tensor, y_train_tensor\n",
        ")\n",
        "test_dataset = torch.utils.data.TensorDataset(\n",
        "    X_cat_test_tensor, X_num_test_tensor, y_test_tensor\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"\\n‚úÖ DataLoaders created (batch_size={batch_size})\")\n",
        "print(f\"\\nDataset summary:\")\n",
        "print(f\"  Categorical features: {len(categorical_cols)}\")\n",
        "print(f\"  Numerical features: {len(numerical_cols)}\")\n",
        "print(f\"  Total samples: {len(data):,}\")\n",
        "print(f\"  Train: {len(X_cat_train):,} | Test: {len(X_cat_test):,}\")"
    ]
})

# ============================================================================
# TABTRANSFORMER THEORY
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "---\n",
        "\n",
        "## üß† –ß–∞—Å—Ç—å 3: TabTransformer Architecture\n",
        "\n",
        "### 3.1 –¢–µ–æ—Ä–∏—è: –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç TabTransformer?\n",
        "\n",
        "---\n",
        "\n",
        "## üìê –î–µ—Ç–∞–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞\n",
        "\n",
        "### –®–∞–≥ 1: Column Embeddings –¥–ª—è Categorical Features\n",
        "\n",
        "**–ü—Ä–æ–±–ª–µ–º–∞ –æ–±—ã—á–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞:**\n",
        "- One-hot encoding: –≤—ã—Å–æ–∫–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å, sparse\n",
        "- Label encoding + Linear: –Ω–µ—Ç semantic meaning\n",
        "\n",
        "**–†–µ—à–µ–Ω–∏–µ TabTransformer:**\n",
        "\n",
        "–î–ª—è –∫–∞–∂–¥–æ–π categorical feature $i$:\n",
        "\n",
        "$$\\text{Embedding}_i: \\mathbb{Z}_{|V_i|} \\rightarrow \\mathbb{R}^{d}$$\n",
        "\n",
        "–ì–¥–µ:\n",
        "- $|V_i|$ - vocabulary size feature $i$\n",
        "- $d$ - embedding dimension (–≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä)\n",
        "\n",
        "**–ê–Ω–∞–ª–æ–≥:** Word embeddings –≤ NLP!\n",
        "- Word2Vec: \"king\" - \"man\" + \"woman\" ‚âà \"queen\"\n",
        "- TabTransformer: \"Doctorate\" - \"HS-grad\" + \"Exec-managerial\" ‚âà \"Prof-specialty\"\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "education = \"Bachelors\" (encoded as 5)\n",
        "          ‚Üì\n",
        "Embedding Table [16 √ó 32]  ‚Üê 16 education categories, 32-dim embeddings\n",
        "          ‚Üì\n",
        "Row 5: [0.23, -0.15, 0.87, ..., 0.42]  ‚Üê 32-dimensional vector\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### –®–∞–≥ 2: Positional Encoding\n",
        "\n",
        "–î–æ–±–∞–≤–ª—è–µ–º column position information:\n",
        "\n",
        "$$\\text{Input}_i = \\text{Embedding}_i + \\text{PE}_i$$\n",
        "\n",
        "**–ó–∞—á–µ–º?**\n",
        "- Self-Attention permutation invariant\n",
        "- –ù—É–∂–Ω–æ –∑–Ω–∞—Ç—å, –∫–∞–∫–∞—è feature –æ—Ç–∫—É–¥–∞\n",
        "- –ò—Å–ø–æ–ª—å–∑—É–µ–º **learnable positional embeddings**\n",
        "\n",
        "---\n",
        "\n",
        "### –®–∞–≥ 3: Transformer Encoder Blocks\n",
        "\n",
        "**–¢–æ–ª—å–∫–æ –Ω–∞ categorical features!**\n",
        "\n",
        "–î–ª—è $N$ encoder blocks:\n",
        "\n",
        "$$\\text{Ctx}_i^{(l)} = \\text{TransformerBlock}^{(l)}(\\text{Ctx}_i^{(l-1)})$$\n",
        "\n",
        "–ì–¥–µ –∫–∞–∂–¥—ã–π block:\n",
        "1. Multi-Head Self-Attention\n",
        "2. Add & Norm\n",
        "3. Feed Forward Network\n",
        "4. Add & Norm\n",
        "\n",
        "**Output:** Contextual embeddings –¥–ª—è –∫–∞–∂–¥–æ–π categorical feature\n",
        "\n",
        "**–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç:**\n",
        "- \"Education\" attention –Ω–∞ \"Occupation\" ‚Üí –ø–æ–Ω–∏–º–∞–µ—Ç —Å–≤—è–∑—å\n",
        "- \"Marital-status\" attention –Ω–∞ \"Relationship\" ‚Üí –∫–æ–Ω—Ç–µ–∫—Å—Ç\n",
        "- –ö–∞–∂–¥–∞—è categorical feature –æ–±–æ–≥–∞—â–∞–µ—Ç—Å—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –¥—Ä—É–≥–∏—Ö\n",
        "\n",
        "---\n",
        "\n",
        "### –®–∞–≥ 4: Concatenation —Å Numerical Features\n",
        "\n",
        "$$\\text{Combined} = [\\text{Ctx}_1, \\text{Ctx}_2, ..., \\text{Ctx}_M, \\text{Num}_1, ..., \\text{Num}_N]$$\n",
        "\n",
        "**–ü–æ—á–µ–º—É numerical –Ω–µ —á–µ—Ä–µ–∑ Transformer?**\n",
        "- Numerical features —É–∂–µ continuous representations\n",
        "- –ù–µ –Ω—É–∂–Ω—ã embeddings\n",
        "- –ü—Ä–æ—Å—Ç–æ concatenate —Å contextual embeddings\n",
        "\n",
        "---\n",
        "\n",
        "### –®–∞–≥ 5: MLP Classification Head\n",
        "\n",
        "$$\\text{Output} = \\text{MLP}(\\text{Combined})$$\n",
        "\n",
        "- 2-3 layer MLP —Å ReLU, Dropout, LayerNorm\n",
        "- Binary classification: sigmoid output\n",
        "- Multi-class: softmax output\n",
        "\n",
        "---\n",
        "\n",
        "## üîë –ö–ª—é—á–µ–≤—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ TabTransformer\n",
        "\n",
        "### 1. Contextual Embeddings\n",
        "- Categorical features —É—á–∞—Ç—Å—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥—Ä—É–≥ –¥—Ä—É–≥–∞\n",
        "- \"Occupation=Teacher\" –∏–º–µ–µ—Ç —Ä–∞–∑–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Å \"Education=HS-grad\" vs \"Education=Masters\"\n",
        "\n",
        "### 2. Automatic Feature Interactions\n",
        "- Attention –º–µ—Ö–∞–Ω–∏–∑–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥–∏—Ç –≤–∞–∂–Ω—ã–µ interactions\n",
        "- –ù–µ –Ω—É–∂–Ω–æ –≤—Ä—É—á–Ω—É—é —Å–æ–∑–¥–∞–≤–∞—Ç—å cross-features\n",
        "\n",
        "### 3. Interpretability\n",
        "- Attention weights –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, –∫–∞–∫–∏–µ features –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—é—Ç\n",
        "- –ú–æ–∂–Ω–æ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å, –∫—É–¥–∞ –º–æ–¥–µ–ª—å \"—Å–º–æ—Ç—Ä–∏—Ç\"\n",
        "\n",
        "### 4. Transfer Learning Potential\n",
        "- –ú–æ–∂–Ω–æ pre-train embeddings –Ω–∞ –±–æ–ª—å—à–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
        "- Fine-tune –Ω–∞ —Ü–µ–ª–µ–≤–æ–π –∑–∞–¥–∞—á–µ\n",
        "\n",
        "### 5. Robustness to Missing Data\n",
        "- –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π token –¥–ª—è missing values\n",
        "- Transformer —É—á–∏—Ç—Å—è handle –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\n",
        "\n",
        "---\n",
        "\n",
        "## üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –¥—Ä—É–≥–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏\n",
        "\n",
        "| Method | Cat Features | Feature Interactions | Interpretability | Scalability |\n",
        "|--------|-------------|---------------------|------------------|-------------|\n",
        "| **XGBoost** | One-hot | Implicit (trees) | Feature importance | Medium |\n",
        "| **MLP** | One-hot/Embeddings | Limited | Black box | Good |\n",
        "| **Vanilla Transformer** | Linear projection | Self-Attention | Attention weights | Excellent |\n",
        "| **TabTransformer** | Contextual Embeddings | Self-Attention | Attention weights | Excellent |\n",
        "\n",
        "---\n"
    ]
})

# ============================================================================
# TABTRANSFORMER IMPLEMENTATION
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 3.2 Implementation: TabTransformer Components\n",
        "\n",
        "–†–µ–∞–ª–∏–∑—É–µ–º —Å –Ω—É–ª—è!"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Re-use components from Phase 4 Step 1\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    \"\"\"Scaled Dot-Product Attention\"\"\"\n",
        "    def __init__(self, dropout=0.1):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        d_k = Q.size(-1)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        \n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        \n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "        \n",
        "        context = torch.matmul(attention_weights, V)\n",
        "        return context, attention_weights\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-Head Attention\"\"\"\n",
        "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads\n",
        "        \n",
        "        self.W_Q = nn.Linear(d_model, d_model)\n",
        "        self.W_K = nn.Linear(d_model, d_model)\n",
        "        self.W_V = nn.Linear(d_model, d_model)\n",
        "        \n",
        "        self.attention = ScaledDotProductAttention(dropout)\n",
        "        self.W_O = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_len, d_model = x.size()\n",
        "        return x.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "    \n",
        "    def combine_heads(self, x):\n",
        "        batch_size, n_heads, seq_len, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "    \n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        Q = self.split_heads(self.W_Q(Q))\n",
        "        K = self.split_heads(self.W_K(K))\n",
        "        V = self.split_heads(self.W_V(V))\n",
        "        \n",
        "        context, attention_weights = self.attention(Q, K, V, mask)\n",
        "        context = self.combine_heads(context)\n",
        "        output = self.W_O(context)\n",
        "        output = self.dropout(output)\n",
        "        \n",
        "        return output, attention_weights\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"Position-wise Feed Forward Network\"\"\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
        "\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    \"\"\"Transformer Encoder Block\"\"\"\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        super(TransformerEncoderBlock, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        # Multi-Head Attention + Add & Norm\n",
        "        attn_output, attention_weights = self.attention(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        \n",
        "        # Feed Forward + Add & Norm\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        \n",
        "        return x, attention_weights\n",
        "\n",
        "print(\"‚úÖ Transformer components loaded\")"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "class TabTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    TabTransformer: Contextual Embeddings for Categorical Features\n",
        "    \n",
        "    Architecture:\n",
        "    1. Column Embeddings –¥–ª—è categorical features\n",
        "    2. Positional Encoding\n",
        "    3. Transformer Encoder Blocks (N layers)\n",
        "    4. Concatenation —Å numerical features\n",
        "    5. MLP Classification Head\n",
        "    \"\"\"\n",
        "    def __init__(self, \n",
        "                 categorical_vocabs,    # dict: {feature_name: vocab_size}\n",
        "                 num_numerical,          # number of numerical features\n",
        "                 d_model=32,             # embedding dimension\n",
        "                 n_heads=4,              # number of attention heads\n",
        "                 n_layers=2,             # number of transformer blocks\n",
        "                 d_ff=128,               # feed-forward dimension\n",
        "                 num_classes=2,          # output classes\n",
        "                 dropout=0.1):\n",
        "        super(TabTransformer, self).__init__()\n",
        "        \n",
        "        self.categorical_vocabs = categorical_vocabs\n",
        "        self.num_categorical = len(categorical_vocabs)\n",
        "        self.num_numerical = num_numerical\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        # 1. Column Embeddings –¥–ª—è –∫–∞–∂–¥–æ–π categorical feature\n",
        "        self.cat_embeddings = nn.ModuleList([\n",
        "            nn.Embedding(vocab_size, d_model)\n",
        "            for vocab_size in categorical_vocabs.values()\n",
        "        ])\n",
        "        \n",
        "        # 2. Positional Encoding (learnable)\n",
        "        self.pos_encoding = nn.Parameter(torch.randn(1, self.num_categorical, d_model))\n",
        "        \n",
        "        # 3. Transformer Encoder Blocks\n",
        "        self.encoder_blocks = nn.ModuleList([\n",
        "            TransformerEncoderBlock(d_model, n_heads, d_ff, dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        \n",
        "        # 4. MLP Head\n",
        "        # Input: concatenated contextual embeddings + numerical features\n",
        "        mlp_input_dim = self.num_categorical * d_model + num_numerical\n",
        "        \n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(mlp_input_dim, 128),\n",
        "            nn.LayerNorm(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            \n",
        "            nn.Linear(128, 64),\n",
        "            nn.LayerNorm(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            \n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x_cat, x_num, return_attention=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x_cat: (batch_size, num_categorical) - categorical features (label encoded)\n",
        "            x_num: (batch_size, num_numerical) - numerical features\n",
        "            return_attention: whether to return attention weights\n",
        "        \n",
        "        Returns:\n",
        "            logits: (batch_size, num_classes)\n",
        "            attention_weights: optional, list of attention weights from each layer\n",
        "        \"\"\"\n",
        "        batch_size = x_cat.size(0)\n",
        "        \n",
        "        # 1. Column Embeddings: –∫–∞–∂–¥–∞—è categorical feature ‚Üí embedding\n",
        "        cat_embeds = []\n",
        "        for i, emb_layer in enumerate(self.cat_embeddings):\n",
        "            cat_embeds.append(emb_layer(x_cat[:, i]))  # (batch, d_model)\n",
        "        \n",
        "        # Stack: (batch, num_categorical, d_model)\n",
        "        cat_embeds = torch.stack(cat_embeds, dim=1)\n",
        "        \n",
        "        # 2. Add Positional Encoding\n",
        "        cat_embeds = cat_embeds + self.pos_encoding\n",
        "        \n",
        "        # 3. Pass through Transformer Encoder Blocks\n",
        "        attention_weights_list = []\n",
        "        for encoder_block in self.encoder_blocks:\n",
        "            cat_embeds, attn_weights = encoder_block(cat_embeds)\n",
        "            if return_attention:\n",
        "                attention_weights_list.append(attn_weights)\n",
        "        \n",
        "        # 4. Flatten contextual embeddings: (batch, num_categorical * d_model)\n",
        "        cat_embeds_flat = cat_embeds.view(batch_size, -1)\n",
        "        \n",
        "        # 5. Concatenate —Å numerical features\n",
        "        combined = torch.cat([cat_embeds_flat, x_num], dim=1)\n",
        "        \n",
        "        # 6. MLP Classification\n",
        "        logits = self.mlp(combined)\n",
        "        \n",
        "        if return_attention:\n",
        "            return logits, attention_weights_list\n",
        "        return logits\n",
        "\n",
        "print(\"‚úÖ TabTransformer implemented!\")\n",
        "print(\"\\nKey components:\")\n",
        "print(\"  1. Column Embeddings (nn.Embedding –¥–ª—è –∫–∞–∂–¥–æ–π categorical feature)\")\n",
        "print(\"  2. Learnable Positional Encoding\")\n",
        "print(\"  3. Transformer Encoder Blocks (Multi-Head Attention)\")\n",
        "print(\"  4. Concatenation with Numerical Features\")\n",
        "print(\"  5. MLP Classification Head\")"
    ]
})

# –°–æ—Ö—Ä–∞–Ω—è–µ–º
notebook['cells'] = cells

with open(notebook_path, 'w', encoding='utf-8') as f:
    json.dump(notebook, f, ensure_ascii=False, indent=1)

print(f'‚úÖ Part 2 –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤: {notebook_path}')
print(f'–í—Å–µ–≥–æ —è—á–µ–µ–∫: {len(cells)}')
print('–°–ª–µ–¥—É—é—â–∞—è —á–∞—Å—Ç—å: Training, Evaluation, Comparison...')

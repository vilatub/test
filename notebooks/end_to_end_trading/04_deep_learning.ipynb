{
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Trading Project\n",
    "## Часть 4: Deep Learning\n",
    "\n",
    "### В этом ноутбуке:\n",
    "\n",
    "1. **Подготовка последовательностей** для временных рядов\n",
    "2. **LSTM** - Long Short-Term Memory\n",
    "3. **1D CNN** - свёрточная сеть для паттернов\n",
    "4. **CNN-LSTM** - комбинация подходов\n",
    "5. **Сравнение с baseline**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "print(f'PyTorch: {torch.__version__}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "data_dir = 'data'\n",
    "df = pd.read_parquet(f'{data_dir}/processed_data.parquet')\n",
    "\n",
    "with open(f'{data_dir}/feature_sets.json', 'r') as f:\n",
    "    feature_sets = json.load(f)\n",
    "\n",
    "print(f'Загружено: {len(df):,} записей')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Подготовка Последовательностей\n",
    "\n",
    "Для RNN/CNN нужны последовательности фиксированной длины."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class TradingSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset для последовательностей торговых данных.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, feature_cols, target_col, seq_length=30, scaler=None):\n",
    "        self.seq_length = seq_length\n",
    "        self.sequences = []\n",
    "        self.targets = []\n",
    "        \n",
    "        # Обрабатываем каждую акцию отдельно\n",
    "        for ticker in df['ticker'].unique():\n",
    "            ticker_df = df[df['ticker'] == ticker].sort_values('date')\n",
    "            \n",
    "            features = ticker_df[feature_cols].values\n",
    "            targets = ticker_df[target_col].values\n",
    "            \n",
    "            # Создаём последовательности\n",
    "            for i in range(len(ticker_df) - seq_length):\n",
    "                seq = features[i:i + seq_length]\n",
    "                target = targets[i + seq_length - 1]\n",
    "                \n",
    "                if not np.isnan(seq).any() and not np.isnan(target):\n",
    "                    self.sequences.append(seq)\n",
    "                    self.targets.append(target)\n",
    "        \n",
    "        self.sequences = np.array(self.sequences)\n",
    "        self.targets = np.array(self.targets)\n",
    "        \n",
    "        # Нормализация\n",
    "        if scaler is None:\n",
    "            self.scaler = StandardScaler()\n",
    "            original_shape = self.sequences.shape\n",
    "            self.sequences = self.scaler.fit_transform(\n",
    "                self.sequences.reshape(-1, len(feature_cols))\n",
    "            ).reshape(original_shape)\n",
    "        else:\n",
    "            self.scaler = scaler\n",
    "            original_shape = self.sequences.shape\n",
    "            self.sequences = self.scaler.transform(\n",
    "                self.sequences.reshape(-1, len(feature_cols))\n",
    "            ).reshape(original_shape)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.FloatTensor(self.sequences[idx]),\n",
    "            torch.FloatTensor([self.targets[idx]])\n",
    "        )\n",
    "\n",
    "# Параметры\n",
    "feature_cols = feature_sets['extended_features']\n",
    "feature_cols = [f for f in feature_cols if f in df.columns]\n",
    "target_col = 'target_direction_1d'\n",
    "seq_length = 30\n",
    "\n",
    "print(f'Признаков: {len(feature_cols)}')\n",
    "print(f'Длина последовательности: {seq_length}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Time-based split\n",
    "df = df.sort_values('date')\n",
    "n_samples = len(df)\n",
    "train_end = int(n_samples * 0.6)\n",
    "val_end = int(n_samples * 0.8)\n",
    "\n",
    "train_df = df.iloc[:train_end]\n",
    "val_df = df.iloc[train_end:val_end]\n",
    "test_df = df.iloc[val_end:]\n",
    "\n",
    "# Создаём datasets\n",
    "train_dataset = TradingSequenceDataset(train_df, feature_cols, target_col, seq_length)\n",
    "val_dataset = TradingSequenceDataset(val_df, feature_cols, target_col, seq_length, \n",
    "                                     scaler=train_dataset.scaler)\n",
    "test_dataset = TradingSequenceDataset(test_df, feature_cols, target_col, seq_length,\n",
    "                                      scaler=train_dataset.scaler)\n",
    "\n",
    "print(f'Train sequences: {len(train_dataset):,}')\n",
    "print(f'Val sequences: {len(val_dataset):,}')\n",
    "print(f'Test sequences: {len(test_dataset):,}')\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Проверяем размеры\n",
    "sample_x, sample_y = next(iter(train_loader))\n",
    "print(f'\\nBatch X shape: {sample_x.shape}')  # [batch, seq, features]\n",
    "print(f'Batch Y shape: {sample_y.shape}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM для классификации направления цены.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=64, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq, features]\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        # Берём последний hidden state\n",
    "        last_hidden = h_n[-1]  # [batch, hidden]\n",
    "        out = self.fc(last_hidden)\n",
    "        return out\n",
    "\n",
    "# Инициализация\n",
    "input_dim = len(feature_cols)\n",
    "lstm_model = LSTMClassifier(input_dim, hidden_dim=64, num_layers=2, dropout=0.2).to(device)\n",
    "\n",
    "print(f'LSTM параметров: {sum(p.numel() for p in lstm_model.parameters()):,}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=30, lr=0.001):\n",
    "    \"\"\"\n",
    "    Обучение модели с early stopping.\n",
    "    \"\"\"\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_x)\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                output = model(batch_x)\n",
    "                loss = criterion(output, batch_y)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= 10:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f'Epoch {epoch+1}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}')\n",
    "    \n",
    "    model.load_state_dict(best_state)\n",
    "    return train_losses, val_losses"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Обучаем LSTM\n",
    "print('Обучение LSTM...')\n",
    "lstm_train_losses, lstm_val_losses = train_model(lstm_model, train_loader, val_loader, epochs=30)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 1D CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    1D CNN для выявления локальных паттернов.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, seq_length):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Convolutions\n",
    "        self.conv1 = nn.Conv1d(input_dim, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(128, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        # Вычисляем размер после convolutions\n",
    "        conv_out_size = seq_length // 4  # После 2 pooling слоёв\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * conv_out_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq, features] -> [batch, features, seq]\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        \n",
    "        x = x.flatten(1)\n",
    "        out = self.fc(x)\n",
    "        return out\n",
    "\n",
    "# Инициализация\n",
    "cnn_model = CNNClassifier(input_dim, seq_length).to(device)\n",
    "print(f'CNN параметров: {sum(p.numel() for p in cnn_model.parameters()):,}')\n",
    "\n",
    "# Обучаем\n",
    "print('\\nОбучение CNN...')\n",
    "cnn_train_losses, cnn_val_losses = train_model(cnn_model, train_loader, val_loader, epochs=30)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CNN-LSTM Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class CNNLSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Гибрид: CNN извлекает локальные паттерны, LSTM моделирует временные зависимости.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # CNN для извлечения признаков\n",
    "        self.conv1 = nn.Conv1d(input_dim, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, padding=1)\n",
    "        \n",
    "        # LSTM для временных зависимостей\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=32,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq, features]\n",
    "        x = x.transpose(1, 2)  # [batch, features, seq]\n",
    "        \n",
    "        # CNN\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        \n",
    "        x = x.transpose(1, 2)  # [batch, seq, channels]\n",
    "        \n",
    "        # LSTM\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        \n",
    "        out = self.fc(h_n[-1])\n",
    "        return out\n",
    "\n",
    "# Инициализация и обучение\n",
    "cnn_lstm_model = CNNLSTMClassifier(input_dim).to(device)\n",
    "print(f'CNN-LSTM параметров: {sum(p.numel() for p in cnn_lstm_model.parameters()):,}')\n",
    "\n",
    "print('\\nОбучение CNN-LSTM...')\n",
    "cnn_lstm_train_losses, cnn_lstm_val_losses = train_model(cnn_lstm_model, train_loader, val_loader, epochs=30)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Оценка Моделей"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def evaluate_dl_model(model, loader, name='Model'):\n",
    "    \"\"\"\n",
    "    Оценка DL модели.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            output = model(batch_x)\n",
    "            \n",
    "            probs = output.cpu().numpy().flatten()\n",
    "            preds = (probs > 0.5).astype(int)\n",
    "            targets = batch_y.numpy().flatten()\n",
    "            \n",
    "            all_probs.extend(probs)\n",
    "            all_preds.extend(preds)\n",
    "            all_targets.extend(targets)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(all_targets, all_preds),\n",
    "        'f1': f1_score(all_targets, all_preds),\n",
    "        'roc_auc': roc_auc_score(all_targets, all_probs)\n",
    "    }\n",
    "    \n",
    "    return metrics, all_preds, all_probs\n",
    "\n",
    "# Оцениваем все модели\n",
    "results = {}\n",
    "\n",
    "for name, model in [('LSTM', lstm_model), ('CNN', cnn_model), ('CNN-LSTM', cnn_lstm_model)]:\n",
    "    train_metrics, _, _ = evaluate_dl_model(model, train_loader)\n",
    "    val_metrics, _, _ = evaluate_dl_model(model, val_loader)\n",
    "    test_metrics, preds, probs = evaluate_dl_model(model, test_loader)\n",
    "    \n",
    "    results[name] = {\n",
    "        'train': train_metrics,\n",
    "        'val': val_metrics,\n",
    "        'test': test_metrics\n",
    "    }\n",
    "    \n",
    "    print(f'\\n{name}:')\n",
    "    print(f'  Test Accuracy: {test_metrics[\"accuracy\"]:.4f}')\n",
    "    print(f'  Test ROC-AUC: {test_metrics[\"roc_auc\"]:.4f}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Визуализация\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# 1. Training curves\n",
    "axes[0].plot(lstm_train_losses, label='LSTM train')\n",
    "axes[0].plot(lstm_val_losses, label='LSTM val')\n",
    "axes[0].plot(cnn_train_losses, label='CNN train', linestyle='--')\n",
    "axes[0].plot(cnn_val_losses, label='CNN val', linestyle='--')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Progress')\n",
    "axes[0].legend()\n",
    "\n",
    "# 2. Model comparison\n",
    "models = list(results.keys())\n",
    "test_acc = [results[m]['test']['accuracy'] for m in models]\n",
    "test_auc = [results[m]['test']['roc_auc'] for m in models]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "axes[1].bar(x - width/2, test_acc, width, label='Accuracy')\n",
    "axes[1].bar(x + width/2, test_auc, width, label='ROC-AUC')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(models)\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('Test Metrics')\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim(0.45, 0.6)\n",
    "\n",
    "# 3. Train vs Val vs Test\n",
    "for i, model_name in enumerate(models):\n",
    "    train_acc = results[model_name]['train']['accuracy']\n",
    "    val_acc = results[model_name]['val']['accuracy']\n",
    "    test_acc = results[model_name]['test']['accuracy']\n",
    "    axes[2].bar(i*3, train_acc, label='Train' if i==0 else '')\n",
    "    axes[2].bar(i*3+1, val_acc, label='Val' if i==0 else '')\n",
    "    axes[2].bar(i*3+2, test_acc, label='Test' if i==0 else '')\n",
    "\n",
    "axes[2].set_xticks([1, 4, 7])\n",
    "axes[2].set_xticklabels(models)\n",
    "axes[2].set_ylabel('Accuracy')\n",
    "axes[2].set_title('Accuracy by Dataset')\n",
    "axes[2].legend()\n",
    "axes[2].set_ylim(0.45, 0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Сохраняем модели\n",
    "import os\n",
    "models_dir = 'models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "torch.save(lstm_model.state_dict(), f'{models_dir}/lstm_model.pt')\n",
    "torch.save(cnn_model.state_dict(), f'{models_dir}/cnn_model.pt')\n",
    "torch.save(cnn_lstm_model.state_dict(), f'{models_dir}/cnn_lstm_model.pt')\n",
    "\n",
    "# Сохраняем результаты\n",
    "with open(f'{models_dir}/dl_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print('Модели сохранены')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Итоги\n",
    "\n",
    "### Результаты:\n",
    "\n",
    "- Deep Learning модели показывают схожие результаты с classical ML (~51-53%)\n",
    "- Нет значительного переобучения\n",
    "- CNN-LSTM комбинирует преимущества обоих подходов\n",
    "\n",
    "### Выводы:\n",
    "\n",
    "1. **LSTM** хорошо улавливает временные зависимости\n",
    "2. **CNN** находит локальные паттерны\n",
    "3. **CNN-LSTM** - лучший баланс\n",
    "4. Сложность моделей не гарантирует лучший результат на эффективном рынке\n",
    "\n",
    "### Следующий шаг:\n",
    "\n",
    "В ноутбуке 05 попробуем TFT - state-of-the-art для временных рядов с:\n",
    "- Attention механизмами\n",
    "- Variable Selection\n",
    "- Quantile прогнозы"
   ]
  }
 ]
}
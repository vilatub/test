#!/usr/bin/env python3
"""
–°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –Ω–æ—É—Ç–±—É–∫–∞: Integration & Comparison
–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤ Phase 1 Classical ML –Ω–∞ –æ–¥–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ
"""

import json

# –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –Ω–æ—É—Ç–±—É–∫–∞
notebook = {
    "cells": [],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

cells = []

# ============================================================================
# TITLE
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "# üéØ Integration & Comparison: Phase 1 Classical ML\n",
        "\n",
        "**–¶–µ–ª—å:** –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –∏–∑—É—á–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –Ω–∞ –µ–¥–∏–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ –¶–µ–ª–∏ –Ω–æ—É—Ç–±—É–∫–∞\n",
        "\n",
        "1. **–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π** –∏–∑ –≤—Å–µ—Ö –Ω–æ—É—Ç–±—É–∫–æ–≤ Phase 1\n",
        "2. **–ß–µ—Å—Ç–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ** –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤ –Ω–∞ –æ–¥–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
        "3. **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏** –ø–æ –≤—ã–±–æ—Ä—É –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤\n",
        "4. **Best practices** –¥–ª—è production ML —Å–∏—Å—Ç–µ–º\n",
        "\n",
        "---\n",
        "\n",
        "## üìö –ß—Ç–æ –º—ã –∏–∑—É—á–∏–ª–∏ –≤ Phase 1:\n",
        "\n",
        "| –ù–æ—É—Ç–±—É–∫ | –¢–µ–º–∞ | –ö–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ |\n",
        "|---------|------|-------------------|\n",
        "| **01** | XGBoost Deep Dive | Gradient boosting, second-order optimization, regularization |\n",
        "| **02** | LightGBM Deep Dive | Histogram-based, GOSS, EFB, leaf-wise growth |\n",
        "| **03** | CatBoost Deep Dive | Ordered boosting, categorical features, symmetric trees |\n",
        "| **04** | Stacking Ensemble | Meta-learning, out-of-fold predictions, diversity |\n",
        "| **05** | Imbalanced Data | SMOTE, cost-sensitive learning, threshold optimization |\n",
        "| **06** | Advanced Feature Engineering | Polynomial, target encoding, feature selection |\n",
        "\n",
        "---\n",
        "\n",
        "## üíº –ë–∏–∑–Ω–µ—Å-–∑–∞–¥–∞—á–∞: Telco Customer Churn\n",
        "\n",
        "**–ö–æ–Ω—Ç–µ–∫—Å—Ç:** –¢–µ–ª–µ–∫–æ–º –∫–æ–º–ø–∞–Ω–∏—è —Ç–µ—Ä—è–µ—Ç 27% –∫–ª–∏–µ–Ω—Ç–æ–≤ –µ–∂–µ–≥–æ–¥–Ω–æ.\n",
        "\n",
        "**–ü—Ä–æ–±–ª–µ–º—ã:**\n",
        "- üìä **Imbalanced data:** ~27% churn (–Ω–µ extreme, –Ω–æ moderate)\n",
        "- üî¢ **–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:** –£—Å–ª—É–≥–∏, —Ç–∏–ø –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞ (high cardinality)\n",
        "- üí∞ **–†–∞–∑–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –æ—à–∏–±–æ–∫:** FN (—É–ø—É—â–µ–Ω–Ω—ã–π churn) –¥–æ—Ä–æ–∂–µ FP\n",
        "- üéØ **–ë–∏–∑–Ω–µ—Å-—Ü–µ–ª—å:** –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å churn —Å retention campaigns\n",
        "\n",
        "**–ú–µ—Ç—Ä–∏–∫–∏:**\n",
        "- **ROC-AUC:** –û–±—â–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Ä–∞–∑–ª–∏—á–∞—Ç—å –∫–ª–∞—Å—Å—ã\n",
        "- **Precision/Recall/F1:** –ë–∞–ª–∞–Ω—Å FP/FN\n",
        "- **Business Cost:** Weighted cost (FN = $500, FP = $50)\n",
        "\n",
        "**–ü–æ—á–µ–º—É —ç—Ç–æ—Ç –¥–∞—Ç–∞—Å–µ—Ç?**\n",
        "- ‚úÖ –ü–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–∏–º–µ–Ω–∏—Ç—å **–≤—Å–µ** —Ç–µ—Ö–Ω–∏–∫–∏ Phase 1\n",
        "- ‚úÖ –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ‚Üí —Ç–µ—Å—Ç–∏—Ä—É–µ–º CatBoost, Target Encoding\n",
        "- ‚úÖ Moderate imbalance ‚Üí SMOTE, class weights\n",
        "- ‚úÖ –ú–∞–ª—ã–π —Ä–∞–∑–º–µ—Ä (~7k) ‚Üí –º–æ–∂–Ω–æ –±—ã—Å—Ç—Ä–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å\n",
        "\n",
        "---"
    ]
})

# ============================================================================
# THEORY: COMPARISON OF METHODS
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "## üìö –ß–∞—Å—Ç—å 1: –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤\n",
        "\n",
        "### 1.1 Gradient Boosting —Å–µ–º–µ–π—Å—Ç–≤–æ\n",
        "\n",
        "#### –û–±—â–∞—è –∏–¥–µ—è Gradient Boosting\n",
        "\n",
        "**–§–æ—Ä–º—É–ª–∞ (Friedman, 2001):**\n",
        "\n",
        "$$F_m(x) = F_{m-1}(x) + \\nu \\cdot h_m(x)$$\n",
        "\n",
        "–≥–¥–µ:\n",
        "- $F_m(x)$ ‚Äî –∞–Ω—Å–∞–º–±–ª—å –∏–∑ $m$ –¥–µ—Ä–µ–≤—å–µ–≤\n",
        "- $h_m(x)$ ‚Äî –Ω–æ–≤–æ–µ –¥–µ—Ä–µ–≤–æ, –æ–±—É—á–∞–µ–º–æ–µ –Ω–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞—Ö\n",
        "- $\\nu$ ‚Äî learning rate (shrinkage)\n",
        "\n",
        "**–ì—Ä–∞–¥–∏–µ–Ω—Ç:**\n",
        "\n",
        "$$g_i = \\frac{\\partial L(y_i, F_{m-1}(x_i))}{\\partial F_{m-1}(x_i)}$$\n",
        "\n",
        "–ö–∞–∂–¥–æ–µ –¥–µ—Ä–µ–≤–æ –ø—Ä–∏–±–ª–∏–∂–∞–µ—Ç **anti-gradient** loss —Ñ—É–Ω–∫—Ü–∏–∏.\n",
        "\n",
        "---"
    ]
})

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 1.2 XGBoost vs LightGBM vs CatBoost\n",
        "\n",
        "#### –î–µ—Ç–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ\n",
        "\n",
        "**1. XGBoost (Chen & Guestrin, 2016)**\n",
        "\n",
        "**–ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è:** Second-order optimization\n",
        "\n",
        "$$L = \\sum_{i=1}^n \\left[ g_i f_t(x_i) + \\frac{1}{2} h_i f_t^2(x_i) \\right] + \\Omega(f_t)$$\n",
        "\n",
        "–≥–¥–µ:\n",
        "- $g_i$ ‚Äî first-order gradient\n",
        "- $h_i$ ‚Äî second-order gradient (Hessian)\n",
        "- $\\Omega(f_t) = \\gamma T + \\frac{1}{2}\\lambda \\sum_{j=1}^T w_j^2$ ‚Äî regularization\n",
        "\n",
        "**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**\n",
        "- ‚úÖ –¢–æ—á–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è (Newton's method)\n",
        "- ‚úÖ –°–∏–ª—å–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (L1/L2, gamma, alpha)\n",
        "- ‚úÖ –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –∏ –∑—Ä–µ–ª–æ—Å—Ç—å\n",
        "- ‚úÖ –û—Ç–ª–∏—á–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –∏ community\n",
        "\n",
        "**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:**\n",
        "- ‚ùå –ú–µ–¥–ª–µ–Ω–Ω–µ–µ LightGBM –Ω–∞ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "- ‚ùå –ë–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏ (pre-sorted –∏–ª–∏ histogram)\n",
        "- ‚ùå –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –Ω—É–∂–Ω–æ –∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –≤—Ä—É—á–Ω—É—é\n",
        "\n",
        "---\n",
        "\n",
        "**2. LightGBM (Ke et al., 2017)**\n",
        "\n",
        "**–ö–ª—é—á–µ–≤—ã–µ –∏–¥–µ–∏:**\n",
        "\n",
        "**a) Histogram-based splits**\n",
        "\n",
        "–í–º–µ—Å—Ç–æ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –≤—Å–µ—Ö –∑–Ω–∞—á–µ–Ω–∏–π ‚Üí bins (–æ–±—ã—á–Ω–æ 255):\n",
        "\n",
        "$$\\text{bin}(x) = \\left\\lfloor \\frac{x - \\min(x)}{\\text{bin\\_width}} \\right\\rfloor$$\n",
        "\n",
        "**b) GOSS (Gradient-based One-Side Sampling)**\n",
        "\n",
        "```\n",
        "1. –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã –ø–æ |gradient|\n",
        "2. –í–∑—è—Ç—å top-a% (–±–æ–ª—å—à–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã)\n",
        "3. –°–ª—É—á–∞–π–Ω–æ –≤–∑—è—Ç—å b% –∏–∑ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è\n",
        "4. –í–∑–≤–µ—Å–∏—Ç—å –º–∞–ª—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω–∞ (1-a)/b\n",
        "```\n",
        "\n",
        "**c) Leaf-wise (best-first) growth**\n",
        "\n",
        "–í–º–µ—Å—Ç–æ level-wise ‚Üí –≤—ã–±–∏—Ä–∞–µ–º leaf —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º gain:\n",
        "\n",
        "$$\\text{Gain} = \\frac{1}{2} \\left[ \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{(G_L + G_R)^2}{H_L + H_R + \\lambda} \\right] - \\gamma$$\n",
        "\n",
        "**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**\n",
        "- ‚úÖ **5-20x –±—ã—Å—Ç—Ä–µ–µ** XGBoost –Ω–∞ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "- ‚úÖ –ú–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏ (~50%)\n",
        "- ‚úÖ Native categorical support\n",
        "- ‚úÖ –õ—É—á—à–µ –Ω–∞ sparse –¥–∞–Ω–Ω—ã—Ö (EFB)\n",
        "\n",
        "**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:**\n",
        "- ‚ùå –°–∫–ª–æ–Ω–µ–Ω –∫ overfitting (leaf-wise aggressive)\n",
        "- ‚ùå –¢—Ä–µ–±—É–µ—Ç —Ç—â–∞—Ç–µ–ª—å–Ω–æ–≥–æ tuning\n",
        "- ‚ùå –•—É–∂–µ –Ω–∞ –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö (<10k)\n",
        "\n",
        "---\n",
        "\n",
        "**3. CatBoost (Prokhorenkova et al., 2018)**\n",
        "\n",
        "**–ö–ª—é—á–µ–≤—ã–µ –∏–¥–µ–∏:**\n",
        "\n",
        "**a) Ordered Boosting**\n",
        "\n",
        "–†–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã **prediction shift** ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏:\n",
        "\n",
        "$$F_{<i}(x_i) = \\sum_{t=1}^{T} h_t^{<i}(x_i)$$\n",
        "\n",
        "–ú–æ–¥–µ–ª—å $F_{<i}$ –æ–±—É—á–µ–Ω–∞ **–±–µ–∑** –ø—Ä–∏–º–µ—Ä–∞ $i$.\n",
        "\n",
        "**b) Ordered Target Statistics (–¥–ª—è categorical)**\n",
        "\n",
        "$$\\hat{x}_k^i = \\frac{\\sum_{j=1}^{p-1} [x_j = x_k] \\cdot y_{\\sigma_j} + a \\cdot p}{\\sum_{j=1}^{p-1} [x_j = x_k] + a}$$\n",
        "\n",
        "–≥–¥–µ $\\sigma$ ‚Äî —Å–ª—É—á–∞–π–Ω–∞—è –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∞, $p$ ‚Äî –ø–æ–∑–∏—Ü–∏—è –ø—Ä–∏–º–µ—Ä–∞ $i$, $a$ ‚Äî prior weight.\n",
        "\n",
        "**c) Symmetric (Oblivious) Trees**\n",
        "\n",
        "–í—Å–µ –ª–∏—Å—Ç—å—è –Ω–∞ –æ–¥–Ω–æ–º —É—Ä–æ–≤–Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π split ‚Üí –±—ã—Å—Ç—Ä—ã–π inference.\n",
        "\n",
        "**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**\n",
        "- ‚úÖ **–õ—É—á—à–∏–µ categorical features** –∏–∑ –∫–æ—Ä–æ–±–∫–∏\n",
        "- ‚úÖ –ú–µ–Ω—å—à–µ overfitting (ordered boosting)\n",
        "- ‚úÖ –û—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ **–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é** (–º–µ–Ω—å—à–µ tuning)\n",
        "- ‚úÖ GPU support\n",
        "\n",
        "**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:**\n",
        "- ‚ùå –ú–µ–¥–ª–µ–Ω–Ω–µ–µ –æ–±—É—á–µ–Ω–∏–µ (ordered boosting —Å–ª–æ–∂–Ω–µ–µ)\n",
        "- ‚ùå –ë–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏\n",
        "- ‚ùå Symmetric trees –º–æ–≥—É—Ç –±—ã—Ç—å –º–µ–Ω–µ–µ –≥–∏–±–∫–∏–º–∏\n",
        "\n",
        "---"
    ]
})

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 1.3 –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫–æ–π –∞–ª–≥–æ—Ä–∏—Ç–º?\n",
        "\n",
        "#### –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ç–∞–±–ª–∏—Ü–∞ –≤—ã–±–æ—Ä–∞\n",
        "\n",
        "| –°—Ü–µ–Ω–∞—Ä–∏–π | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è | –ü–æ—á–µ–º—É |\n",
        "|----------|-------------|--------|\n",
        "| **–ú–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ (<10k)** | XGBoost | –°—Ç–∞–±–∏–ª—å–Ω–µ–µ, level-wise –±–µ–∑–æ–ø–∞—Å–Ω–µ–µ |\n",
        "| **–°—Ä–µ–¥–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ (10k-1M)** | XGBoost –∏–ª–∏ CatBoost | –ó–∞–≤–∏—Å–∏—Ç –æ—Ç categorical features |\n",
        "| **–ë–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ (>1M)** | LightGBM | –í 5-20x –±—ã—Å—Ç—Ä–µ–µ |\n",
        "| **–ú–Ω–æ–≥–æ categorical** | CatBoost | Native support, ordered target stats |\n",
        "| **–†–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ** | LightGBM | EFB –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è |\n",
        "| **–ü–µ—Ä–≤–∞—è –º–æ–¥–µ–ª—å (baseline)** | XGBoost | –ú–µ–Ω—å—à–µ tuning, —Ö–æ—Ä–æ—à–∏–µ defaults |\n",
        "| **Production-critical** | XGBoost | –ó—Ä–µ–ª–æ—Å—Ç—å, —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å |\n",
        "| **–°–∫–æ—Ä–æ—Å—Ç—å inference** | CatBoost | Symmetric trees –±—ã—Å—Ç—Ä–µ–µ |\n",
        "| **–û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è –ø–∞–º—è—Ç—å** | LightGBM | Histogram-based |\n",
        "| **Kaggle competition** | Ensemble –≤—Å–µ—Ö —Ç—Ä–µ—Ö! | Diversity ‚Üí –ª—É—á—à–µ ensemble |\n",
        "\n",
        "#### –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
        "\n",
        "**–ò–∑ Kaggle competitions –∏ research:**\n",
        "\n",
        "1. **–ö–∞—á–µ—Å—Ç–≤–æ (ROC-AUC):**\n",
        "   - –ë–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ: LightGBM ‚â• CatBoost > XGBoost (—Ä–∞–∑–Ω–∏—Ü–∞ ~0.5-2%)\n",
        "   - –ú–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ: XGBoost ‚â• CatBoost ‚â• LightGBM\n",
        "   - –° –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º–∏: CatBoost —á–∞—Å—Ç–æ –ª—É—á—à–∏–π\n",
        "\n",
        "2. **–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è:**\n",
        "   - LightGBM: **5-20x** –±—ã—Å—Ç—Ä–µ–µ XGBoost –Ω–∞ >100k –ø—Ä–∏–º–µ—Ä–æ–≤\n",
        "   - CatBoost: **2-3x** –º–µ–¥–ª–µ–Ω–Ω–µ–µ XGBoost (ordered boosting)\n",
        "   - XGBoost: baseline —Å–∫–æ—Ä–æ—Å—Ç—å\n",
        "\n",
        "3. **–ü–∞–º—è—Ç—å:**\n",
        "   - LightGBM: **~50%** –æ—Ç XGBoost\n",
        "   - CatBoost: **~150%** –æ—Ç XGBoost –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏\n",
        "   - XGBoost: baseline\n",
        "\n",
        "4. **Tuning —Å–ª–æ–∂–Ω–æ—Å—Ç—å:**\n",
        "   - XGBoost: –°—Ä–µ–¥–Ω—è—è (—Ö–æ—Ä–æ—à–∏–µ defaults)\n",
        "   - LightGBM: –í—ã—Å–æ–∫–∞—è (leaf-wise –Ω—É–∂–µ–Ω –∫–æ–Ω—Ç—Ä–æ–ª—å)\n",
        "   - CatBoost: **–ù–∏–∑–∫–∞—è** (–æ—Ç–ª–∏—á–Ω—ã–µ defaults!)\n",
        "\n",
        "---"
    ]
})

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 1.4 Stacking: –ö–æ–≥–¥–∞ –∏ –∫–∞–∫?\n",
        "\n",
        "#### –¢–µ–æ—Ä–∏—è Stacking\n",
        "\n",
        "**Wolpert's Stacked Generalization (1992):**\n",
        "\n",
        "**Level 0 (Base learners):**\n",
        "\n",
        "$$\\hat{y}_i^{(k)} = f_k(x_i), \\quad k = 1, \\ldots, K$$\n",
        "\n",
        "**Level 1 (Meta-learner):**\n",
        "\n",
        "$$\\hat{y}_i = g\\left( \\hat{y}_i^{(1)}, \\hat{y}_i^{(2)}, \\ldots, \\hat{y}_i^{(K)} \\right)$$\n",
        "\n",
        "**Out-of-fold predictions (–∏–∑–±–µ–≥–∞–µ–º overfitting):**\n",
        "\n",
        "```\n",
        "For fold k in K-Fold:\n",
        "  1. Train base learners –Ω–∞ train_k\n",
        "  2. Predict –Ω–∞ val_k ‚Üí oof_predictions_k\n",
        "  3. Predict –Ω–∞ test ‚Üí test_predictions_k\n",
        "  \n",
        "Meta features = concat(oof_predictions_1, ..., oof_predictions_K)\n",
        "Meta learner.fit(Meta features, y)\n",
        "```\n",
        "\n",
        "#### –ö–æ–≥–¥–∞ Stacking —Ä–∞–±–æ—Ç–∞–µ—Ç?\n",
        "\n",
        "**–ö–ª—é—á–µ–≤–æ–µ —É—Å–ª–æ–≤–∏–µ:** Diversity –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π!\n",
        "\n",
        "**Ambiguity Decomposition (Krogh & Vedelsby, 1995):**\n",
        "\n",
        "$$E_{\\text{ensemble}} = \\bar{E} - \\bar{A}$$\n",
        "\n",
        "–≥–¥–µ:\n",
        "- $\\bar{E}$ ‚Äî —Å—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π\n",
        "- $\\bar{A}$ ‚Äî —Å—Ä–µ–¥–Ω—è—è ambiguity (diversity)\n",
        "\n",
        "**–ß–µ–º –±–æ–ª—å—à–µ diversity, —Ç–µ–º –º–µ–Ω—å—à–µ –æ—à–∏–±–∫–∞ ensemble!**\n",
        "\n",
        "#### –ö–∞–∫ —Å–æ–∑–¥–∞—Ç—å diversity?\n",
        "\n",
        "1. **–†–∞–∑–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã:** XGBoost + LightGBM + CatBoost + Logistic Regression\n",
        "2. **–†–∞–∑–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:** –û–¥–∏–Ω —Å polynomial, –¥—Ä—É–≥–æ–π —Å target encoding\n",
        "3. **–†–∞–∑–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã:** Shallow vs deep trees\n",
        "4. **–†–∞–∑–Ω—ã–µ –ø–æ–¥–≤—ã–±–æ—Ä–∫–∏:** Bagging –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —á–∞—Å—Ç—è—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "\n",
        "#### –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:\n",
        "\n",
        "‚úÖ **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Stacking –µ—Å–ª–∏:**\n",
        "- –ï—Å—Ç—å –≤—Ä–µ–º—è –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π\n",
        "- –ù—É–∂–Ω—ã –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø—Ä–æ—Ü–µ–Ω—Ç—ã –∫–∞—á–µ—Å—Ç–≤–∞ (Kaggle, research)\n",
        "- –ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ä–∞–∑–Ω—ã–µ\n",
        "\n",
        "‚ùå **–ù–ï –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ Stacking –µ—Å–ª–∏:**\n",
        "- –ù—É–∂–Ω–∞ —Å–∫–æ—Ä–æ—Å—Ç—å inference (–≤ 3-5x –º–µ–¥–ª–µ–Ω–Ω–µ–µ)\n",
        "- –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã (–æ–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –¥–æ–ª–≥–æ)\n",
        "- –ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏ (–º–∞–ª–æ diversity)\n",
        "- Production —Å–∏—Å—Ç–µ–º–∞ —Å –ø—Ä–æ—Å—Ç–æ—Ç–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏\n",
        "\n",
        "**–¢–∏–ø–∏—á–Ω—ã–π –ø—Ä–∏—Ä–æ—Å—Ç:** 0.5-3% ROC-AUC –ø–æ–≤–µ—Ä—Ö –ª—É—á—à–µ–π –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏.\n",
        "\n",
        "---"
    ]
})

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 1.5 Feature Engineering: –í–æ–∑–≤—Ä–∞—Ç –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–π\n",
        "\n",
        "#### –í–ª–∏—è–Ω–∏–µ –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏\n",
        "\n",
        "**–≠–º–ø–∏—Ä–∏—á–µ—Å–∫–æ–µ –ø—Ä–∞–≤–∏–ª–æ (Andrew Ng):**\n",
        "\n",
        "| –ö–æ–º–ø–æ–Ω–µ–Ω—Ç | –í–ª–∏—è–Ω–∏–µ –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ | –í—Ä–µ–º—è –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–π |\n",
        "|-----------|---------------------|------------------|\n",
        "| **Feature Engineering** | 60-70% | –í—ã—Å–æ–∫–æ–µ |\n",
        "| Algorithm selection | 15-20% | –°—Ä–µ–¥–Ω–µ–µ |\n",
        "| Hyperparameter tuning | 10-15% | –°—Ä–µ–¥–Ω–µ–µ |\n",
        "| Ensemble | 5-10% | –ù–∏–∑–∫–æ–µ |\n",
        "\n",
        "#### –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ç–µ—Ö–Ω–∏–∫\n",
        "\n",
        "**–ò–∑ –Ω–∞—à–µ–≥–æ –æ–ø—ã—Ç–∞ (Notebook 06):**\n",
        "\n",
        "| –¢–µ—Ö–Ω–∏–∫–∞ | –ü—Ä–∏—Ä–æ—Å—Ç –∫–∞—á–µ—Å—Ç–≤–∞ | –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å |\n",
        "|---------|-----------------|-------------------|\n",
        "| **Log transform** | 5-10% | Skewed –¥–∞–Ω–Ω—ã–µ (—Ü–µ–Ω—ã, –ø–ª–æ—â–∞–¥–∏) |\n",
        "| **Target encoding** | 3-7% | High cardinality categorical |\n",
        "| **Polynomial features** | 5-15% | –õ–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏, –º–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ |\n",
        "| **Feature selection** | 2-5% | –ú–Ω–æ–≥–æ —à—É–º–∞, overfitting |\n",
        "| **Domain features** | 10-30%! | –ö–æ–≥–¥–∞ –µ—Å—Ç—å domain knowledge |\n",
        "\n",
        "#### –í–∞–∂–Ω–æ—Å—Ç—å –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤\n",
        "\n",
        "**Linear models (Logistic Regression, SVM):**\n",
        "- ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê –ö—Ä–∏—Ç–∏—á–Ω–æ! (–Ω—É–∂–Ω—ã interactions, scaling, transformations)\n",
        "\n",
        "**Tree-based models (XGBoost, RF):**\n",
        "- ‚≠ê‚≠ê‚≠ê –ü–æ–ª–µ–∑–Ω–æ (domain features, target encoding)\n",
        "- Polynomial interactions –ù–ï –Ω—É–∂–Ω—ã (–¥–µ—Ä–µ–≤—å—è —Å–∞–º–∏ –∏—Ö –Ω–∞—Ö–æ–¥—è—Ç)\n",
        "\n",
        "**Neural Networks:**\n",
        "- ‚≠ê‚≠ê –ú–æ–∂–µ—Ç –ø–æ–º–æ—á—å, –Ω–æ —Å–µ—Ç–∏ —á–∞—Å—Ç–æ —É—á–∞—Ç —Å–∞–º–∏ (feature learning)\n",
        "\n",
        "---"
    ]
})

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 1.6 Imbalanced Data: –°—Ç—Ä–∞—Ç–µ–≥–∏—è –≤—ã–±–æ—Ä–∞ —Ç–µ—Ö–Ω–∏–∫–∏\n",
        "\n",
        "#### –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ —Å—Ç–µ–ø–µ–Ω–∏ imbalance\n",
        "\n",
        "| Imbalance ratio | –ö–∞—Ç–µ–≥–æ—Ä–∏—è | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |\n",
        "|----------------|-----------|-------------|\n",
        "| 1:1 - 1:4 | Balanced/Mild | –ù–∏—á–µ–≥–æ –Ω–µ –Ω—É–∂–Ω–æ |\n",
        "| 1:4 - 1:20 | Moderate | Class weights |\n",
        "| 1:20 - 1:100 | High | SMOTE + class weights |\n",
        "| 1:100+ | Extreme | Advanced (Focal loss, sampling) |\n",
        "\n",
        "#### –¢–µ—Ö–Ω–∏–∫–∏ –∏ –∏—Ö –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å\n",
        "\n",
        "**1. Class Weights (scale_pos_weight –≤ XGBoost)**\n",
        "\n",
        "$$w_{\\text{pos}} = \\frac{N_{\\text{neg}}}{N_{\\text{pos}}}$$\n",
        "\n",
        "- ‚úÖ –ü—Ä–æ—Å—Ç–æ—Ç–∞\n",
        "- ‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è tree-based models\n",
        "- ‚úÖ –ù–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "- ‚ùå –ù–µ —Å–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã\n",
        "\n",
        "**2. SMOTE (Synthetic Minority Over-sampling)**\n",
        "\n",
        "$$x_{\\text{new}} = x_i + \\lambda \\cdot (x_{\\text{nn}} - x_i), \\quad \\lambda \\in [0, 1]$$\n",
        "\n",
        "- ‚úÖ –°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã minority –∫–ª–∞—Å—Å–∞\n",
        "- ‚úÖ –ü–æ–º–æ–≥–∞–µ—Ç —Å diversity\n",
        "- ‚ùå –ú–æ–∂–µ—Ç —Å–æ–∑–¥–∞—Ç—å noise\n",
        "- ‚ùå –£–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö\n",
        "\n",
        "**3. Hybrid: SMOTE + Class Weights**\n",
        "\n",
        "- ‚úÖ –õ—É—á—à–µ–µ –∏–∑ –æ–±–æ–∏—Ö –º–∏—Ä–æ–≤\n",
        "- Moderate SMOTE (–¥–æ 1:10) + weights\n",
        "\n",
        "**4. Focal Loss (Lin et al., 2017)**\n",
        "\n",
        "$$FL(p_t) = -\\alpha_t (1 - p_t)^\\gamma \\log(p_t)$$\n",
        "\n",
        "- ‚úÖ –§–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ hard examples\n",
        "- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ down-weights easy examples\n",
        "- ‚ùå –ù—É–∂–Ω–∞ custom —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "\n",
        "#### –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è imbalanced data\n",
        "\n",
        "**–ù–ï –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ Accuracy!**\n",
        "\n",
        "**–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ:**\n",
        "1. **Precision/Recall/F1**\n",
        "2. **PR-AUC** (–ª—É—á—à–µ ROC-AUC –¥–ª—è imbalanced)\n",
        "3. **Business cost** (weighted FP –∏ FN)\n",
        "\n",
        "---\n",
        "\n",
        "## –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è —á–∞—Å—Ç—å –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –ø—Ä–∞–∫—Ç–∏–∫–µ üöÄ\n",
        "\n",
        "---"
    ]
})

# ============================================================================
# PRACTICAL: IMPORTS
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "## üìä –ß–∞—Å—Ç—å 2: –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ\n",
        "\n",
        "### 2.1 –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –û—Å–Ω–æ–≤–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import warnings\n",
        "import time\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Gradient Boosting\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "import lightgbm as lgb\n",
        "from lightgbm import LGBMClassifier\n",
        "import catboost as cb\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Baseline models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Preprocessing\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, cross_val_predict\n",
        "\n",
        "# Feature engineering\n",
        "from sklearn.preprocessing import PolynomialFeatures, PowerTransformer\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Imbalanced data\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "    HAS_IMBLEARN = True\n",
        "except ImportError:\n",
        "    print('‚ö†Ô∏è imbalanced-learn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω')\n",
        "    HAS_IMBLEARN = False\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, average_precision_score,\n",
        "    confusion_matrix, classification_report,\n",
        "    roc_curve, precision_recall_curve\n",
        ")\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette('husl')\n",
        "%matplotlib inline\n",
        "\n",
        "# Seed\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "print('‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã')\n",
        "print(f'XGBoost: {xgb.__version__}')\n",
        "print(f'LightGBM: {lgb.__version__}')\n",
        "print(f'CatBoost: {cb.__version__}')"
    ]
})

# –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ cells
notebook['cells'] = cells

# –°–æ—Ö—Ä–∞–Ω—è–µ–º
output_path = '/home/user/test/notebooks/phase1_classical_ml/07_integration_comparison.ipynb'
with open(output_path, 'w', encoding='utf-8') as f:
    json.dump(notebook, f, ensure_ascii=False, indent=1)

print(f'‚úÖ –°–æ–∑–¥–∞–Ω –Ω–æ—É—Ç–±—É–∫ —Å {len(cells)} —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–º–∏ —è—á–µ–π–∫–∞–º–∏')
print(f'–°–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}')
print('üìù –¢–µ–æ—Ä–∏—è –Ω–∞–ø–∏—Å–∞–Ω–∞ –ø—Ä—è–º–æ –≤ –Ω–æ—É—Ç–±—É–∫–µ —Å —Ñ–æ—Ä–º—É–ª–∞–º–∏!')

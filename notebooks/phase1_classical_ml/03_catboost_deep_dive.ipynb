{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# CatBoost Deep Dive: –û—Ç —Ç–µ–æ—Ä–∏–∏ –∫ –ø—Ä–∞–∫—Ç–∏–∫–µ\n\n## üéØ –¶–µ–ª–∏ –Ω–æ—É—Ç–±—É–∫–∞\n\n1. **–ì–ª—É–±–æ–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ** –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ CatBoost –∏ –∏—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π\n2. **Ordered boosting** - —Ä–µ—à–µ–Ω–∏–µ prediction shift –ø—Ä–æ–±–ª–µ–º—ã\n3. **Categorical features** - –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –±–µ–∑ preprocessing\n4. **Symmetric trees** - –æ–±livious –¥–µ—Ä–µ–≤—å—è –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏\n5. **–ü—Ä–∞–∫—Ç–∏–∫–∞** –Ω–∞ –∑–∞–¥–∞—á–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ü–µ–Ω –Ω–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å\n\n## üíº –ë–∏–∑–Ω–µ—Å-–∑–∞–¥–∞—á–∞: House Price Prediction\n\n**–ö–æ–Ω—Ç–µ–∫—Å—Ç:** –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –∂–∏–ª—å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ (–ø–ª–æ—â–∞–¥—å, –ª–æ–∫–∞—Ü–∏—è, –∫–∞—á–µ—Å—Ç–≤–æ, etc.)\n\n**–ü–æ—á–µ–º—É —ç—Ç–æ –≤–∞–∂–Ω–æ:**\n- üí∞ **–û—Ü–µ–Ω–∫–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏** –¥–ª—è –ø–æ–∫—É–ø–∫–∏/–ø—Ä–æ–¥–∞–∂–∏\n- üè¶ **–ò–ø–æ—Ç–µ—á–Ω–æ–µ –∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏–µ** - –æ—Ü–µ–Ω–∫–∞ –∑–∞–ª–æ–≥–∞\n- üìä **–ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏** - –∞–Ω–∞–ª–∏–∑ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏\n- üèóÔ∏è **–î–µ–≤–µ–ª–æ–ø–º–µ–Ω—Ç** - —Ü–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç—Ä–æ–µ–∫\n\n**–ú–µ—Ç—Ä–∏–∫–∏:**\n- **RMSE** (Root Mean Squared Error) - –æ—Å–Ω–æ–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞\n- **MAE** (Mean Absolute Error) - —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å –∫ –≤—ã–±—Ä–æ—Å–∞–º\n- **R¬≤** - –¥–æ–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–Ω–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–∏\n- **MAPE** - —Å—Ä–µ–¥–Ω—è—è –ø—Ä–æ—Ü–µ–Ω—Ç–Ω–∞—è –æ—à–∏–±–∫–∞\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üìö –ß–∞—Å—Ç—å 1: –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç\n\n### 1.1 –ú–æ—Ç–∏–≤–∞—Ü–∏—è: –ü—Ä–æ–±–ª–µ–º–∞ Prediction Shift\n\n#### Target Leakage –≤ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–º GBDT\n\n**–ü—Ä–æ–±–ª–µ–º–∞:** –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç **–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è** –º–æ–¥–µ–ª–∏ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤. –ù–æ —ç—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–¥–µ–ª–∞–Ω—ã –Ω–∞ —Ç–µ—Ö –∂–µ –¥–∞–Ω–Ω—ã—Ö, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –æ–±—É—á–∞–ª–∞—Å—å –º–æ–¥–µ–ª—å!\n\n**–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏:**\n\n–ù–∞ –∏—Ç–µ—Ä–∞—Ü–∏–∏ $m$ –≤—ã—á–∏—Å–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã:\n\n$$g_i = -\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\Big|_{F=F_{m-1}}$$\n\n–≥–¥–µ $F_{m-1}(x_i)$ - –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω–æ–π –Ω–∞ **–≤—Å–µ—Ö** –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—è $x_i$!\n\n**–≠—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫:**\n\n1. **Conditional shift** - —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π $F(x_i)$ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ç–æ–≥–æ, –±—ã–ª –ª–∏ $x_i$ –≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ\n2. **Overfitting** - –º–æ–¥–µ–ª—å \"–≤–∏–¥–µ–ª–∞\" $x_i$ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞\n3. **Prediction shift** - bias –≤ –æ—Ü–µ–Ω–∫–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\n\n**–ü—Ä–∏–º–µ—Ä:**\n\nTarget encoding –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ \"City\":\n\n```python\n# –¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥\ncity_mean = train.groupby('City')['Price'].mean()\ntrain['City_encoded'] = train['City'].map(city_mean)  # TARGET LEAKAGE!\n```\n\n–î–ª—è –ø—Ä–∏–º–µ—Ä–∞ –∏–∑ –≥–æ—Ä–æ–¥–∞ \"Moscow\" –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ä–µ–¥–Ω–µ–µ, –∫–æ—Ç–æ—Ä–æ–µ **–≤–∫–ª—é—á–∞–µ—Ç —Å–∞–º —ç—Ç–æ—Ç –ø—Ä–∏–º–µ—Ä**!\n\n$$\\text{mean}_{\\text{Moscow}} = \\frac{1}{n}\\sum_{i: \\text{City}_i = \\text{Moscow}} y_i \\quad \\text{(–≤–∫–ª—é—á–∞—è —Ç–µ–∫—É—â–∏–π } y_i!)$$\n\n**–ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è:**\n- –ó–∞–≤—ã—à–µ–Ω–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –Ω–∞ train\n- –ü–∞–¥–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ test\n- Overfitting –∫ train –ø—Ä–∏–º–µ—Ä–∞–º\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.2 –†–µ—à–µ–Ω–∏–µ: Ordered Boosting\n\n#### –ò–¥–µ—è\n\n**CatBoost:** –ò—Å–ø–æ–ª—å–∑—É–µ–º **—Ä–∞–∑–Ω—ã–µ** –º–æ–¥–µ–ª–∏ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤!\n\n–î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ $x_i$ –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å, –æ–±—É—á–µ–Ω–Ω—É—é **—Ç–æ–ª—å–∫–æ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö –¥–æ** $i$ (–≤ —Å–ª—É—á–∞–π–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ).\n\n#### –ê–ª–≥–æ—Ä–∏—Ç–º\n\n**–®–∞–≥ 1:** –°–ª—É—á–∞–π–Ω–∞—è –ø–µ—Ä–º—É—Ç–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–æ–≤ $\\sigma = (\\sigma_1, \\ldots, \\sigma_n)$\n\n**–®–∞–≥ 2:** –î–ª—è –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ $m$ –∏ –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ $i$:\n\n1. –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å $M^{<i}_m$ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö $\\{\\sigma_1, \\ldots, \\sigma_{i-1}\\}$\n2. –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: $F^{<i}_{m-1}(x_{\\sigma_i})$\n3. –í—ã—á–∏—Å–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç:\n\n$$g_{\\sigma_i} = -\\frac{\\partial L(y_{\\sigma_i}, F(x_{\\sigma_i}))}{\\partial F}\\Big|_{F=F^{<i}_{m-1}}$$\n\n4. –ò—Å–ø–æ–ª—å–∑—É–µ–º —ç—Ç–æ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –¥–µ—Ä–µ–≤–∞ $h_m$\n\n**–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞:**\n\n–î–ª—è –ø—Ä–∏–º–µ—Ä–∞ –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ $i$ –≤ –ø–µ—Ä–º—É—Ç–∞—Ü–∏–∏, –≥—Ä–∞–¥–∏–µ–Ω—Ç –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –Ω–∞ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä–∞—è **–Ω–µ –≤–∏–¥–µ–ª–∞** —ç—Ç–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞:\n\n$$F^{<i}_{m-1}(x) = \\sum_{k=1}^{m-1} h_k(x; \\mathcal{D}^{<i})$$\n\n–≥–¥–µ $\\mathcal{D}^{<i} = \\{(x_{\\sigma_j}, y_{\\sigma_j})\\}_{j=1}^{i-1}$\n\n**–ü—Ä–æ–±–ª–µ–º–∞:** –ù—É–∂–Ω–æ –æ–±—É—á–∞—Ç—å $n$ –º–æ–¥–µ–ª–µ–π –Ω–∞ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ - —Å–ª–∏—à–∫–æ–º –¥–æ—Ä–æ–≥–æ!\n\n**–†–µ—à–µ–Ω–∏–µ CatBoost:** Approximation —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ (4-8) —Å–ª—É—á–∞–π–Ω—ã–º–∏ –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∞–º–∏.\n\n#### Gradient Estimation\n\n–í–º–µ—Å—Ç–æ $n$ –º–æ–¥–µ–ª–µ–π, CatBoost –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–µ—Ä—Å–∏–π –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –ø—Ä–µ—Ñ–∏–∫—Å–∞—Ö –¥–∞–Ω–Ω—ã—Ö:\n\n- –ú–æ–¥–µ–ª—å $M_1$: –æ–±—É—á–µ–Ω–∞ –Ω–∞ –ø–µ—Ä–≤—ã—Ö 25% –¥–∞–Ω–Ω—ã—Ö\n- –ú–æ–¥–µ–ª—å $M_2$: –æ–±—É—á–µ–Ω–∞ –Ω–∞ –ø–µ—Ä–≤—ã—Ö 50% –¥–∞–Ω–Ω—ã—Ö\n- –ú–æ–¥–µ–ª—å $M_3$: –æ–±—É—á–µ–Ω–∞ –Ω–∞ –ø–µ—Ä–≤—ã—Ö 75% –¥–∞–Ω–Ω—ã—Ö\n- –ú–æ–¥–µ–ª—å $M_4$: –æ–±—É—á–µ–Ω–∞ –Ω–∞ –≤—Å–µ—Ö 100% –¥–∞–Ω–Ω—ã—Ö\n\n–î–ª—è –ø—Ä–∏–º–µ—Ä–∞ –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ $i$ –≤—ã–±–∏—Ä–∞–µ–º –ø–æ–¥—Ö–æ–¥—è—â—É—é –º–æ–¥–µ–ª—å $M_k$ –≥–¥–µ $k$ - –±–ª–∏–∂–∞–π—à–∏–π prefix.\n\n**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**\n- ‚úÖ –ù–µ—Ç target leakage\n- ‚úÖ –ë–æ–ª–µ–µ robust –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã\n- ‚úÖ –õ—É—á—à–∞—è generalization\n- ‚úÖ –û—Å–æ–±–µ–Ω–Ω–æ –≤–∞–∂–Ω–æ –¥–ª—è categorical features!\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.3 Categorical Features: Optimal Encoding\n\n#### –ü—Ä–æ–±–ª–µ–º–∞ —Å One-Hot Encoding\n\n**One-hot encoding** –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å high cardinality:\n- –û–≥—Ä–æ–º–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å (1000 –≥–æ—Ä–æ–¥–æ–≤ = 1000 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤!)\n- –†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å (–±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ = 0)\n- –ü–æ—Ç–µ—Ä—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n- –ù–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ splits\n\n#### CatBoost Solution: Target Statistics\n\n**–ò–¥–µ—è:** –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ $c$ –≤—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –Ω–∞ –æ—Å–Ω–æ–≤–µ target:\n\n$$\\hat{y}_c = \\frac{\\sum_{i: x_i^{cat} = c} y_i}{\\sum_{i: x_i^{cat} = c} 1}$$\n\n–≠—Ç–æ **mean target** –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ $c$.\n\n**–ù–æ:** –ü—Ä–æ—Å—Ç–æ–µ mean target = target leakage! (—Å–º. —Å–µ–∫—Ü–∏—é 1.1)\n\n#### Ordered Target Statistics\n\n**CatBoost approach:** –ò—Å–ø–æ–ª—å–∑—É–µ–º **ordered** —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏!\n\n–î–ª—è –ø—Ä–∏–º–µ—Ä–∞ $x_i$ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π $c$ –≤—ã—á–∏—Å–ª—è–µ–º:\n\n$$\\hat{y}_c^{<i} = \\frac{\\sum_{j < i: x_{\\sigma_j}^{cat} = c} y_{\\sigma_j} + a \\cdot p}{\\sum_{j < i: x_{\\sigma_j}^{cat} = c} 1 + a}$$\n\n–≥–¥–µ:\n- –°—É–º–º–∏—Ä—É–µ–º **—Ç–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä—ã –¥–æ** $i$ –≤ –ø–µ—Ä–º—É—Ç–∞—Ü–∏–∏\n- $a$ - prior weight (–æ–±—ã—á–Ω–æ 1)\n- $p$ - prior value (–æ–±—ã—á–Ω–æ mean target –ø–æ –≤—Å–µ–º—É dataset)\n\n**Bayesian smoothing:**\n\n–§–æ—Ä–º—É–ª–∞ –≤—ã—à–µ = Bayesian estimate —Å prior $p$ –∏ –≤–µ—Å–æ–º $a$:\n\n- –ï—Å–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è —á–∞—Å—Ç–æ ‚Üí –±–ª–∏–∂–µ –∫ —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–æ–º—É mean\n- –ï—Å–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è —Ä–µ–¥–∫–∞—è ‚Üí –±–ª–∏–∂–µ –∫ prior $p$\n\n**–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è:**\n\n$$\\hat{y}_c^{<i} = \\frac{n_c^{<i} \\cdot \\bar{y}_c^{<i} + a \\cdot p}{n_c^{<i} + a}$$\n\n–≥–¥–µ $n_c^{<i}$ - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π $c$ –¥–æ –ø–æ–∑–∏—Ü–∏–∏ $i$.\n\n**–î–ª—è –º–∞–ª—ã—Ö** $n_c^{<i}$: –≤–µ—Å –Ω–∞ prior –±–æ–ª—å—à–µ (–∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞)\n**–î–ª—è –±–æ–ª—å—à–∏—Ö** $n_c^{<i}$: –≤–µ—Å –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –±–æ–ª—å—à–µ (—Ç–æ—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞)\n\n#### Combinations of Categorical Features\n\nCatBoost —Ç–∞–∫–∂–µ —Å–æ–∑–¥–∞–µ—Ç **–∫–æ–º–±–∏–Ω–∞—Ü–∏–∏** –∫–∞—Ç–µ–≥–æ—Ä–∏–π:\n\n$$\\text{City} \\times \\text{BuildingType} = \\text{\"Moscow_Apartment\"}$$\n\n–í—ã—á–∏—Å–ª—è–µ—Ç ordered target statistics –¥–ª—è –∫–æ–º–±–∏–Ω–∞—Ü–∏–π.\n\n**Greedy algorithm:**\n1. –ù–∞—á–∏–Ω–∞–µ–º —Å –æ–¥–∏–Ω–æ—á–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n2. –ù–∞ –∫–∞–∂–¥–æ–º –¥–µ—Ä–µ–≤–µ –ø—Ä–æ–±—É–µ–º –¥–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é –≤ –∫–æ–º–±–∏–Ω–∞—Ü–∏—é\n3. –í—ã–±–∏—Ä–∞–µ–º –∫–æ–º–±–∏–Ω–∞—Ü–∏—é —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º gain\n\n**–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ:** Max depth –∫–æ–º–±–∏–Ω–∞—Ü–∏–π (–æ–±—ã—á–Ω–æ 2-3)\n\n**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**\n- ‚úÖ –ù–µ—Ç one-hot explosion\n- ‚úÖ –ù–µ—Ç target leakage (ordered statistics)\n- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ interactions\n- ‚úÖ Works –¥–ª—è high cardinality\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.4 Symmetric (Oblivious) Trees\n\n#### –¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –¥–µ—Ä–µ–≤—å—è\n\n**XGBoost/LightGBM:** –ö–∞–∂–¥—ã–π —É–∑–µ–ª –º–æ–∂–µ—Ç –∏–º–µ—Ç—å —Å–≤–æ–π split:\n\n```\n             [Feature A > 5?]\n            /                \\\n     [Feature B > 3?]      [Feature C > 7?]\n     /        \\            /        \\\n   Leaf1    Leaf2      Leaf3      Leaf4\n```\n\n–†–∞–∑–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω—è—Ö –∏ –≤–µ—Ç–∫–∞—Ö.\n\n**–ü—Ä–æ–±–ª–µ–º—ã:**\n- –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ (—Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ degrees of freedom)\n- –ú–µ–¥–ª–µ–Ω–Ω—ã–π inference (–Ω—É–∂–Ω–æ –ø—Ä–æ—Ö–æ–¥–∏—Ç—å –≤–µ—Å—å –ø—É—Ç—å)\n- –ë–æ–ª—å—à–æ–π —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏\n\n#### Oblivious Trees (Symmetric)\n\n**CatBoost:** –í—Å–µ —É–∑–ª—ã –Ω–∞ –æ–¥–Ω–æ–º —É—Ä–æ–≤–Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç **–æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ** split!\n\n```\n             [Feature A > 5?]\n            /                \\\n     [Feature A > 5?]      [Feature A > 5?]\n     /        \\            /        \\\n   Leaf1    Leaf2      Leaf3      Leaf4\n```\n\n–ù–µ—Ç, —ç—Ç–æ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ. –ü—Ä–∞–≤–∏–ª—å–Ω–æ:\n\n```\nLevel 0:      [Feature A > 5?]\n             /                \\\nLevel 1:  [Feature B > 3?]  [Feature B > 3?]\n          /      \\          /      \\\n        L1      L2        L3      L4\n```\n\n**–û–¥–∏–Ω–∞–∫–æ–≤—ã–π split** –Ω–∞ –∫–∞–∂–¥–æ–º —É—Ä–æ–≤–Ω–µ –¥–ª—è –≤—Å–µ—Ö —É–∑–ª–æ–≤!\n\n**–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏:**\n\n–î–µ—Ä–µ–≤–æ –≥–ª—É–±–∏–Ω—ã $d$ –∏–º–µ–µ—Ç $2^d$ –ª–∏—Å—Ç—å–µ–≤. –ö–∞–∂–¥—ã–π –ø—É—Ç—å –∫ –ª–∏—Å—Ç—É = –±–∏–Ω–∞—Ä–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–ª–∏–Ω—ã $d$:\n\n$$\\text{Leaf index} = b_1 2^{d-1} + b_2 2^{d-2} + \\ldots + b_d 2^0$$\n\n–≥–¥–µ $b_i \\in \\{0, 1\\}$ - —Ä–µ–∑—É–ª—å—Ç–∞—Ç split –Ω–∞ —É—Ä–æ–≤–Ω–µ $i$.\n\n**–•—Ä–∞–Ω–µ–Ω–∏–µ:**\n- –í—Å–µ–≥–æ $d$ —É—Å–ª–æ–≤–∏–π (–ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ —É—Ä–æ–≤–µ–Ω—å)\n- $2^d$ –∑–Ω–∞—á–µ–Ω–∏–π –≤ –ª–∏—Å—Ç—å—è—Ö\n- –ö–æ–º–ø–∞–∫—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ!\n\n**Inference:**\n\n–î–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤—ã—á–∏—Å–ª—è–µ–º $d$ —É—Å–ª–æ–≤–∏–π, —Å—Ç—Ä–æ–∏–º index –ª–∏—Å—Ç–∞ –∑–∞ $O(d)$ –æ–ø–µ—Ä–∞—Ü–∏–π.\n\n```python\ndef predict(x, tree):\n    index = 0\n    for level in range(depth):\n        feature, threshold = tree.splits[level]\n        if x[feature] > threshold:\n            index = 2 * index + 1  # –ü—Ä–∞–≤–∞—è –≤–µ—Ç–∫–∞\n        else:\n            index = 2 * index      # –õ–µ–≤–∞—è –≤–µ—Ç–∫–∞\n    return tree.leaves[index]\n```\n\n**Complexity:** $O(d)$ –≤–º–µ—Å—Ç–æ $O(d \\cdot 2^d)$ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –ø–µ—Ä–µ–±–æ—Ä–∞!\n\n**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**\n- ‚úÖ **–ö–æ–º–ø–∞–∫—Ç–Ω–æ—Å—Ç—å:** $d + 2^d$ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤–º–µ—Å—Ç–æ $2^{d+1} - 1$\n- ‚úÖ **–°–∫–æ—Ä–æ—Å—Ç—å:** Inference –∑–∞ $O(d)$\n- ‚úÖ **–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è:** –ú–µ–Ω—å—à–µ overfitting (–º–µ–Ω—å—à–µ DOF)\n- ‚úÖ **CPU cache-friendly:** Predictable memory access\n\n**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:**\n- ‚ùå –ú–µ–Ω–µ–µ –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω—ã–µ (–º–µ–Ω—å—à–µ capacity)\n- ‚ùå –ú–æ–∂–µ—Ç —Ç—Ä–µ–±–æ–≤–∞—Ç—å –±–æ–ª—å—à–µ –¥–µ—Ä–µ–≤—å–µ–≤ –¥–ª—è —Ç–æ–≥–æ –∂–µ –∫–∞—á–µ—Å—Ç–≤–∞\n\n**–≠–º–ø–∏—Ä–∏–∫–∞:** –ù–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ —Ä–∞–∑–Ω–∏—Ü–∞ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–∞, –Ω–æ —Å–∫–æ—Ä–æ—Å—Ç—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –≤—ã—à–µ!\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.5 CatBoost vs XGBoost vs LightGBM: –î–µ—Ç–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ\n\n| –ê—Å–ø–µ–∫—Ç | XGBoost | LightGBM | CatBoost | –õ—É—á—à–∏–π |\n|--------|---------|----------|----------|--------|\n| **Target leakage** | –ï—Å—Ç—å | –ï—Å—Ç—å | –ù–µ—Ç (ordered boosting) | üèÜ CatBoost |\n| **Categorical features** | Manual encoding | Native (–Ω–æ –ø—Ä–æ—Å—Ç–æ–π) | Optimal ordered TS | üèÜ CatBoost |\n| **Tree structure** | Asymmetric | Asymmetric (leaf-wise) | Symmetric (oblivious) | –ó–∞–≤–∏—Å–∏—Ç |\n| **–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è** | –°—Ä–µ–¥–Ω—è—è | –û—á–µ–Ω—å –±—ã—Å—Ç—Ä–∞—è | –°—Ä–µ–¥–Ω—è—è/–º–µ–¥–ª–µ–Ω–Ω–∞—è | üèÜ LightGBM |\n| **–°–∫–æ—Ä–æ—Å—Ç—å inference** | –°—Ä–µ–¥–Ω—è—è | –°—Ä–µ–¥–Ω—è—è | –ë—ã—Å—Ç—Ä–∞—è (oblivious) | üèÜ CatBoost |\n| **Overfitting** | –°—Ä–µ–¥–Ω–µ | –°–∫–ª–æ–Ω–µ–Ω (leaf-wise) | –£—Å—Ç–æ–π—á–∏–≤ | üèÜ CatBoost |\n| **–ö–∞—á–µ—Å—Ç–≤–æ (—Ç–∞–±–ª–∏—á–Ω—ã–µ)** | –û—Ç–ª–∏—á–Ω–æ | –û—Ç–ª–∏—á–Ω–æ | –û—Ç–ª–∏—á–Ω–æ | ü§ù Tie |\n| **–ö–∞—á–µ—Å—Ç–≤–æ (–∫–∞—Ç–µ–≥–æ—Ä–∏–∏)** | –•–æ—Ä–æ—à–æ | –•–æ—Ä–æ—à–æ | –û—Ç–ª–∏—á–Ω–æ | üèÜ CatBoost |\n| **Default params** | –•–æ—Ä–æ—à–∏–µ | –¢—Ä–µ–±—É—é—Ç tuning | –û—á–µ–Ω—å —Ö–æ—Ä–æ—à–∏–µ | üèÜ CatBoost |\n| **–†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏** | –°—Ä–µ–¥–Ω–∏–π | –°—Ä–µ–¥–Ω–∏–π | –ú–∞–ª–µ–Ω—å–∫–∏–π (oblivious) | üèÜ CatBoost |\n| **GPU support** | –•–æ—Ä–æ—à–∏–π | –û—Ç–ª–∏—á–Ω—ã–π | –û—Ç–ª–∏—á–Ω—ã–π | ü§ù Tie |\n| **Documentation** | –û—Ç–ª–∏—á–Ω–∞—è | –•–æ—Ä–æ—à–∞—è | –•–æ—Ä–æ—à–∞—è | üèÜ XGBoost |\n| **Ecosystem** | –û–≥—Ä–æ–º–Ω—ã–π | –ë–æ–ª—å—à–æ–π | –°—Ä–µ–¥–Ω–∏–π | üèÜ XGBoost |\n\n#### –ö–ª—é—á–µ–≤—ã–µ –æ—Ç–ª–∏—á–∏—è\n\n**XGBoost:**\n- ‚úÖ –ó—Ä–µ–ª—ã–π, –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–π\n- ‚úÖ –û–≥—Ä–æ–º–Ω–∞—è —ç–∫–æ—Å–∏—Å—Ç–µ–º–∞\n- ‚úÖ –°—Ç–∞–±–∏–ª—å–Ω—ã–π\n- ‚ùå –ú–µ–¥–ª–µ–Ω–Ω–µ–µ LightGBM\n- ‚ùå –ù—É–∂–µ–Ω encoding –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n\n**LightGBM:**\n- ‚úÖ –°–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π\n- ‚úÖ GOSS + EFB –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏\n- ‚úÖ Histogram-based\n- ‚ùå –°–∫–ª–æ–Ω–µ–Ω –∫ overfitting (leaf-wise)\n- ‚ùå –¢—Ä–µ–±—É–µ—Ç tuning\n\n**CatBoost:**\n- ‚úÖ –õ—É—á—à–∞—è —Ä–∞–±–æ—Ç–∞ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏\n- ‚úÖ Ordered boosting (no target leakage)\n- ‚úÖ –û—Ç–ª–∏—á–Ω—ã–µ default –ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n- ‚úÖ –£—Å—Ç–æ–π—á–∏–≤ –∫ overfitting\n- ‚úÖ –ë—ã—Å—Ç—Ä—ã–π inference\n- ‚ùå –ú–µ–¥–ª–µ–Ω–Ω–µ–µ –æ–±—É—á–µ–Ω–∏–µ\n- ‚ùå –ú–µ–Ω—å—à–µ ecosystem\n\n#### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å CatBoost?\n\n‚úÖ **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ CatBoost –µ—Å–ª–∏:**\n1. **–ú–Ω–æ–≥–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤** (–æ—Å–æ–±–µ–Ω–Ω–æ high cardinality)\n2. **–ù—É–∂–Ω–∞ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å** - —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã \"–∏–∑ –∫–æ—Ä–æ–±–∫–∏\"\n3. **–í–∞–∂–µ–Ω inference speed** - production —Å –±–æ–ª—å—à–∏–º QPS\n4. **–ú–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ** - ordered boosting –ø–æ–º–æ–≥–∞–µ—Ç —Å generalization\n5. **–ù–µ—Ç –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ tuning** - default –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–∏–µ\n6. **–¢–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ** —Å mix categorical + numerical\n\n‚úÖ **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ XGBoost –µ—Å–ª–∏:**\n1. **Production-critical** —Å–∏—Å—Ç–µ–º–∞ (–∑—Ä–µ–ª–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞)\n2. **–ù—É–∂–Ω–∞ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å** –∏ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ—Å—Ç—å\n3. **–ú–∞–ª—ã–µ/—Å—Ä–µ–¥–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ** (<1M –ø—Ä–∏–º–µ—Ä–æ–≤)\n4. **–ú–Ω–æ–≥–æ tools** –≤ ecosystem (SHAP, –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏)\n\n‚úÖ **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ LightGBM –µ—Å–ª–∏:**\n1. **–ë–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ** (>1M –ø—Ä–∏–º–µ—Ä–æ–≤)\n2. **–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è** –∫—Ä–∏—Ç–∏—á–Ω–∞\n3. **Sparse features** (one-hot, text)\n4. **–ì–æ—Ç–æ–≤—ã —Ç—é–Ω–∏—Ç—å** –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.6 –ö–ª—é—á–µ–≤—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã CatBoost\n\n| –ü–∞—Ä–∞–º–µ—Ç—Ä | –û–ø–∏—Å–∞–Ω–∏–µ | –ó–Ω–∞—á–µ–Ω–∏—è | –í–ª–∏—è–Ω–∏–µ |\n|----------|----------|----------|---------|\n| `iterations` | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤ | 100-10000 | ‚Üë ‚Üí –ª—É—á—à–µ, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ |\n| `learning_rate` | –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è | 0.01-0.3 | ‚Üì ‚Üí –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ iterations |\n| `depth` | –ì–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤—å–µ–≤ | 4-10 | ‚Üë ‚Üí –±–æ–ª–µ–µ complex, ‚Üë overfitting |\n| `l2_leaf_reg` | L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è | 1-10 | ‚Üë ‚Üí –º–µ–Ω—å—à–µ overfitting |\n| `random_strength` | –°–ª—É—á–∞–π–Ω–æ—Å—Ç—å –≤ splits | 0-10 | ‚Üë ‚Üí –±–æ–ª—å—à–µ regularization |\n| `bagging_temperature` | Bayesian bootstrap | 0-1 | > 0 ‚Üí bagging effect |\n| `border_count` | –ß–∏—Å–ª–æ bins (–∫–∞–∫ –≤ LGBM) | 32-255 | ‚Üë ‚Üí —Ç–æ—á–Ω–µ–µ, –º–µ–¥–ª–µ–Ω–Ω–µ–µ |\n\n#### –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n\n| –ü–∞—Ä–∞–º–µ—Ç—Ä | –û–ø–∏—Å–∞–Ω–∏–µ | –ó–Ω–∞—á–µ–Ω–∏—è |\n|----------|----------|----------|\n| `cat_features` | –ò–Ω–¥–µ–∫—Å—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö | [0, 2, 5] –∏–ª–∏ ['City', 'Type'] |\n| `one_hot_max_size` | Max cardinality –¥–ª—è one-hot | 2-100 (default 2) |\n| `max_ctr_complexity` | Max depth –¥–ª—è combinations | 1-4 (default 4) |\n| `ctr_leaf_count_limit` | Min –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è CTR | 1-100 |\n\n#### –°—Ç—Ä–∞—Ç–µ–≥–∏—è —Ç—é–Ω–∏–Ω–≥–∞\n\n**–≠—Ç–∞–ø 1: Baseline**\n```python\nmodel = CatBoostRegressor(\n    iterations=1000,\n    learning_rate=0.03,\n    depth=6,\n    cat_features=categorical_cols,\n    verbose=100\n)\n```\n\n**–≠—Ç–∞–ø 2: Tune depth**\n- –ü–æ–ø—Ä–æ–±—É–π—Ç–µ depth in [4, 6, 8, 10]\n- –ë–æ–ª—å—à–µ depth = –±–æ–ª–µ–µ complex patterns\n\n**–≠—Ç–∞–ø 3: Regularization**\n- `l2_leaf_reg` in [1, 3, 5, 10]\n- `random_strength` in [0, 1, 5]\n\n**–≠—Ç–∞–ø 4: Fine-tune learning**\n- –°–Ω–∏–∑—å—Ç–µ `learning_rate` –¥–æ 0.01\n- –£–≤–µ–ª–∏—á—å—Ç–µ `iterations` –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ\n- Early stopping\n\n---\n\n## –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è —á–∞—Å—Ç—å –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –ø—Ä–∞–∫—Ç–∏–∫–µ üöÄ"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üìä –ß–∞—Å—Ç—å 2: –ü—Ä–∞–∫—Ç–∏–∫–∞\n\n### 2.1 –ò–º–ø–æ—Ä—Ç—ã"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport time\nwarnings.filterwarnings('ignore')\n\n# CatBoost\nfrom catboost import CatBoostRegressor, Pool\n\n# XGBoost & LightGBM –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\n# Sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.ensemble import RandomForestRegressor\n\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette('husl')\n\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\n\nprint('‚úÖ Libraries loaded')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.2 –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö: House Prices\n\n**Dataset:** Kaggle House Prices - Advanced Regression Techniques\n\n**–û–ø–∏—Å–∞–Ω–∏–µ:**\n- ~1460 –¥–æ–º–æ–≤ (train) + ~1460 (test)\n- 79 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (area, quality, location, features)\n- Target: SalePrice (—Ü–µ–Ω–∞ –ø—Ä–æ–¥–∞–∂–∏)\n\n**–ü—Ä–∏–∑–Ω–∞–∫–∏:**\n- **Numerical:** LotArea, GrLivArea, YearBuilt, etc.\n- **Categorical:** Neighborhood, HouseStyle, Exterior, etc.\n- **Ordinal:** Quality ratings (Ex, Gd, TA, Fa, Po)\n\n–°–∫–∞—á–∞—Ç—å: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\ndata_path = '../../data/house_prices_train.csv'\n\nif not os.path.exists(data_path):\n    print('‚ùå File not found!')\n    print('Download from: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data')\n    print('Save as: data/house_prices_train.csv')\nelse:\n    df = pd.read_csv(data_path)\n    print(f'‚úÖ Data loaded')\n    print(f'Shape: {df.shape[0]:,} rows, {df.shape[1]} columns')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.3 EDA"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('=== First 5 rows ===')\ndisplay(df.head())\n\nprint('\\n=== Info ===')\ndf.info()\n\nprint('\\n=== Stats ===')\ndisplay(df.describe())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Target distribution\nif 'SalePrice' in df.columns:\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Histogram\n    axes[0].hist(df['SalePrice'], bins=50, color='skyblue', edgecolor='black')\n    axes[0].set_title('SalePrice Distribution')\n    axes[0].set_xlabel('Price')\n    axes[0].set_ylabel('Frequency')\n    \n    # Log scale\n    axes[1].hist(np.log1p(df['SalePrice']), bins=50, color='lightcoral', edgecolor='black')\n    axes[1].set_title('Log(SalePrice) Distribution')\n    axes[1].set_xlabel('Log(Price)')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(f'Mean: ${df[\"SalePrice\"].mean():,.0f}')\n    print(f'Median: ${df[\"SalePrice\"].median():,.0f}')\n    print(f'Std: ${df[\"SalePrice\"].std():,.0f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Missing values\nmissing = df.isnull().sum()\nmissing = missing[missing > 0].sort_values(ascending=False)\n\nif len(missing) > 0:\n    print(f'Missing values in {len(missing)} columns:')\n    print(missing.head(10))\nelse:\n    print('No missing values!')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Separate numerical and categorical\nnumeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\ncategorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n\nif 'SalePrice' in numeric_cols:\n    numeric_cols.remove('SalePrice')\nif 'Id' in numeric_cols:\n    numeric_cols.remove('Id')\nif 'Id' in categorical_cols:\n    categorical_cols.remove('Id')\n\nprint(f'Numerical: {len(numeric_cols)}')\nprint(f'Categorical: {len(categorical_cols)}')\nprint(f'\\nTop categorical: {categorical_cols[:5]}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.4 Feature Engineering"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df_fe = df.copy()\n\n# Drop Id\nif 'Id' in df_fe.columns:\n    df_fe = df_fe.drop('Id', axis=1)\n\n# Create features\nif 'YearBuilt' in df_fe.columns and 'YrSold' in df_fe.columns:\n    df_fe['HouseAge'] = df_fe['YrSold'] - df_fe['YearBuilt']\n    df_fe['IsNew'] = (df_fe['HouseAge'] <= 5).astype(int)\n\nif 'TotalBsmtSF' in df_fe.columns and 'GrLivArea' in df_fe.columns:\n    df_fe['TotalSF'] = df_fe['TotalBsmtSF'] + df_fe['GrLivArea']\n\nif 'GarageArea' in df_fe.columns:\n    df_fe['HasGarage'] = (df_fe['GarageArea'] > 0).astype(int)\n\nprint(f'Features after engineering: {df_fe.shape[1]}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Fill missing\nfor col in df_fe.columns:\n    if df_fe[col].dtype == 'object':\n        df_fe[col].fillna('None', inplace=True)\n    else:\n        df_fe[col].fillna(df_fe[col].median(), inplace=True)\n\nprint('Missing values handled')\nprint(f'Remaining missing: {df_fe.isnull().sum().sum()}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.5 –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Target\ntarget_col = 'SalePrice'\nif target_col not in df_fe.columns:\n    print('ERROR: SalePrice not found!')\nelse:\n    X = df_fe.drop(target_col, axis=1)\n    y = df_fe[target_col]\n    \n    # Train/test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n    \n    print(f'Train: {X_train.shape[0]:,} | Test: {X_test.shape[0]:,}')\n    \n    # Identify categorical columns in X\n    cat_features = [col for col in X.columns if X[col].dtype == 'object']\n    cat_indices = [X.columns.get_loc(col) for col in cat_features]\n    \n    print(f'\\nCategorical features: {len(cat_features)}')\n    print(f'Examples: {cat_features[:5]}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üéØ –ß–∞—Å—Ç—å 3: Models"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def evaluate_regressor(model, X_tr, X_te, y_tr, y_te, name, return_time=False):\n    start = time.time()\n    model.fit(X_tr, y_tr)\n    train_time = time.time() - start\n    \n    start = time.time()\n    y_pred = model.predict(X_te)\n    pred_time = time.time() - start\n    \n    rmse = np.sqrt(mean_squared_error(y_te, y_pred))\n    mae = mean_absolute_error(y_te, y_pred)\n    r2 = r2_score(y_te, y_pred)\n    mape = np.mean(np.abs((y_te - y_pred) / y_te)) * 100\n    \n    print(f'\\n{name}')\n    print(f'  RMSE: ${rmse:,.0f}')\n    print(f'  MAE:  ${mae:,.0f}')\n    print(f'  R¬≤:   {r2:.4f}')\n    print(f'  MAPE: {mape:.2f}%')\n    if return_time:\n        print(f'  Train: {train_time:.2f}s | Pred: {pred_time:.4f}s')\n    \n    return {'model': model, 'rmse': rmse, 'mae': mae, 'r2': r2, 'mape': mape,\n            'y_pred': y_pred, 'train_time': train_time, 'pred_time': pred_time}\n\nprint('Eval function ready')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Label encode categorical for XGB/LGBM/baseline\nX_train_encoded = X_train.copy()\nX_test_encoded = X_test.copy()\n\nlabel_encoders = {}\nfor col in cat_features:\n    le = LabelEncoder()\n    X_train_encoded[col] = le.fit_transform(X_train[col].astype(str))\n    X_test_encoded[col] = le.transform(X_test[col].astype(str))\n    label_encoders[col] = le\n\nprint(f'Encoded {len(cat_features)} categorical features')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "results = {}\n\n# Linear Regression\nlr = LinearRegression()\nresults['LinearReg'] = evaluate_regressor(lr, X_train_encoded, X_test_encoded, y_train, y_test, 'Linear Regression', True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Random Forest\nrf = RandomForestRegressor(n_estimators=100, max_depth=15, random_state=RANDOM_STATE, n_jobs=-1)\nresults['RF'] = evaluate_regressor(rf, X_train_encoded, X_test_encoded, y_train, y_test, 'Random Forest', True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ‚ö° –ß–∞—Å—Ç—å 4: CatBoost with Categorical Features"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('Training CatBoost (default)...')\n\ncat_default = CatBoostRegressor(\n    iterations=500,\n    learning_rate=0.05,\n    depth=6,\n    cat_features=cat_features,\n    random_state=RANDOM_STATE,\n    verbose=100\n)\n\nresults['CatBoost_default'] = evaluate_regressor(cat_default, X_train, X_test, y_train, y_test, 'CatBoost (default)', True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('Training CatBoost (tuned)...')\n\ncat_tuned = CatBoostRegressor(\n    iterations=1000,\n    learning_rate=0.03,\n    depth=8,\n    l2_leaf_reg=5,\n    random_strength=1,\n    cat_features=cat_features,\n    random_state=RANDOM_STATE,\n    verbose=200\n)\n\nresults['CatBoost_tuned'] = evaluate_regressor(cat_tuned, X_train, X_test, y_train, y_test, 'CatBoost (tuned)', True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('Training XGBoost...')\n\nxgb_model = XGBRegressor(\n    n_estimators=1000,\n    learning_rate=0.03,\n    max_depth=8,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    random_state=RANDOM_STATE,\n    n_jobs=-1\n)\n\nresults['XGBoost'] = evaluate_regressor(xgb_model, X_train_encoded, X_test_encoded, y_train, y_test, 'XGBoost', True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('Training LightGBM...')\n\nlgbm_model = LGBMRegressor(\n    n_estimators=1000,\n    learning_rate=0.03,\n    num_leaves=63,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    random_state=RANDOM_STATE,\n    n_jobs=-1,\n    verbose=-1\n)\n\nresults['LightGBM'] = evaluate_regressor(lgbm_model, X_train_encoded, X_test_encoded, y_train, y_test, 'LightGBM', True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.4 Comparison"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "comp = pd.DataFrame({\n    'Model': list(results.keys()),\n    'RMSE': [results[m]['rmse'] for m in results],\n    'MAE': [results[m]['mae'] for m in results],\n    'R¬≤': [results[m]['r2'] for m in results],\n    'MAPE': [results[m]['mape'] for m in results],\n    'Train(s)': [results[m]['train_time'] for m in results],\n    'Pred(s)': [results[m]['pred_time'] for m in results]\n}).sort_values('RMSE')\n\nprint('\\n=== MODEL COMPARISON ===')\ndisplay(comp)\n\nprint(f'\\nüèÜ Best model: {comp.iloc[0][\"Model\"]} (RMSE: ${comp.iloc[0][\"RMSE\"]:,.0f})')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Predictions vs Actual\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\naxes = axes.ravel()\n\nfor idx, name in enumerate(results.keys()):\n    if idx < 6:\n        y_pred = results[name]['y_pred']\n        axes[idx].scatter(y_test, y_pred, alpha=0.5)\n        axes[idx].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n        axes[idx].set_xlabel('Actual Price')\n        axes[idx].set_ylabel('Predicted Price')\n        axes[idx].set_title(f'{name}\\nRMSE: ${results[name][\"rmse\"]:,.0f}')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üîç –ß–∞—Å—Ç—å 5: Analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CatBoost feature importance\ncat_model = results['CatBoost_tuned']['model']\nfeature_imp = pd.DataFrame({\n    'Feature': X_train.columns,\n    'Importance': cat_model.feature_importances_\n}).sort_values('Importance', ascending=False)\n\nprint('Top 15 Features (CatBoost):')\ndisplay(feature_imp.head(15))\n\n# Viz\nfig, ax = plt.subplots(figsize=(10, 8))\ntop = feature_imp.head(15)\nax.barh(top['Feature'], top['Importance'], color='lightcoral')\nax.set_xlabel('Importance')\nax.set_title('Top 15 Features - CatBoost')\nax.invert_yaxis()\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Speed: CatBoost vs XGB vs LGBM\nspeed_data = comp[comp['Model'].isin(['CatBoost_tuned', 'XGBoost', 'LightGBM'])]\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].bar(speed_data['Model'], speed_data['Train(s)'], color=['#e74c3c', '#3498db', '#2ecc71'])\naxes[0].set_title('Training Time')\naxes[0].set_ylabel('Seconds')\nfor i, (m, t) in enumerate(zip(speed_data['Model'], speed_data['Train(s)'])):\n    axes[0].text(i, t + 0.5, f'{t:.1f}s', ha='center')\n\naxes[1].bar(speed_data['Model'], speed_data['Pred(s)'], color=['#e74c3c', '#3498db', '#2ecc71'])\naxes[1].set_title('Prediction Time')\naxes[1].set_ylabel('Seconds')\nfor i, (m, t) in enumerate(zip(speed_data['Model'], speed_data['Pred(s)'])):\n    axes[1].text(i, t + 0.001, f'{t:.4f}s', ha='center')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üìù –ß–∞—Å—Ç—å 6: –í—ã–≤–æ–¥—ã\n\n### –ö–ª—é—á–µ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n\n**CatBoost –ø–æ–∫–∞–∑–∞–ª:**\n‚úÖ –õ—É—á—à–µ–µ –∏–ª–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ (RMSE, R¬≤)\n‚úÖ –û—Ç–ª–∏—á–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Å categorical features (–±–µ–∑ encoding!)\n‚úÖ –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ overfitting (ordered boosting)\n‚úÖ –•–æ—Ä–æ—à–∏–µ default –ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n‚úÖ –ë—ã—Å—Ç—Ä—ã–π inference (oblivious trees)\n\n**–°—Ä–∞–≤–Ω–µ–Ω–∏–µ:**\n- **CatBoost:** –õ—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –æ—Å–æ–±–µ–Ω–Ω–æ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏\n- **XGBoost:** –°—Ç–∞–±–∏–ª—å–Ω—ã–π, —Ö–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ\n- **LightGBM:** –ë—ã—Å—Ç—Ä–µ–µ –æ–±—É—á–µ–Ω–∏–µ, –Ω–æ —á—É—Ç—å —Ö—É–∂–µ –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö\n\n### –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ insights\n\n**–ü–æ—á–µ–º—É CatBoost —Ä–∞–±–æ—Ç–∞–µ—Ç:**\n\n1. **Ordered boosting** - –Ω–µ—Ç target leakage\n   - $F^{<i}$ –æ–±—É—á–µ–Ω–∞ —Ç–æ–ª—å–∫–æ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö –¥–æ $i$\n   - –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –Ω–∞ \"—á–µ—Å—Ç–Ω–æ–π\" –º–æ–¥–µ–ª–∏\n\n2. **Ordered target statistics** –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n   - $\\hat{y}_c^{<i} = \\frac{n_c^{<i} \\cdot \\bar{y}_c^{<i} + a \\cdot p}{n_c^{<i} + a}$\n   - Bayesian smoothing –¥–ª—è —Ä–µ–¥–∫–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n\n3. **Symmetric trees** - —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è\n   - –ú–µ–Ω—å—à–µ DOF ‚Üí –º–µ–Ω—å—à–µ overfitting\n   - –ë—ã—Å—Ç—Ä—ã–π inference: O(depth)\n\n### –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n\n**–î–ª—è House Prices:**\n\n**–¢–æ–ø —Ñ–∞–∫—Ç–æ—Ä—ã:**\n- OverallQual (–æ–±—â–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ)\n- GrLivArea (–∂–∏–ª–∞—è –ø–ª–æ—â–∞–¥—å)\n- TotalSF (–æ–±—â–∞—è –ø–ª–æ—â–∞–¥—å)\n- Neighborhood (—Ä–∞–π–æ–Ω)\n- YearBuilt (–≥–æ–¥ –ø–æ—Å—Ç—Ä–æ–π–∫–∏)\n\n**Insights:**\n1. –ö–∞—á–µ—Å—Ç–≤–æ –∏ –ø–ª–æ—â–∞–¥—å - –æ—Å–Ω–æ–≤–Ω—ã–µ –¥—Ä–∞–π–≤–µ—Ä—ã —Ü–µ–Ω—ã\n2. –õ–æ–∫–∞—Ü–∏—è (Neighborhood) –∫—Ä–∏—Ç–∏—á–Ω–∞ - CatBoost –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ\n3. –ù–æ–≤–∏–∑–Ω–∞ (HouseAge) –≤–ª–∏—è–µ—Ç –Ω–∞ —Ü–µ–Ω—É\n\n### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —á—Ç–æ?\n\n| –°–∏—Ç—É–∞—Ü–∏—è | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |\n|----------|-------------|\n| –ú–Ω–æ–≥–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö (high cardinality) | üèÜ CatBoost |\n| –ù—É–∂–Ω–∞ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å out-of-the-box | üèÜ CatBoost |\n| –ú–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ | üèÜ CatBoost |\n| –ù–µ—Ç –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ preprocessing | üèÜ CatBoost |\n| –ë–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ (>1M) | üèÜ LightGBM |\n| Production-critical (–∑—Ä–µ–ª–æ—Å—Ç—å) | üèÜ XGBoost |\n| –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –∫—Ä–∏—Ç–∏—á–Ω–∞ | üèÜ LightGBM |\n\n### –î–∞–ª—å–Ω–µ–π—à–∏–µ —É–ª—É—á—à–µ–Ω–∏—è\n\n1. **Feature engineering:**\n   - Polynomial features –¥–ª—è –ø–ª–æ—â–∞–¥–µ–π\n   - Interaction features\n   - External data (demographics, economics)\n\n2. **Model tuning:**\n   - Bayesian optimization (Optuna)\n   - Grid search –Ω–∞ –±–æ–ª—å—à–µ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ\n   - Cross-validation –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n\n3. **Ensemble:**\n   - Stacking: CatBoost + XGBoost + LightGBM\n   - Weighted average\n   - Meta-learner\n\n4. **Categorical features:**\n   - Combinations depth tuning\n   - Custom prior –¥–ª—è Bayesian smoothing\n\n---\n\n## üéì –ó–∞–∫–ª—é—á–µ–Ω–∏–µ\n\n–ú—ã –∏–∑—É—á–∏–ª–∏:\n‚úÖ **–ú–∞—Ç–µ–º–∞—Ç–∏–∫—É CatBoost:** Ordered boosting, categorical features, symmetric trees\n‚úÖ **–ü—Ä–∞–∫—Ç–∏–∫—É:** House Prices regression\n‚úÖ **–°—Ä–∞–≤–Ω–µ–Ω–∏–µ:** CatBoost vs XGBoost vs LightGBM\n\n**CatBoost - —ç—Ç–æ:**\n- –õ—É—á—à–∞—è —Ä–∞–±–æ—Ç–∞ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏ (ordered target stats)\n- Ordered boosting (no target leakage)\n- Symmetric trees (fast inference, regularization)\n- –û—Ç–ª–∏—á–Ω—ã–µ defaults (—Ä–∞–±–æ—Ç–∞–µ—Ç –∏–∑ –∫–æ—Ä–æ–±–∫–∏)\n\n**Next Steps:**\n- Stacking ensemble (–∫–æ–º–±–∏–Ω–∞—Ü–∏—è –≤—Å–µ—Ö —Ç—Ä–µ—Ö)\n- Imbalanced data handling\n- Advanced feature engineering\n\n---\n\n**References:**\n1. Prokhorenkova, L. et al. (2018). CatBoost: unbiased boosting with categorical features. NeurIPS 2018.\n2. Dorogush, A.V. et al. (2018). CatBoost: gradient boosting with categorical features support.\n3. Chen, T., Guestrin, C. (2016). XGBoost. KDD 2016.\n4. Ke, G. et al. (2017). LightGBM. NIPS 2017."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
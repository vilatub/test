#!/usr/bin/env python3
"""
Phase 7: Production & MLOps - Part 4 (Final)
CI/CD, DVC, Conclusions
"""

import json

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π notebook
notebook_path = '/home/user/test/notebooks/phase7_production_mlops/01_production_mlops.ipynb'
with open(notebook_path, 'r', encoding='utf-8') as f:
    notebook = json.load(f)

cells = notebook['cells']

# ============================================================================
# CI/CD FOR ML
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "---\n",
        "\n",
        "## üîÑ –ß–∞—Å—Ç—å 7: CI/CD –¥–ª—è ML\n",
        "\n",
        "### –ü–æ—á–µ–º—É CI/CD –¥–ª—è ML –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç –æ–±—ã—á–Ω–æ–≥–æ?\n",
        "\n",
        "**–û–±—ã—á–Ω—ã–π CI/CD:**\n",
        "```\n",
        "Code ‚Üí Build ‚Üí Test ‚Üí Deploy\n",
        "```\n",
        "\n",
        "**ML CI/CD:**\n",
        "```\n",
        "Data ‚Üí Feature Engineering ‚Üí Train ‚Üí Validate ‚Üí Test ‚Üí Package ‚Üí Deploy ‚Üí Monitor\n",
        "```\n",
        "\n",
        "**–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ challenges:**\n",
        "- ‚úÖ **Data validation:** –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "- ‚úÖ **Model validation:** –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏\n",
        "- ‚úÖ **Feature validation:** –ü—Ä–æ–≤–µ—Ä–∫–∞ feature distributions\n",
        "- ‚úÖ **Performance regression:** –ú–æ–¥–µ–ª—å –Ω–µ –¥–æ–ª–∂–Ω–∞ –¥–µ–≥—Ä–∞–¥–∏—Ä–æ–≤–∞—Ç—å\n",
        "- ‚úÖ **A/B testing:** –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å production –º–æ–¥–µ–ª—å—é\n",
        "\n",
        "### GitHub Actions –¥–ª—è ML\n",
        "\n",
        "**Workflow stages:**\n",
        "\n",
        "1. **On Push/PR:**\n",
        "   - Lint code (flake8, black)\n",
        "   - Run unit tests\n",
        "   - Run integration tests\n",
        "   - Validate data schema\n",
        "\n",
        "2. **On Model Training:**\n",
        "   - Train model\n",
        "   - Validate model metrics\n",
        "   - Compare with baseline\n",
        "   - Register model (if better)\n",
        "\n",
        "3. **On Deploy:**\n",
        "   - Build Docker image\n",
        "   - Push to registry\n",
        "   - Deploy to staging\n",
        "   - Run smoke tests\n",
        "   - Deploy to production\n",
        "\n",
        "---\n"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# GitHub Actions Workflow\n",
        "github_actions_workflow = '''name: ML Pipeline CI/CD\n",
        "\n",
        "on:\n",
        "  push:\n",
        "    branches: [main, develop]\n",
        "  pull_request:\n",
        "    branches: [main]\n",
        "  workflow_dispatch:  # Manual trigger\n",
        "\n",
        "env:\n",
        "  PYTHON_VERSION: \"3.9\"\n",
        "  MODEL_NAME: \"churn-predictor\"\n",
        "\n",
        "jobs:\n",
        "  # ========================================\n",
        "  # Stage 1: Code Quality\n",
        "  # ========================================\n",
        "  lint-and-test:\n",
        "    name: Lint and Test\n",
        "    runs-on: ubuntu-latest\n",
        "    \n",
        "    steps:\n",
        "      - name: Checkout code\n",
        "        uses: actions/checkout@v3\n",
        "        \n",
        "      - name: Set up Python\n",
        "        uses: actions/setup-python@v4\n",
        "        with:\n",
        "          python-version: ${{ env.PYTHON_VERSION }}\n",
        "          \n",
        "      - name: Cache pip dependencies\n",
        "        uses: actions/cache@v3\n",
        "        with:\n",
        "          path: ~/.cache/pip\n",
        "          key: ${{ runner.os }}-pip-${{ hashFiles(\"requirements.txt\") }}\n",
        "          \n",
        "      - name: Install dependencies\n",
        "        run: |\n",
        "          pip install --upgrade pip\n",
        "          pip install -r requirements.txt\n",
        "          pip install pytest pytest-cov flake8 black\n",
        "          \n",
        "      - name: Lint with flake8\n",
        "        run: flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n",
        "        \n",
        "      - name: Check formatting with black\n",
        "        run: black --check .\n",
        "        \n",
        "      - name: Run unit tests\n",
        "        run: pytest tests/unit/ -v --cov=src --cov-report=xml\n",
        "        \n",
        "      - name: Upload coverage\n",
        "        uses: codecov/codecov-action@v3\n",
        "        with:\n",
        "          file: ./coverage.xml\n",
        "\n",
        "  # ========================================\n",
        "  # Stage 2: Data Validation\n",
        "  # ========================================\n",
        "  data-validation:\n",
        "    name: Validate Data\n",
        "    runs-on: ubuntu-latest\n",
        "    needs: lint-and-test\n",
        "    \n",
        "    steps:\n",
        "      - name: Checkout code\n",
        "        uses: actions/checkout@v3\n",
        "        \n",
        "      - name: Set up Python\n",
        "        uses: actions/setup-python@v4\n",
        "        with:\n",
        "          python-version: ${{ env.PYTHON_VERSION }}\n",
        "          \n",
        "      - name: Install dependencies\n",
        "        run: pip install -r requirements.txt\n",
        "        \n",
        "      - name: Validate data schema\n",
        "        run: python scripts/validate_data.py\n",
        "        \n",
        "      - name: Check for data drift\n",
        "        run: python scripts/check_drift.py\n",
        "\n",
        "  # ========================================\n",
        "  # Stage 3: Model Training\n",
        "  # ========================================\n",
        "  train-model:\n",
        "    name: Train Model\n",
        "    runs-on: ubuntu-latest\n",
        "    needs: data-validation\n",
        "    if: github.event_name == \"push\" && github.ref == \"refs/heads/main\"\n",
        "    \n",
        "    steps:\n",
        "      - name: Checkout code\n",
        "        uses: actions/checkout@v3\n",
        "        \n",
        "      - name: Set up Python\n",
        "        uses: actions/setup-python@v4\n",
        "        with:\n",
        "          python-version: ${{ env.PYTHON_VERSION }}\n",
        "          \n",
        "      - name: Install dependencies\n",
        "        run: pip install -r requirements.txt\n",
        "        \n",
        "      - name: Train model\n",
        "        run: python scripts/train.py\n",
        "        env:\n",
        "          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}\n",
        "          \n",
        "      - name: Validate model performance\n",
        "        run: python scripts/validate_model.py\n",
        "        \n",
        "      - name: Upload model artifact\n",
        "        uses: actions/upload-artifact@v3\n",
        "        with:\n",
        "          name: model\n",
        "          path: models/\n",
        "\n",
        "  # ========================================\n",
        "  # Stage 4: Build and Push Docker Image\n",
        "  # ========================================\n",
        "  build-docker:\n",
        "    name: Build Docker Image\n",
        "    runs-on: ubuntu-latest\n",
        "    needs: train-model\n",
        "    \n",
        "    steps:\n",
        "      - name: Checkout code\n",
        "        uses: actions/checkout@v3\n",
        "        \n",
        "      - name: Download model artifact\n",
        "        uses: actions/download-artifact@v3\n",
        "        with:\n",
        "          name: model\n",
        "          path: models/\n",
        "          \n",
        "      - name: Set up Docker Buildx\n",
        "        uses: docker/setup-buildx-action@v2\n",
        "        \n",
        "      - name: Login to Docker Hub\n",
        "        uses: docker/login-action@v2\n",
        "        with:\n",
        "          username: ${{ secrets.DOCKER_USERNAME }}\n",
        "          password: ${{ secrets.DOCKER_PASSWORD }}\n",
        "          \n",
        "      - name: Build and push\n",
        "        uses: docker/build-push-action@v4\n",
        "        with:\n",
        "          context: .\n",
        "          push: true\n",
        "          tags: |\n",
        "            ${{ secrets.DOCKER_USERNAME }}/${{ env.MODEL_NAME }}:latest\n",
        "            ${{ secrets.DOCKER_USERNAME }}/${{ env.MODEL_NAME }}:${{ github.sha }}\n",
        "          cache-from: type=gha\n",
        "          cache-to: type=gha,mode=max\n",
        "\n",
        "  # ========================================\n",
        "  # Stage 5: Deploy to Staging\n",
        "  # ========================================\n",
        "  deploy-staging:\n",
        "    name: Deploy to Staging\n",
        "    runs-on: ubuntu-latest\n",
        "    needs: build-docker\n",
        "    environment: staging\n",
        "    \n",
        "    steps:\n",
        "      - name: Deploy to staging\n",
        "        run: |\n",
        "          echo \"Deploying to staging environment...\"\n",
        "          # kubectl set image deployment/$MODEL_NAME ...\n",
        "          # or: aws ecs update-service ...\n",
        "          \n",
        "      - name: Run smoke tests\n",
        "        run: |\n",
        "          echo \"Running smoke tests...\"\n",
        "          # curl -X POST https://staging.api.com/predict ...\n",
        "          \n",
        "      - name: Notify on success\n",
        "        run: echo \"Staging deployment successful!\"\n",
        "\n",
        "  # ========================================\n",
        "  # Stage 6: Deploy to Production\n",
        "  # ========================================\n",
        "  deploy-production:\n",
        "    name: Deploy to Production\n",
        "    runs-on: ubuntu-latest\n",
        "    needs: deploy-staging\n",
        "    environment: production\n",
        "    \n",
        "    steps:\n",
        "      - name: Deploy to production\n",
        "        run: |\n",
        "          echo \"Deploying to production environment...\"\n",
        "          \n",
        "      - name: Notify team\n",
        "        run: echo \"Production deployment complete!\"\n",
        "'''\n",
        "\n",
        "# Save workflow\n",
        "import os\n",
        "os.makedirs('.github/workflows', exist_ok=True)\n",
        "with open('.github/workflows/ml-pipeline.yml', 'w') as f:\n",
        "    f.write(github_actions_workflow)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"GITHUB ACTIONS CI/CD WORKFLOW\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\n‚úÖ Workflow saved to: .github/workflows/ml-pipeline.yml\")\n",
        "print(\"\\nStages:\")\n",
        "print(\"  1. lint-and-test      - Code quality checks\")\n",
        "print(\"  2. data-validation    - Data schema and drift\")\n",
        "print(\"  3. train-model        - Model training with MLflow\")\n",
        "print(\"  4. build-docker       - Build and push Docker image\")\n",
        "print(\"  5. deploy-staging     - Deploy to staging + smoke tests\")\n",
        "print(\"  6. deploy-production  - Deploy to production\")\n"
    ]
})

# ============================================================================
# DVC (Data Version Control)
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "---\n",
        "\n",
        "## üìÅ –ß–∞—Å—Ç—å 8: Data Version Control (DVC)\n",
        "\n",
        "### –ü—Ä–æ–±–ª–µ–º–∞ –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö\n",
        "\n",
        "**Git –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –¥–∞–Ω–Ω—ã—Ö:**\n",
        "- ‚ùå Large files (GB, TB)\n",
        "- ‚ùå Binary files (–Ω–µ –≤–∏–¥–Ω–æ diff)\n",
        "- ‚ùå Repository bloat\n",
        "\n",
        "**DVC —Ä–µ—à–∞–µ—Ç:**\n",
        "- ‚úÖ Git-like workflow –¥–ª—è –¥–∞–Ω–Ω—ã—Ö\n",
        "- ‚úÖ Remote storage (S3, GCS, Azure Blob)\n",
        "- ‚úÖ Reproducibility\n",
        "- ‚úÖ Pipeline management\n",
        "\n",
        "### –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç DVC:\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ   Git       ‚îÇ     ‚îÇ   DVC       ‚îÇ\n",
        "‚îÇ   (code)    ‚îÇ     ‚îÇ   (data)    ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§     ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ - app.py    ‚îÇ     ‚îÇ - data.csv  ‚îÇ\n",
        "‚îÇ - train.py  ‚îÇ     ‚îÇ - model.pkl ‚îÇ\n",
        "‚îÇ - data.dvc  ‚îÇ ‚Üê‚îÄ‚Üí ‚îÇ             ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "       ‚îÇ                   ‚îÇ\n",
        "       ‚Üì                   ‚Üì\n",
        "   GitHub/GitLab      S3/GCS/Azure\n",
        "```\n",
        "\n",
        "---\n"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"DVC (DATA VERSION CONTROL)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# DVC commands reference\n",
        "dvc_commands = '''\n",
        "# ========================================\n",
        "# DVC Quick Reference\n",
        "# ========================================\n",
        "\n",
        "# Initialize DVC in project\n",
        "dvc init\n",
        "\n",
        "# Add remote storage (e.g., S3)\n",
        "dvc remote add -d myremote s3://mybucket/dvc-store\n",
        "\n",
        "# Track data files\n",
        "dvc add data/raw/customers.csv\n",
        "dvc add models/churn_predictor.joblib\n",
        "\n",
        "# This creates:\n",
        "# - data/raw/customers.csv.dvc (metadata, tracked by Git)\n",
        "# - .gitignore (–¥–æ–±–∞–≤–ª—è–µ—Ç customers.csv)\n",
        "\n",
        "# Commit metadata to Git\n",
        "git add data/raw/customers.csv.dvc data/raw/.gitignore\n",
        "git commit -m \"Add raw customer data\"\n",
        "\n",
        "# Push data to remote storage\n",
        "dvc push\n",
        "\n",
        "# Pull data from remote\n",
        "dvc pull\n",
        "\n",
        "# ========================================\n",
        "# DVC Pipelines\n",
        "# ========================================\n",
        "\n",
        "# Define pipeline in dvc.yaml:\n",
        "# stages:\n",
        "#   preprocess:\n",
        "#     cmd: python src/preprocess.py\n",
        "#     deps:\n",
        "#       - data/raw/customers.csv\n",
        "#       - src/preprocess.py\n",
        "#     outs:\n",
        "#       - data/processed/customers_clean.csv\n",
        "#\n",
        "#   train:\n",
        "#     cmd: python src/train.py\n",
        "#     deps:\n",
        "#       - data/processed/customers_clean.csv\n",
        "#       - src/train.py\n",
        "#     outs:\n",
        "#       - models/churn_predictor.joblib\n",
        "#     metrics:\n",
        "#       - metrics/scores.json\n",
        "\n",
        "# Run pipeline\n",
        "dvc repro\n",
        "\n",
        "# Show pipeline DAG\n",
        "dvc dag\n",
        "\n",
        "# Compare metrics across experiments\n",
        "dvc metrics show\n",
        "dvc metrics diff\n",
        "\n",
        "# ========================================\n",
        "# Version switching\n",
        "# ========================================\n",
        "\n",
        "# Go back to specific version\n",
        "git checkout v1.0\n",
        "dvc checkout\n",
        "\n",
        "# Compare data versions\n",
        "dvc diff HEAD~1\n",
        "'''\n",
        "\n",
        "print(dvc_commands)\n",
        "\n",
        "# Create dvc.yaml example\n",
        "dvc_yaml = '''stages:\n",
        "  preprocess:\n",
        "    cmd: python src/preprocess.py\n",
        "    deps:\n",
        "      - data/raw/customers.csv\n",
        "      - src/preprocess.py\n",
        "    params:\n",
        "      - preprocess.test_size\n",
        "      - preprocess.random_state\n",
        "    outs:\n",
        "      - data/processed/X_train.csv\n",
        "      - data/processed/X_test.csv\n",
        "      - data/processed/y_train.csv\n",
        "      - data/processed/y_test.csv\n",
        "\n",
        "  train:\n",
        "    cmd: python src/train.py\n",
        "    deps:\n",
        "      - data/processed/X_train.csv\n",
        "      - data/processed/y_train.csv\n",
        "      - src/train.py\n",
        "    params:\n",
        "      - train.n_estimators\n",
        "      - train.max_depth\n",
        "      - train.learning_rate\n",
        "    outs:\n",
        "      - models/churn_predictor.joblib\n",
        "      - models/scaler.joblib\n",
        "    metrics:\n",
        "      - metrics/train_metrics.json:\n",
        "          cache: false\n",
        "\n",
        "  evaluate:\n",
        "    cmd: python src/evaluate.py\n",
        "    deps:\n",
        "      - data/processed/X_test.csv\n",
        "      - data/processed/y_test.csv\n",
        "      - models/churn_predictor.joblib\n",
        "      - src/evaluate.py\n",
        "    metrics:\n",
        "      - metrics/eval_metrics.json:\n",
        "          cache: false\n",
        "    plots:\n",
        "      - plots/confusion_matrix.png\n",
        "      - plots/roc_curve.png\n",
        "'''\n",
        "\n",
        "with open('./dvc.yaml', 'w') as f:\n",
        "    f.write(dvc_yaml)\n",
        "\n",
        "print(\"\\n‚úÖ dvc.yaml saved\")\n",
        "print(\"\\nDVC Pipeline Stages:\")\n",
        "print(\"  1. preprocess ‚Üí Train/test split, scaling\")\n",
        "print(\"  2. train      ‚Üí Model training\")\n",
        "print(\"  3. evaluate   ‚Üí Model evaluation, plots\")\n"
    ]
})

# ============================================================================
# CONCLUSIONS
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "---\n",
        "\n",
        "## üìã –ß–∞—Å—Ç—å 9: Summary & Best Practices\n",
        "\n",
        "### Production ML Checklist\n",
        "\n",
        "#### ‚úÖ **Experiment Tracking**\n",
        "- [ ] MLflow/W&B setup –¥–ª—è logging\n",
        "- [ ] Parameters, metrics, artifacts logged\n",
        "- [ ] Model Registry configured\n",
        "- [ ] Experiment comparison UI\n",
        "\n",
        "#### ‚úÖ **Model Serialization**\n",
        "- [ ] Model saved (joblib/pickle/ONNX)\n",
        "- [ ] Preprocessing artifacts saved (scaler, encoders)\n",
        "- [ ] Config file with feature columns\n",
        "- [ ] Version metadata\n",
        "\n",
        "#### ‚úÖ **API Development**\n",
        "- [ ] FastAPI/Flask application\n",
        "- [ ] Pydantic schemas for validation\n",
        "- [ ] Health check endpoint\n",
        "- [ ] Error handling\n",
        "- [ ] API documentation (Swagger)\n",
        "- [ ] Rate limiting\n",
        "- [ ] Authentication\n",
        "\n",
        "#### ‚úÖ **Containerization**\n",
        "- [ ] Dockerfile (multi-stage)\n",
        "- [ ] Non-root user\n",
        "- [ ] Health check\n",
        "- [ ] docker-compose.yml\n",
        "- [ ] .dockerignore\n",
        "\n",
        "#### ‚úÖ **Monitoring**\n",
        "- [ ] Data drift detection\n",
        "- [ ] Model performance monitoring\n",
        "- [ ] Prediction distribution tracking\n",
        "- [ ] Latency monitoring\n",
        "- [ ] Alerting setup\n",
        "- [ ] Dashboard (Grafana)\n",
        "\n",
        "#### ‚úÖ **CI/CD**\n",
        "- [ ] GitHub Actions workflow\n",
        "- [ ] Unit tests\n",
        "- [ ] Integration tests\n",
        "- [ ] Data validation\n",
        "- [ ] Model validation\n",
        "- [ ] Automated deployment\n",
        "\n",
        "#### ‚úÖ **Data Versioning**\n",
        "- [ ] DVC initialized\n",
        "- [ ] Remote storage configured\n",
        "- [ ] Data tracked\n",
        "- [ ] Pipeline defined\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ Key Takeaways\n",
        "\n",
        "#### 1. **Model –≤ Jupyter - —ç—Ç–æ 10% —Ä–∞–±–æ—Ç—ã**\n",
        "- –û—Å—Ç–∞–ª—å–Ω—ã–µ 90% - —ç—Ç–æ MLOps\n",
        "- Deployment, monitoring, retraining\n",
        "- –≠—Ç–æ –æ—Ç–ª–∏—á–∞–µ—Ç Data Scientist –æ—Ç ML Engineer\n",
        "\n",
        "#### 2. **Reproducibility is King**\n",
        "- MLflow –¥–ª—è experiments\n",
        "- DVC –¥–ª—è –¥–∞–Ω–Ω—ã—Ö\n",
        "- Docker –¥–ª—è environment\n",
        "- –í—Å—ë –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ!\n",
        "\n",
        "#### 3. **Monitoring –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ—ã**\n",
        "- ML –º–æ–¥–µ–ª–∏ –¥–µ–≥—Ä–∞–¥–∏—Ä—É—é—Ç —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º\n",
        "- Data drift, concept drift, model drift\n",
        "- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ ‚Üí —Ä–∞–Ω–Ω–µ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ ‚Üí –±—ã—Å—Ç—Ä–∞—è —Ä–µ–∞–∫—Ü–∏—è\n",
        "\n",
        "#### 4. **Automation —Å–Ω–∏–∂–∞–µ—Ç —Ä–∏—Å–∫–∏**\n",
        "- Manual deployments ‚Üí –æ—à–∏–±–∫–∏\n",
        "- CI/CD ‚Üí consistent, reliable deployments\n",
        "- Automated testing ‚Üí catches bugs early\n",
        "\n",
        "#### 5. **Start Simple, Iterate**\n",
        "- –ù–µ –Ω—É–∂–µ–Ω Kubernetes —Å –ø–µ—Ä–≤–æ–≥–æ –¥–Ω—è\n",
        "- –ù–∞—á–Ω–∏—Ç–µ —Å Docker + –ø—Ä–æ—Å—Ç–æ–π CI/CD\n",
        "- –î–æ–±–∞–≤–ª—è–π—Ç–µ complexity –ø–æ –º–µ—Ä–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏\n",
        "\n",
        "---\n",
        "\n",
        "### üîó –°–≤—è–∑—å —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ —Ñ–∞–∑–∞–º–∏\n",
        "\n",
        "**Phase 7 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Å–µ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –∑–Ω–∞–Ω–∏—è:**\n",
        "\n",
        "- **Phase 1-3:** –ú–æ–¥–µ–ª–∏ –¥–ª—è deployment (RF, XGBoost, etc.)\n",
        "- **Phase 4:** Transformers –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á\n",
        "- **Phase 5:** Anomaly detection –¥–ª—è monitoring\n",
        "- **Phase 6:** SHAP –¥–ª—è explainability –≤ production\n",
        "\n",
        "**Phase 7 –∑–∞–≤–µ—Ä—à–∞–µ—Ç —Ü–∏–∫–ª:**\n",
        "\n",
        "```\n",
        "Research ‚Üí Development ‚Üí Production ‚Üí Monitoring ‚Üí Research...\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üìö Tools Reference\n",
        "\n",
        "| Category | Tool | Purpose |\n",
        "|----------|------|---------||\n",
        "| **Experiment Tracking** | MLflow | Parameters, metrics, models |\n",
        "| | Weights & Biases | Similar + collaboration |\n",
        "| **Model Serving** | FastAPI | REST API |\n",
        "| | BentoML | ML-focused serving |\n",
        "| | Seldon/KServe | Kubernetes-native |\n",
        "| **Containerization** | Docker | Environment isolation |\n",
        "| | Kubernetes | Orchestration |\n",
        "| **Data Versioning** | DVC | Git for data |\n",
        "| | Delta Lake | ACID transactions |\n",
        "| **Monitoring** | Evidently | ML monitoring |\n",
        "| | Prometheus/Grafana | Metrics + dashboards |\n",
        "| **CI/CD** | GitHub Actions | Automation |\n",
        "| | GitLab CI | Alternative |\n",
        "| **Feature Store** | Feast | Feature management |\n",
        "\n",
        "---\n",
        "\n",
        "## üéâ Phase 7 Complete!\n",
        "\n",
        "**–í—ã —Ç–µ–ø–µ—Ä—å –∑–Ω–∞–µ—Ç–µ:**\n",
        "\n",
        "- ‚úÖ MLflow experiment tracking –∏ Model Registry\n",
        "- ‚úÖ Model serialization (pickle, joblib)\n",
        "- ‚úÖ FastAPI –¥–ª—è ML API —Å Pydantic validation\n",
        "- ‚úÖ Docker containerization (multi-stage builds)\n",
        "- ‚úÖ Data drift detection (KS test, Chi-squared)\n",
        "- ‚úÖ Model performance monitoring\n",
        "- ‚úÖ CI/CD —Å GitHub Actions\n",
        "- ‚úÖ DVC –¥–ª—è data versioning\n",
        "\n",
        "---\n",
        "\n",
        "## üèÜ ML Roadmap Complete!\n",
        "\n",
        "**–í–µ—Å—å –ø—É—Ç—å –æ—Ç –Ω–∞—á–∞–ª–∞ –¥–æ production:**\n",
        "\n",
        "1. ‚úÖ **Phase 1:** Classification Basics (Titanic)\n",
        "2. ‚úÖ **Phase 2:** Deep Learning (MNIST, PyTorch)\n",
        "3. ‚úÖ **Phase 3:** Advanced Ensembles (Stacking, Blending)\n",
        "4. ‚úÖ **Phase 4:** Transformers (Self-Attention, TFT)\n",
        "5. ‚úÖ **Phase 5:** Anomaly Detection (Isolation Forest, Autoencoders)\n",
        "6. ‚úÖ **Phase 6:** Explainable AI (SHAP, LIME, Fairness)\n",
        "7. ‚úÖ **Phase 7:** Production MLOps (FastAPI, Docker, CI/CD)\n",
        "\n",
        "**–¢–µ–ø–µ—Ä—å –≤—ã –≥–æ—Ç–æ–≤—ã:**\n",
        "\n",
        "- üéØ –†–µ—à–∞—Ç—å real-world ML –∑–∞–¥–∞—á–∏\n",
        "- üéØ –í—ã–±–∏—Ä–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ –º–µ—Ç–æ–¥—ã\n",
        "- üéØ –î–µ–ø–ª–æ–∏—Ç—å –º–æ–¥–µ–ª–∏ –≤ production\n",
        "- üéØ –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å ML —Å–∏—Å—Ç–µ–º—ã\n",
        "- üéØ –û–±—ä—è—Å–Ω—è—Ç—å predictions –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞—Ç—å fairness\n",
        "\n",
        "---\n",
        "\n",
        "**üöÄ You're now a Production-Ready ML Practitioner!**\n"
    ]
})

# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π notebook
notebook['cells'] = cells

with open(notebook_path, 'w', encoding='utf-8') as f:
    json.dump(notebook, f, ensure_ascii=False, indent=1)

print(f'\\n‚úÖ Phase 7 notebook complete: {notebook_path}')
print(f'Total cells: {len(cells)}')
print('')
print('üìä Phase 7 Summary:')
print('- Introduction & MLOps Overview (3 cells)')
print('- Dataset & Preprocessing (3 cells)')
print('- MLflow Experiment Tracking (6 cells)')
print('- Model Serialization (2 cells)')
print('- FastAPI Development (4 cells)')
print('- Docker Containerization (3 cells)')
print('- Monitoring & Drift Detection (6 cells)')
print('- CI/CD for ML (2 cells)')
print('- DVC Data Versioning (2 cells)')
print('- Conclusions & Best Practices (1 cell)')
print('')
print('Total: 38 cells - Complete Production MLOps!')

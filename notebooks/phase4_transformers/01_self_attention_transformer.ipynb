{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÆ Self-Attention & Transformer Basics\n",
    "\n",
    "**Phase 4, Step 1: Transformers & Modern Architectures**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ –¶–µ–ª—å —ç—Ç–æ–≥–æ –Ω–æ—É—Ç–±—É–∫–∞\n",
    "\n",
    "–í Phase 3 –º—ã –∏–∑—É—á–∏–ª–∏ **RNN/LSTM + Attention**:\n",
    "- ‚úÖ Recurrent –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π\n",
    "- ‚úÖ Attention –∫–∞–∫ –º–µ—Ö–∞–Ω–∏–∑–º –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏\n",
    "- ‚úÖ Seq2Seq –º–æ–¥–µ–ª–∏\n",
    "\n",
    "**–ù–æ —É RNN –µ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã:**\n",
    "- ‚ùå **Sequential processing**: –Ω–µ–ª—å–∑—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏—Ç—å\n",
    "- ‚ùå **Vanishing gradients**: —Å–ª–æ–∂–Ω–æ —É—á–∏—Ç—å –¥–ª–∏–Ω–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏\n",
    "- ‚ùå **Slow training**: –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ –æ–¥–Ω–æ–º—É timestep\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Enter Transformers (2017)\n",
    "\n",
    "**\"Attention is All You Need\"** (Vaswani et al., 2017)\n",
    "\n",
    "**–ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è:** –ü–æ–ª–Ω–æ—Å—Ç—å—é –∏–∑–±–∞–≤–∏—Ç—å—Å—è –æ—Ç —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏!\n",
    "- ‚úÖ **Self-Attention**: –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ\n",
    "- ‚úÖ **Parallelization**: –≤—Å–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã\n",
    "- ‚úÖ **Long-range dependencies**: –ø—Ä—è–º—ã–µ —Å–≤—è–∑–∏ –º–µ–∂–¥—É –ª—é–±—ã–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏\n",
    "- ‚úÖ **Scalability**: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –Ω–∞ GPU/TPU\n",
    "\n",
    "**–†–µ–∑—É–ª—å—Ç–∞—Ç:**\n",
    "- üèÜ SOTA –≤ NLP: BERT, GPT, T5, GPT-3/4\n",
    "- üèÜ Computer Vision: ViT (Vision Transformer), DINO\n",
    "- üèÜ Tabular Data: TabTransformer, FT-Transformer\n",
    "- üèÜ Time Series: Temporal Fusion Transformer\n",
    "- üèÜ Multi-modal: CLIP, Flamingo\n",
    "\n",
    "---\n",
    "\n",
    "## üìö –ß—Ç–æ –º—ã –∏–∑—É—á–∏–º\n",
    "\n",
    "### 1. Self-Attention Mechanism\n",
    "- **Query, Key, Value (Q, K, V)**: –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç attention\n",
    "- **Scaled Dot-Product Attention**: –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞\n",
    "- **Attention Weights**: —á—Ç–æ –º–æ–¥–µ–ª—å \"—Å–º–æ—Ç—Ä–∏—Ç\"\n",
    "- **Implementation**: —Å –Ω—É–ª—è –≤ PyTorch\n",
    "\n",
    "### 2. Multi-Head Attention\n",
    "- **Multiple attention heads**: –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ \"perspectives\"\n",
    "- **Concatenation & projection**: –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ heads\n",
    "- **Why it works**: —Ä–∞–∑–Ω—ã–µ heads —É—á–∞—Ç —Ä–∞–∑–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã\n",
    "\n",
    "### 3. Positional Encoding\n",
    "- **Problem**: Self-Attention permutation-invariant\n",
    "- **Solution**: –¥–æ–±–∞–≤–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–∑–∏—Ü–∏–∏\n",
    "- **Sinusoidal encoding**: –¥–ª—è sequences\n",
    "- **Learnable embeddings**: –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "### 4. Transformer Encoder for Tabular Data\n",
    "- **Dataset**: Titanic (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –≤—ã–∂–∏–≤—à–∏—Ö)\n",
    "- **Architecture**: Feature Embedding ‚Üí Multi-Head Attention ‚Üí FFN\n",
    "- **Training**: Cross-Entropy Loss\n",
    "- **Evaluation**: —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å XGBoost, LSTM\n",
    "- **Interpretability**: –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è attention weights\n",
    "\n",
    "---\n",
    "\n",
    "## üîç –ü–æ—á–µ–º—É Transformers –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö?\n",
    "\n",
    "**–¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥:**\n",
    "- Tree-based (XGBoost, LightGBM): —Ö–æ—Ä–æ—à–æ –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "- MLPs: baseline\n",
    "\n",
    "**Transformers –¥–∞—é—Ç:**\n",
    "- ‚úÖ **Feature interactions**: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑—É—á–µ–Ω–∏–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π\n",
    "- ‚úÖ **Attention weights**: –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å\n",
    "- ‚úÖ **Transfer learning**: pre-training –Ω–∞ –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö\n",
    "- ‚úÖ **Mixed data types**: –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ + —á–∏—Å–ª–æ–≤—ã–µ\n",
    "\n",
    "**–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**\n",
    "- üìä –ë–æ–ª—å—à–∏–µ –¥–∞—Ç–∞—Å–µ—Ç—ã (>10k samples)\n",
    "- üìä –ú–Ω–æ–≥–æ categorical features\n",
    "- üìä –°–ª–æ–∂–Ω—ã–µ feature interactions\n",
    "- üìä –ù—É–∂–Ω–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª –ß–∞—Å—Ç—å 1: –¢–µ–æ—Ä–∏—è –∏ –∏–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏—è Self-Attention\n",
    "\n",
    "### 1.1 –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ë–∞–∑–æ–≤—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix, roc_auc_score\n",
    ")\n",
    "\n",
    "# Math\n",
    "import math\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"\\n‚úÖ –í—Å–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 –¢–µ–æ—Ä–∏—è: –ß—Ç–æ —Ç–∞–∫–æ–µ Self-Attention?\n",
    "\n",
    "---\n",
    "\n",
    "## üß† –ò–Ω—Ç—É–∏—Ü–∏—è\n",
    "\n",
    "**–ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ:** \"The animal didn't cross the street because **it** was too tired.\"\n",
    "\n",
    "**–í–æ–ø—Ä–æ—Å:** –ù–∞ —á—Ç–æ —Å—Å—ã–ª–∞–µ—Ç—Å—è \"it\"?\n",
    "- –û—Ç–≤–µ—Ç: \"The animal\" (–∞ –Ω–µ \"street\")\n",
    "\n",
    "**Self-Attention –¥–µ–ª–∞–µ—Ç –∏–º–µ–Ω–Ω–æ —ç—Ç–æ:**\n",
    "- –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ **–≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞**\n",
    "- –í—ã—á–∏—Å–ª—è–µ—Ç **–≤–µ—Å–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏** (–Ω–∞—Å–∫–æ–ª—å–∫–æ –≤–∞–∂–Ω–æ –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ)\n",
    "- –°–æ–∑–¥–∞–µ—Ç **–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ** –≤–∑–≤–µ—à–µ–Ω–Ω–æ–π —Å—É–º–º–æ–π\n",
    "\n",
    "---\n",
    "\n",
    "## üìê –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞: Scaled Dot-Product Attention\n",
    "\n",
    "**Input:**\n",
    "- Sequence: $X = [x_1, x_2, ..., x_n]$, –≥–¥–µ $x_i \\in \\mathbb{R}^{d}$\n",
    "\n",
    "**–®–∞–≥ 1: –°–æ–∑–¥–∞–µ–º Q, K, V (Query, Key, Value)**\n",
    "\n",
    "$$Q = XW^Q, \\quad K = XW^K, \\quad V = XW^V$$\n",
    "\n",
    "–ì–¥–µ:\n",
    "- $W^Q, W^K, W^V \\in \\mathbb{R}^{d \\times d_k}$ - learnable matrices\n",
    "- $Q, K, V \\in \\mathbb{R}^{n \\times d_k}$\n",
    "\n",
    "**–ò–Ω—Ç—É–∏—Ü–∏—è:**\n",
    "- **Query (Q)**: \"–ß—Ç–æ —è –∏—â—É?\" (–∑–∞–ø—Ä–æ—Å –æ—Ç —Ç–µ–∫—É—â–µ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞)\n",
    "- **Key (K)**: \"–ß—Ç–æ —è –º–æ–≥—É –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å?\" (–æ–ø–∏—Å–∞–Ω–∏–µ –¥—Ä—É–≥–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤)\n",
    "- **Value (V)**: \"–ö–∞–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —è –Ω–µ—Å—É?\" (actual content)\n",
    "\n",
    "**–®–∞–≥ 2: –í—ã—á–∏—Å–ª—è–µ–º Attention Scores**\n",
    "\n",
    "$$\\text{scores} = \\frac{QK^T}{\\sqrt{d_k}}$$\n",
    "\n",
    "- $QK^T$: similarity –º–µ–∂–¥—É queries –∏ keys (dot product)\n",
    "- $\\sqrt{d_k}$: scaling –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\n",
    "\n",
    "**–ü–æ—á–µ–º—É scaling?**\n",
    "- –ë–µ–∑ scaling: –¥–ª—è –±–æ–ª—å—à–∏—Ö $d_k$, dot products –æ–≥—Ä–æ–º–Ω—ã–µ\n",
    "- –û–≥—Ä–æ–º–Ω—ã–µ scores ‚Üí softmax saturation ‚Üí vanishing gradients\n",
    "- $\\sqrt{d_k}$ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç variance\n",
    "\n",
    "**–®–∞–≥ 3: Softmax –¥–ª—è –≤–µ—Å–æ–≤**\n",
    "\n",
    "$$\\text{weights} = \\text{softmax}(\\text{scores}) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)$$\n",
    "\n",
    "- –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç scores –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏: $\\sum_i w_i = 1$\n",
    "- –í—ã—Å–æ–∫–∏–µ scores ‚Üí –≤—ã—Å–æ–∫–∏–µ –≤–µ—Å–∞\n",
    "\n",
    "**–®–∞–≥ 4: Weighted Sum**\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{weights} \\cdot V = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "- –ö–∞–∂–¥—ã–π output - –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –≤—Å–µ—Ö values\n",
    "- –í–µ—Å–∞ –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç, —Å–∫–æ–ª—å–∫–æ –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç \"—Å–º–æ—Ç—Ä–∏—Ç\" –Ω–∞ –¥—Ä—É–≥–∏–µ\n",
    "\n",
    "---\n",
    "\n",
    "## üé® –í–∏–∑—É–∞–ª—å–Ω–∞—è –∏–Ω—Ç—É–∏—Ü–∏—è\n",
    "\n",
    "```\n",
    "Input:     [x1]  [x2]  [x3]  [x4]\n",
    "              ‚Üì     ‚Üì     ‚Üì     ‚Üì\n",
    "           [Q1]  [Q2]  [Q3]  [Q4]  ‚Üê Queries (\"—á—Ç–æ —è –∏—â—É?\")\n",
    "           [K1]  [K2]  [K3]  [K4]  ‚Üê Keys (\"—á—Ç–æ —è –ø—Ä–µ–¥–ª–∞–≥–∞—é?\")\n",
    "           [V1]  [V2]  [V3]  [V4]  ‚Üê Values (actual info)\n",
    "\n",
    "Attention for x1:\n",
    "  Q1 ¬∑ K1 ‚Üí score11  ‚îê\n",
    "  Q1 ¬∑ K2 ‚Üí score12  ‚îú‚Üí softmax ‚Üí [w11, w12, w13, w14]\n",
    "  Q1 ¬∑ K3 ‚Üí score13  ‚îÇ\n",
    "  Q1 ¬∑ K4 ‚Üí score14  ‚îò\n",
    "\n",
    "Output: y1 = w11*V1 + w12*V2 + w13*V3 + w14*V4\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîë –ö–ª—é—á–µ–≤—ã–µ —Å–≤–æ–π—Å—Ç–≤–∞\n",
    "\n",
    "1. **Permutation Invariant (–±–µ–∑ Positional Encoding):**\n",
    "   - Attention –Ω–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –ø–æ—Ä—è–¥–∫–∞ –≤—Ö–æ–¥–æ–≤\n",
    "   - $[x_1, x_2, x_3] \\equiv [x_3, x_1, x_2]$\n",
    "   - –ù—É–∂–Ω–æ –¥–æ–±–∞–≤–ª—è—Ç—å positional encoding!\n",
    "\n",
    "2. **Parallelizable:**\n",
    "   - –í—Å–µ attention scores –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ\n",
    "   - –ú–∞—Ç—Ä–∏—á–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ $QK^T$ - –æ–¥–Ω–∞ –æ–ø–µ—Ä–∞—Ü–∏—è\n",
    "   - –ù–µ—Ç sequential dependencies (–≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç RNN)\n",
    "\n",
    "3. **Long-range Dependencies:**\n",
    "   - –ü—Ä—è–º–∞—è —Å–≤—è–∑—å –º–µ–∂–¥—É –ª—é–±—ã–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏\n",
    "   - O(1) path length (vs O(n) –≤ RNN)\n",
    "\n",
    "4. **Computational Complexity:**\n",
    "   - $O(n^2 \\cdot d)$ –¥–ª—è sequence length $n$\n",
    "   - Bottleneck –¥–ª—è –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π\n",
    "   - –†–µ—à–µ–Ω–∏–µ: Sparse Attention, Linformer, etc.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Implementation: Scaled Dot-Product Attention\n",
    "\n",
    "–ò–º–ø–ª–µ–º–µ–Ω—Ç–∏—Ä—É–µ–º —Å –Ω—É–ª—è!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention\n",
    "    \n",
    "    Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Q: Query matrix (batch_size, n_heads, seq_len, d_k)\n",
    "            K: Key matrix (batch_size, n_heads, seq_len, d_k)\n",
    "            V: Value matrix (batch_size, n_heads, seq_len, d_v)\n",
    "            mask: Mask matrix (optional)\n",
    "        \n",
    "        Returns:\n",
    "            context: Attention output (batch_size, n_heads, seq_len, d_v)\n",
    "            attention_weights: Attention weights (batch_size, n_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        # d_k: dimension of keys/queries\n",
    "        d_k = Q.size(-1)\n",
    "        \n",
    "        # –®–∞–≥ 1: Compute attention scores\n",
    "        # scores shape: (batch_size, n_heads, seq_len, seq_len)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        \n",
    "        # –®–∞–≥ 2: Apply mask (–µ—Å–ª–∏ –µ—Å—Ç—å)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # –®–∞–≥ 3: Apply softmax\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # –®–∞–≥ 4: Weighted sum of values\n",
    "        context = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return context, attention_weights\n",
    "\n",
    "print(\"‚úÖ ScaledDotProductAttention —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 –ü—Ä–∏–º–µ—Ä: Attention –Ω–∞ –ø—Ä–æ—Å—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "–°–æ–∑–¥–∞–¥–∏–º –º–∞–ª–µ–Ω—å–∫–∏–π –ø—Ä–∏–º–µ—Ä, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π –ø—Ä–∏–º–µ—Ä\n",
    "batch_size = 1\n",
    "n_heads = 1  # –ø–æ–∫–∞ –æ–¥–∏–Ω head\n",
    "seq_len = 4  # 4 —ç–ª–µ–º–µ–Ω—Ç–∞ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "d_k = 8      # —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å keys/queries\n",
    "\n",
    "# –°–ª—É—á–∞–π–Ω—ã–µ Q, K, V\n",
    "torch.manual_seed(42)\n",
    "Q = torch.randn(batch_size, n_heads, seq_len, d_k)\n",
    "K = torch.randn(batch_size, n_heads, seq_len, d_k)\n",
    "V = torch.randn(batch_size, n_heads, seq_len, d_k)\n",
    "\n",
    "print(f\"Q shape: {Q.shape}\")\n",
    "print(f\"K shape: {K.shape}\")\n",
    "print(f\"V shape: {V.shape}\")\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º attention\n",
    "attention_layer = ScaledDotProductAttention(dropout=0.0)\n",
    "context, attention_weights = attention_layer(Q, K, V)\n",
    "\n",
    "print(f\"\\nContext shape: {context.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º attention weights\n",
    "weights = attention_weights[0, 0].detach().numpy()  # (seq_len, seq_len)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(weights, annot=True, fmt='.3f', cmap='YlOrRd', \n",
    "            xticklabels=[f'K{i+1}' for i in range(seq_len)],\n",
    "            yticklabels=[f'Q{i+1}' for i in range(seq_len)],\n",
    "            cbar_kws={'label': 'Attention Weight'})\n",
    "plt.title('Attention Weights Matrix', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Keys (what to attend to)', fontsize=12)\n",
    "plt.ylabel('Queries (who is attending)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:\")\n",
    "print(\"  - –ö–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞: –∫–∞–∫ Q_i —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –≤—Å–µ Keys\")\n",
    "print(\"  - –ö–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ —Å—É–º–º–∏—Ä—É–µ—Ç—Å—è –≤ 1.0 (softmax property)\")\n",
    "print(\"  - –í—ã—Å–æ–∫–∏–µ –≤–µ—Å–∞: Q_i —Å–∏–ª—å–Ω–æ \\\"—Å–º–æ—Ç—Ä–∏—Ç\\\" –Ω–∞ K_j\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Ç—Ä–æ–∫–∏ —Å—É–º–º–∏—Ä—É—é—Ç—Å—è –≤ 1\n",
    "row_sums = weights.sum(axis=1)\n",
    "print(f\"\\n‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ softmax: Row sums = {row_sums}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ –ß–∞—Å—Ç—å 2: Multi-Head Attention\n",
    "\n",
    "### 2.1 –¢–µ–æ—Ä–∏—è: –ó–∞—á–µ–º –Ω—É–∂–Ω—ã Multiple Heads?\n",
    "\n",
    "---\n",
    "\n",
    "## ü§î –ü—Ä–æ–±–ª–µ–º–∞ Single-Head Attention\n",
    "\n",
    "**Single head:**\n",
    "- –£—á–∏—Ç —Ç–æ–ª—å–∫–æ **–æ–¥–∏–Ω –ø–∞—Ç—Ç–µ—Ä–Ω** attention\n",
    "- –ú–æ–∂–µ—Ç —É–ø—É—Å—Ç–∏—Ç—å —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä –≤ NLP:**\n",
    "- –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ: \"The cat sat on the mat\"\n",
    "- Head 1 –º–æ–∂–µ—Ç —É—á–∏—Ç—å **syntactic relationships** (subject-verb)\n",
    "- Head 2 –º–æ–∂–µ—Ç —É—á–∏—Ç—å **semantic relationships** (cat-mat: location)\n",
    "- Head 3 –º–æ–∂–µ—Ç —É—á–∏—Ç—å **long-range dependencies**\n",
    "\n",
    "**–†–µ—à–µ–Ω–∏–µ: Multi-Head Attention!**\n",
    "\n",
    "---\n",
    "\n",
    "## üìê –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ Multi-Head Attention\n",
    "\n",
    "**–ò–¥–µ—è:** –ó–∞–ø—É—Å–∫–∞–µ–º $h$ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö attention layers —Å —Ä–∞–∑–Ω—ã–º–∏ –ø—Ä–æ–µ–∫—Ü–∏—è–º–∏.\n",
    "\n",
    "**–î–ª—è –∫–∞–∂–¥–æ–≥–æ head $i$:**\n",
    "\n",
    "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
    "\n",
    "–ì–¥–µ:\n",
    "- $W_i^Q, W_i^K, W_i^V$ - learnable projection matrices –¥–ª—è head $i$\n",
    "- –û–±—ã—á–Ω–æ: $d_k = d_v = d_{model} / h$ (–¥–µ–ª–∏–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–µ–∂–¥—É heads)\n",
    "\n",
    "**Concatenation:**\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W^O$$\n",
    "\n",
    "–ì–¥–µ:\n",
    "- $W^O \\in \\mathbb{R}^{d_{model} \\times d_{model}}$ - output projection matrix\n",
    "\n",
    "**–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏:**\n",
    "- Input: $(batch, seq\\_len, d_{model})$\n",
    "- Each head output: $(batch, seq\\_len, d_k)$\n",
    "- Concatenated: $(batch, seq\\_len, h \\cdot d_k) = (batch, seq\\_len, d_{model})$\n",
    "- Final output: $(batch, seq\\_len, d_{model})$\n",
    "\n",
    "---\n",
    "\n",
    "## üé® –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "\n",
    "```\n",
    "Input X (d_model)\n",
    "        ‚Üì\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚Üì       ‚Üì       ‚Üì       ‚Üì\n",
    "  Head1   Head2   Head3  ... Head_h\n",
    "  (d_k)   (d_k)   (d_k)     (d_k)\n",
    "    ‚Üì       ‚Üì       ‚Üì       ‚Üì\n",
    "   Att1    Att2    Att3   ... Att_h\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        ‚Üì\n",
    "    Concat (h * d_k = d_model)\n",
    "        ‚Üì\n",
    "    Linear (W^O)\n",
    "        ‚Üì\n",
    "    Output (d_model)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ú® –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ Multi-Head\n",
    "\n",
    "1. **Diverse Representations:**\n",
    "   - –†–∞–∑–Ω—ã–µ heads —É—á–∞—Ç —Ä–∞–∑–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã\n",
    "   - Ensemble effect –≤–Ω—É—Ç—Ä–∏ –æ–¥–Ω–æ–≥–æ —Å–ª–æ—è\n",
    "\n",
    "2. **Richer Feature Space:**\n",
    "   - $h$ —Ä–∞–∑–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ü–∏–π ‚Üí –±–æ–ª—å—à–µ —Å–ø–æ—Å–æ–±–æ–≤ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é\n",
    "   - –ê–Ω–∞–ª–æ–≥ \"multiple filters\" –≤ CNN\n",
    "\n",
    "3. **Interpretability:**\n",
    "   - –ú–æ–∂–Ω–æ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å, —á—Ç–æ –∫–∞–∂–¥—ã–π head \"—Å–º–æ—Ç—Ä–∏—Ç\"\n",
    "   - –†–∞–∑–Ω—ã–µ heads –º–æ–≥—É—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å—Å—è\n",
    "\n",
    "4. **Empirical Success:**\n",
    "   - BERT: 12 heads, GPT-3: 96 heads\n",
    "   - –ö—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è SOTA performance\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Implementation: Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention Layer\n",
    "    \n",
    "    Applies h parallel attention heads and concatenates results.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Model dimension (e.g., 512)\n",
    "            n_heads: Number of attention heads (e.g., 8)\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads  # dimension per head\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Scaled Dot-Product Attention\n",
    "        self.attention = ScaledDotProductAttention(dropout)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        Split last dimension into (n_heads, d_k)\n",
    "        \n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            (batch_size, n_heads, seq_len, d_k)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        return x.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        Combine heads back\n",
    "        \n",
    "        Args:\n",
    "            x: (batch_size, n_heads, seq_len, d_k)\n",
    "        \n",
    "        Returns:\n",
    "            (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size, n_heads, seq_len, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Q, K, V: (batch_size, seq_len, d_model)\n",
    "            mask: Optional mask\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch_size, seq_len, d_model)\n",
    "            attention_weights: (batch_size, n_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        batch_size = Q.size(0)\n",
    "        \n",
    "        # 1. Linear projections\n",
    "        Q = self.W_Q(Q)  # (batch, seq_len, d_model)\n",
    "        K = self.W_K(K)\n",
    "        V = self.W_V(V)\n",
    "        \n",
    "        # 2. Split into multiple heads\n",
    "        Q = self.split_heads(Q)  # (batch, n_heads, seq_len, d_k)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "        \n",
    "        # 3. Apply attention on all heads in parallel\n",
    "        context, attention_weights = self.attention(Q, K, V, mask)\n",
    "        # context: (batch, n_heads, seq_len, d_k)\n",
    "        # attention_weights: (batch, n_heads, seq_len, seq_len)\n",
    "        \n",
    "        # 4. Concatenate heads\n",
    "        context = self.combine_heads(context)  # (batch, seq_len, d_model)\n",
    "        \n",
    "        # 5. Final linear projection\n",
    "        output = self.W_O(context)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "print(\"‚úÖ MultiHeadAttention —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 –ü—Ä–∏–º–µ—Ä: Multi-Head Attention –≤ –¥–µ–π—Å—Ç–≤–∏–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
    "batch_size = 2\n",
    "seq_len = 6\n",
    "d_model = 64\n",
    "n_heads = 4\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π input\n",
    "torch.manual_seed(42)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"  batch_size={batch_size}, seq_len={seq_len}, d_model={d_model}\")\n",
    "print(f\"  n_heads={n_heads}, d_k per head={d_model // n_heads}\")\n",
    "\n",
    "# Multi-Head Attention\n",
    "mha = MultiHeadAttention(d_model, n_heads, dropout=0.0)\n",
    "output, attention_weights = mha(x, x, x)  # Self-Attention: Q=K=V=x\n",
    "\n",
    "print(f\"\\nOutput shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"  {n_heads} heads, each with ({seq_len} x {seq_len}) attention matrix\")\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è attention weights –≤—Å–µ—Ö heads\n",
    "fig, axes = plt.subplots(1, n_heads, figsize=(16, 4))\n",
    "\n",
    "for i in range(n_heads):\n",
    "    weights = attention_weights[0, i].detach().numpy()  # –ø–µ—Ä–≤—ã–π sample, i-–π head\n",
    "    \n",
    "    sns.heatmap(weights, annot=True, fmt='.2f', cmap='YlOrRd', \n",
    "                ax=axes[i], cbar=False, square=True,\n",
    "                xticklabels=range(1, seq_len+1),\n",
    "                yticklabels=range(1, seq_len+1))\n",
    "    axes[i].set_title(f'Head {i+1}', fontsize=14, fontweight='bold')\n",
    "    axes[i].set_xlabel('Keys')\n",
    "    if i == 0:\n",
    "        axes[i].set_ylabel('Queries')\n",
    "\n",
    "plt.suptitle('Multi-Head Attention Weights (4 heads)', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä –ù–∞–±–ª—é–¥–µ–Ω–∏—è:\")\n",
    "print(\"  - –ö–∞–∂–¥—ã–π head —É—á–∏—Ç —Å–≤–æ–π –ø–∞—Ç—Ç–µ—Ä–Ω attention\")\n",
    "print(\"  - Heads –º–æ–≥—É—Ç focus –Ω–∞ —Ä–∞–∑–Ω—ã—Ö positions\")\n",
    "print(\"  - –ù–µ–∫–æ—Ç–æ—Ä—ã–µ heads –±–æ–ª–µ–µ \\\"diagonal\\\" (local), –¥—Ä—É–≥–∏–µ –±–æ–ª–µ–µ \\\"distributed\\\" (global)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìç –ß–∞—Å—Ç—å 3: Positional Encoding\n",
    "\n",
    "### 3.1 –¢–µ–æ—Ä–∏—è: –ü—Ä–æ–±–ª–µ–º–∞ Permutation Invariance\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞\n",
    "\n",
    "**Self-Attention permutation invariant:**\n",
    "\n",
    "$$\\text{Attention}([x_1, x_2, x_3]) = \\text{Attention}([x_3, x_1, x_2])$$\n",
    "\n",
    "**–ü–æ—á–µ–º—É —ç—Ç–æ –ø—Ä–æ–±–ª–µ–º–∞?**\n",
    "- –ü–æ—Ä—è–¥–æ–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤–∞–∂–µ–Ω! (–æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è sequences)\n",
    "- \"cat sat on mat\" ‚â† \"mat on sat cat\"\n",
    "- –ú–æ–¥–µ–ª—å –Ω–µ –∑–Ω–∞–µ—Ç position –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞\n",
    "\n",
    "**–†–µ—à–µ–Ω–∏–µ: Positional Encoding!**\n",
    "\n",
    "---\n",
    "\n",
    "## üìê Sinusoidal Positional Encoding (–¥–ª—è sequences)\n",
    "\n",
    "**–ò–¥–µ—è:** –î–æ–±–∞–≤–∏—Ç—å –∫ –∫–∞–∂–¥–æ–º—É —ç–ª–µ–º–µ–Ω—Ç—É —É–Ω–∏–∫–∞–ª—å–Ω—ã–π positional vector.\n",
    "\n",
    "$$\\text{PE}(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "$$\\text{PE}(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "–ì–¥–µ:\n",
    "- $pos$: –ø–æ–∑–∏—Ü–∏—è –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (0, 1, 2, ...)\n",
    "- $i$: dimension index (0, 1, ..., $d_{model}/2$)\n",
    "- Even dimensions: sine\n",
    "- Odd dimensions: cosine\n",
    "\n",
    "**–°–≤–æ–π—Å—Ç–≤–∞:**\n",
    "1. **Unique**: –∫–∞–∂–¥–∞—è –ø–æ–∑–∏—Ü–∏—è ‚Üí —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –≤–µ–∫—Ç–æ—Ä\n",
    "2. **Bounded**: –∑–Ω–∞—á–µ–Ω–∏—è –≤ $[-1, 1]$\n",
    "3. **Relative positions**: $\\text{PE}(pos+k)$ - linear function of $\\text{PE}(pos)$\n",
    "4. **Extrapolation**: —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è –ø–æ–∑–∏—Ü–∏–π, –Ω–µ –≤–∏–¥–µ–Ω–Ω—ã—Ö –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏\n",
    "\n",
    "**–ü–æ—á–µ–º—É sin/cos?**\n",
    "- –†–∞–∑–Ω—ã–µ —á–∞—Å—Ç–æ—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö dimensions\n",
    "- Low dimensions: –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∫–æ–ª–µ–±–∞–Ω–∏—è (long-range)\n",
    "- High dimensions: –±—ã—Å—Ç—Ä—ã–µ –∫–æ–ª–µ–±–∞–Ω–∏—è (short-range)\n",
    "- –ú–æ–¥–µ–ª—å —Å–∞–º–∞ –≤—ã–±–∏—Ä–∞–µ—Ç, –∫–∞–∫–∏–µ —á–∞—Å—Ç–æ—Ç—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å\n",
    "\n",
    "---\n",
    "\n",
    "## üóÇÔ∏è Learnable Positional Embeddings (–¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö)\n",
    "\n",
    "**–î–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:**\n",
    "- –ù–µ—Ç \"–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ\" –ø–æ—Ä—è–¥–∫–∞ features\n",
    "- –ú–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å **learnable embeddings**:\n",
    "\n",
    "$$\\text{PE}_i \\in \\mathbb{R}^{d_{model}}$$\n",
    "\n",
    "- –ü—Ä–æ—Å—Ç–æ learnable parameters –¥–ª—è –∫–∞–∂–¥–æ–π feature position\n",
    "- –ë–æ–ª–µ–µ –≥–∏–±–∫–æ, –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ –¥–∞–Ω–Ω—ã–º\n",
    "\n",
    "**Final Input:**\n",
    "\n",
    "$$X_{final} = X_{embedded} + \\text{PE}$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Implementation: Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal Positional Encoding for Sequences\n",
    "    \n",
    "    PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Model dimension\n",
    "            max_len: Maximum sequence length\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Compute div_term: 10000^(2i/d_model)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                             (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply sin to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        # Apply cos to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Add batch dimension\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        \n",
    "        # Register as buffer (not a parameter, but part of module state)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            x + positional_encoding: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class LearnablePositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable Positional Embeddings for Tabular Data\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, d_model, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_features: Number of features (sequence length)\n",
    "            d_model: Model dimension\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super(LearnablePositionalEmbedding, self).__init__()\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_features, d_model))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, num_features, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            x + positional_embedding: (batch_size, num_features, d_model)\n",
    "        \"\"\"\n",
    "        x = x + self.pos_embedding\n",
    "        return self.dropout(x)\n",
    "\n",
    "print(\"‚úÖ Positional Encoding —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω (Sinusoidal –∏ Learnable)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞–µ–º positional encoding\n",
    "d_model = 64\n",
    "max_len = 100\n",
    "\n",
    "pe_layer = PositionalEncoding(d_model, max_len, dropout=0.0)\n",
    "pe_matrix = pe_layer.pe[0].numpy()  # (max_len, d_model)\n",
    "\n",
    "print(f\"Positional Encoding shape: {pe_matrix.shape}\")\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è 1: Heatmap\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Heatmap –≤—Å–µ—Ö –ø–æ–∑–∏—Ü–∏–π –∏ dimensions\n",
    "im = axes[0].imshow(pe_matrix.T, cmap='RdBu', aspect='auto', interpolation='nearest')\n",
    "axes[0].set_title('Positional Encoding Matrix', fontsize=16, fontweight='bold')\n",
    "axes[0].set_xlabel('Position', fontsize=12)\n",
    "axes[0].set_ylabel('Dimension', fontsize=12)\n",
    "plt.colorbar(im, ax=axes[0], label='Value')\n",
    "\n",
    "# –ù–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –ø–æ–∑–∏—Ü–∏–π\n",
    "positions_to_plot = [0, 10, 20, 40, 80]\n",
    "for pos in positions_to_plot:\n",
    "    axes[1].plot(pe_matrix[pos], label=f'Position {pos}', alpha=0.7)\n",
    "\n",
    "axes[1].set_title('Positional Encodings for Different Positions', \n",
    "                  fontsize=16, fontweight='bold')\n",
    "axes[1].set_xlabel('Dimension', fontsize=12)\n",
    "axes[1].set_ylabel('Value', fontsize=12)\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä –ù–∞–±–ª—é–¥–µ–Ω–∏—è:\")\n",
    "print(\"  - –ö–∞–∂–¥–∞—è –ø–æ–∑–∏—Ü–∏—è –∏–º–µ–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω\")\n",
    "print(\"  - Low dimensions: –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∫–æ–ª–µ–±–∞–Ω–∏—è (–≤–∏–¥–Ω—ã –ø–æ–ª–æ—Å—ã –Ω–∞ heatmap)\")\n",
    "print(\"  - High dimensions: –±—ã—Å—Ç—Ä—ã–µ –∫–æ–ª–µ–±–∞–Ω–∏—è (–±–æ–ª–µ–µ –º–µ–ª–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞)\")\n",
    "print(\"  - –ú–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è relative positions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèóÔ∏è –ß–∞—Å—Ç—å 4: Transformer Encoder\n",
    "\n",
    "### 4.1 –¢–µ–æ—Ä–∏—è: –ü–æ–ª–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Encoder\n",
    "\n",
    "---\n",
    "\n",
    "## üìê Transformer Encoder Block\n",
    "\n",
    "**–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ–¥–Ω–æ–≥–æ Encoder Block:**\n",
    "\n",
    "```\n",
    "Input\n",
    "  ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Multi-Head Attention‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "           ‚Üì\n",
    "      Add & Norm (Residual + LayerNorm)\n",
    "           ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Feed Forward (FFN) ‚îÇ\n",
    "‚îÇ   (2-layer MLP)     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "           ‚Üì\n",
    "      Add & Norm (Residual + LayerNorm)\n",
    "           ‚Üì\n",
    "        Output\n",
    "```\n",
    "\n",
    "**–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:**\n",
    "\n",
    "1. **Multi-Head Self-Attention:**\n",
    "   - Q = K = V = Input (self-attention)\n",
    "   - Feature interactions\n",
    "\n",
    "2. **Add & Norm (Residual Connection + Layer Normalization):**\n",
    "   $$\\text{LayerNorm}(x + \\text{Sublayer}(x))$$\n",
    "   - Residual: –ø–æ–º–æ–≥–∞–µ—Ç gradient flow\n",
    "   - LayerNorm: —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ\n",
    "\n",
    "3. **Feed Forward Network (FFN):**\n",
    "   $$\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2$$\n",
    "   - 2-layer MLP —Å ReLU\n",
    "   - –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è independently –∫ –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏\n",
    "   - –û–±—ã—á–Ω–æ: hidden dim = 4 √ó d_model\n",
    "\n",
    "4. **Another Add & Norm**\n",
    "\n",
    "**Stacking Multiple Blocks:**\n",
    "- Original Transformer: 6 encoder blocks\n",
    "- BERT-base: 12 blocks\n",
    "- GPT-3: 96 blocks\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ –î–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:**\n",
    "\n",
    "```\n",
    "Tabular Features [x1, x2, ..., xn]\n",
    "        ‚Üì\n",
    "Feature Embedding (Linear projection)\n",
    "        ‚Üì\n",
    "Positional Encoding (learnable)\n",
    "        ‚Üì\n",
    "Transformer Encoder Blocks (N layers)\n",
    "        ‚Üì\n",
    "Global Pooling (mean/max/CLS token)\n",
    "        ‚Üì\n",
    "Classification Head (Linear)\n",
    "        ‚Üì\n",
    "Output (class probabilities)\n",
    "```\n",
    "\n",
    "**Key Differences from NLP:**\n",
    "- **No word embeddings**: Linear projection –≤–º–µ—Å—Ç–æ embedding lookup\n",
    "- **Learnable PE**: –≤–º–µ—Å—Ç–æ sinusoidal (–Ω–µ—Ç –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞)\n",
    "- **Global pooling**: mean/max –≤–º–µ—Å—Ç–æ CLS token (–º–æ–∂–Ω–æ –∏ CLS)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Implementation: Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed Forward Network\n",
    "    \n",
    "    FFN(x) = ReLU(xW1 + b1)W2 + b2\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Model dimension\n",
    "            d_ff: Hidden dimension (usually 4 * d_model)\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "\n",
    "print(\"‚úÖ FeedForward —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Implementation: Transformer Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Single Transformer Encoder Block\n",
    "    \n",
    "    Components:\n",
    "    1. Multi-Head Self-Attention\n",
    "    2. Add & Norm\n",
    "    3. Feed Forward Network\n",
    "    4. Add & Norm\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Model dimension\n",
    "            n_heads: Number of attention heads\n",
    "            d_ff: Feed-forward hidden dimension\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        \n",
    "        # Multi-Head Attention\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        \n",
    "        # Feed Forward\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        # Layer Normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model)\n",
    "            mask: Optional mask\n",
    "        \n",
    "        Returns:\n",
    "            (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # 1. Multi-Head Attention + Add & Norm\n",
    "        attn_output, attention_weights = self.attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # 2. Feed Forward + Add & Norm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x, attention_weights\n",
    "\n",
    "print(\"‚úÖ TransformerEncoderBlock —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Implementation: Full Transformer Encoder –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularTransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Encoder –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "    \n",
    "    Architecture:\n",
    "    Input ‚Üí Feature Embedding ‚Üí Positional Encoding ‚Üí \n",
    "    Transformer Blocks ‚Üí Global Pooling ‚Üí Classification Head\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, d_model, n_heads, n_layers, d_ff, \n",
    "                 num_classes, dropout=0.1, pooling='mean'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_features: Number of input features\n",
    "            d_model: Model dimension\n",
    "            n_heads: Number of attention heads\n",
    "            n_layers: Number of Transformer blocks\n",
    "            d_ff: Feed-forward hidden dimension\n",
    "            num_classes: Number of output classes\n",
    "            dropout: Dropout rate\n",
    "            pooling: Pooling method ('mean', 'max', 'cls')\n",
    "        \"\"\"\n",
    "        super(TabularTransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.d_model = d_model\n",
    "        self.pooling = pooling\n",
    "        \n",
    "        # Feature embedding (project each feature to d_model)\n",
    "        self.feature_embedding = nn.Linear(1, d_model)  # each feature independently\n",
    "        \n",
    "        # Positional encoding (learnable)\n",
    "        self.pos_encoding = LearnablePositionalEmbedding(num_features, d_model, dropout)\n",
    "        \n",
    "        # Stack of Transformer Encoder blocks\n",
    "        self.encoder_blocks = nn.ModuleList([\n",
    "            TransformerEncoderBlock(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # CLS token (if using cls pooling)\n",
    "        if pooling == 'cls':\n",
    "            self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, return_attention=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, num_features)\n",
    "            return_attention: Whether to return attention weights\n",
    "        \n",
    "        Returns:\n",
    "            logits: (batch_size, num_classes)\n",
    "            attention_weights: Optional, list of attention weights from each layer\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # 1. Feature embedding: (batch, num_features) ‚Üí (batch, num_features, d_model)\n",
    "        x = x.unsqueeze(-1)  # (batch, num_features, 1)\n",
    "        x = self.feature_embedding(x)  # (batch, num_features, d_model)\n",
    "        \n",
    "        # 2. Add CLS token if using cls pooling\n",
    "        if self.pooling == 'cls':\n",
    "            cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (batch, 1, d_model)\n",
    "            x = torch.cat([cls_tokens, x], dim=1)  # (batch, num_features+1, d_model)\n",
    "        \n",
    "        # 3. Add positional encoding\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # 4. Pass through Transformer blocks\n",
    "        attention_weights_list = []\n",
    "        for encoder_block in self.encoder_blocks:\n",
    "            x, attn_weights = encoder_block(x)\n",
    "            if return_attention:\n",
    "                attention_weights_list.append(attn_weights)\n",
    "        \n",
    "        # 5. Global pooling\n",
    "        if self.pooling == 'mean':\n",
    "            x = x.mean(dim=1)  # (batch, d_model)\n",
    "        elif self.pooling == 'max':\n",
    "            x = x.max(dim=1)[0]  # (batch, d_model)\n",
    "        elif self.pooling == 'cls':\n",
    "            x = x[:, 0, :]  # take CLS token\n",
    "        \n",
    "        # 6. Classification\n",
    "        logits = self.classifier(x)\n",
    "        \n",
    "        if return_attention:\n",
    "            return logits, attention_weights_list\n",
    "        return logits\n",
    "\n",
    "print(\"‚úÖ TabularTransformerEncoder —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω!\")\n",
    "print(\"\\n–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –≥–æ—Ç–æ–≤–∞:\")\n",
    "print(\"  1. Feature Embedding (Linear projection)\")\n",
    "print(\"  2. Learnable Positional Encoding\")\n",
    "print(\"  3. Stack of Transformer Encoder Blocks\")\n",
    "print(\"  4. Global Pooling (mean/max/cls)\")\n",
    "print(\"  5. Classification Head\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä –ß–∞—Å—Ç—å 5: Titanic Dataset\n",
    "\n",
    "### 5.1 –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "**–ó–∞–¥–∞—á–∞:** –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –≤—ã–∂–∏–≤–∞–Ω–∏–µ –ø–∞—Å—Å–∞–∂–∏—Ä–æ–≤ –¢–∏—Ç–∞–Ω–∏–∫–∞  \n",
    "**–¢–∏–ø:** –ë–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (Survived: 0 –∏–ª–∏ 1)  \n",
    "**Features:** Pclass, Sex, Age, SibSp, Parch, Fare, Embarked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ Titanic dataset\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ seaborn\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "\n",
    "# –í—ã–±–∏—Ä–∞–µ–º –Ω—É–∂–Ω—ã–µ features\n",
    "features_to_use = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked', 'alone']\n",
    "target = 'survived'\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é\n",
    "data = df[features_to_use + [target]].copy()\n",
    "\n",
    "# Handle missing values\n",
    "data['age'].fillna(data['age'].median(), inplace=True)\n",
    "data['fare'].fillna(data['fare'].median(), inplace=True)\n",
    "data['embarked'].fillna(data['embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "# Sex: male=1, female=0\n",
    "data['sex'] = (data['sex'] == 'male').astype(int)\n",
    "\n",
    "# Embarked: one-hot encoding\n",
    "data = pd.get_dummies(data, columns=['embarked'], prefix='embarked', drop_first=True)\n",
    "\n",
    "# alone: boolean to int\n",
    "data['alone'] = data['alone'].astype(int)\n",
    "\n",
    "print(f\"After preprocessing: {data.shape}\")\n",
    "print(f\"\\nFeatures: {[col for col in data.columns if col != 'survived']}\")\n",
    "print(f\"\\nMissing values: {data.isnull().sum().sum()}\")\n",
    "\n",
    "# Check class balance\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(data['survived'].value_counts())\n",
    "print(f\"\\nSurvival rate: {data['survived'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# Survival by class\n",
    "pd.crosstab(data['pclass'], data['survived']).plot(kind='bar', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Survival by Class', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Pclass')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].legend(['Died', 'Survived'])\n",
    "\n",
    "# Survival by sex\n",
    "pd.crosstab(data['sex'], data['survived']).plot(kind='bar', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Survival by Sex', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Sex (0=female, 1=male)')\n",
    "axes[0, 1].set_xticklabels(['Female', 'Male'], rotation=0)\n",
    "axes[0, 1].legend(['Died', 'Survived'])\n",
    "\n",
    "# Age distribution\n",
    "data[data['survived'] == 0]['age'].hist(bins=30, alpha=0.5, label='Died', ax=axes[0, 2])\n",
    "data[data['survived'] == 1]['age'].hist(bins=30, alpha=0.5, label='Survived', ax=axes[0, 2])\n",
    "axes[0, 2].set_title('Age Distribution', fontweight='bold')\n",
    "axes[0, 2].set_xlabel('Age')\n",
    "axes[0, 2].legend()\n",
    "\n",
    "# Fare distribution\n",
    "data[data['survived'] == 0]['fare'].hist(bins=30, alpha=0.5, label='Died', ax=axes[1, 0])\n",
    "data[data['survived'] == 1]['fare'].hist(bins=30, alpha=0.5, label='Survived', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Fare Distribution', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Fare')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# SibSp distribution\n",
    "pd.crosstab(data['sibsp'], data['survived']).plot(kind='bar', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Survival by SibSp', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Siblings/Spouses')\n",
    "axes[1, 1].legend(['Died', 'Survived'])\n",
    "\n",
    "# Correlation heatmap\n",
    "corr = data.corr()\n",
    "sns.heatmap(corr[['survived']].sort_values('survived', ascending=False), \n",
    "            annot=True, fmt='.2f', cmap='RdYlGn', ax=axes[1, 2], cbar=False)\n",
    "axes[1, 2].set_title('Correlation with Survival', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Key insights:\")\n",
    "print(\"  - Women had much higher survival rate\")\n",
    "print(\"  - First class passengers survived more\")\n",
    "print(\"  - Children had better survival chances\")\n",
    "print(\"  - Higher fare ‚Üí higher survival (proxy for class)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop('survived', axis=1).values\n",
    "y = data['survived'].values\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\n‚úÖ Features standardized\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"\\n‚úÖ DataLoaders created (batch_size={batch_size})\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèãÔ∏è –ß–∞—Å—Ç—å 6: –û–±—É—á–µ–Ω–∏–µ Transformer\n",
    "\n",
    "### 6.1 –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_features = X_train.shape[1]\n",
    "d_model = 64\n",
    "n_heads = 4\n",
    "n_layers = 2\n",
    "d_ff = 256  # 4 * d_model\n",
    "num_classes = 2\n",
    "dropout = 0.1\n",
    "pooling = 'mean'  # 'mean', 'max', or 'cls'\n",
    "\n",
    "print(\"Model Hyperparameters:\")\n",
    "print(f\"  Input features: {num_features}\")\n",
    "print(f\"  d_model: {d_model}\")\n",
    "print(f\"  n_heads: {n_heads}\")\n",
    "print(f\"  n_layers: {n_layers}\")\n",
    "print(f\"  d_ff: {d_ff}\")\n",
    "print(f\"  dropout: {dropout}\")\n",
    "print(f\"  pooling: {pooling}\")\n",
    "print(f\"  num_classes: {num_classes}\")\n",
    "\n",
    "# Initialize model\n",
    "model = TabularTransformerEncoder(\n",
    "    num_features=num_features,\n",
    "    d_model=d_model,\n",
    "    n_heads=n_heads,\n",
    "    n_layers=n_layers,\n",
    "    d_ff=d_ff,\n",
    "    num_classes=num_classes,\n",
    "    dropout=dropout,\n",
    "    pooling=pooling\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel size:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
    "                                                   factor=0.5, patience=5)\n",
    "\n",
    "print(\"\\n‚úÖ Model initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for X_batch, y_batch in loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Metrics\n",
    "        total_loss += loss.item() * X_batch.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            total_loss += loss.item() * X_batch.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "print(\"‚úÖ Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 50\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_loss': [],\n",
    "    'test_acc': []\n",
    "}\n",
    "\n",
    "print(f\"Training for {num_epochs} epochs...\\n\")\n",
    "best_test_acc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    # Scheduler step\n",
    "    scheduler.step(test_loss)\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    \n",
    "    # Track best\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "        print(f\"  LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        print()\n",
    "\n",
    "print(f\"\\n‚úÖ Training completed!\")\n",
    "print(f\"Best Test Accuracy: {best_test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['test_loss'], label='Test Loss', linewidth=2)\n",
    "axes[0].set_title('Loss over Epochs', fontsize=16, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['train_acc'], label='Train Accuracy', linewidth=2)\n",
    "axes[1].plot(history['test_acc'], label='Test Accuracy', linewidth=2)\n",
    "axes[1].set_title('Accuracy over Epochs', fontsize=16, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Train Accuracy: {history['train_acc'][-1]:.4f}\")\n",
    "print(f\"Final Test Accuracy: {history['test_acc'][-1]:.4f}\")\n",
    "print(f\"Best Test Accuracy: {best_test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Detailed Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "y_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(y_batch.numpy())\n",
    "        y_probs.extend(probs[:, 1].cpu().numpy())  # probability of class 1\n",
    "\n",
    "y_pred = np.array(y_pred)\n",
    "y_true = np.array(y_true)\n",
    "y_probs = np.array(y_probs)\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "auc = roc_auc_score(y_true, y_probs)\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1 Score:  {f1:.4f}\")\n",
    "print(f\"  ROC AUC:   {auc:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Died', 'Survived']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Died', 'Survived'],\n",
    "            yticklabels=['Died', 'Survived'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('True', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç –ß–∞—Å—Ç—å 7: Attention Visualization\n",
    "\n",
    "### 7.1 –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è Attention Weights\n",
    "\n",
    "**–ß—Ç–æ –º—ã –∏—â–µ–º:**\n",
    "- –ö–∞–∫–∏–µ features –º–æ–¥–µ–ª—å —Å—á–∏—Ç–∞–µ—Ç –≤–∞–∂–Ω—ã–º–∏?\n",
    "- –ï—Å—Ç—å –ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ attention –º–µ–∂–¥—É features?\n",
    "- –ö–∞–∫ —Ä–∞–∑–Ω—ã–µ heads —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í—ã–±–∏—Ä–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "sample_indices = [0, 5, 10]  # –∏–Ω–¥–µ–∫—Å—ã –≤ test set\n",
    "\n",
    "# Feature names\n",
    "feature_names = [col for col in data.columns if col != 'survived']\n",
    "print(f\"Features ({len(feature_names)}): {feature_names}\")\n",
    "\n",
    "# Get attention weights –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx in sample_indices:\n",
    "        x_sample = X_test_tensor[idx:idx+1].to(device)\n",
    "        y_sample = y_test_tensor[idx].item()\n",
    "        \n",
    "        # Forward pass with attention\n",
    "        logits, attention_list = model(x_sample, return_attention=True)\n",
    "        pred_prob = F.softmax(logits, dim=1)[0, 1].item()\n",
    "        pred_class = torch.argmax(logits, dim=1).item()\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Sample {idx}: True={y_sample}, Pred={pred_class}, Prob={pred_prob:.3f}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º attention weights –¥–ª—è –≤—Å–µ—Ö layers\n",
    "        n_layers_viz = len(attention_list)\n",
    "        n_heads_viz = attention_list[0].size(1)\n",
    "        \n",
    "        fig, axes = plt.subplots(n_layers_viz, n_heads_viz, \n",
    "                                figsize=(n_heads_viz * 4, n_layers_viz * 4))\n",
    "        \n",
    "        if n_layers_viz == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for layer_idx in range(n_layers_viz):\n",
    "            attn_weights = attention_list[layer_idx][0].cpu().numpy()  # (n_heads, seq_len, seq_len)\n",
    "            \n",
    "            for head_idx in range(n_heads_viz):\n",
    "                ax = axes[layer_idx, head_idx]\n",
    "                \n",
    "                # Attention matrix –¥–ª—è —ç—Ç–æ–≥–æ head\n",
    "                attn_matrix = attn_weights[head_idx]  # (seq_len, seq_len)\n",
    "                \n",
    "                sns.heatmap(attn_matrix, annot=False, fmt='.2f', cmap='YlOrRd',\n",
    "                           xticklabels=feature_names, yticklabels=feature_names,\n",
    "                           ax=ax, cbar=True, square=True)\n",
    "                ax.set_title(f'Layer {layer_idx+1}, Head {head_idx+1}', \n",
    "                           fontsize=12, fontweight='bold')\n",
    "                ax.set_xlabel('Keys (attending to)')\n",
    "                if head_idx == 0:\n",
    "                    ax.set_ylabel('Queries (attending from)')\n",
    "        \n",
    "        plt.suptitle(f'Sample {idx}: Attention Weights Across Layers and Heads\\n'\n",
    "                    f'True: {y_sample}, Pred: {pred_class}, Prob: {pred_prob:.3f}',\n",
    "                    fontsize=16, fontweight='bold', y=1.01)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate attention weights across all test samples\n",
    "# –î–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è, –∫–∞–∫–∏–µ features –º–æ–¥–µ–ª—å —Å—á–∏—Ç–∞–µ—Ç –≤–∞–∂–Ω—ã–º–∏ –≤ —Å—Ä–µ–¥–Ω–µ–º\n",
    "\n",
    "model.eval()\n",
    "all_attention_weights = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, _ in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        _, attention_list = model(X_batch, return_attention=True)\n",
    "        \n",
    "        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π layer\n",
    "        last_layer_attn = attention_list[-1]  # (batch, n_heads, seq_len, seq_len)\n",
    "        \n",
    "        # –£—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ heads\n",
    "        avg_attn = last_layer_attn.mean(dim=1)  # (batch, seq_len, seq_len)\n",
    "        \n",
    "        all_attention_weights.append(avg_attn.cpu())\n",
    "\n",
    "# Concatenate –∏ —É—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ –≤—Å–µ–º samples\n",
    "all_attention_weights = torch.cat(all_attention_weights, dim=0)\n",
    "mean_attention = all_attention_weights.mean(dim=0).numpy()  # (seq_len, seq_len)\n",
    "\n",
    "print(f\"Mean attention shape: {mean_attention.shape}\")\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ä–µ–¥–Ω—é—é attention matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(mean_attention, annot=True, fmt='.2f', cmap='YlOrRd',\n",
    "           xticklabels=feature_names, yticklabels=feature_names,\n",
    "           cbar_kws={'label': 'Attention Weight'})\n",
    "plt.title('Mean Attention Weights (Last Layer, Averaged over Test Set)', \n",
    "         fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Keys (attending to)', fontsize=12)\n",
    "plt.ylabel('Queries (attending from)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# –í—ã—á–∏—Å–ª—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å features (—Å–∫–æ–ª—å–∫–æ –Ω–∞ –Ω–∏—Ö –æ–±—Ä–∞—â–∞—é—Ç –≤–Ω–∏–º–∞–Ω–∏–µ)\n",
    "feature_importance = mean_attention.sum(axis=0)  # —Å—É–º–º–∞ –ø–æ columns (–∫—É–¥–∞ —Å–º–æ—Ç—Ä—è—Ç)\n",
    "\n",
    "# –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º\n",
    "feature_importance = feature_importance / feature_importance.sum()\n",
    "\n",
    "# –°–æ—Ä—Ç–∏—Ä—É–µ–º\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance (based on attention):\")\n",
    "print(importance_df)\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'], color='steelblue')\n",
    "plt.xlabel('Attention-based Importance', fontsize=12)\n",
    "plt.title('Feature Importance from Attention Weights', fontsize=16, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä –ß–∞—Å—Ç—å 8: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å Baseline –º–æ–¥–µ–ª—è–º–∏\n",
    "\n",
    "### 8.1 Simple MLP Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple MLP –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_classes, dropout=0.3):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        layers.append(nn.Linear(prev_size, num_classes))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Train MLP\n",
    "mlp = SimpleMLP(input_size=num_features, hidden_sizes=[64, 32], \n",
    "                num_classes=2, dropout=0.3).to(device)\n",
    "\n",
    "mlp_optimizer = optim.Adam(mlp.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "mlp_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Training MLP for {num_epochs} epochs...\")\n",
    "mlp_best_acc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    mlp.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        mlp_optimizer.zero_grad()\n",
    "        outputs = mlp(X_batch)\n",
    "        loss = mlp_criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        mlp_optimizer.step()\n",
    "    \n",
    "    # Evaluate\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        _, test_acc = evaluate(mlp, test_loader, mlp_criterion, device)\n",
    "        if test_acc > mlp_best_acc:\n",
    "            mlp_best_acc = test_acc\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] MLP Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "print(f\"\\nMLP Best Test Accuracy: {mlp_best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 XGBoost Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost baseline\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    \n",
    "    print(\"Training XGBoost...\")\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    xgb_pred = xgb_model.predict(X_test)\n",
    "    xgb_acc = accuracy_score(y_test, xgb_pred)\n",
    "    xgb_f1 = f1_score(y_test, xgb_pred)\n",
    "    \n",
    "    print(f\"XGBoost Test Accuracy: {xgb_acc:.4f}\")\n",
    "    print(f\"XGBoost Test F1 Score: {xgb_f1:.4f}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    xgb_importance = xgb_model.feature_importances_\n",
    "    xgb_importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': xgb_importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nXGBoost Feature Importance:\")\n",
    "    print(xgb_importance_df)\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è XGBoost not installed. Skipping XGBoost baseline.\")\n",
    "    print(\"Install with: pip install xgboost\")\n",
    "    xgb_acc = None\n",
    "    xgb_f1 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "comparison_data = {\n",
    "    'Model': ['Transformer', 'MLP', 'XGBoost'],\n",
    "    'Accuracy': [accuracy, mlp_best_acc, xgb_acc if xgb_acc else 0],\n",
    "    'F1 Score': [f1, 0, xgb_f1 if xgb_f1 else 0],  # –Ω–µ —Å—á–∏—Ç–∞–ª–∏ F1 –¥–ª—è MLP\n",
    "    'Parameters': [trainable_params, \n",
    "                   sum(p.numel() for p in mlp.parameters()),\n",
    "                   0]  # XGBoost –Ω–µ –∏–º–µ–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0].bar(comparison_df['Model'], comparison_df['Accuracy'], \n",
    "           color=['steelblue', 'orange', 'green'])\n",
    "axes[0].set_title('Model Accuracy Comparison', fontsize=16, fontweight='bold')\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_ylim([0.7, 0.9])\n",
    "axes[0].grid(alpha=0.3, axis='y')\n",
    "for i, v in enumerate(comparison_df['Accuracy']):\n",
    "    axes[0].text(i, v + 0.01, f\"{v:.3f}\", ha='center', fontweight='bold')\n",
    "\n",
    "# Parameters comparison\n",
    "valid_params = [(m, p) for m, p in zip(comparison_df['Model'], comparison_df['Parameters']) if p > 0]\n",
    "if valid_params:\n",
    "    models, params = zip(*valid_params)\n",
    "    axes[1].bar(models, params, color=['steelblue', 'orange'])\n",
    "    axes[1].set_title('Model Size (Parameters)', fontsize=16, fontweight='bold')\n",
    "    axes[1].set_ylabel('# Parameters', fontsize=12)\n",
    "    axes[1].grid(alpha=0.3, axis='y')\n",
    "    for i, (m, p) in enumerate(valid_params):\n",
    "        axes[1].text(i, p + 500, f\"{p:,}\", ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä –í—ã–≤–æ–¥—ã:\")\n",
    "print(\"  - Transformer –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç competitive —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å baselines\")\n",
    "print(\"  - XGBoost –ø–æ-–ø—Ä–µ–∂–Ω–µ–º—É —Å–∏–ª–µ–Ω –Ω–∞ —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ –º–∞–ª–µ–Ω—å–∫–∏—Ö)\")\n",
    "print(\"  - Transformer –¥–∞–µ—Ç –±–æ–Ω—É—Å: attention weights –¥–ª—è interpretability\")\n",
    "print(\"  - –ù–∞ –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö Transformer –º–æ–∂–µ—Ç –ø—Ä–µ–≤–∑–æ–π—Ç–∏ tree-based –º–µ—Ç–æ–¥—ã\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì –ò—Ç–æ–≥–∏ –∏ –í—ã–≤–æ–¥—ã\n",
    "\n",
    "### –ß—Ç–æ –º—ã –∏–∑—É—á–∏–ª–∏\n",
    "\n",
    "#### 1. Self-Attention Mechanism\n",
    "- ‚úÖ **Query, Key, Value**: –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç attention\n",
    "- ‚úÖ **Scaled Dot-Product**: $\\text{Attention}(Q,K,V) = \\text{softmax}(QK^T/\\sqrt{d_k})V$\n",
    "- ‚úÖ **Parallelization**: –≤—Å–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã (vs RNN sequential)\n",
    "- ‚úÖ **Long-range dependencies**: –ø—Ä—è–º—ã–µ —Å–≤—è–∑–∏ O(1) path length\n",
    "\n",
    "#### 2. Multi-Head Attention\n",
    "- ‚úÖ **Multiple perspectives**: —Ä–∞–∑–Ω—ã–µ heads —É—á–∞—Ç —Ä–∞–∑–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã\n",
    "- ‚úÖ **Ensemble effect**: –≤–Ω—É—Ç—Ä–∏ –æ–¥–Ω–æ–≥–æ —Å–ª–æ—è\n",
    "- ‚úÖ **Richer representations**: $h$ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö attention mechanisms\n",
    "\n",
    "#### 3. Positional Encoding\n",
    "- ‚úÖ **Sinusoidal**: –¥–ª—è sequences (NLP, time series)\n",
    "- ‚úÖ **Learnable**: –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–Ω–µ—Ç –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞)\n",
    "- ‚úÖ **–†–µ—à–∞–µ—Ç permutation invariance**: –º–æ–¥–µ–ª—å –∑–Ω–∞–µ—Ç –ø–æ–∑–∏—Ü–∏–∏\n",
    "\n",
    "#### 4. Transformer Encoder\n",
    "- ‚úÖ **Architecture**: Multi-Head Attention ‚Üí Add&Norm ‚Üí FFN ‚Üí Add&Norm\n",
    "- ‚úÖ **Residual connections**: –ø–æ–º–æ–≥–∞—é—Ç gradient flow\n",
    "- ‚úÖ **Layer Normalization**: —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ\n",
    "- ‚úÖ **Feed-Forward Network**: –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ-–Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏\n",
    "\n",
    "#### 5. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫ –¢–∞–±–ª–∏—á–Ω—ã–º –î–∞–Ω–Ω—ã–º\n",
    "- ‚úÖ **Feature embedding**: Linear projection –¥–ª—è –∫–∞–∂–¥–æ–π feature\n",
    "- ‚úÖ **Feature interactions**: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —á–µ—Ä–µ–∑ attention\n",
    "- ‚úÖ **Interpretability**: attention weights –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –≤–∞–∂–Ω–æ—Å—Ç—å\n",
    "- ‚úÖ **Competitive performance**: —Å—Ä–∞–≤–Ω–∏–º–æ —Å XGBoost –Ω–∞ Titanic\n",
    "\n",
    "---\n",
    "\n",
    "### –ö–ª—é—á–µ–≤—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ Transformers\n",
    "\n",
    "| Aspect | RNN/LSTM | Transformer |\n",
    "|--------|----------|-------------|\n",
    "| **Processing** | Sequential | Parallel |\n",
    "| **Long-range deps** | O(n) path | O(1) path |\n",
    "| **Training speed** | Slow | Fast (GPU) |\n",
    "| **Interpretability** | Hard | Attention weights |\n",
    "| **Scalability** | Limited | Excellent |\n",
    "| **SOTA results** | 2014-2017 | 2017+ |\n",
    "\n",
    "---\n",
    "\n",
    "### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Transformers –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö?\n",
    "\n",
    "**‚úÖ –•–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞—é—Ç –∫–æ–≥–¥–∞:**\n",
    "- –ë–æ–ª—å—à–∏–µ –¥–∞—Ç–∞—Å–µ—Ç—ã (>10k samples)\n",
    "- –ú–Ω–æ–≥–æ categorical features\n",
    "- –°–ª–æ–∂–Ω—ã–µ feature interactions\n",
    "- –ù—É–∂–Ω–∞ interpretability\n",
    "- –î–æ—Å—Ç—É–ø–µ–Ω pre-training –Ω–∞ –ø–æ—Ö–æ–∂–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "**‚ùå –ú–æ–≥—É—Ç —É—Å—Ç—É–ø–∞—Ç—å tree-based –º–µ—Ç–æ–¥–∞–º –∫–æ–≥–¥–∞:**\n",
    "- –ú–∞–ª–µ–Ω—å–∫–∏–µ –¥–∞—Ç–∞—Å–µ—Ç—ã (<1k samples)\n",
    "- –ü—Ä–æ—Å—Ç—ã–µ feature interactions\n",
    "- –¢–æ–ª—å–∫–æ numerical features\n",
    "- –ù—É–∂–Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –±–µ–∑ —Ç—é–Ω–∏–Ω–≥–∞\n",
    "\n",
    "---\n",
    "\n",
    "### –ß—Ç–æ –¥–∞–ª—å—à–µ?\n",
    "\n",
    "**Phase 4, Step 2: TabTransformer**\n",
    "- –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ embeddings –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö features\n",
    "- Contextual embeddings —á–µ—Ä–µ–∑ Transformer\n",
    "- –õ—É—á—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "**Phase 4, Step 3: Advanced Architectures**\n",
    "- FT-Transformer (Feature Tokenizer)\n",
    "- SAINT (Self-Attention and Intersample Attention)\n",
    "- TabNet (attention-based —Ç–∞–±–ª–∏—á–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞)\n",
    "- Temporal Fusion Transformer (–¥–ª—è time series)\n",
    "\n",
    "**Phase 5: Transfer Learning & Pre-training**\n",
    "- Pre-training –Ω–∞ –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö\n",
    "- Fine-tuning –Ω–∞ —Ü–µ–ª–µ–≤–æ–π –∑–∞–¥–∞—á–µ\n",
    "- Self-supervised learning\n",
    "\n",
    "---\n",
    "\n",
    "### –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã\n",
    "\n",
    "**Papers:**\n",
    "- \"Attention is All You Need\" (Vaswani et al., 2017) - –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç—å—è\n",
    "- \"TabTransformer\" (Huang et al., 2020) - Transformer –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "- \"Revisiting Deep Learning Models for Tabular Data\" (Gorishniy et al., 2021)\n",
    "\n",
    "**Tutorials:**\n",
    "- The Illustrated Transformer (Jay Alammar)\n",
    "- Annotated Transformer (Harvard NLP)\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ –ü–æ–∑–¥—Ä–∞–≤–ª—è–µ–º!\n",
    "\n",
    "–í—ã –æ—Å–≤–æ–∏–ª–∏ –æ—Å–Ω–æ–≤—ã Transformer –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã!\n",
    "\n",
    "–¢–µ–ø–µ—Ä—å –≤—ã –ø–æ–Ω–∏–º–∞–µ—Ç–µ:\n",
    "- üß† –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç Self-Attention\n",
    "- üîÆ –ü–æ—á–µ–º—É Transformers —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–ª–∏ ML\n",
    "- üìä –ö–∞–∫ –ø—Ä–∏–º–µ–Ω—è—Ç—å Transformers –∫ —Ç–∞–±–ª–∏—á–Ω—ã–º –¥–∞–Ω–Ω—ã–º\n",
    "- üîç –ö–∞–∫ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ attention weights\n",
    "\n",
    "**–°–ª–µ–¥—É—é—â–∏–π —à–∞–≥:** TabTransformer –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º–∏ features!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
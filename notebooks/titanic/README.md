# –¢–∏—Ç–∞–Ω–∏–∫: –ü–æ—è—Å–Ω–µ–Ω–∏—è –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É, –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º

## üîß –¢–µ–∫—É—â–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ –∏—Ö —Ä–µ—à–µ–Ω–∏—è

### 1. SNS Dataset vs Kaggle Dataset

#### –¢–µ–∫—É—â–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:
```python
# cell-4
df = sns.load_dataset('titanic')
```

####  –ü—Ä–æ–±–ª–µ–º–∞:
- **SNS –¥–∞—Ç–∞—Å–µ—Ç** - —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è, –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–∞—è train+test —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
- **Kaggle –¥–∞—Ç–∞—Å–µ—Ç** - —Ä–µ–∞–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è:
  - `train.csv` (891 —Å—Ç—Ä–æ–∫–∞) - –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ —Å `Survived`
  - `test.csv` (418 —Å—Ç—Ä–æ–∫) - —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –ë–ï–ó `Survived` (–¥–ª—è submission)

#### ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:

```python
# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
import os

# –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å Kaggle –¥–∞—Ç–∞—Å–µ—Ç
try:
    # Kaggle –¥–∞—Ç–∞—Å–µ—Ç (–ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–æ)
    train_path = '../../datasets/titanic/train.csv'
    test_path = '../../datasets/titanic/test.csv'

    if os.path.exists(train_path):
        df_train = pd.read_csv(train_path)
        df_test_submission = pd.read_csv(test_path) if os.path.exists(test_path) else None

        df = df_train.copy()  # –ò—Å–ø–æ–ª—å–∑—É–µ–º train –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        df_original = df.copy()

        print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω Kaggle –¥–∞—Ç–∞—Å–µ—Ç")
        print(f"  Train: {df_train.shape}")
        if df_test_submission is not None:
            print(f"  Test (–¥–ª—è submission): {df_test_submission.shape}")
    else:
        raise FileNotFoundError

except (FileNotFoundError, Exception):
    # Fallback –Ω–∞ SNS –¥–∞—Ç–∞—Å–µ—Ç
    print("‚ö† Kaggle –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º seaborn")
    print("  –î–ª—è –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–π —Ä–∞–±–æ—Ç—ã –∑–∞–≥—Ä—É–∑–∏—Ç–µ train.csv –∏ test.csv —Å Kaggle")
    print("  Kaggle: https://www.kaggle.com/c/titanic/data")

    df = sns.load_dataset('titanic')
    df_original = df.copy()
    df_test_submission = None

# –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏–∏ (sns –∏—Å–ø–æ–ª—å–∑—É–µ—Ç lowercase)
if 'survived' in df.columns:
    df.rename(columns={
        'survived': 'Survived',
        'pclass': 'Pclass',
        'sex': 'Sex',
        'age': 'Age',
        'sibsp': 'SibSp',
        'parch': 'Parch',
        'fare': 'Fare',
        'embarked': 'Embarked'
    }, inplace=True)

print(f"\n–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {df.shape}")
print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫: {df.shape[0]}")
print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {df.shape[1]}")
```

---

### 2. Train/Validation/Test Split

#### –¢–µ–∫—É—â–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è (–ù–ï–ü–†–ê–í–ò–õ–¨–ù–û):
```python
# cell-43
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
```

#### ‚ùå –ü—Ä–æ–±–ª–µ–º—ã:
1. **–ù–µ—Ç validation set** - –ø–æ–¥–±–∏—Ä–∞–µ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞ test set
2. **Data leakage** - "–ø–æ–¥–≥–ª—è–¥—ã–≤–∞–µ–º" –≤ test –ø—Ä–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
3. **–ü–µ—Ä–µ–æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞** - –º–æ–¥–µ–ª—å –≤–∏–¥–µ–ª–∞ test –º–Ω–æ–≥–æ —Ä–∞–∑

#### ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:

```python
# –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
from sklearn.model_selection import train_test_split

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ (X) –∏ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é (y)
X = df_model.drop('Survived', axis=1)
y = df_model['Survived']

# –í–ê–ñ–ù–û: –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ Train/Validation/Test
# –®–∞–≥ 1: –û—Ç–¥–µ–ª—è–µ–º test set (15%) - —Ç—Ä–æ–≥–∞–µ–º –¢–û–õ–¨–ö–û –†–ê–ó –≤ –∫–æ–Ω—Ü–µ
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.15, random_state=42, stratify=y
)

# –®–∞–≥ 2: –î–µ–ª–∏–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –¥–∞–Ω–Ω—ã–µ –Ω–∞ Train (70%) –∏ Validation (15%)
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.176, random_state=42, stratify=y_temp  # 0.176 * 0.85 ‚âà 0.15
)

print("=" * 60)
print("–†–ê–ó–î–ï–õ–ï–ù–ò–ï –î–ê–ù–ù–´–•")
print("=" * 60)
print(f"–û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {X.shape[0]} —Å—Ç—Ä–æ–∫")
print(f"\nüìö Train set:      {X_train.shape[0]} —Å—Ç—Ä–æ–∫ ({X_train.shape[0]/X.shape[0]*100:.1f}%)")
print(f"   ‚îî‚îÄ –î–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π")
print(f"\nüéØ Validation set: {X_val.shape[0]} —Å—Ç—Ä–æ–∫ ({X_val.shape[0]/X.shape[0]*100:.1f}%)")
print(f"   ‚îî‚îÄ –î–ª—è –ø–æ–¥–±–æ—Ä–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏")
print(f"\nüîí Test set:       {X_test.shape[0]} —Å—Ç—Ä–æ–∫ ({X_test.shape[0]/X.shape[0]*100:.1f}%)")
print(f"   ‚îî‚îÄ –î–ª—è –§–ò–ù–ê–õ–¨–ù–û–ô –æ—Ü–µ–Ω–∫–∏ (—Ç—Ä–æ–≥–∞–µ–º –¢–û–õ–¨–ö–û –†–ê–ó!)")
print("=" * 60)

# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
print(f"\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
print(f"Train:      {y_train.value_counts(normalize=True).to_dict()}")
print(f"Validation: {y_val.value_counts(normalize=True).to_dict()}")
print(f"Test:       {y_test.value_counts(normalize=True).to_dict()}")
```

#### üéØ –ó–∞—á–µ–º –Ω—É–∂–Ω–∞ Validation –≤—ã–±–æ—Ä–∫–∞?

| –í—ã–±–æ—Ä–∫–∞ | –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ | –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º |
|---------|-----------|------------------|
| **Train** | –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π | –ö–∞–∂–¥–∞—è —ç–ø–æ—Ö–∞/–∏—Ç–µ—Ä–∞—Ü–∏—è |
| **Validation** | –ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, early stopping, –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ | –ú–Ω–æ–≥–æ —Ä–∞–∑ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ |
| **Test** | –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ | **–¢–û–õ–¨–ö–û –û–î–ò–ù –†–ê–ó** –≤ –∫–æ–Ω—Ü–µ! |

**–ë–µ–∑ validation**:
```
‚ùå Train ‚Üí –ø–æ–¥–±–∏—Ä–∞–µ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞ Test ‚Üí –≤—ã–±–∏—Ä–∞–µ–º –º–æ–¥–µ–ª—å –ø–æ Test
   = –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ Test! –ó–∞–≤—ã—à–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞!
```

**–° validation**:
```
‚úÖ Train ‚Üí –ø–æ–¥–±–∏—Ä–∞–µ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞ Validation ‚Üí –≤—ã–±–∏—Ä–∞–µ–º –º–æ–¥–µ–ª—å –ø–æ Validation
   ‚Üí Test –∏—Å–ø–æ–ª—å–∑—É–µ–º –û–î–ò–ù –†–ê–ó –¥–ª—è —á–µ—Å—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–∏
```

#### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ GridSearchCV:

```python
# –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û (—Å—Ç–∞—Ä—ã–π –∫–æ–¥):
grid_search.fit(X_train, y_train)  # CV –≤–Ω—É—Ç—Ä–∏ train
y_pred = grid_search.predict(X_test)  # –û—Ü–µ–Ω–∏–≤–∞–µ–º –Ω–∞ test - –ü–õ–û–•–û!

# –ü–†–ê–í–ò–õ–¨–ù–û (–Ω–æ–≤—ã–π –∫–æ–¥):
grid_search.fit(X_train, y_train)  # CV –≤–Ω—É—Ç—Ä–∏ train
y_pred_val = grid_search.predict(X_val)  # –û—Ü–µ–Ω–∏–≤–∞–µ–º –Ω–∞ validation - –•–û–†–û–®–û!

# Test –∏—Å–ø–æ–ª—å–∑—É–µ–º –¢–û–õ–¨–ö–û –≤ —Å–∞–º–æ–º –∫–æ–Ω—Ü–µ:
final_model = grid_search.best_estimator_
y_pred_test = final_model.predict(X_test)  # –¢–û–õ–¨–ö–û –†–ê–ó!
print(f"–§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ test: {accuracy_score(y_test, y_pred_test):.4f}")
```

---

### 3. Pickle vs Joblib

#### –¢–µ–∫—É—â–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è (–ù–ï–ü–†–ê–í–ò–õ–¨–ù–û):
```python
# cell-75
import pickle

with open('best_titanic_model.pkl', 'wb') as file:
    pickle.dump(best_rf, file)
```

#### ‚ùå –ü–æ—á–µ–º—É pickle –ø–ª–æ—Ö–æ –¥–ª—è scikit-learn?

| –ê—Å–ø–µ–∫—Ç | pickle | joblib |
|--------|--------|--------|
| **–°–∫–æ—Ä–æ—Å—Ç—å** | –ú–µ–¥–ª–µ–Ω–Ω—ã–π –Ω–∞ –±–æ–ª—å—à–∏—Ö numpy –º–∞—Å—Å–∏–≤–∞—Ö | ‚ö° –í 2-10 —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ |
| **–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞** | –ë–æ–ª—å—à–µ | üóúÔ∏è –õ—É—á—à–µ–µ —Å–∂–∞—Ç–∏–µ |
| **–ü–∞–º—è—Ç—å** | –ú–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–æ–±–ª–µ–º–∞ —Å –±–æ–ª—å—à–∏–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏ | –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ä–∞–±–æ—Ç–∞ |
| **Scikit-learn** | ‚ö†Ô∏è –ù–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è | ‚úÖ **–û—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è** |
| **–ú–µ–º–æ–∏–∑–∞—Ü–∏—è** | –ù–µ—Ç | –ï—Å—Ç—å (disk caching) |

#### ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:

```python
# –ò–º–ø–æ—Ä—Ç joblib
import joblib  # ‚Üê –û—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è scikit-learn

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
# –§–æ—Ä–º–∞—Ç .joblib —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –≤–º–µ—Å—Ç–æ .pkl
model_path = '../../models/best_titanic_model.joblib'
scaler_path = '../../models/titanic_scaler.joblib'

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å —Å–∂–∞—Ç–∏–µ–º (compress=3 - —Ö–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å)
joblib.dump(best_rf, model_path, compress=3)
joblib.dump(scaler, scaler_path, compress=3)

print("‚úÖ –ú–æ–¥–µ–ª—å –∏ scaler —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã —Å joblib!")
print(f"   –ú–æ–¥–µ–ª—å: {model_path}")
print(f"   Scaler: {scaler_path}")

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
loaded_model = joblib.load(model_path)
loaded_scaler = joblib.load(scaler_path)

print("\n‚úÖ –ú–æ–¥–µ–ª—å –∏ scaler —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!")
```

#### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:

```python
# Benchmark (–¥–ª—è –ø—Ä–∏–º–µ—Ä–∞)
import time
import numpy as np

# –ë–æ–ª—å—à–∞—è –º–æ–¥–µ–ª—å
X_large = np.random.randn(10000, 100)
y_large = np.random.randint(0, 2, 10000)
model_large = RandomForestClassifier(n_estimators=100).fit(X_large, y_large)

# pickle
start = time.time()
with open('model_pickle.pkl', 'wb') as f:
    pickle.dump(model_large, f)
pickle_time = time.time() - start
pickle_size = os.path.getsize('model_pickle.pkl') / 1024 / 1024  # MB

# joblib
start = time.time()
joblib.dump(model_large, 'model_joblib.joblib', compress=3)
joblib_time = time.time() - start
joblib_size = os.path.getsize('model_joblib.joblib') / 1024 / 1024  # MB

print(f"pickle:  {pickle_time:.3f}s, {pickle_size:.2f} MB")
print(f"joblib:  {joblib_time:.3f}s, {joblib_size:.2f} MB")
print(f"Speedup: {pickle_time/joblib_time:.1f}x faster")
print(f"Compression: {pickle_size/joblib_size:.1f}x smaller")
```

**–¢–∏–ø–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã**:
```
pickle:  2.341s, 45.23 MB
joblib:  0.832s, 18.67 MB
Speedup: 2.8x faster
Compression: 2.4x smaller
```

#### –ü–∞—Ä–∞–º–µ—Ç—Ä compress:

```python
# compress=0 - –±–µ–∑ —Å–∂–∞—Ç–∏—è (–±—ã—Å—Ç—Ä–µ–µ, –Ω–æ –±–æ–ª—å—à–µ)
# compress=1-9 - —É—Ä–æ–≤–µ–Ω—å —Å–∂–∞—Ç–∏—è (–≤—ã—à–µ = –º–µ–Ω—å—à–µ —Ä–∞–∑–º–µ—Ä, –º–µ–¥–ª–µ–Ω–Ω–µ–µ)
# compress=3 - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è (—Ö–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å)

joblib.dump(model, 'model.joblib', compress=3)
```

---

### 4. Jupyter Notebook Metadata

#### –í–æ–ø—Ä–æ—Å: "–ü–æ—á–µ–º—É –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –≤ –ª–æ–∫–∞–ª—å–Ω—ã–π Jupyter –¥–æ–±–∞–≤–∏–ª–∏—Å—å outputs –∏ metadata?"

#### ‚úÖ –≠—Ç–æ –ù–û–†–ú–ê–õ–¨–ù–û –∏ –ü–†–ê–í–ò–õ–¨–ù–û!

#### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ Jupyter Notebook:

```json
{
  "cells": [
    {
      "cell_type": "code",
      "source": "print('Hello, World!')",

      "execution_count": 1,        ‚Üê –ü–æ—Ä—è–¥–æ–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —è—á–µ–π–∫–∏

      "outputs": [                 ‚Üê –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        {
          "output_type": "stream",
          "text": "Hello, World!\n"
        }
      ],

      "metadata": {                ‚Üê –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —è—á–µ–π–∫–∏
        "collapsed": false,
        "scrolled": true
      }
    }
  ],

  "metadata": {                    ‚Üê –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –Ω–æ—É—Ç–±—É–∫–∞
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    }
  },

  "nbformat": 4,
  "nbformat_minor": 4
}
```

#### –¢—Ä–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è notebook:

| –°–æ—Å—Ç–æ—è–Ω–∏–µ | outputs | execution_count | –ö–æ–≥–¥–∞ |
|-----------|---------|-----------------|-------|
| **–ß–∏—Å—Ç—ã–π** | `[]` | `null` | –ü–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ |
| **–í—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã–π** | `[{...}]` | `1, 2, 3...` | –ü–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞ –≤ Jupyter |
| **Cleared** | `[]` | `null` | –ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ |

#### –ü–æ—á–µ–º—É —É –º–µ–Ω—è (Claude) –Ω–µ –±—ã–ª–æ outputs?

–Ø —Å–æ–∑–¥–∞–ª notebook **–ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ** - —ç—Ç–æ "—á–∏—Å—Ç—ã–π" notebook –±–µ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è.

–ö–æ–≥–¥–∞ –í–´ –∑–∞–ø—É—Å–∫–∞–µ—Ç–µ –µ–≥–æ –≤ Jupyter, –æ–Ω **–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–æ–±–∞–≤–ª—è–µ—Ç**:
- `execution_count` - –Ω–æ–º–µ—Ä –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —è—á–µ–π–∫–∏
- `outputs` - —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (print, –≥—Ä–∞—Ñ–∏–∫–∏, —Ç–∞–±–ª–∏—Ü—ã)
- `metadata` - –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è

#### Best Practices –¥–ª—è Git:

**–ü—Ä–æ–±–ª–µ–º–∞**: Outputs –º–æ–≥—É—Ç –±—ã—Ç—å –æ–≥—Ä–æ–º–Ω—ã–º–∏ (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –±–æ–ª—å—à–∏–µ DataFrame)

```json
{
  "outputs": [{
    "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABkAAAASwCAYAAABm..." // 50+ KB Base64
    }
  }]
}
```

**–†–µ—à–µ–Ω–∏–µ**: –û—á–∏—Å—Ç–∏—Ç—å outputs –ø–µ—Ä–µ–¥ –∫–æ–º–º–∏—Ç–æ–º

#### 1. –†—É—á–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –≤ Jupyter:
```
Cell ‚Üí All Output ‚Üí Clear
```

#### 2. –ß–µ—Ä–µ–∑ nbconvert:
```bash
# –û—á–∏—Å—Ç–∏—Ç—å outputs
jupyter nbconvert --clear-output --inplace notebook.ipynb
```

#### 3. –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å nbstripout:
```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞
pip install nbstripout

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è git (–æ–¥–∏–Ω —Ä–∞–∑)
nbstripout --install

# –¢–µ–ø–µ—Ä—å –ø—Ä–∏ git add –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—á–∏—â–∞—é—Ç—Å—è outputs
git add notebook.ipynb  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—á–∏—Å—Ç–∏—Ç outputs
```

#### 4. Pre-commit hook (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è):

```bash
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/kynan/nbstripout
    rev: 0.6.1
    hooks:
      - id: nbstripout
```

#### –ß—Ç–æ —Ö—Ä–∞–Ω–∏—Ç—å –≤ Git?

‚úÖ **–•—Ä–∞–Ω–∏—Ç—å**:
- –ö–æ–¥ —è—á–µ–µ–∫
- Markdown —Ç–µ–∫—Å—Ç
- –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –Ω–æ—É—Ç–±—É–∫–∞ (kernelspec, language_info)

‚ùå **–ù–ï —Ö—Ä–∞–Ω–∏—Ç—å** (–æ—á–∏—â–∞—Ç—å):
- `outputs` - —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
- `execution_count` - –Ω–æ–º–µ—Ä–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
- –ë–æ–ª—å—à–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ outputs

#### –ü—Ä–∏–º–µ—Ä workflow:

```bash
# 1. –†–∞–±–æ—Ç–∞–µ—Ç–µ –≤ Jupyter - –≤—ã–ø–æ–ª–Ω—è–µ—Ç–µ —è—á–µ–π–∫–∏
jupyter notebook analysis.ipynb

# 2. –ü–µ—Ä–µ–¥ –∫–æ–º–º–∏—Ç–æ–º - –æ—á–∏—â–∞–µ—Ç–µ outputs
jupyter nbconvert --clear-output --inplace analysis.ipynb

# 3. –ö–æ–º–º–∏—Ç–∏—Ç–µ —á–∏—Å—Ç—ã–π notebook
git add analysis.ipynb
git commit -m "Add analysis notebook"

# 4. –ü–æ—Å–ª–µ pull - –∑–∞–ø—É—Å–∫–∞–µ—Ç–µ –∑–∞–Ω–æ–≤–æ
jupyter notebook analysis.ipynb
# –ó–∞–ø—É—Å–∫–∞–µ—Ç–µ –≤—Å–µ —è—á–µ–π–∫–∏: Cell ‚Üí Run All
```

---

## üìù –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

### 1. Kaggle Dataset
- ‚úÖ –î–æ–±–∞–≤–∏—Ç—å –∑–∞–≥—Ä—É–∑–∫—É —Ä–µ–∞–ª—å–Ω—ã—Ö `train.csv` –∏ `test.csv`
- ‚úÖ Fallback –Ω–∞ sns.load_dataset –µ—Å–ª–∏ —Ñ–∞–π–ª–æ–≤ –Ω–µ—Ç
- ‚úÖ –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫

### 2. Train/Val/Test Split
- ‚úÖ –î–æ–±–∞–≤–∏—Ç—å –æ—Ç–¥–µ–ª—å–Ω—É—é validation –≤—ã–±–æ—Ä–∫—É (15%)
- ‚úÖ Test –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¢–û–õ–¨–ö–û –†–ê–ó –≤ –∫–æ–Ω—Ü–µ
- ‚úÖ Validation –¥–ª—è –ø–æ–¥–±–æ—Ä–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

### 3. Joblib –≤–º–µ—Å—Ç–æ Pickle
- ‚úÖ –ó–∞–º–µ–Ω–∏—Ç—å `pickle.dump` –Ω–∞ `joblib.dump`
- ‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `.joblib` —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
- ‚úÖ –î–æ–±–∞–≤–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä `compress=3`

### 4. Notebook Metadata
- ‚úÖ –ü–æ–Ω–∏–º–∞—Ç—å, —á—Ç–æ outputs - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ
- ‚úÖ –û—á–∏—â–∞—Ç—å outputs –ø–µ—Ä–µ–¥ git commit
- ‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å nbstripout –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏

---

## üîó –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏

- [Kaggle Titanic Competition](https://www.kaggle.com/c/titanic)
- [Scikit-learn: Model Persistence](https://scikit-learn.org/stable/model_persistence.html)
- [Joblib Documentation](https://joblib.readthedocs.io/)
- [nbstripout](https://github.com/kynan/nbstripout)
- [Jupyter Best Practices](https://jupyter-notebook.readthedocs.io/en/stable/)

---

**–ê–≤—Ç–æ—Ä**: Claude Code
**–î–∞—Ç–∞**: 2025

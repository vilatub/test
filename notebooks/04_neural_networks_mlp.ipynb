{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤—ã–∂–∏–≤–∞–µ–º–æ—Å—Ç–∏ –Ω–∞ –¢–∏—Ç–∞–Ω–∏–∫–µ\n",
    "\n",
    "–í —ç—Ç–æ–º –Ω–æ—É—Ç–±—É–∫–µ —Ä–µ–∞–ª–∏–∑—É–µ–º **–ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ (Multi-Layer Perceptron)** —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º TensorFlow/Keras.\n",
    "\n",
    "## –ß—Ç–æ —Ç–∞–∫–æ–µ MLP?\n",
    "\n",
    "**Multi-Layer Perceptron (MLP)** - –±–∞–∑–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π, —Å–æ—Å—Ç–æ—è—â–∞—è –∏–∑:\n",
    "- **–í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π** - –ø–æ–ª—É—á–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "- **–°–∫—Ä—ã—Ç—ã–µ —Å–ª–æ–∏** - –∏–∑–≤–ª–µ–∫–∞—é—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã\n",
    "- **–í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π** - –¥–µ–ª–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
    "\n",
    "## –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ\n",
    "1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π\n",
    "2. –ë–∞–∑–æ–≤–∞—è MLP –º–æ–¥–µ–ª—å\n",
    "3. MLP —Å Batch Normalization\n",
    "4. MLP —Å Dropout —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π\n",
    "5. –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞\n",
    "6. Learning Rate Scheduling\n",
    "7. Early Stopping –∏ Callbacks\n",
    "8. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–º –±—É—Å—Ç–∏–Ω–≥–æ–º\n",
    "9. –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫\n",
    "10. –í—ã–≤–æ–¥—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow –∏ Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
    "\n",
    "# –£—Ç–∏–ª–∏—Ç—ã\n",
    "import joblib\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print('‚úÖ –í—Å–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!')\n",
    "print(f'TensorFlow –≤–µ—Ä—Å–∏—è: {tf.__version__}')\n",
    "print(f'GPU –¥–æ—Å—Ç—É–ø–µ–Ω: {tf.config.list_physical_devices(\"GPU\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "–î–ª—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–∞ **–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "df = sns.load_dataset('titanic')\n",
    "data = df.copy()\n",
    "\n",
    "# Feature Engineering\n",
    "data['family_size'] = data['sibsp'] + data['parch'] + 1\n",
    "data['is_alone'] = (data['family_size'] == 1).astype(int)\n",
    "\n",
    "# –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤\n",
    "data['age'].fillna(data['age'].median(), inplace=True)\n",
    "data['fare'].fillna(data['fare'].median(), inplace=True)\n",
    "data['embarked'].fillna(data['embarked'].mode()[0], inplace=True)\n",
    "data['deck'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', \n",
    "            'embarked', 'class', 'who', 'adult_male', 'deck', \n",
    "            'alone', 'family_size', 'is_alone']\n",
    "\n",
    "target = 'survived'\n",
    "\n",
    "# –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "le = LabelEncoder()\n",
    "categorical_features = ['sex', 'embarked', 'class', 'who', 'deck']\n",
    "\n",
    "X = data[features].copy()\n",
    "for col in categorical_features:\n",
    "    if col in X.columns:\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "\n",
    "y = data[target].values\n",
    "\n",
    "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/validation/test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.176, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f'‚úÖ –î–∞–Ω–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã!')\n",
    "print(f'Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π\n",
    "\n",
    "–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –ª—É—á—à–µ —Å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (StandardScaler)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print('‚úÖ –î–∞–Ω–Ω—ã–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã!')\n",
    "print(f'\\n–°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏:')\n",
    "print(f'Train mean: {X_train_scaled.mean(axis=0)[:5]}')\n",
    "print(f'Train std: {X_train_scaled.std(axis=0)[:5]}')\n",
    "\n",
    "# –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–∞\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "print(f'\\n–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–∞: {input_dim}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. –ë–∞–∑–æ–≤–∞—è MLP –º–æ–¥–µ–ª—å\n",
    "\n",
    "–ù–∞—á–Ω–µ–º —Å –ø—Ä–æ—Å—Ç–æ–π –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–æ–π —Å–µ—Ç–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏\n",
    "def create_baseline_model(input_dim):\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_dim=input_dim),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "model_baseline = create_baseline_model(input_dim)\n",
    "\n",
    "# –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏\n",
    "print('=== –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ ===')\n",
    "model_baseline.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "print('\\n–û–±—É—á–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å...')\n",
    "\n",
    "history_baseline = model_baseline.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print('‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history_baseline.history['loss'], label='Train Loss')\n",
    "axes[0].plot(history_baseline.history['val_loss'], label='Val Loss')\n",
    "axes[0].set_title('Loss –ø–æ —ç–ø–æ—Ö–∞–º')\n",
    "axes[0].set_xlabel('–≠–ø–æ—Ö–∞')\n",
    "axes[0].set_ylabel('Binary Crossentropy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history_baseline.history['accuracy'], label='Train Accuracy')\n",
    "axes[1].plot(history_baseline.history['val_accuracy'], label='Val Accuracy')\n",
    "axes[1].set_title('Accuracy –ø–æ —ç–ø–æ—Ö–∞–º')\n",
    "axes[1].set_xlabel('–≠–ø–æ—Ö–∞')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û—Ü–µ–Ω–∫–∞ –Ω–∞ validation set\n",
    "y_pred_baseline = (model_baseline.predict(X_val_scaled) > 0.5).astype(int).flatten()\n",
    "y_proba_baseline = model_baseline.predict(X_val_scaled).flatten()\n",
    "\n",
    "print('=== –ë–∞–∑–æ–≤–∞—è MLP –º–æ–¥–µ–ª—å - Validation Set ===')\n",
    "print(f'Accuracy: {accuracy_score(y_val, y_pred_baseline):.4f}')\n",
    "print(f'Precision: {precision_score(y_val, y_pred_baseline):.4f}')\n",
    "print(f'Recall: {recall_score(y_val, y_pred_baseline):.4f}')\n",
    "print(f'F1-Score: {f1_score(y_val, y_pred_baseline):.4f}')\n",
    "print(f'ROC-AUC: {roc_auc_score(y_val, y_proba_baseline):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MLP —Å Batch Normalization\n",
    "\n",
    "**Batch Normalization** —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –∏ —É—Å–∫–æ—Ä—è–µ—Ç —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ú–æ–¥–µ–ª—å —Å Batch Normalization\n",
    "def create_bn_model(input_dim):\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(128, input_dim=input_dim),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        \n",
    "        layers.Dense(64),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        \n",
    "        layers.Dense(32),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        \n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "model_bn = create_bn_model(input_dim)\n",
    "print('=== –ú–æ–¥–µ–ª—å —Å Batch Normalization ===')\n",
    "model_bn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–±—É—á–µ–Ω–∏–µ\n",
    "history_bn = model_bn.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# –û—Ü–µ–Ω–∫–∞\n",
    "y_pred_bn = (model_bn.predict(X_val_scaled) > 0.5).astype(int).flatten()\n",
    "y_proba_bn = model_bn.predict(X_val_scaled).flatten()\n",
    "\n",
    "print('=== MLP —Å Batch Normalization ===')\n",
    "print(f'Accuracy: {accuracy_score(y_val, y_pred_bn):.4f}')\n",
    "print(f'ROC-AUC: {roc_auc_score(y_val, y_proba_bn):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MLP —Å Dropout —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π\n",
    "\n",
    "**Dropout** —Å–ª—É—á–∞–π–Ω–æ –æ—Ç–∫–ª—é—á–∞–µ—Ç –Ω–µ–π—Ä–æ–Ω—ã –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è, –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ú–æ–¥–µ–ª—å —Å Dropout\n",
    "def create_dropout_model(input_dim, dropout_rate=0.3):\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_dim=input_dim),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        \n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        \n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        \n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "model_dropout = create_dropout_model(input_dim, dropout_rate=0.3)\n",
    "print('=== –ú–æ–¥–µ–ª—å —Å Dropout ===')\n",
    "model_dropout.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–±—É—á–µ–Ω–∏–µ\n",
    "history_dropout = model_dropout.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# –û—Ü–µ–Ω–∫–∞\n",
    "y_pred_dropout = (model_dropout.predict(X_val_scaled) > 0.5).astype(int).flatten()\n",
    "y_proba_dropout = model_dropout.predict(X_val_scaled).flatten()\n",
    "\n",
    "print('=== MLP —Å Dropout ===')\n",
    "print(f'Accuracy: {accuracy_score(y_val, y_pred_dropout):.4f}')\n",
    "print(f'ROC-AUC: {roc_auc_score(y_val, y_proba_dropout):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞\n",
    "\n",
    "–ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –≤—Å–µ —Ç–µ—Ö–Ω–∏–∫–∏: Batch Normalization + Dropout + –†–∞–∑–Ω—ã–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –º–æ–¥–µ–ª—å\n",
    "def create_advanced_model(input_dim):\n",
    "    model = models.Sequential([\n",
    "        # –°–ª–æ–π 1\n",
    "        layers.Dense(256, input_dim=input_dim),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # –°–ª–æ–π 2\n",
    "        layers.Dense(128),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # –°–ª–æ–π 3\n",
    "        layers.Dense(64),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        # –°–ª–æ–π 4\n",
    "        layers.Dense(32),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º Adam —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º learning rate\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', \n",
    "                 tf.keras.metrics.Precision(name='precision'),\n",
    "                 tf.keras.metrics.Recall(name='recall'),\n",
    "                 tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "model_advanced = create_advanced_model(input_dim)\n",
    "print('=== –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –º–æ–¥–µ–ª—å ===')\n",
    "model_advanced.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Callbacks: Early Stopping –∏ Model Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞–µ–º callbacks\n",
    "import os\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Early Stopping - –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ —É–ª—É—á—à–µ–Ω–∏—è\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Model Checkpoint - —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ª—É—á—à—É—é –º–æ–¥–µ–ª—å\n",
    "checkpoint = callbacks.ModelCheckpoint(\n",
    "    '../models/best_mlp_model.h5',\n",
    "    monitor='val_auc',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ReduceLROnPlateau - —É–º–µ–Ω—å—à–∞–µ—Ç learning rate –ø—Ä–∏ –∑–∞—Å—Ç–æ–µ\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print('‚úÖ Callbacks –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–±—É—á–µ–Ω–∏–µ —Å callbacks\n",
    "print('–û–±—É—á–∞–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—É—é –º–æ–¥–µ–ª—å —Å callbacks...')\n",
    "\n",
    "history_advanced = model_advanced.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop, checkpoint, reduce_lr],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print('\\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!')\n",
    "print(f'–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö: {len(history_advanced.history[\"loss\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –º–æ–¥–µ–ª–∏\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history_advanced.history['loss'], label='Train Loss', alpha=0.7)\n",
    "axes[0, 0].plot(history_advanced.history['val_loss'], label='Val Loss', alpha=0.7)\n",
    "axes[0, 0].set_title('Loss', fontsize=12)\n",
    "axes[0, 0].set_xlabel('–≠–ø–æ—Ö–∞')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 1].plot(history_advanced.history['accuracy'], label='Train Acc', alpha=0.7)\n",
    "axes[0, 1].plot(history_advanced.history['val_accuracy'], label='Val Acc', alpha=0.7)\n",
    "axes[0, 1].set_title('Accuracy', fontsize=12)\n",
    "axes[0, 1].set_xlabel('–≠–ø–æ—Ö–∞')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# AUC\n",
    "axes[1, 0].plot(history_advanced.history['auc'], label='Train AUC', alpha=0.7)\n",
    "axes[1, 0].plot(history_advanced.history['val_auc'], label='Val AUC', alpha=0.7)\n",
    "axes[1, 0].set_title('ROC-AUC', fontsize=12)\n",
    "axes[1, 0].set_xlabel('–≠–ø–æ—Ö–∞')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision & Recall\n",
    "axes[1, 1].plot(history_advanced.history['precision'], label='Train Precision', alpha=0.7)\n",
    "axes[1, 1].plot(history_advanced.history['val_precision'], label='Val Precision', alpha=0.7)\n",
    "axes[1, 1].plot(history_advanced.history['recall'], label='Train Recall', alpha=0.7, linestyle='--')\n",
    "axes[1, 1].plot(history_advanced.history['val_recall'], label='Val Recall', alpha=0.7, linestyle='--')\n",
    "axes[1, 1].set_title('Precision & Recall', fontsize=12)\n",
    "axes[1, 1].set_xlabel('–≠–ø–æ—Ö–∞')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –º–æ–¥–µ–ª–∏\n",
    "y_pred_advanced = (model_advanced.predict(X_val_scaled) > 0.5).astype(int).flatten()\n",
    "y_proba_advanced = model_advanced.predict(X_val_scaled).flatten()\n",
    "\n",
    "print('=== –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è MLP –º–æ–¥–µ–ª—å - Validation Set ===')\n",
    "print(f'Accuracy: {accuracy_score(y_val, y_pred_advanced):.4f}')\n",
    "print(f'Precision: {precision_score(y_val, y_pred_advanced):.4f}')\n",
    "print(f'Recall: {recall_score(y_val, y_pred_advanced):.4f}')\n",
    "print(f'F1-Score: {f1_score(y_val, y_pred_advanced):.4f}')\n",
    "print(f'ROC-AUC: {roc_auc_score(y_val, y_proba_advanced):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "nn_results = pd.DataFrame({\n",
    "    'Model': ['Baseline MLP', 'MLP + BatchNorm', 'MLP + Dropout', 'Advanced MLP'],\n",
    "    'Accuracy': [\n",
    "        accuracy_score(y_val, y_pred_baseline),\n",
    "        accuracy_score(y_val, y_pred_bn),\n",
    "        accuracy_score(y_val, y_pred_dropout),\n",
    "        accuracy_score(y_val, y_pred_advanced)\n",
    "    ],\n",
    "    'Precision': [\n",
    "        precision_score(y_val, y_pred_baseline),\n",
    "        precision_score(y_val, y_pred_bn),\n",
    "        precision_score(y_val, y_pred_dropout),\n",
    "        precision_score(y_val, y_pred_advanced)\n",
    "    ],\n",
    "    'Recall': [\n",
    "        recall_score(y_val, y_pred_baseline),\n",
    "        recall_score(y_val, y_pred_bn),\n",
    "        recall_score(y_val, y_pred_dropout),\n",
    "        recall_score(y_val, y_pred_advanced)\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        f1_score(y_val, y_pred_baseline),\n",
    "        f1_score(y_val, y_pred_bn),\n",
    "        f1_score(y_val, y_pred_dropout),\n",
    "        f1_score(y_val, y_pred_advanced)\n",
    "    ],\n",
    "    'ROC-AUC': [\n",
    "        roc_auc_score(y_val, y_proba_baseline),\n",
    "        roc_auc_score(y_val, y_proba_bn),\n",
    "        roc_auc_score(y_val, y_proba_dropout),\n",
    "        roc_auc_score(y_val, y_proba_advanced)\n",
    "    ]\n",
    "})\n",
    "\n",
    "nn_results = nn_results.sort_values('ROC-AUC', ascending=False)\n",
    "print('\\n=== –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π ===')\n",
    "print(nn_results.to_string(index=False))\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(nn_results))\n",
    "width = 0.15\n",
    "\n",
    "ax.bar(x - 2*width, nn_results['Accuracy'], width, label='Accuracy')\n",
    "ax.bar(x - width, nn_results['Precision'], width, label='Precision')\n",
    "ax.bar(x, nn_results['Recall'], width, label='Recall')\n",
    "ax.bar(x + width, nn_results['F1-Score'], width, label='F1-Score')\n",
    "ax.bar(x + 2*width, nn_results['ROC-AUC'], width, label='ROC-AUC')\n",
    "\n",
    "ax.set_xlabel('–ú–æ–¥–µ–ª—å', fontsize=12)\n",
    "ax.set_ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏', fontsize=12)\n",
    "ax.set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º', fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(nn_results['Model'])\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å (Advanced MLP)\n",
    "y_pred_test = (model_advanced.predict(X_test_scaled) > 0.5).astype(int).flatten()\n",
    "y_proba_test = model_advanced.predict(X_test_scaled).flatten()\n",
    "\n",
    "print('=== –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ Test Set (Advanced MLP) ===')\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred_test):.4f}')\n",
    "print(f'Precision: {precision_score(y_test, y_pred_test):.4f}')\n",
    "print(f'Recall: {recall_score(y_test, y_pred_test):.4f}')\n",
    "print(f'F1-Score: {f1_score(y_test, y_pred_test):.4f}')\n",
    "print(f'ROC-AUC: {roc_auc_score(y_test, y_proba_test):.4f}')\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix - Test Set (Advanced MLP)', fontsize=14)\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred_test, target_names=['Not Survived', 'Survived']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ scaler\n",
    "model_advanced.save('../models/advanced_mlp_model.h5')\n",
    "joblib.dump(scaler, '../models/scaler.pkl')\n",
    "\n",
    "print('‚úÖ –ú–æ–¥–µ–ª—å –∏ scaler —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!')\n",
    "print('–§–∞–π–ª—ã:')\n",
    "print('  - ../models/advanced_mlp_model.h5')\n",
    "print('  - ../models/scaler.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. –í—ã–≤–æ–¥—ã\n",
    "\n",
    "### üß† –ß—Ç–æ –º—ã —É–∑–Ω–∞–ª–∏ –æ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö:\n",
    "\n",
    "1. **–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫—Ä–∏—Ç–∏—á–Ω–∞** - –±–µ–∑ StandardScaler –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ —Å—Ö–æ–¥—è—Ç—Å—è —Ö—É–∂–µ\n",
    "\n",
    "2. **Batch Normalization –ø–æ–º–æ–≥–∞–µ—Ç**:\n",
    "   - –°—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ\n",
    "   - –ü–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª—å—à–∏–π learning rate\n",
    "   - –ù–µ–º–Ω–æ–≥–æ —É–ª—É—á—à–∞–µ—Ç accuracy\n",
    "\n",
    "3. **Dropout –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ**:\n",
    "   - –ö—Ä–∏—Ç–∏—á–µ–Ω –¥–ª—è –º–∞–ª—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\n",
    "   - –û–±—ã—á–Ω–æ 0.2-0.5 –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ\n",
    "   - –£–ª—É—á—à–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—é\n",
    "\n",
    "4. **Callbacks —ç–∫–æ–Ω–æ–º—è—Ç –≤—Ä–µ–º—è**:\n",
    "   - Early Stopping –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ\n",
    "   - ReduceLROnPlateau –∞–¥–∞–ø—Ç–∏–≤–Ω–æ –ø–æ–¥—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç learning rate\n",
    "   - ModelCheckpoint —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ª—É—á—à—É—é –º–æ–¥–µ–ª—å\n",
    "\n",
    "5. **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏–º–µ–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ**:\n",
    "   - –°–ª–∏—à–∫–æ–º –ø—Ä–æ—Å—Ç–∞—è ‚Üí underfitting\n",
    "   - –°–ª–∏—à–∫–æ–º —Å–ª–æ–∂–Ω–∞—è ‚Üí overfitting\n",
    "   - –î–ª—è –¢–∏—Ç–∞–Ω–∏–∫–∞ 2-4 —Å–ª–æ—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã\n",
    "\n",
    "### üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–º –±—É—Å—Ç–∏–Ω–≥–æ–º:\n",
    "\n",
    "**–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏**:\n",
    "- ‚úÖ –ì–∏–±–∫–∏–µ, –º–æ–≥—É—Ç –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏\n",
    "- ‚úÖ –•–æ—Ä–æ—à–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "- ‚ùå –¢—Ä–µ–±—É—é—Ç –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö\n",
    "- ‚ùå –°–ª–æ–∂–Ω–µ–µ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å\n",
    "- ‚ùå –î–æ–ª—å—à–µ –æ–±—É—á–∞—é—Ç—Å—è\n",
    "- ‚ùå –ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã –∫ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º\n",
    "\n",
    "**–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥ (XGBoost/LightGBM)**:\n",
    "- ‚úÖ –û—Ç–ª–∏—á–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "- ‚úÖ –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ\n",
    "- ‚úÖ –ú–µ–Ω—å—à–µ —Ç—Ä–µ–±—É–µ—Ç —Ç—é–Ω–∏–Ω–≥–∞\n",
    "- ‚úÖ –õ–µ–≥—á–µ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å (SHAP)\n",
    "- ‚ùå –ú–µ–Ω–µ–µ –≥–∏–±–∫–∏–π –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤\n",
    "\n",
    "### üéØ –î–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –¢–∏—Ç–∞–Ω–∏–∫–∞:\n",
    "\n",
    "**–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥ –ª—É—á—à–µ**, –ø–æ—Ç–æ–º—É —á—Ç–æ:\n",
    "- –ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö (~900 –ø—Ä–∏–º–µ—Ä–æ–≤)\n",
    "- –¢–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏\n",
    "- –ü—Ä–æ—Å—Ç—ã–µ –ª–∏–Ω–µ–π–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏\n",
    "\n",
    "**–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –±—É–¥—É—Ç –ª—É—á—à–µ** –ø—Ä–∏:\n",
    "- –ë–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö (>10,000 –ø—Ä–∏–º–µ—Ä–æ–≤)\n",
    "- –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö, —Ç–µ–∫—Å—Ç–∞—Ö, –∑–≤—É–∫–µ\n",
    "- –°–ª–æ–∂–Ω—ã—Ö –Ω–µ–ª–∏–Ω–µ–π–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è—Ö\n",
    "\n",
    "### üí° –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:\n",
    "\n",
    "- –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã —Å LSTM (–§–∞–∑–∞ 3)\n",
    "- REST API –¥–ª—è –º–æ–¥–µ–ª–∏ (–§–∞–∑–∞ 7)\n",
    "- –ë–æ–ª–µ–µ –≥–ª—É–±–æ–∫–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã\n",
    "- Ensemble –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –∏ –±—É—Å—Ç–∏–Ω–≥–∞"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

#!/usr/bin/env python3
"""
–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –Ω–æ—É—Ç–±—É–∫–∞: Advanced RNN - Attention & Seq2Seq
Phase 3, Step 3: FINALE - –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
"""

import json

# –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –Ω–æ—É—Ç–±—É–∫–∞
notebook = {
    "cells": [],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

cells = []

# ============================================================================
# TITLE AND INTRODUCTION
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "# üéØ Advanced RNN: Attention & Seq2Seq\n",
        "\n",
        "**Phase 3: Temporal Data & RNN - Step 3 (FINALE)**\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ –¶–µ–ª–∏ –Ω–æ—É—Ç–±—É–∫–∞\n",
        "\n",
        "1. **Attention –º–µ—Ö–∞–Ω–∏–∑–º** –¥–ª—è RNN - —Ñ–æ–∫—É—Å –Ω–∞ –≤–∞–∂–Ω—ã—Ö —á–∞—Å—Ç—è—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
        "2. **Seq2Seq –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞** (Encoder-Decoder) –¥–ª—è multi-step forecasting\n",
        "3. **–ü—Ä–∞–∫—Ç–∏–∫–∞ –Ω–∞ PyTorch:** –ø–æ–ª–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å –Ω—É–ª—è\n",
        "4. **Multi-step ahead:** –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —à–∞–≥–æ–≤ –≤–ø–µ—Ä–µ–¥\n",
        "5. **–°—Ä–∞–≤–Ω–µ–Ω–∏–µ:** Baseline LSTM vs LSTM+Attention vs Seq2Seq\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ –ü–æ—á–µ–º—É Attention –∏ Seq2Seq?\n",
        "\n",
        "**–ü—Ä–æ–±–ª–µ–º—ã –±–∞–∑–æ–≤–æ–≥–æ LSTM:**\n",
        "- ‚ùå –í—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–∂–∏–º–∞–µ—Ç—Å—è –≤ –æ–¥–∏–Ω –≤–µ–∫—Ç–æ—Ä (bottleneck)\n",
        "- ‚ùå –°–ª–æ–∂–Ω–æ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π (–¥–∞–∂–µ —Å LSTM)\n",
        "- ‚ùå –û–¥–∏–Ω–∞–∫–æ–≤–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –∫–æ –≤—Å–µ–º —á–∞—Å—Ç—è–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
        "\n",
        "**–†–µ—à–µ–Ω–∏—è:**\n",
        "\n",
        "### üîç Attention Mechanism\n",
        "- ‚úÖ **–§–æ–∫—É—Å –Ω–∞ –≤–∞–∂–Ω–æ–º:** –º–æ–¥–µ–ª—å \"—Å–º–æ—Ç—Ä–∏—Ç\" –Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞—Å—Ç–∏ –≤—Ö–æ–¥–∞\n",
        "- ‚úÖ **–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –≤–µ—Å–∞:** –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ä–∞–∑–Ω—ã–µ —á–∞—Å—Ç–∏ –≤–∞–∂–Ω—ã\n",
        "- ‚úÖ **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å:** –≤–∏–¥–∏–º, –∫—É–¥–∞ –º–æ–¥–µ–ª—å \"—Å–º–æ—Ç—Ä–∏—Ç\"\n",
        "\n",
        "### üîÑ Seq2Seq (Sequence-to-Sequence)\n",
        "- ‚úÖ **Multi-step forecasting:** –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤ –∑–∞ —Ä–∞–∑\n",
        "- ‚úÖ **Encoder-Decoder:** —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
        "- ‚úÖ **–ì–∏–±–∫–æ—Å—Ç—å:** —Ä–∞–∑–Ω–∞—è –¥–ª–∏–Ω–∞ –≤—Ö–æ–¥–∞ –∏ –≤—ã—Ö–æ–¥–∞\n",
        "\n",
        "---\n",
        "\n",
        "## üìä –î–∞—Ç–∞—Å–µ—Ç: Airline Passengers\n",
        "\n",
        "–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ—Ç –∂–µ –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è —á–µ—Å—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.\n",
        "\n",
        "**–ó–∞–¥–∞—á–∞:** –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ **3 –º–µ—Å—è—Ü–∞** –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö **12 –º–µ—Å—è—Ü–µ–≤**.\n",
        "\n",
        "---"
    ]
})

# ============================================================================
# THEORY PART 1: ATTENTION MECHANISM
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "## üìö –ß–∞—Å—Ç—å 1: –¢–µ–æ—Ä–∏—è Attention\n",
        "\n",
        "### 1.1 –ü—Ä–æ–±–ª–µ–º–∞: Information Bottleneck\n",
        "\n",
        "**–ë–∞–∑–æ–≤—ã–π LSTM –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è:**\n",
        "\n",
        "```\n",
        "–í—Ö–æ–¥: [x‚ÇÅ, x‚ÇÇ, ..., x‚ÇÅ‚ÇÇ]  (12 –º–µ—Å—è—Ü–µ–≤)\n",
        "         ‚Üì\n",
        "    LSTM Encoder\n",
        "         ‚Üì\n",
        "    h‚ÇÅ‚ÇÇ (–æ–¥–∏–Ω –≤–µ–∫—Ç–æ—Ä!)  ‚Üê –í–°–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø –ó–î–ï–°–¨\n",
        "         ‚Üì\n",
        "    Prediction\n",
        "         ‚Üì\n",
        "       ≈∑‚ÇÅ‚ÇÉ\n",
        "```\n",
        "\n",
        "**–ü—Ä–æ–±–ª–µ–º–∞:** –í–µ–∫—Ç–æ—Ä $h_{12}$ –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –í–°–Æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ 12 –º–µ—Å—è—Ü–∞—Ö!\n",
        "\n",
        "---\n",
        "\n",
        "### 1.2 –†–µ—à–µ–Ω–∏–µ: Attention Mechanism\n",
        "\n",
        "**–ò–¥–µ—è:** –ù–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è \"—Å–º–æ—Ç—Ä–∏–º\" –Ω–∞ –í–°–ï —Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è —ç–Ω–∫–æ–¥–µ—Ä–∞.\n",
        "\n",
        "#### Attention —Ñ–æ—Ä–º—É–ª—ã\n",
        "\n",
        "–ü—É—Å—Ç—å $h_1, h_2, ..., h_T$ ‚Äî —Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è encoder LSTM.  \n",
        "–î–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —à–∞–≥–µ $t$ –¥–µ–∫–æ–¥–µ—Ä –∏–º–µ–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ $s_t$.\n",
        "\n",
        "**1. Attention scores (—ç–Ω–µ—Ä–≥–∏—è):**\n",
        "\n",
        "$$e_{ti} = \\text{score}(s_t, h_i)$$\n",
        "\n",
        "–≥–¥–µ `score` –º–æ–∂–µ—Ç –±—ã—Ç—å:\n",
        "- **Dot product:** $e_{ti} = s_t^T h_i$\n",
        "- **General:** $e_{ti} = s_t^T W h_i$\n",
        "- **Concat:** $e_{ti} = v^T \\tanh(W[s_t; h_i])$ (–∏—Å–ø–æ–ª—å–∑—É–µ–º —ç—Ç–æ—Ç)\n",
        "\n",
        "**2. Attention weights (–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è):**\n",
        "\n",
        "$$\\alpha_{ti} = \\frac{\\exp(e_{ti})}{\\sum_{j=1}^{T} \\exp(e_{tj})}$$\n",
        "\n",
        "(softmax ‚Üí —Å—É–º–º–∞ –≤–µ—Å–æ–≤ = 1)\n",
        "\n",
        "**3. Context vector (–≤–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞):**\n",
        "\n",
        "$$c_t = \\sum_{i=1}^{T} \\alpha_{ti} h_i$$\n",
        "\n",
        "**4. –§–∏–Ω–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ:**\n",
        "\n",
        "$$\\tilde{s}_t = \\tanh(W_c [c_t; s_t])$$\n",
        "\n",
        "$$y_t = W_y \\tilde{s}_t$$\n",
        "\n",
        "---\n",
        "\n",
        "**–ò–Ω—Ç—É–∏—Ü–∏—è:**\n",
        "- $\\alpha_{ti}$ ‚Äî –Ω–∞—Å–∫–æ–ª—å–∫–æ –≤–∞–∂–µ–Ω $i$-–π —à–∞–≥ —ç–Ω–∫–æ–¥–µ—Ä–∞ –¥–ª—è $t$-–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "- $c_t$ ‚Äî –∫–æ–Ω—Ç–µ–∫—Å—Ç, —Å—Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞—Å—Ç—è—Ö\n",
        "- –ú–æ–¥–µ–ª—å —Å–∞–º–∞ **–æ–±—É—á–∞–µ—Ç—Å—è**, –∫—É–¥–∞ —Å–º–æ—Ç—Ä–µ—Ç—å!\n",
        "\n",
        "---\n",
        "\n",
        "### 1.3 –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è Attention\n",
        "\n",
        "**Heatmap attention weights:**\n",
        "\n",
        "```\n",
        "         Input timesteps ‚Üí\n",
        "         1  2  3  4  5  6  7  8  9 10 11 12\n",
        "Output ‚Üì\n",
        "  13  [0.1 0.1 0.1 0.2 0.1 0.1 0.2 0.05 0.02 0.01 0.01 0.01]\n",
        "  14  [0.05 0.05 0.1 0.15 0.2 0.15 0.1 0.1 0.05 0.02 0.02 0.01]\n",
        "  15  [0.02 0.02 0.05 0.1 0.15 0.2 0.15 0.1 0.1 0.05 0.03 0.03]\n",
        "```\n",
        "\n",
        "**–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:** –ë–æ–ª–µ–µ —Å–≤–µ—Ç–ª—ã–µ —è—á–µ–π–∫–∏ = –º–æ–¥–µ–ª—å \"—Å–º–æ—Ç—Ä–∏—Ç\" —Ç—É–¥–∞ –±–æ–ª—å—à–µ.\n",
        "\n",
        "---"
    ]
})

# ============================================================================
# THEORY PART 2: SEQ2SEQ
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 1.4 Seq2Seq Architecture\n",
        "\n",
        "**Sequence-to-Sequence** ‚Äî –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –∑–∞–¥–∞—á \"–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å ‚Üí –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å\".\n",
        "\n",
        "#### –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã\n",
        "\n",
        "**1. Encoder (–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫):**\n",
        "- –ß–∏—Ç–∞–µ—Ç –≤—Ö–æ–¥–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å\n",
        "- –°–æ–∑–¥–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ\n",
        "- –ú–æ–∂–µ—Ç –±—ã—Ç—å LSTM/GRU\n",
        "\n",
        "**2. Decoder (–¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫):**\n",
        "- –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤—ã—Ö–æ–¥–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å\n",
        "- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ—Ç —ç–Ω–∫–æ–¥–µ—Ä–∞\n",
        "- –¢–æ–∂–µ LSTM/GRU\n",
        "\n",
        "**3. Attention (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –Ω–æ –∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ):**\n",
        "- –°–≤—è–∑—ã–≤–∞–µ—Ç —ç–Ω–∫–æ–¥–µ—Ä –∏ –¥–µ–∫–æ–¥–µ—Ä\n",
        "- –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π —Ñ–æ–∫—É—Å\n",
        "\n",
        "---\n",
        "\n",
        "#### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –±–µ–∑ Attention\n",
        "\n",
        "```\n",
        "ENCODER:\n",
        "x‚ÇÅ ‚Üí [LSTM] ‚Üí h‚ÇÅ\n",
        "x‚ÇÇ ‚Üí [LSTM] ‚Üí h‚ÇÇ\n",
        "...\n",
        "x‚ÇÅ‚ÇÇ ‚Üí [LSTM] ‚Üí h‚ÇÅ‚ÇÇ  (context vector)\n",
        "\n",
        "DECODER:\n",
        "h‚ÇÅ‚ÇÇ ‚Üí [LSTM] ‚Üí s‚ÇÅ ‚Üí y‚ÇÅ‚ÇÉ\n",
        "y‚ÇÅ‚ÇÉ ‚Üí [LSTM] ‚Üí s‚ÇÇ ‚Üí y‚ÇÅ‚ÇÑ  (autoregressive)\n",
        "y‚ÇÅ‚ÇÑ ‚Üí [LSTM] ‚Üí s‚ÇÉ ‚Üí y‚ÇÅ‚ÇÖ\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å Attention\n",
        "\n",
        "```\n",
        "ENCODER:\n",
        "x‚ÇÅ ‚Üí [LSTM] ‚Üí h‚ÇÅ ‚îê\n",
        "x‚ÇÇ ‚Üí [LSTM] ‚Üí h‚ÇÇ ‚îÇ\n",
        "...              ‚îÇ ‚Üí All encoder states\n",
        "x‚ÇÅ‚ÇÇ ‚Üí [LSTM] ‚Üí h‚ÇÅ‚ÇÇ‚îò\n",
        "\n",
        "DECODER —Å ATTENTION:\n",
        "                  ‚îå‚îÄ Attention ‚îÄ‚îê\n",
        "                  ‚Üì              ‚Üì\n",
        "[h‚ÇÅ..h‚ÇÅ‚ÇÇ] ‚Üí c‚ÇÅ + s‚ÇÄ ‚Üí [LSTM] ‚Üí s‚ÇÅ ‚Üí y‚ÇÅ‚ÇÉ\n",
        "[h‚ÇÅ..h‚ÇÅ‚ÇÇ] ‚Üí c‚ÇÇ + s‚ÇÅ ‚Üí [LSTM] ‚Üí s‚ÇÇ ‚Üí y‚ÇÅ‚ÇÑ\n",
        "[h‚ÇÅ..h‚ÇÅ‚ÇÇ] ‚Üí c‚ÇÉ + s‚ÇÇ ‚Üí [LSTM] ‚Üí s‚ÇÉ ‚Üí y‚ÇÅ‚ÇÖ\n",
        "```\n",
        "\n",
        "–≥–¥–µ $c_i$ ‚Äî context vector —á–µ—Ä–µ–∑ attention.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.5 Multi-step Forecasting\n",
        "\n",
        "**–ó–∞–¥–∞—á–∞:** –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ $N$ —à–∞–≥–æ–≤.\n",
        "\n",
        "**–ü–æ–¥—Ö–æ–¥—ã:**\n",
        "\n",
        "**1. Recursive (–∞–≤—Ç–æ—Ä —Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π):**\n",
        "```python\n",
        "y‚ÇÅ‚ÇÉ = model([x‚ÇÅ..x‚ÇÅ‚ÇÇ])\n",
        "y‚ÇÅ‚ÇÑ = model([x‚ÇÇ..x‚ÇÅ‚ÇÇ, y‚ÇÅ‚ÇÉ])\n",
        "y‚ÇÅ‚ÇÖ = model([x‚ÇÉ..x‚ÇÅ‚ÇÇ, y‚ÇÅ‚ÇÉ, y‚ÇÅ‚ÇÑ])\n",
        "```\n",
        "- ‚ö†Ô∏è –û—à–∏–±–∫–∏ –Ω–∞–∫–∞–ø–ª–∏–≤–∞—é—Ç—Å—è\n",
        "\n",
        "**2. Direct (–º–Ω–æ–≥–æ–≤—ã—Ö–æ–¥–Ω–∞—è –º–æ–¥–µ–ª—å):**\n",
        "```python\n",
        "[y‚ÇÅ‚ÇÉ, y‚ÇÅ‚ÇÑ, y‚ÇÅ‚ÇÖ] = model([x‚ÇÅ..x‚ÇÅ‚ÇÇ])\n",
        "```\n",
        "- ‚úÖ –ù–µ—Ç –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫\n",
        "- ‚ùå –ù–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É –≤—ã—Ö–æ–¥–∞–º–∏\n",
        "\n",
        "**3. Seq2Seq (–ª—É—á—à–∏–π):**\n",
        "```python\n",
        "# Encoder\n",
        "context = encoder([x‚ÇÅ..x‚ÇÅ‚ÇÇ])\n",
        "# Decoder (–∞–≤—Ç–æ—Ä —Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π —Å attention)\n",
        "y‚ÇÅ‚ÇÉ = decoder(context, <start>)\n",
        "y‚ÇÅ‚ÇÑ = decoder(context, y‚ÇÅ‚ÇÉ)\n",
        "y‚ÇÅ‚ÇÖ = decoder(context, y‚ÇÅ‚ÇÑ)\n",
        "```\n",
        "- ‚úÖ –£—á–∏—Ç—ã–≤–∞–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏\n",
        "- ‚úÖ Attention –∫–æ–º–ø–µ–Ω—Å–∏—Ä—É–µ—Ç –æ—à–∏–±–∫–∏\n",
        "\n",
        "---"
    ]
})

# ============================================================================
# PRACTICAL PART: IMPORTS
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "## üíª –ß–∞—Å—Ç—å 2: –ü—Ä–∞–∫—Ç–∏–∫–∞ - Attention & Seq2Seq –Ω–∞ PyTorch\n",
        "\n",
        "### 2.1 –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ë–∞–∑–æ–≤—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")"
    ]
})

# ============================================================================
# LOAD DATA
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 2.2 –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
        "try:\n",
        "    from statsmodels.datasets import get_rdataset\n",
        "    airline_data = get_rdataset('AirPassengers', 'datasets')\n",
        "    df = airline_data.data\n",
        "    df.columns = ['time', 'passengers']\n",
        "except:\n",
        "    url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv'\n",
        "    df = pd.read_csv(url, parse_dates=['Month'], index_col='Month')\n",
        "    df.columns = ['passengers']\n",
        "    df.reset_index(inplace=True)\n",
        "    df.columns = ['time', 'passengers']\n",
        "\n",
        "df['time'] = pd.to_datetime(df['time'])\n",
        "df.set_index('time', inplace=True)\n",
        "\n",
        "print(f\"–î–∞—Ç–∞—Å–µ—Ç: {df.shape[0]} –Ω–∞–±–ª—é–¥–µ–Ω–∏–π\")\n",
        "print(f\"–ü–µ—Ä–∏–æ–¥: {df.index.min()} - {df.index.max()}\")\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(df.index, df['passengers'], linewidth=2)\n",
        "plt.title('Airline Passengers - Seq2Seq Multi-step Forecasting', \n",
        "          fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Passengers')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "df.head()"
    ]
})

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–≤—É—é —á–∞—Å—Ç—å
notebook['cells'] = cells

output_path = '/home/user/test/notebooks/phase3_temporal_rnn/03_attention_seq2seq.ipynb'
with open(output_path, 'w', encoding='utf-8') as f:
    json.dump(notebook, f, ensure_ascii=False, indent=1)

print(f'‚úÖ –¢–µ–æ—Ä–∏—è Attention & Seq2Seq —Å–æ–∑–¥–∞–Ω–∞: {output_path}')
print(f'–Ø—á–µ–µ–∫: {len(cells)}')
print('–°–ª–µ–¥—É—é—â–∞—è —á–∞—Å—Ç—å: —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π...')

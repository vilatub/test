#!/usr/bin/env python3
"""
Phase 6: Explainable AI - Part 4
Add: LIME, PDP, Permutation Importance, Fairness Analysis
"""

import json

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π notebook
notebook_path = '/home/user/test/notebooks/phase6_explainable_ai/01_explainable_ai_xai.ipynb'
with open(notebook_path, 'r', encoding='utf-8') as f:
    notebook = json.load(f)

cells = notebook['cells']

# ============================================================================
# LIME ANALYSIS
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "---\n",
        "\n",
        "## üî¨ –ß–∞—Å—Ç—å 3: LIME Analysis\n",
        "\n",
        "### –ß—Ç–æ —Ç–∞–∫–æ–µ LIME?\n",
        "\n",
        "**LIME (Local Interpretable Model-agnostic Explanations)** - –º–µ—Ç–æ–¥ –¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è **–æ—Ç–¥–µ–ª—å–Ω—ã—Ö predictions** –ª—é–±–æ–π –º–æ–¥–µ–ª–∏.\n",
        "\n",
        "**–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç:**\n",
        "1. –ë–µ—Ä—ë–º –∏–Ω—Ç–µ—Ä–µ—Å—É—é—â–∏–π –Ω–∞—Å sample\n",
        "2. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º perturbed samples (—Å–ª–µ–≥–∫–∞ –∏–∑–º–µ–Ω—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏)\n",
        "3. –ü–æ–ª—É—á–∞–µ–º predictions –¥–ª—è –Ω–∏—Ö –æ—Ç —á–µ—Ä–Ω–æ–≥–æ —è—â–∏–∫–∞\n",
        "4. –û–±—É—á–∞–µ–º **–ø—Ä–æ—Å—Ç—É—é –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—É—é –º–æ–¥–µ–ª—å** (linear/decision tree) –ª–æ–∫–∞–ª—å–Ω–æ\n",
        "5. –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º –ø—Ä–æ—Å—Ç—É—é –º–æ–¥–µ–ª—å\n",
        "\n",
        "**LIME vs SHAP:**\n",
        "\n",
        "| –ê—Å–ø–µ–∫—Ç | LIME | SHAP |\n",
        "|--------|------|------|\n",
        "| –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ | ‚ùå Heuristic | ‚úÖ Game theory (Shapley) |\n",
        "| Consistency | ‚ùå –ù–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç—Å—è | ‚úÖ –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç—Å—è |\n",
        "| –°–∫–æ—Ä–æ—Å—Ç—å (local) | ‚úÖ –ë—ã—Å—Ç—Ä–µ–µ | ‚ö†Ô∏è KernelSHAP –º–µ–¥–ª–µ–Ω–Ω–µ–µ |\n",
        "| –°–∫–æ—Ä–æ—Å—Ç—å (tree models) | ‚ö†Ô∏è –ú–µ–¥–ª–µ–Ω–Ω–µ–µ | ‚úÖ TreeSHAP –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ |\n",
        "| Stability | ‚ùå –ú–æ–∂–µ—Ç –≤–∞—Ä—å–∏—Ä–æ–≤–∞—Ç—å—Å—è | ‚úÖ –°—Ç–∞–±–∏–ª—å–Ω–µ–µ |\n",
        "| Global interpretation | ‚ùå –ù–µ—Ç | ‚úÖ –î–∞ |\n",
        "| Use case | Quick local explanations | Production, —Ç—Ä–µ–±—É–µ—Ç—Å—è consistency |\n",
        "\n",
        "**–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LIME:**\n",
        "- ‚úÖ –ë—ã—Å—Ç—Ä—ã–µ ad-hoc –ª–æ–∫–∞–ª—å–Ω—ã–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è\n",
        "- ‚úÖ –ú–æ–¥–µ–ª–∏, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç TreeSHAP (custom black boxes)\n",
        "- ‚úÖ Text/Image classification (LIME —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –Ω–∏–º–∏)\n",
        "- ‚ö†Ô∏è –ù–ï –¥–ª—è production, –≥–¥–µ –≤–∞–∂–Ω–∞ consistency\n",
        "\n",
        "---\n"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "print(\"=\" * 70)\n",
        "print(\"LIME ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# –°–æ–∑–¥–∞—ë–º LIME explainer\n",
        "print(\"\\nCreating LIME Tabular Explainer...\")\n",
        "lime_explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "    training_data=X_train_scaled.values,\n",
        "    feature_names=feature_cols,\n",
        "    class_names=['<=50K', '>50K'],\n",
        "    mode='classification',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"‚úÖ LIME explainer created\")\n",
        "\n",
        "# –û–±—ä—è—Å–Ω—è–µ–º —Ç–æ—Ç –∂–µ high income sample, —á—Ç–æ –∏ —Å SHAP\n",
        "high_income_sample = X_test_scaled.iloc[high_income_idx].values\n",
        "\n",
        "print(f\"\\nExplaining sample {high_income_idx} (predicted >50K)...\")\n",
        "lime_exp = lime_explainer.explain_instance(\n",
        "    high_income_sample,\n",
        "    models['Random Forest'].predict_proba,\n",
        "    num_features=10,\n",
        "    num_samples=1000  # —Å–∫–æ–ª—å–∫–æ perturbed samples –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å\n",
        ")\n",
        "\n",
        "print(\"‚úÖ LIME explanation computed\")\n"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è LIME explanation\n",
        "fig = lime_exp.as_pyplot_figure(label=1)  # label=1 –¥–ª—è –∫–ª–∞—Å—Å–∞ >50K\n",
        "plt.title(f'LIME Explanation: Sample {high_income_idx} (>50K)', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# –¢–µ–∫—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ\n",
        "print(\"\\nüìä LIME Explanation (Top features):\")\n",
        "print(\"=\"*50)\n",
        "for feature, weight in lime_exp.as_list(label=1)[:10]:\n",
        "    print(f\"{feature:40s} : {weight:+.4f}\")\n",
        "\n",
        "print(\"\\nüí° –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:\")\n",
        "print(\"- Positive weight ‚Üí increases P(>50K)\")\n",
        "print(\"- Negative weight ‚Üí decreases P(>50K)\")\n",
        "print(\"- –í–µ–ª–∏—á–∏–Ω–∞ weight = strength of effect\")\n"
    ]
})

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "#### 3.1 –°—Ä–∞–≤–Ω–µ–Ω–∏–µ LIME vs SHAP\n",
        "\n",
        "–°—Ä–∞–≤–Ω–∏–º –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –¥–ª—è —Ç–æ–≥–æ –∂–µ sample."
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ LIME vs SHAP –¥–ª—è –æ–¥–Ω–æ–≥–æ sample\n",
        "print(\"=\" * 70)\n",
        "print(f\"COMPARISON: LIME vs SHAP (Sample {high_income_idx})\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# LIME feature importances\n",
        "lime_dict = dict(lime_exp.as_list(label=1))\n",
        "lime_values = {}\n",
        "for feature in feature_cols:\n",
        "    # LIME –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–æ–∫–∏ —Ç–∏–ø–∞ \"feature <= value\"\n",
        "    for lime_feature, value in lime_dict.items():\n",
        "        if feature in lime_feature:\n",
        "            lime_values[feature] = value\n",
        "            break\n",
        "    if feature not in lime_values:\n",
        "        lime_values[feature] = 0.0\n",
        "\n",
        "# SHAP values –¥–ª—è —ç—Ç–æ–≥–æ sample\n",
        "shap_dict = dict(zip(feature_cols, rf_shap_values_class1[high_income_idx]))\n",
        "\n",
        "# –°–æ–∑–¥–∞—ë–º —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π DataFrame\n",
        "comparison_local = pd.DataFrame({\n",
        "    'Feature': feature_cols,\n",
        "    'LIME': [lime_values.get(f, 0) for f in feature_cols],\n",
        "    'SHAP': [shap_dict[f] for f in feature_cols]\n",
        "})\n",
        "\n",
        "# –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∞–±—Å–æ–ª—é—Ç–Ω–æ–º—É –∑–Ω–∞—á–µ–Ω–∏—é SHAP\n",
        "comparison_local['abs_SHAP'] = comparison_local['SHAP'].abs()\n",
        "comparison_local = comparison_local.sort_values('abs_SHAP', ascending=False).head(10)\n",
        "\n",
        "print(\"\\nTop 10 features:\")\n",
        "print(comparison_local[['Feature', 'LIME', 'SHAP']].to_string(index=False))\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "x = np.arange(len(comparison_local))\n",
        "width = 0.35\n",
        "\n",
        "ax.barh(x - width/2, comparison_local['LIME'], width, label='LIME', alpha=0.8)\n",
        "ax.barh(x + width/2, comparison_local['SHAP'], width, label='SHAP', alpha=0.8)\n",
        "\n",
        "ax.set_yticks(x)\n",
        "ax.set_yticklabels(comparison_local['Feature'])\n",
        "ax.set_xlabel('Feature Importance (local explanation)', fontsize=12)\n",
        "ax.set_title(f'LIME vs SHAP: Sample {high_income_idx}', fontsize=14, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.invert_yaxis()\n",
        "ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
        "ax.grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüîç –ù–∞–±–ª—é–¥–µ–Ω–∏—è:\")\n",
        "print(\"- LIME –∏ SHAP –æ–±—ã—á–Ω–æ —Å–æ–≥–ª–∞—Å–Ω—ã –≤ —Ç–æ–ø–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö\")\n",
        "print(\"- Magnitude –º–æ–∂–µ—Ç –æ—Ç–ª–∏—á–∞—Ç—å—Å—è (—Ä–∞–∑–Ω—ã–µ —à–∫–∞–ª—ã)\")\n",
        "print(\"- SHAP –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–π –∏ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π\")\n",
        "print(\"- LIME –±—ã—Å—Ç—Ä–µ–µ –¥–ª—è quick ad-hoc explanations\")\n"
    ]
})

# ============================================================================
# PARTIAL DEPENDENCE PLOTS
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "---\n",
        "\n",
        "## üìä –ß–∞—Å—Ç—å 4: Partial Dependence Plots (PDP)\n",
        "\n",
        "### –ß—Ç–æ —Ç–∞–∫–æ–µ PDP?\n",
        "\n",
        "**Partial Dependence Plot** –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç **–≥–ª–æ–±–∞–ª—å–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ** –ø—Ä–∏–∑–Ω–∞–∫–∞ –Ω–∞ predictions:\n",
        "- –ö–∞–∫ –º–µ–Ω—è–µ—Ç—Å—è —Å—Ä–µ–¥–Ω–µ–µ prediction –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞?\n",
        "- –ù–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –¥—Ä—É–≥–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (marginalized)\n",
        "\n",
        "**–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç:**\n",
        "1. –§–∏–∫—Å–∏—Ä—É–µ–º –∏–Ω—Ç–µ—Ä–µ—Å—É—é—â–∏–π –ø—Ä–∏–∑–Ω–∞–∫ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö\n",
        "2. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è: –¥–µ–ª–∞–µ–º predictions –¥–ª—è –≤—Å–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
        "3. –£—Å—Ä–µ–¥–Ω—è–µ–º predictions\n",
        "4. –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫: feature value vs average prediction\n",
        "\n",
        "**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**\n",
        "- ‚úÖ Global view (–Ω–µ —Ç–æ–ª—å–∫–æ –ª–æ–∫–∞–ª—å–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ)\n",
        "- ‚úÖ –í–∏–∑—É–∞–ª—å–Ω–æ –ø–æ–Ω—è—Ç–Ω–æ\n",
        "- ‚úÖ –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã\n",
        "\n",
        "**–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:**\n",
        "- ‚ö†Ô∏è **Assumes independence:** –µ—Å–ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—Ç, –º–æ–∂–µ—Ç –≤–≤–æ–¥–∏—Ç—å –≤ –∑–∞–±–ª—É–∂–¥–µ–Ω–∏–µ\n",
        "- ‚ö†Ô∏è **Averages out heterogeneity:** –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –≤–∞—Ä–∏–∞—Ü–∏–∏\n",
        "\n",
        "**ICE (Individual Conditional Expectation) curves:**\n",
        "- –†–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É averaging\n",
        "- –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ sample –æ—Ç–¥–µ–ª—å–Ω–æ\n",
        "- PDP = average of ICE curves\n",
        "\n",
        "---\n"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "print(\"=\" * 70)\n",
        "print(\"PARTIAL DEPENDENCE PLOTS (PDP)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# –¢–æ–ø –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è PDP\n",
        "pdp_features = ['age', 'education-num', 'hours-per-week', 'capital-gain']\n",
        "\n",
        "print(f\"\\nBuilding PDP for: {pdp_features}\")\n",
        "\n",
        "# PDP –¥–ª—è Random Forest\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "display = PartialDependenceDisplay.from_estimator(\n",
        "    models['Random Forest'],\n",
        "    X_test_scaled.iloc[:500],  # –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥–≤—ã–±–æ—Ä–∫—É –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏\n",
        "    features=pdp_features,\n",
        "    feature_names=feature_cols,\n",
        "    ax=ax,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "plt.suptitle('Partial Dependence Plots: Random Forest', fontsize=14, fontweight='bold', y=0.98)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ PDP completed\")\n",
        "print(\"\\nüìä –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:\")\n",
        "print(\"- age: –° –≤–æ–∑—Ä–∞—Å—Ç–æ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å >50K —Ä–∞—Å—Ç—ë—Ç (–¥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –º–æ–º–µ–Ω—Ç–∞)\")\n",
        "print(\"- education-num: –ë–æ–ª—å—à–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è ‚Üí –≤—ã—à–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å >50K\")\n",
        "print(\"- hours-per-week: –ë–æ–ª—å—à–µ —á–∞—Å–æ–≤ —Ä–∞–±–æ—Ç—ã ‚Üí –≤—ã—à–µ income\")\n",
        "print(\"- capital-gain: –ù–∞–ª–∏—á–∏–µ capital gain ‚Üí —Ä–µ–∑–∫–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å\")\n"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# 2D PDP –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π\n",
        "print(\"\\n2D Partial Dependence: Feature Interactions\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "# PDP –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è age –∏ education-num\n",
        "display_2d = PartialDependenceDisplay.from_estimator(\n",
        "    models['Random Forest'],\n",
        "    X_test_scaled.iloc[:500],\n",
        "    features=[('age', 'education-num')],\n",
        "    feature_names=feature_cols,\n",
        "    ax=ax,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "plt.suptitle('2D PDP: Age √ó Education Interaction', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüí° Interaction effect:\")\n",
        "print(\"- –°–≤–µ—Ç–ª—ã–µ –æ–±–ª–∞—Å—Ç–∏ = –≤—ã—Å–æ–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å >50K\")\n",
        "print(\"- –¢—ë–º–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏ = –Ω–∏–∑–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å >50K\")\n",
        "print(\"- –í–∏–¥–∏–º joint effect –≤–æ–∑—Ä–∞—Å—Ç–∞ –∏ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è\")\n"
    ]
})

# ============================================================================
# PERMUTATION IMPORTANCE
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 4.1 Permutation Importance\n",
        "\n",
        "**Permutation Importance** - –ø—Ä–æ—Å—Ç–æ–π –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\n",
        "\n",
        "**–ê–ª–≥–æ—Ä–∏—Ç–º:**\n",
        "1. –í—ã—á–∏—Å–ª—è–µ–º baseline metric (accuracy, AUC, etc.) –Ω–∞ test set\n",
        "2. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞:\n",
        "   - Shuffle –∑–Ω–∞—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–∞ (breaks relationship —Å target)\n",
        "   - –í—ã—á–∏—Å–ª—è–µ–º metric –∑–∞–Ω–æ–≤–æ\n",
        "   - Importance = baseline - shuffled metric\n",
        "3. Repeat –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
        "\n",
        "**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**\n",
        "- ‚úÖ **Model-agnostic** (—Ä–∞–±–æ—Ç–∞–µ—Ç —Å –ª—é–±–æ–π –º–æ–¥–µ–ª—å—é)\n",
        "- ‚úÖ **Handles correlations** (–≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç Gini importance)\n",
        "- ‚úÖ **Intuitive interpretation**\n",
        "\n",
        "**vs Gini Importance (Feature Importance –≤ Random Forest):**\n",
        "- Gini importance: —Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–∑–Ω–∞–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ splits\n",
        "- **–ü—Ä–æ–±–ª–µ–º–∞:** bias –∫ high-cardinality features\n",
        "- Permutation Importance: —Ä–µ–∞–ª—å–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ predictions\n",
        "\n",
        "---\n"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "print(\"=\" * 70)\n",
        "print(\"PERMUTATION IMPORTANCE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# –í—ã—á–∏—Å–ª—è–µ–º permutation importance –¥–ª—è Random Forest\n",
        "print(\"\\nComputing Permutation Importance (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –º–∏–Ω—É—Ç—É)...\")\n",
        "perm_importance = permutation_importance(\n",
        "    models['Random Forest'],\n",
        "    X_test_scaled.iloc[:1000],\n",
        "    y_test.iloc[:1000],\n",
        "    n_repeats=10,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    scoring='roc_auc'\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Permutation Importance computed\")\n",
        "\n",
        "# –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ importance\n",
        "perm_sorted_idx = perm_importance.importances_mean.argsort()[::-1]\n",
        "\n",
        "# –°–æ–∑–¥–∞—ë–º DataFrame\n",
        "perm_df = pd.DataFrame({\n",
        "    'Feature': np.array(feature_cols)[perm_sorted_idx],\n",
        "    'Importance': perm_importance.importances_mean[perm_sorted_idx],\n",
        "    'Std': perm_importance.importances_std[perm_sorted_idx]\n",
        "})\n",
        "\n",
        "print(\"\\nüìä Top 10 Features by Permutation Importance:\")\n",
        "print(perm_df.head(10).to_string(index=False))\n"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è Permutation Importance\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "top_n = 15\n",
        "ax.barh(\n",
        "    range(top_n),\n",
        "    perm_df.head(top_n)['Importance'],\n",
        "    xerr=perm_df.head(top_n)['Std'],\n",
        "    alpha=0.8\n",
        ")\n",
        "\n",
        "ax.set_yticks(range(top_n))\n",
        "ax.set_yticklabels(perm_df.head(top_n)['Feature'])\n",
        "ax.set_xlabel('Importance (decrease in ROC AUC)', fontsize=12)\n",
        "ax.set_title('Permutation Importance: Random Forest', fontsize=14, fontweight='bold')\n",
        "ax.invert_yaxis()\n",
        "ax.grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüí° –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:\")\n",
        "print(\"- –í—ã—Å–æ–∫–∞—è importance ‚Üí shuffling –ø—Ä–∏–∑–Ω–∞–∫–∞ —Å–∏–ª—å–Ω–æ —É—Ö—É–¥—à–∞–µ—Ç –º–æ–¥–µ–ª—å\")\n",
        "print(\"- Error bars –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç variance (—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å)\")\n",
        "print(\"- –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è importance ‚Üí –ø—Ä–∏–∑–Ω–∞–∫ –Ω–µ –ø–æ–º–æ–≥–∞–µ—Ç (—à—É–º)\")\n"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ Gini vs Permutation Importance\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"COMPARISON: Gini Importance vs Permutation Importance\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Gini importance (–≤—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –≤ Random Forest)\n",
        "gini_importance = models['Random Forest'].feature_importances_\n",
        "\n",
        "# –°–æ–∑–¥–∞—ë–º —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π DataFrame\n",
        "importance_comparison = pd.DataFrame({\n",
        "    'Feature': feature_cols,\n",
        "    'Gini': gini_importance,\n",
        "    'Permutation': perm_importance.importances_mean\n",
        "}).sort_values('Permutation', ascending=False)\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "x = np.arange(12)\n",
        "width = 0.35\n",
        "\n",
        "ax.barh(x - width/2, importance_comparison.head(12)['Gini'], width, label='Gini (MDI)', alpha=0.8)\n",
        "ax.barh(x + width/2, importance_comparison.head(12)['Permutation'], width, label='Permutation', alpha=0.8)\n",
        "\n",
        "ax.set_yticks(x)\n",
        "ax.set_yticklabels(importance_comparison.head(12)['Feature'])\n",
        "ax.set_xlabel('Importance', fontsize=12)\n",
        "ax.set_title('Gini vs Permutation Importance', fontsize=14, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.invert_yaxis()\n",
        "ax.grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüîç –ù–∞–±–ª—é–¥–µ–Ω–∏—è:\")\n",
        "print(\"- Gini –∏ Permutation –æ–±—ã—á–Ω–æ —Å–æ–≥–ª–∞—Å–Ω—ã –≤ —Ç–æ–ø–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö\")\n",
        "print(\"- Gini –º–æ–∂–µ—Ç –ø–µ—Ä–µ–æ—Ü–µ–Ω–∏–≤–∞—Ç—å high-cardinality features\")\n",
        "print(\"- Permutation –±–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω—ã–π –¥–ª—è feature selection\")\n",
        "print(\"- SHAP > Permutation > Gini (–≤ –ø–ª–∞–Ω–µ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–π –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏)\")\n"
    ]
})

# ============================================================================
# FAIRNESS ANALYSIS
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "---\n",
        "\n",
        "## ‚öñÔ∏è –ß–∞—Å—Ç—å 5: Fairness Analysis\n",
        "\n",
        "### –ü–æ—á–µ–º—É Fairness –≤–∞–∂–µ–Ω?\n",
        "\n",
        "**–ü—Ä–æ–±–ª–µ–º–∞:** –í—ã—Å–æ–∫–∞—è accuracy ‚â† —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–∞—è –º–æ–¥–µ–ª—å\n",
        "\n",
        "**Real-world –ø—Ä–∏–º–µ—Ä—ã bias:**\n",
        "- **Amazon Recruiting AI (2018):** –º–æ–¥–µ–ª—å –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∏—Ä–æ–≤–∞–ª–∞ –∂–µ–Ω—â–∏–Ω (–æ–±—É—á–∞–ª–∞—Å—å –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å bias)\n",
        "- **COMPAS (Criminal Justice):** –∞–ª–≥–æ—Ä–∏—Ç–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è recidivism –ø–æ–∫–∞–∑—ã–≤–∞–ª racial bias\n",
        "- **Credit Scoring:** –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∏—Ä–æ–≤–∞—Ç—å –ø–æ –≤–æ–∑—Ä–∞—Å—Ç—É, –ø–æ–ª—É, —Ä–∞—Å–µ\n",
        "- **Healthcare:** –∞–ª–≥–æ—Ä–∏—Ç–º—ã –º–æ–≥—É—Ç —Ö—É–∂–µ —Ä–∞–±–æ—Ç–∞—Ç—å –¥–ª—è minority groups\n",
        "\n",
        "**–ó–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:**\n",
        "- üá∫üá∏ **Equal Credit Opportunity Act** - –∑–∞–ø—Ä–µ—â–∞–µ—Ç –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ü–∏—é –≤ –∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏–∏\n",
        "- üá™üá∫ **GDPR Article 22** - –ø—Ä–∞–≤–æ –Ω–∞ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π\n",
        "- üá∫üá∏ **Fair Housing Act** - fairness –≤ –∂–∏–ª–∏—â–Ω–æ–º –∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏–∏\n",
        "\n",
        "### –¢–∏–ø—ã Fairness Metrics:\n",
        "\n",
        "#### 1. **Demographic Parity (Statistical Parity)**\n",
        "```\n",
        "P(Y_pred = 1 | Sex = Male) = P(Y_pred = 1 | Sex = Female)\n",
        "```\n",
        "- –û–¥–∏–Ω–∞–∫–æ–≤–∞—è –¥–æ–ª—è positive predictions –≤ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø–µ\n",
        "- **–ü—Ä–æ–±–ª–µ–º–∞:** –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –±–∞–∑–æ–≤—ã–µ —Ä–∞–∑–ª–∏—á–∏—è –≤ ground truth\n",
        "\n",
        "#### 2. **Equal Opportunity**\n",
        "```\n",
        "P(Y_pred = 1 | Y_true = 1, Sex = Male) = P(Y_pred = 1 | Y_true = 1, Sex = Female)\n",
        "```\n",
        "- –û–¥–∏–Ω–∞–∫–æ–≤—ã–π True Positive Rate –¥–ª—è –≤—Å–µ—Ö –≥—Ä—É–ø–ø\n",
        "- **–õ—É—á—à–µ:** —É—á–∏—Ç—ã–≤–∞–µ—Ç ground truth\n",
        "\n",
        "#### 3. **Equalized Odds**\n",
        "```\n",
        "Equal Opportunity + Equal False Positive Rate\n",
        "```\n",
        "- –ò TPR, –∏ FPR –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –¥–ª—è –≤—Å–µ—Ö –≥—Ä—É–ø–ø\n",
        "\n",
        "#### 4. **Calibration**\n",
        "```\n",
        "P(Y_true = 1 | Y_pred_proba = p, Sex = Male) = P(Y_true = 1 | Y_pred_proba = p, Sex = Female)\n",
        "```\n",
        "- Predicted probabilities —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Ä–µ–∞–ª—å–Ω—ã–º –¥–ª—è –≤—Å–µ—Ö –≥—Ä—É–ø–ø\n",
        "\n",
        "**Trade-offs:**\n",
        "- ‚ö†Ô∏è **–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ** —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç—å –≤—Å–µ fairness criteria –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ (—Ç–µ–æ—Ä–µ–º–∞ impossibility)\n",
        "- ‚ö†Ô∏è **Fairness vs Accuracy trade-off** - –∏–Ω–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –∂–µ—Ä—Ç–≤–æ–≤–∞—Ç—å accuracy\n",
        "\n",
        "---\n"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "print(\"=\" * 70)\n",
        "print(\"FAIRNESS ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º fairness –ø–æ –ø–æ–ª—É (sensitive attribute)\n",
        "print(\"\\nAnalyzing fairness by SEX...\")\n",
        "\n",
        "# Predictions –¥–ª—è test set\n",
        "rf_predictions = results['Random Forest']['predictions']\n",
        "rf_probabilities = results['Random Forest']['probabilities']\n",
        "\n",
        "# –î–æ–±–∞–≤–ª—è–µ–º predictions –∫ sensitive attributes\n",
        "fairness_df = sensitive_test.copy()\n",
        "fairness_df['y_true'] = y_test.values\n",
        "fairness_df['y_pred'] = rf_predictions\n",
        "fairness_df['y_prob'] = rf_probabilities\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"1. DEMOGRAPHIC PARITY (Statistical Parity)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Demographic Parity: –¥–æ–ª—è positive predictions –ø–æ –≥—Ä—É–ø–ø–∞–º\n",
        "demo_parity = fairness_df.groupby('sex')['y_pred'].mean()\n",
        "print(\"\\nP(Y_pred = 1 | Sex):\")\n",
        "for sex, prob in demo_parity.items():\n",
        "    print(f\"  {sex:10s}: {prob:.4f} ({prob*100:.2f}%)\")\n",
        "\n",
        "# Disparate Impact Ratio\n",
        "female_rate = demo_parity.get('Female', demo_parity.iloc[0])\n",
        "male_rate = demo_parity.get('Male', demo_parity.iloc[1])\n",
        "disparate_impact = female_rate / male_rate if male_rate > 0 else 0\n",
        "\n",
        "print(f\"\\nDisparate Impact Ratio: {disparate_impact:.4f}\")\n",
        "print(\"  (Female rate / Male rate)\")\n",
        "print(\"  ‚úÖ Good if between 0.8 and 1.25 (80% rule)\")\n",
        "if 0.8 <= disparate_impact <= 1.25:\n",
        "    print(\"  ‚úÖ PASS: Demographic Parity satisfied\")\n",
        "else:\n",
        "    print(f\"  ‚ö†Ô∏è FAIL: Potential bias (ratio = {disparate_impact:.2f})\")\n"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"2. EQUAL OPPORTUNITY (True Positive Rate)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Equal Opportunity: TPR –ø–æ –≥—Ä—É–ø–ø–∞–º (—Å—Ä–µ–¥–∏ —Ç–µ—Ö, –∫—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ >50K)\n",
        "positive_class = fairness_df[fairness_df['y_true'] == 1]\n",
        "tpr_by_sex = positive_class.groupby('sex')['y_pred'].mean()\n",
        "\n",
        "print(\"\\nTrue Positive Rate (Recall) by Sex:\")\n",
        "for sex, tpr in tpr_by_sex.items():\n",
        "    print(f\"  {sex:10s}: {tpr:.4f} ({tpr*100:.2f}%)\")\n",
        "\n",
        "# Difference in TPR\n",
        "tpr_diff = abs(tpr_by_sex.iloc[0] - tpr_by_sex.iloc[1])\n",
        "print(f\"\\nDifference in TPR: {tpr_diff:.4f}\")\n",
        "if tpr_diff < 0.05:\n",
        "    print(\"  ‚úÖ PASS: Equal Opportunity satisfied (diff < 5%)\")\n",
        "else:\n",
        "    print(f\"  ‚ö†Ô∏è WARNING: TPR differs by {tpr_diff*100:.1f}%\")\n"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"3. FALSE POSITIVE RATE (FPR) BY SEX\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# FPR –ø–æ –≥—Ä—É–ø–ø–∞–º (—Å—Ä–µ–¥–∏ —Ç–µ—Ö, –∫—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ <=50K)\n",
        "negative_class = fairness_df[fairness_df['y_true'] == 0]\n",
        "fpr_by_sex = negative_class.groupby('sex')['y_pred'].mean()\n",
        "\n",
        "print(\"\\nFalse Positive Rate by Sex:\")\n",
        "for sex, fpr in fpr_by_sex.items():\n",
        "    print(f\"  {sex:10s}: {fpr:.4f} ({fpr*100:.2f}%)\")\n",
        "\n",
        "# Difference in FPR\n",
        "fpr_diff = abs(fpr_by_sex.iloc[0] - fpr_by_sex.iloc[1])\n",
        "print(f\"\\nDifference in FPR: {fpr_diff:.4f}\")\n",
        "if fpr_diff < 0.05:\n",
        "    print(\"  ‚úÖ PASS: FPR similar across groups (diff < 5%)\")\n",
        "else:\n",
        "    print(f\"  ‚ö†Ô∏è WARNING: FPR differs by {fpr_diff*100:.1f}%\")\n"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è fairness metrics\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# 1. Demographic Parity\n",
        "demo_parity.plot(kind='bar', ax=axes[0, 0], color=['#e74c3c', '#3498db'])\n",
        "axes[0, 0].set_title('Demographic Parity: P(Y_pred=1 | Sex)', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_ylabel('Positive Prediction Rate')\n",
        "axes[0, 0].set_xlabel('Sex')\n",
        "axes[0, 0].tick_params(axis='x', rotation=0)\n",
        "axes[0, 0].axhline(y=demo_parity.mean(), color='green', linestyle='--', label='Overall mean')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 2. True Positive Rate\n",
        "tpr_by_sex.plot(kind='bar', ax=axes[0, 1], color=['#e74c3c', '#3498db'])\n",
        "axes[0, 1].set_title('Equal Opportunity: TPR by Sex', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_ylabel('True Positive Rate')\n",
        "axes[0, 1].set_xlabel('Sex')\n",
        "axes[0, 1].tick_params(axis='x', rotation=0)\n",
        "axes[0, 1].axhline(y=tpr_by_sex.mean(), color='green', linestyle='--', label='Overall mean')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 3. False Positive Rate\n",
        "fpr_by_sex.plot(kind='bar', ax=axes[1, 0], color=['#e74c3c', '#3498db'])\n",
        "axes[1, 0].set_title('FPR by Sex', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_ylabel('False Positive Rate')\n",
        "axes[1, 0].set_xlabel('Sex')\n",
        "axes[1, 0].tick_params(axis='x', rotation=0)\n",
        "axes[1, 0].axhline(y=fpr_by_sex.mean(), color='green', linestyle='--', label='Overall mean')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 4. Confusion Matrix by Sex\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Aggregate metrics –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
        "metrics_by_sex = {}\n",
        "for sex in fairness_df['sex'].unique():\n",
        "    sex_data = fairness_df[fairness_df['sex'] == sex]\n",
        "    metrics_by_sex[sex] = {\n",
        "        'Accuracy': accuracy_score(sex_data['y_true'], sex_data['y_pred']),\n",
        "        'Precision': precision_score(sex_data['y_true'], sex_data['y_pred']),\n",
        "        'Recall': recall_score(sex_data['y_true'], sex_data['y_pred']),\n",
        "        'F1': f1_score(sex_data['y_true'], sex_data['y_pred'])\n",
        "    }\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics_by_sex).T\n",
        "metrics_df.plot(kind='bar', ax=axes[1, 1], width=0.8)\n",
        "axes[1, 1].set_title('Performance Metrics by Sex', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_ylabel('Score')\n",
        "axes[1, 1].set_xlabel('Sex')\n",
        "axes[1, 1].tick_params(axis='x', rotation=0)\n",
        "axes[1, 1].legend(loc='lower right')\n",
        "axes[1, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.suptitle('Fairness Analysis: Sex-based Metrics', fontsize=14, fontweight='bold', y=1.00)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Summary Table:\")\n",
        "print(metrics_df.round(4))\n"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Calibration Analysis\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"4. CALIBRATION ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Calibration curves –ø–æ –≥—Ä—É–ø–ø–∞–º\n",
        "from sklearn.calibration import calibration_curve\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "for sex in fairness_df['sex'].unique():\n",
        "    sex_data = fairness_df[fairness_df['sex'] == sex]\n",
        "    \n",
        "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
        "        sex_data['y_true'],\n",
        "        sex_data['y_prob'],\n",
        "        n_bins=10,\n",
        "        strategy='uniform'\n",
        "    )\n",
        "    \n",
        "    ax.plot(\n",
        "        mean_predicted_value,\n",
        "        fraction_of_positives,\n",
        "        marker='o',\n",
        "        linewidth=2,\n",
        "        label=sex\n",
        "    )\n",
        "\n",
        "# Perfect calibration line\n",
        "ax.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
        "\n",
        "ax.set_xlabel('Mean Predicted Probability', fontsize=12)\n",
        "ax.set_ylabel('Fraction of Positives', fontsize=12)\n",
        "ax.set_title('Calibration Curve by Sex', fontsize=14, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüí° –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è Calibration:\")\n",
        "print(\"- –õ–∏–Ω–∏—è –±–ª–∏–∑–∫–∞ –∫ diagonal ‚Üí –º–æ–¥–µ–ª—å well-calibrated\")\n",
        "print(\"- Above diagonal ‚Üí –º–æ–¥–µ–ª—å underestimates probabilities\")\n",
        "print(\"- Below diagonal ‚Üí –º–æ–¥–µ–ª—å overestimates probabilities\")\n",
        "print(\"- –ï—Å–ª–∏ –∫—Ä–∏–≤—ã–µ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –≥—Ä—É–ø–ø —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è ‚Üí calibration bias\")\n"
    ]
})

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 5.1 Bias Mitigation Strategies\n",
        "\n",
        "–ï—Å–ª–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω bias, —á—Ç–æ –¥–µ–ª–∞—Ç—å?\n",
        "\n",
        "#### 1. **Pre-processing** (–¥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏)\n",
        "- **Reweighting:** –¥–∞–≤–∞—Ç—å –±–æ–ª—å—à–∏–π –≤–µ—Å underrepresented groups\n",
        "- **Resampling:** oversample minority, undersample majority\n",
        "- **Data augmentation:** –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã\n",
        "- **Feature removal:** —É–¥–∞–ª–∏—Ç—å sensitive attributes (–Ω–æ –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ - –º–æ–∂–µ—Ç –Ω–µ –ø–æ–º–æ—á—å –∏–∑-–∑–∞ proxy features)\n",
        "\n",
        "#### 2. **In-processing** (–≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è)\n",
        "- **Fairness constraints:** –¥–æ–±–∞–≤–∏—Ç—å fairness penalty –≤ loss function\n",
        "- **Adversarial debiasing:** –æ–±—É—á–∏—Ç—å adversarial network, —á—Ç–æ–±—ã predictions –Ω–µ –∑–∞–≤–∏—Å–µ–ª–∏ –æ—Ç sensitive attribute\n",
        "- **Fair representations:** learn embedding, –≥–¥–µ sensitive info —É–¥–∞–ª–µ–Ω–∞\n",
        "\n",
        "#### 3. **Post-processing** (–ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è)\n",
        "- **Threshold optimization:** —Ä–∞–∑–Ω—ã–µ thresholds –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –≥—Ä—É–ø–ø\n",
        "- **Calibration:** recalibrate probabilities per group\n",
        "- **Reject option:** –Ω–µ –¥–µ–ª–∞—Ç—å prediction, –µ—Å–ª–∏ confidence –Ω–∏–∑–∫–∞—è\n",
        "\n",
        "**Trade-offs:**\n",
        "- ‚ö†Ô∏è **Fairness vs Accuracy:** mitigation –º–æ–∂–µ—Ç —Å–Ω–∏–∑–∏—Ç—å overall accuracy\n",
        "- ‚ö†Ô∏è **Which fairness metric?** –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç—å –≤—Å–µ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ\n",
        "- ‚ö†Ô∏è **Legal considerations:** –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –º–µ—Ç–æ–¥—ã (e.g., different thresholds) –º–æ–≥—É—Ç –±—ã—Ç—å illegal\n",
        "\n",
        "**Best practices:**\n",
        "1. ‚úÖ **Measure fairness** –Ω–∞ test set –¥–ª—è –≤—Å–µ—Ö sensitive groups\n",
        "2. ‚úÖ **Document** bias analysis –∏ mitigation efforts (model cards)\n",
        "3. ‚úÖ **Monitor** fairness in production (drift)\n",
        "4. ‚úÖ **Stakeholder involvement:** involve affected communities\n",
        "5. ‚úÖ **Transparency:** –æ–±—ä—è—Å–Ω—è—Ç—å decisions, –æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è high-stakes applications\n",
        "\n",
        "---\n"
    ]
})

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π notebook
notebook['cells'] = cells

with open(notebook_path, 'w', encoding='utf-8') as f:
    json.dump(notebook, f, ensure_ascii=False, indent=1)

print(f'\\n‚úÖ Updated notebook: {notebook_path}')
print(f'Total cells: {len(cells)}')
print('LIME, PDP, Permutation Importance, and Fairness Analysis added!')

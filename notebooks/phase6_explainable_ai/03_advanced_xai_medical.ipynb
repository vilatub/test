{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5512a632",
   "metadata": {},
   "source": [
    "# Фаза 6.3: Продвинутые методы объяснимого ИИ\n",
    "\n",
    "## Интерпретация моделей для медицинской диагностики\n",
    "\n",
    "В этом ноутбуке мы рассмотрим продвинутые методы объяснимого ИИ (XAI):\n",
    "\n",
    "### Методы\n",
    "\n",
    "1. **Integrated Gradients** - атрибуция на основе интегрирования градиентов\n",
    "2. **Anchor Explanations** - объяснения в виде правил (if-then)\n",
    "3. **Concept-based Explanations (TCAV-like)** - объяснения через высокоуровневые концепции\n",
    "\n",
    "### Задача\n",
    "\n",
    "Предсказание болезни сердца на основе клинических показателей. Медицинская область требует особенно высокого уровня объяснимости, так как решения влияют на здоровье пациентов.\n",
    "\n",
    "### Датасет\n",
    "\n",
    "Синтетический датасет с клиническими показателями (~8,000 пациентов), включающий понятные медицинские признаки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a50e855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Глубокое обучение\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"Библиотеки загружены успешно\")\n",
    "print(f\"TensorFlow версия: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd45f352",
   "metadata": {},
   "source": [
    "## 1. Создание медицинского датасета\n",
    "\n",
    "### Описание признаков\n",
    "\n",
    "Наш датасет содержит клинические показатели пациентов:\n",
    "\n",
    "**Демографические:**\n",
    "- **age** - возраст (лет)\n",
    "- **sex** - пол (0 = женский, 1 = мужской)\n",
    "\n",
    "**Клинические измерения:**\n",
    "- **blood_pressure** - систолическое давление (мм рт. ст.)\n",
    "- **cholesterol** - холестерин (мг/дл)\n",
    "- **blood_sugar** - уровень сахара натощак (мг/дл)\n",
    "- **heart_rate** - пульс в покое (уд/мин)\n",
    "\n",
    "**Результаты обследований:**\n",
    "- **ecg_abnormal** - аномалии на ЭКГ (0/1)\n",
    "- **exercise_angina** - стенокардия при нагрузке (0/1)\n",
    "- **st_depression** - депрессия сегмента ST\n",
    "- **vessels_colored** - число основных сосудов, окрашенных при флюороскопии (0-3)\n",
    "\n",
    "### Целевая переменная\n",
    "- **heart_disease** - наличие болезни сердца (0/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcca1224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heart_disease_data(n_samples=8000):\n",
    "    \"\"\"\n",
    "    Создание синтетического датасета для диагностики болезни сердца.\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "    n_samples : int\n",
    "        Количество пациентов\n",
    "\n",
    "    Возвращает:\n",
    "    -----------\n",
    "    DataFrame с клиническими показателями и диагнозом\n",
    "    \"\"\"\n",
    "    # Базовые характеристики\n",
    "    age = np.random.normal(55, 12, n_samples).clip(25, 85)\n",
    "    sex = np.random.binomial(1, 0.6, n_samples)  # 60% мужчин\n",
    "\n",
    "    # Клинические показатели (зависят от возраста и пола)\n",
    "    blood_pressure = (120 + 0.5 * (age - 50) + 5 * sex +\n",
    "                      np.random.normal(0, 15, n_samples)).clip(90, 200)\n",
    "\n",
    "    cholesterol = (200 + 0.8 * (age - 50) + 10 * sex +\n",
    "                   np.random.normal(0, 30, n_samples)).clip(120, 400)\n",
    "\n",
    "    blood_sugar = (90 + 0.3 * (age - 50) +\n",
    "                   np.random.normal(0, 20, n_samples)).clip(60, 200)\n",
    "\n",
    "    heart_rate = (72 - 0.1 * (age - 50) +\n",
    "                  np.random.normal(0, 10, n_samples)).clip(50, 120)\n",
    "\n",
    "    # Результаты обследований\n",
    "    ecg_prob = 0.1 + 0.005 * (age - 50) + 0.05 * sex\n",
    "    ecg_abnormal = np.random.binomial(1, ecg_prob.clip(0, 1))\n",
    "\n",
    "    exercise_angina_prob = 0.15 + 0.008 * (age - 50) + 0.1 * sex\n",
    "    exercise_angina = np.random.binomial(1, exercise_angina_prob.clip(0, 1))\n",
    "\n",
    "    st_depression = (0.5 + 0.02 * (age - 50) +\n",
    "                     np.random.exponential(0.5, n_samples)).clip(0, 5)\n",
    "\n",
    "    vessels_colored = np.random.poisson(0.5 + 0.02 * (age - 50), n_samples).clip(0, 3)\n",
    "\n",
    "    # Вероятность болезни сердца (логистическая модель)\n",
    "    logit = (-5 +\n",
    "             0.05 * (age - 50) +\n",
    "             0.5 * sex +\n",
    "             0.02 * (blood_pressure - 120) +\n",
    "             0.01 * (cholesterol - 200) +\n",
    "             0.01 * (blood_sugar - 100) +\n",
    "             0.02 * (heart_rate - 72) +\n",
    "             1.0 * ecg_abnormal +\n",
    "             1.5 * exercise_angina +\n",
    "             0.8 * st_depression +\n",
    "             0.7 * vessels_colored)\n",
    "\n",
    "    prob = 1 / (1 + np.exp(-logit))\n",
    "    heart_disease = np.random.binomial(1, prob)\n",
    "\n",
    "    # Создаём DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'age': age,\n",
    "        'sex': sex,\n",
    "        'blood_pressure': blood_pressure,\n",
    "        'cholesterol': cholesterol,\n",
    "        'blood_sugar': blood_sugar,\n",
    "        'heart_rate': heart_rate,\n",
    "        'ecg_abnormal': ecg_abnormal,\n",
    "        'exercise_angina': exercise_angina,\n",
    "        'st_depression': st_depression,\n",
    "        'vessels_colored': vessels_colored,\n",
    "        'heart_disease': heart_disease\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "# Создаём датасет\n",
    "df = create_heart_disease_data(n_samples=8000)\n",
    "\n",
    "print(f\"Размер датасета: {df.shape}\")\n",
    "print(f\"\\nРаспределение диагнозов:\")\n",
    "print(df['heart_disease'].value_counts())\n",
    "print(f\"\\nДоля больных: {df['heart_disease'].mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61637856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Статистика по признакам\n",
    "print(\"Статистика признаков:\")\n",
    "print(df.describe().round(2))\n",
    "\n",
    "# Корреляция с целевой переменной\n",
    "correlations = df.corr()['heart_disease'].drop('heart_disease').sort_values(ascending=False)\n",
    "print(\"\\nКорреляция с болезнью сердца:\")\n",
    "print(correlations.round(3))\n",
    "\n",
    "# Визуализация\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Распределения ключевых признаков\n",
    "key_features = ['age', 'blood_pressure', 'cholesterol', 'st_depression', 'vessels_colored', 'heart_rate']\n",
    "\n",
    "for i, col in enumerate(key_features):\n",
    "    ax = axes[i]\n",
    "    for label in [0, 1]:\n",
    "        subset = df[df['heart_disease'] == label][col]\n",
    "        ax.hist(subset, bins=30, alpha=0.5,\n",
    "                label='Здоров' if label == 0 else 'Болен', density=True)\n",
    "    ax.set_title(col)\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle('Распределение признаков по диагнозу', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e803971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка данных\n",
    "feature_cols = ['age', 'sex', 'blood_pressure', 'cholesterol', 'blood_sugar',\n",
    "                'heart_rate', 'ecg_abnormal', 'exercise_angina',\n",
    "                'st_depression', 'vessels_colored']\n",
    "\n",
    "X = df[feature_cols].values\n",
    "y = df['heart_disease'].values\n",
    "\n",
    "# Разделение\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Масштабирование\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Обучающая выборка: {X_train.shape}\")\n",
    "print(f\"Тестовая выборка: {X_test.shape}\")\n",
    "\n",
    "# Построение нейронной сети\n",
    "def build_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Обучение\n",
    "model = build_model(X_train_scaled.shape[1])\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Оценка\n",
    "y_pred_proba = model.predict(X_test_scaled, verbose=0).flatten()\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nПроизводительность модели:\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.3f}\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Здоров', 'Болен']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe732ba",
   "metadata": {},
   "source": [
    "## 2. Integrated Gradients\n",
    "\n",
    "### Теория\n",
    "\n",
    "Integrated Gradients (IG) - это метод атрибуции, который удовлетворяет двум важным аксиомам:\n",
    "\n",
    "1. **Чувствительность (Sensitivity)** - если признак влияет на предсказание, он должен получить ненулевую атрибуцию\n",
    "2. **Инвариантность к реализации (Implementation Invariance)** - атрибуции не должны зависеть от внутренней реализации модели\n",
    "\n",
    "### Формула\n",
    "\n",
    "$$IG_i(x) = (x_i - x'_i) \\times \\int_{\\alpha=0}^{1} \\frac{\\partial F(x' + \\alpha(x - x'))}{\\partial x_i} d\\alpha$$\n",
    "\n",
    "где:\n",
    "- $x$ - входной пример\n",
    "- $x'$ - базовый пример (baseline, обычно нули)\n",
    "- $F$ - функция модели\n",
    "- $\\alpha$ - параметр интерполяции\n",
    "\n",
    "### Интуиция\n",
    "\n",
    "Мы \"идём\" от базового примера к нашему входу и накапливаем градиенты вдоль пути. Это показывает, как каждый признак способствует изменению предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3b4abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrated_gradients(model, input_data, baseline=None, steps=50):\n",
    "    \"\"\"\n",
    "    Вычисление Integrated Gradients для объяснения предсказаний.\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "    model : Keras model\n",
    "        Обученная модель\n",
    "    input_data : array\n",
    "        Входные данные для объяснения\n",
    "    baseline : array\n",
    "        Базовый пример (по умолчанию нули)\n",
    "    steps : int\n",
    "        Число шагов интегрирования\n",
    "\n",
    "    Возвращает:\n",
    "    -----------\n",
    "    attributions : array\n",
    "        Атрибуции для каждого признака\n",
    "    \"\"\"\n",
    "    if baseline is None:\n",
    "        baseline = np.zeros_like(input_data)\n",
    "\n",
    "    # Преобразуем в тензоры\n",
    "    input_tensor = tf.constant(input_data, dtype=tf.float32)\n",
    "    baseline_tensor = tf.constant(baseline, dtype=tf.float32)\n",
    "\n",
    "    # Создаём интерполированные входы\n",
    "    alphas = tf.linspace(0.0, 1.0, steps + 1)\n",
    "\n",
    "    # Для batch обработки\n",
    "    if len(input_data.shape) == 1:\n",
    "        input_tensor = tf.expand_dims(input_tensor, 0)\n",
    "        baseline_tensor = tf.expand_dims(baseline_tensor, 0)\n",
    "\n",
    "    # Интерполяция между baseline и input\n",
    "    interpolated_inputs = []\n",
    "    for alpha in alphas:\n",
    "        interpolated = baseline_tensor + alpha * (input_tensor - baseline_tensor)\n",
    "        interpolated_inputs.append(interpolated)\n",
    "\n",
    "    interpolated_inputs = tf.concat(interpolated_inputs, axis=0)\n",
    "\n",
    "    # Вычисляем градиенты\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(interpolated_inputs)\n",
    "        predictions = model(interpolated_inputs)\n",
    "\n",
    "    gradients = tape.gradient(predictions, interpolated_inputs)\n",
    "\n",
    "    # Интегрируем градиенты (трапецеидальное правило)\n",
    "    gradients = gradients.numpy()\n",
    "    avg_gradients = np.mean(gradients[:-1] + gradients[1:], axis=0) / 2\n",
    "\n",
    "    # Атрибуции = средний градиент * (input - baseline)\n",
    "    attributions = avg_gradients * (input_data - baseline)\n",
    "\n",
    "    return attributions\n",
    "\n",
    "print(\"Функция Integrated Gradients определена\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73db27bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применяем Integrated Gradients к тестовым примерам\n",
    "print(\"Вычисление Integrated Gradients...\")\n",
    "\n",
    "# Выбираем примеры для объяснения\n",
    "n_explain = 100\n",
    "X_explain = X_test_scaled[:n_explain]\n",
    "y_explain = y_test[:n_explain]\n",
    "pred_explain = y_pred_proba[:n_explain]\n",
    "\n",
    "# Вычисляем атрибуции для каждого примера\n",
    "all_attributions = []\n",
    "\n",
    "for i in range(n_explain):\n",
    "    attr = integrated_gradients(model, X_explain[i], steps=50)\n",
    "    all_attributions.append(attr.flatten())\n",
    "\n",
    "attributions = np.array(all_attributions)\n",
    "\n",
    "print(f\"Атрибуции вычислены: {attributions.shape}\")\n",
    "\n",
    "# Средняя важность признаков (по абсолютным значениям)\n",
    "mean_abs_attr = np.abs(attributions).mean(axis=0)\n",
    "\n",
    "# Визуализация\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Средняя важность\n",
    "ax1 = axes[0]\n",
    "sorted_idx = np.argsort(mean_abs_attr)\n",
    "ax1.barh([feature_cols[i] for i in sorted_idx], mean_abs_attr[sorted_idx])\n",
    "ax1.set_xlabel('Средняя |атрибуция|')\n",
    "ax1.set_title('Integrated Gradients: Важность признаков')\n",
    "\n",
    "# Распределение атрибуций\n",
    "ax2 = axes[1]\n",
    "# Beeswarm-подобный plot\n",
    "for i, col in enumerate(feature_cols):\n",
    "    y_jitter = np.random.normal(i, 0.1, n_explain)\n",
    "    colors = ['red' if a > 0 else 'blue' for a in attributions[:, i]]\n",
    "    ax2.scatter(attributions[:, i], y_jitter, c=colors, alpha=0.3, s=10)\n",
    "\n",
    "ax2.set_yticks(range(len(feature_cols)))\n",
    "ax2.set_yticklabels(feature_cols)\n",
    "ax2.axvline(x=0, color='black', linewidth=0.5)\n",
    "ax2.set_xlabel('Атрибуция')\n",
    "ax2.set_title('Распределение атрибуций (красный = увеличивает риск)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d93ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объяснение конкретных пациентов\n",
    "def explain_patient(idx, X_scaled, X_original, y_true, y_pred, attributions, feature_names):\n",
    "    \"\"\"Детальное объяснение предсказания для пациента.\"\"\"\n",
    "    attr = attributions[idx]\n",
    "    pred = y_pred[idx]\n",
    "    true = y_true[idx]\n",
    "\n",
    "    print(f\"Пациент {idx}\")\n",
    "    print(f\"Истинный диагноз: {'Болен' if true else 'Здоров'}\")\n",
    "    print(f\"Предсказание: {pred:.3f} ({'Болен' if pred > 0.5 else 'Здоров'})\")\n",
    "    print(\"\\nВлияние признаков:\")\n",
    "\n",
    "    # Сортируем по абсолютной атрибуции\n",
    "    sorted_idx = np.argsort(np.abs(attr))[::-1]\n",
    "\n",
    "    for i in sorted_idx:\n",
    "        name = feature_names[i]\n",
    "        value = X_original[idx, i]\n",
    "        attribution = attr[i]\n",
    "        direction = \"↑\" if attribution > 0 else \"↓\"\n",
    "\n",
    "        print(f\"  {name}: {value:.1f} → {direction} ({attribution:+.3f})\")\n",
    "\n",
    "# Примеры пациентов\n",
    "# Истинно положительный (больной, правильно распознан)\n",
    "tp_idx = np.where((y_explain == 1) & (pred_explain > 0.7))[0]\n",
    "if len(tp_idx) > 0:\n",
    "    print(\"=\" * 50)\n",
    "    print(\"ИСТИННО ПОЛОЖИТЕЛЬНЫЙ (высокий риск)\")\n",
    "    print(\"=\" * 50)\n",
    "    explain_patient(tp_idx[0], X_explain, X_test[:n_explain],\n",
    "                   y_explain, pred_explain, attributions, feature_cols)\n",
    "\n",
    "# Истинно отрицательный (здоровый, правильно распознан)\n",
    "tn_idx = np.where((y_explain == 0) & (pred_explain < 0.3))[0]\n",
    "if len(tn_idx) > 0:\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"ИСТИННО ОТРИЦАТЕЛЬНЫЙ (низкий риск)\")\n",
    "    print(\"=\" * 50)\n",
    "    explain_patient(tn_idx[0], X_explain, X_test[:n_explain],\n",
    "                   y_explain, pred_explain, attributions, feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45096499",
   "metadata": {},
   "source": [
    "## 3. Anchor Explanations\n",
    "\n",
    "### Теория\n",
    "\n",
    "Anchors - это объяснения в виде правил \"if-then\", которые достаточны для предсказания. Anchor - это набор условий, которые \"закрепляют\" предсказание с высокой вероятностью.\n",
    "\n",
    "### Формальное определение\n",
    "\n",
    "Anchor $A$ для примера $x$ с предсказанием $f(x)$:\n",
    "\n",
    "$$P(f(z) = f(x) | A(z) = 1) \\geq \\tau$$\n",
    "\n",
    "где:\n",
    "- $z$ - примеры из распределения данных\n",
    "- $A(z) = 1$ означает, что пример $z$ удовлетворяет условиям anchor\n",
    "- $\\tau$ - требуемая точность (precision)\n",
    "\n",
    "### Преимущества\n",
    "\n",
    "1. **Понятность** - правила легко интерпретировать\n",
    "2. **Локальная точность** - высокая точность в области применения\n",
    "3. **Actionable** - понятно, что нужно изменить\n",
    "\n",
    "### Пример\n",
    "\n",
    "\"ЕСЛИ возраст > 60 И холестерин > 250 И exercise_angina = 1, ТО высокий риск болезни сердца (точность 95%)\"\n",
    "\n",
    "Мы реализуем упрощённую версию поиска anchors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e66b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAnchorExplainer:\n",
    "    \"\"\"\n",
    "    Упрощённая реализация Anchor Explanations.\n",
    "\n",
    "    Ищет правила (условия на признаки), которые с высокой\n",
    "    вероятностью приводят к определённому предсказанию.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, X_train, feature_names, percentiles=[25, 50, 75]):\n",
    "        self.model = model\n",
    "        self.X_train = X_train\n",
    "        self.feature_names = feature_names\n",
    "        self.n_features = len(feature_names)\n",
    "\n",
    "        # Вычисляем пороги для каждого признака\n",
    "        self.thresholds = {}\n",
    "        for i, name in enumerate(feature_names):\n",
    "            values = X_train[:, i]\n",
    "            self.thresholds[i] = np.percentile(values, percentiles)\n",
    "\n",
    "    def _generate_conditions(self, instance):\n",
    "        \"\"\"Генерация возможных условий для примера.\"\"\"\n",
    "        conditions = []\n",
    "\n",
    "        for i in range(self.n_features):\n",
    "            value = instance[i]\n",
    "            thresholds = self.thresholds[i]\n",
    "\n",
    "            # Условия вида \"feature < threshold\" или \"feature >= threshold\"\n",
    "            for t in thresholds:\n",
    "                if value < t:\n",
    "                    conditions.append((i, '<', t))\n",
    "                else:\n",
    "                    conditions.append((i, '>=', t))\n",
    "\n",
    "        return conditions\n",
    "\n",
    "    def _evaluate_anchor(self, conditions, target_pred, n_samples=500):\n",
    "        \"\"\"Оценка качества anchor (precision).\"\"\"\n",
    "        # Генерируем примеры из обучающей выборки\n",
    "        indices = np.random.choice(len(self.X_train), n_samples, replace=True)\n",
    "        samples = self.X_train[indices].copy()\n",
    "\n",
    "        # Фильтруем примеры, удовлетворяющие условиям\n",
    "        mask = np.ones(n_samples, dtype=bool)\n",
    "        for feat_idx, op, threshold in conditions:\n",
    "            if op == '<':\n",
    "                mask &= samples[:, feat_idx] < threshold\n",
    "            else:\n",
    "                mask &= samples[:, feat_idx] >= threshold\n",
    "\n",
    "        if mask.sum() < 10:\n",
    "            return 0, 0  # Недостаточно примеров\n",
    "\n",
    "        # Предсказания для отфильтрованных примеров\n",
    "        filtered_samples = samples[mask]\n",
    "        predictions = (self.model.predict(filtered_samples, verbose=0) > 0.5).astype(int).flatten()\n",
    "\n",
    "        # Precision = доля правильных предсказаний\n",
    "        precision = (predictions == target_pred).mean()\n",
    "        coverage = mask.sum() / n_samples\n",
    "\n",
    "        return precision, coverage\n",
    "\n",
    "    def explain(self, instance, min_precision=0.9, max_conditions=4):\n",
    "        \"\"\"\n",
    "        Найти anchor для примера.\n",
    "\n",
    "        Параметры:\n",
    "        ----------\n",
    "        instance : array\n",
    "            Пример для объяснения\n",
    "        min_precision : float\n",
    "            Минимальная требуемая точность\n",
    "        max_conditions : int\n",
    "            Максимальное число условий\n",
    "\n",
    "        Возвращает:\n",
    "        -----------\n",
    "        anchor : list of conditions\n",
    "        precision : float\n",
    "        coverage : float\n",
    "        \"\"\"\n",
    "        # Предсказание для примера\n",
    "        pred = int(self.model.predict(instance.reshape(1, -1), verbose=0) > 0.5)\n",
    "\n",
    "        # Генерируем условия\n",
    "        all_conditions = self._generate_conditions(instance)\n",
    "\n",
    "        # Жадный поиск лучшего anchor\n",
    "        best_anchor = []\n",
    "        best_precision = 0\n",
    "        best_coverage = 0\n",
    "\n",
    "        current_anchor = []\n",
    "        remaining_conditions = all_conditions.copy()\n",
    "\n",
    "        for _ in range(max_conditions):\n",
    "            best_next = None\n",
    "            best_next_score = -1\n",
    "\n",
    "            for cond in remaining_conditions:\n",
    "                test_anchor = current_anchor + [cond]\n",
    "                precision, coverage = self._evaluate_anchor(test_anchor, pred)\n",
    "\n",
    "                # Скор = precision * sqrt(coverage)\n",
    "                score = precision * np.sqrt(coverage) if coverage > 0 else 0\n",
    "\n",
    "                if score > best_next_score:\n",
    "                    best_next_score = score\n",
    "                    best_next = cond\n",
    "                    best_next_precision = precision\n",
    "                    best_next_coverage = coverage\n",
    "\n",
    "            if best_next is None:\n",
    "                break\n",
    "\n",
    "            current_anchor.append(best_next)\n",
    "            remaining_conditions.remove(best_next)\n",
    "\n",
    "            if best_next_precision >= min_precision:\n",
    "                best_anchor = current_anchor.copy()\n",
    "                best_precision = best_next_precision\n",
    "                best_coverage = best_next_coverage\n",
    "\n",
    "        return best_anchor, best_precision, best_coverage, pred\n",
    "\n",
    "    def format_anchor(self, anchor, pred):\n",
    "        \"\"\"Форматирование anchor в читаемый вид.\"\"\"\n",
    "        if not anchor:\n",
    "            return \"Не найдено правило с достаточной точностью\"\n",
    "\n",
    "        conditions_str = []\n",
    "        for feat_idx, op, threshold in anchor:\n",
    "            name = self.feature_names[feat_idx]\n",
    "            conditions_str.append(f\"{name} {op} {threshold:.1f}\")\n",
    "\n",
    "        rule = \" И \".join(conditions_str)\n",
    "        outcome = \"Высокий риск\" if pred == 1 else \"Низкий риск\"\n",
    "\n",
    "        return f\"ЕСЛИ {rule}, ТО {outcome}\"\n",
    "\n",
    "print(\"Класс SimpleAnchorExplainer определён\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b59ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём explainer\n",
    "anchor_explainer = SimpleAnchorExplainer(\n",
    "    model, X_train_scaled, feature_cols\n",
    ")\n",
    "\n",
    "# Объясняем несколько пациентов\n",
    "print(\"Поиск Anchor объяснений...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Пациенты с высоким риском\n",
    "high_risk_idx = np.where(pred_explain > 0.7)[0][:3]\n",
    "\n",
    "print(\"\\nПАЦИЕНТЫ С ВЫСОКИМ РИСКОМ:\")\n",
    "for idx in high_risk_idx:\n",
    "    anchor, precision, coverage, pred = anchor_explainer.explain(\n",
    "        X_explain[idx], min_precision=0.85\n",
    "    )\n",
    "\n",
    "    rule = anchor_explainer.format_anchor(anchor, pred)\n",
    "\n",
    "    print(f\"\\nПациент {idx} (предсказание: {pred_explain[idx]:.3f})\")\n",
    "    print(f\"  Правило: {rule}\")\n",
    "    print(f\"  Точность: {precision*100:.1f}%\")\n",
    "    print(f\"  Покрытие: {coverage*100:.1f}%\")\n",
    "\n",
    "# Пациенты с низким риском\n",
    "low_risk_idx = np.where(pred_explain < 0.3)[0][:3]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nПАЦИЕНТЫ С НИЗКИМ РИСКОМ:\")\n",
    "for idx in low_risk_idx:\n",
    "    anchor, precision, coverage, pred = anchor_explainer.explain(\n",
    "        X_explain[idx], min_precision=0.85\n",
    "    )\n",
    "\n",
    "    rule = anchor_explainer.format_anchor(anchor, pred)\n",
    "\n",
    "    print(f\"\\nПациент {idx} (предсказание: {pred_explain[idx]:.3f})\")\n",
    "    print(f\"  Правило: {rule}\")\n",
    "    print(f\"  Точность: {precision*100:.1f}%\")\n",
    "    print(f\"  Покрытие: {coverage*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45710348",
   "metadata": {},
   "source": [
    "## 4. Concept-based Explanations (TCAV-like)\n",
    "\n",
    "### Теория\n",
    "\n",
    "TCAV (Testing with Concept Activation Vectors) объясняет предсказания модели через высокоуровневые понятия, понятные человеку.\n",
    "\n",
    "### Идея\n",
    "\n",
    "Вместо объяснения через отдельные признаки, мы используем **концепции** - группы признаков, объединённые смыслом:\n",
    "\n",
    "- **Сердечно-сосудистые факторы**: давление, холестерин, пульс\n",
    "- **Возрастные факторы**: возраст, пол\n",
    "- **Результаты тестов**: ЭКГ, стенокардия, ST-депрессия, сосуды\n",
    "\n",
    "### Алгоритм (упрощённый)\n",
    "\n",
    "1. Определяем концепции как группы признаков\n",
    "2. Для каждой концепции вычисляем \"направление\" в пространстве активаций\n",
    "3. Измеряем, насколько предсказание чувствительно к движению в этом направлении\n",
    "\n",
    "### Преимущества\n",
    "\n",
    "- Объяснения на уровне медицинских понятий\n",
    "- Легче для понимания врачами\n",
    "- Можно проверять гипотезы о модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c486934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение медицинских концепций\n",
    "CONCEPTS = {\n",
    "    'Сердечно-сосудистые': ['blood_pressure', 'cholesterol', 'heart_rate'],\n",
    "    'Демографические': ['age', 'sex'],\n",
    "    'Результаты тестов': ['ecg_abnormal', 'exercise_angina', 'st_depression', 'vessels_colored'],\n",
    "    'Метаболические': ['blood_sugar', 'cholesterol']\n",
    "}\n",
    "\n",
    "# Индексы признаков для каждой концепции\n",
    "concept_indices = {}\n",
    "for concept_name, features in CONCEPTS.items():\n",
    "    indices = [feature_cols.index(f) for f in features if f in feature_cols]\n",
    "    concept_indices[concept_name] = indices\n",
    "    print(f\"{concept_name}: {features} → индексы {indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd270432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concept_sensitivity(model, X, concept_indices, epsilon=0.1, n_samples=100):\n",
    "    \"\"\"\n",
    "    Вычисление чувствительности предсказания к концепции.\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "    model : Keras model\n",
    "        Обученная модель\n",
    "    X : array\n",
    "        Данные для анализа\n",
    "    concept_indices : list\n",
    "        Индексы признаков концепции\n",
    "    epsilon : float\n",
    "        Величина возмущения\n",
    "    n_samples : int\n",
    "        Число примеров для анализа\n",
    "\n",
    "    Возвращает:\n",
    "    -----------\n",
    "    sensitivity : float\n",
    "        Средняя чувствительность\n",
    "    sensitivities : array\n",
    "        Чувствительность для каждого примера\n",
    "    \"\"\"\n",
    "    # Выбираем подмножество данных\n",
    "    if len(X) > n_samples:\n",
    "        idx = np.random.choice(len(X), n_samples, replace=False)\n",
    "        X_sample = X[idx]\n",
    "    else:\n",
    "        X_sample = X\n",
    "\n",
    "    # Базовые предсказания\n",
    "    base_preds = model.predict(X_sample, verbose=0).flatten()\n",
    "\n",
    "    # Возмущаем признаки концепции\n",
    "    X_perturbed = X_sample.copy()\n",
    "    for i in concept_indices:\n",
    "        X_perturbed[:, i] += epsilon * np.std(X_sample[:, i])\n",
    "\n",
    "    # Предсказания после возмущения\n",
    "    perturbed_preds = model.predict(X_perturbed, verbose=0).flatten()\n",
    "\n",
    "    # Чувствительность = изменение предсказания\n",
    "    sensitivities = perturbed_preds - base_preds\n",
    "\n",
    "    return np.mean(sensitivities), sensitivities\n",
    "\n",
    "# Вычисляем чувствительность для каждой концепции\n",
    "print(\"Анализ чувствительности к концепциям:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "concept_results = {}\n",
    "\n",
    "for concept_name, indices in concept_indices.items():\n",
    "    mean_sens, all_sens = concept_sensitivity(\n",
    "        model, X_test_scaled, indices, epsilon=0.5\n",
    "    )\n",
    "\n",
    "    concept_results[concept_name] = {\n",
    "        'mean': mean_sens,\n",
    "        'std': np.std(all_sens),\n",
    "        'sensitivities': all_sens\n",
    "    }\n",
    "\n",
    "    direction = \"увеличивает\" if mean_sens > 0 else \"уменьшает\"\n",
    "    print(f\"\\n{concept_name}:\")\n",
    "    print(f\"  Среднее изменение: {mean_sens:+.4f}\")\n",
    "    print(f\"  Интерпретация: {direction} риск болезни сердца\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b7cf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация чувствительности к концепциям\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# 1. Средняя чувствительность\n",
    "ax1 = axes[0]\n",
    "concepts = list(concept_results.keys())\n",
    "means = [concept_results[c]['mean'] for c in concepts]\n",
    "stds = [concept_results[c]['std'] for c in concepts]\n",
    "\n",
    "colors = ['red' if m > 0 else 'blue' for m in means]\n",
    "bars = ax1.barh(concepts, means, xerr=stds, color=colors, alpha=0.7, capsize=5)\n",
    "\n",
    "ax1.axvline(x=0, color='black', linewidth=0.5)\n",
    "ax1.set_xlabel('Средняя чувствительность')\n",
    "ax1.set_title('Влияние концепций на риск болезни сердца\\n(красный = увеличивает риск)')\n",
    "\n",
    "# 2. Распределение чувствительности\n",
    "ax2 = axes[1]\n",
    "for i, concept in enumerate(concepts):\n",
    "    sens = concept_results[concept]['sensitivities']\n",
    "    y_jitter = np.random.normal(i, 0.1, len(sens))\n",
    "    ax2.scatter(sens, y_jitter, alpha=0.3, s=20)\n",
    "\n",
    "ax2.set_yticks(range(len(concepts)))\n",
    "ax2.set_yticklabels(concepts)\n",
    "ax2.axvline(x=0, color='black', linewidth=0.5)\n",
    "ax2.set_xlabel('Чувствительность')\n",
    "ax2.set_title('Распределение чувствительности по примерам')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0039f850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объяснение конкретного пациента через концепции\n",
    "def explain_with_concepts(model, patient_data, concept_indices, feature_names, epsilon=0.5):\n",
    "    \"\"\"\n",
    "    Объяснение предсказания через концепции.\n",
    "    \"\"\"\n",
    "    # Базовое предсказание\n",
    "    base_pred = model.predict(patient_data.reshape(1, -1), verbose=0)[0, 0]\n",
    "\n",
    "    explanations = []\n",
    "\n",
    "    for concept_name, indices in concept_indices.items():\n",
    "        # Возмущаем концепцию\n",
    "        perturbed = patient_data.copy()\n",
    "        for i in indices:\n",
    "            perturbed[i] += epsilon * np.std(X_train_scaled[:, i])\n",
    "\n",
    "        new_pred = model.predict(perturbed.reshape(1, -1), verbose=0)[0, 0]\n",
    "        change = new_pred - base_pred\n",
    "\n",
    "        explanations.append({\n",
    "            'concept': concept_name,\n",
    "            'change': change,\n",
    "            'features': [feature_names[i] for i in indices]\n",
    "        })\n",
    "\n",
    "    return base_pred, sorted(explanations, key=lambda x: abs(x['change']), reverse=True)\n",
    "\n",
    "# Объясняем пациента с высоким риском\n",
    "high_risk_patient_idx = np.argmax(pred_explain)\n",
    "patient_data = X_explain[high_risk_patient_idx]\n",
    "\n",
    "base_pred, explanations = explain_with_concepts(\n",
    "    model, patient_data, concept_indices, feature_cols\n",
    ")\n",
    "\n",
    "print(\"ОБЪЯСНЕНИЕ ЧЕРЕЗ КОНЦЕПЦИИ\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nПациент {high_risk_patient_idx}\")\n",
    "print(f\"Базовое предсказание: {base_pred:.3f} ({'Высокий риск' if base_pred > 0.5 else 'Низкий риск'})\")\n",
    "print(\"\\nВлияние концепций (при увеличении):\")\n",
    "\n",
    "for exp in explanations:\n",
    "    direction = \"↑\" if exp['change'] > 0 else \"↓\"\n",
    "    print(f\"\\n  {exp['concept']}:\")\n",
    "    print(f\"    Изменение: {exp['change']:+.4f} {direction}\")\n",
    "    print(f\"    Признаки: {', '.join(exp['features'])}\")\n",
    "\n",
    "    if exp['change'] > 0.01:\n",
    "        print(\"    → Увеличивает риск\")\n",
    "    elif exp['change'] < -0.01:\n",
    "        print(\"    → Уменьшает риск\")\n",
    "    else:\n",
    "        print(\"    → Минимальное влияние\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212e7aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сравнение всех методов XAI\n",
    "print(\"=\" * 60)\n",
    "print(\"СРАВНЕНИЕ МЕТОДОВ ОБЪЯСНИМОГО ИИ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. Integrated Gradients\")\n",
    "print(\"   Тип: Атрибуция на уровне признаков\")\n",
    "print(\"   Формат: Числовой вклад каждого признака\")\n",
    "print(\"   Плюсы: Теоретические гарантии, точность\")\n",
    "print(\"   Минусы: Числа сложно интерпретировать\")\n",
    "print(\"   Аудитория: Data scientists, исследователи\")\n",
    "\n",
    "print(\"\\n2. Anchor Explanations\")\n",
    "print(\"   Тип: Правила (if-then)\")\n",
    "print(\"   Формат: 'ЕСЛИ условия, ТО предсказание'\")\n",
    "print(\"   Плюсы: Понятные правила, actionable\")\n",
    "print(\"   Минусы: Упрощение, может терять нюансы\")\n",
    "print(\"   Аудитория: Врачи, пациенты, регуляторы\")\n",
    "\n",
    "print(\"\\n3. Concept-based (TCAV-like)\")\n",
    "print(\"   Тип: Высокоуровневые концепции\")\n",
    "print(\"   Формат: Влияние медицинских понятий\")\n",
    "print(\"   Плюсы: Соответствует экспертному мышлению\")\n",
    "print(\"   Минусы: Требует определения концепций\")\n",
    "print(\"   Аудитория: Врачи, медицинские эксперты\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"РЕКОМЕНДАЦИИ ДЛЯ МЕДИЦИНСКИХ ПРИЛОЖЕНИЙ\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n• Для пациентов: Anchor Explanations\")\n",
    "print(\"  'Высокий риск из-за повышенного давления и холестерина'\")\n",
    "print(\"\\n• Для врачей: Concept-based + Anchors\")\n",
    "print(\"  Связь с медицинскими понятиями + конкретные правила\")\n",
    "print(\"\\n• Для аудита: Integrated Gradients\")\n",
    "print(\"  Полная атрибуция для проверки модели\")\n",
    "print(\"\\n• Рекомендация: Комбинация всех трёх методов\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e530550",
   "metadata": {},
   "source": [
    "## Заключение\n",
    "\n",
    "### Ключевые результаты\n",
    "\n",
    "1. **Integrated Gradients** показал, что наибольший вклад в предсказание вносят:\n",
    "   - exercise_angina (стенокардия при нагрузке)\n",
    "   - st_depression (депрессия сегмента ST)\n",
    "   - vessels_colored (число поражённых сосудов)\n",
    "\n",
    "2. **Anchor Explanations** нашёл интерпретируемые правила, например:\n",
    "   - \"ЕСЛИ st_depression >= 1.2 И exercise_angina = 1, ТО высокий риск\"\n",
    "\n",
    "3. **Concept-based анализ** показал, что:\n",
    "   - \"Результаты тестов\" - наиболее влиятельная концепция\n",
    "   - \"Сердечно-сосудистые факторы\" также значимы\n",
    "\n",
    "### Особенности датасета\n",
    "\n",
    "- 8,000 синтетических пациентов\n",
    "- 10 клинических признаков\n",
    "- Реалистичная зависимость диагноза от признаков\n",
    "\n",
    "### Практическое применение\n",
    "\n",
    "1. **Клиническая поддержка** - помощь врачам в принятии решений\n",
    "2. **Коммуникация с пациентами** - понятные объяснения рисков\n",
    "3. **Регуляторное соответствие** - документирование решений ИИ\n",
    "4. **Проверка модели** - выявление нежелательных паттернов\n",
    "\n",
    "### Ограничения и предупреждения\n",
    "\n",
    "- Это учебный пример, не для реального медицинского использования\n",
    "- Реальные медицинские модели требуют тщательной валидации\n",
    "- Объяснения должны проверяться медицинскими экспертами\n",
    "\n",
    "### Дальнейшее развитие\n",
    "\n",
    "- Интеграция с электронными медицинскими картами\n",
    "- Онлайн-обучение для персонализации\n",
    "- Мультимодальные объяснения (текст + визуализация)\n",
    "- Валидация объяснений с врачами"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† RNN/LSTM/GRU –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤\n",
    "\n",
    "**Phase 3: Temporal Data & RNN - Step 2**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ –¶–µ–ª–∏ –Ω–æ—É—Ç–±—É–∫–∞\n",
    "\n",
    "1. **–ü–æ–Ω—è—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã RNN:** Vanilla RNN, LSTM, GRU\n",
    "2. **–ü—Ä–æ–±–ª–µ–º—ã RNN:** vanishing/exploding gradients\n",
    "3. **–ü—Ä–∞–∫—Ç–∏–∫–∞ –Ω–∞ PyTorch:** —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "4. **–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∫–ª–∞—Å—Å–∏–∫–æ–π:** RNN vs ARIMA/SARIMA\n",
    "5. **–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏:** Bidirectional, Stacked RNN\n",
    "\n",
    "---\n",
    "\n",
    "## üìä –î–∞—Ç–∞—Å–µ—Ç: Airline Passengers\n",
    "\n",
    "**–ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ –¥–∞—Ç–∞—Å–µ—Ç** –¥–ª—è —á–µ—Å—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å ARIMA/SARIMA/Prophet.\n",
    "\n",
    "**–ó–∞—á–µ–º Deep Learning?**\n",
    "- üìà **–ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã:** RNN –º–æ–≥—É—Ç –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏\n",
    "- üîó **–î–æ–ª–≥–æ—Å—Ä–æ—á–Ω–∞—è –ø–∞–º—è—Ç—å:** LSTM –¥–ª—è long-term dependencies\n",
    "- üß† **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:** –Ω–µ –Ω—É–∂–Ω–æ –≤—Ä—É—á–Ω—É—é —Å–æ–∑–¥–∞–≤–∞—Ç—å –ª–∞–≥–∏\n",
    "- üéØ **Multivariate –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å:** –ª–µ–≥–∫–æ –¥–æ–±–∞–≤–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "\n",
    "**–ß–µ—Å—Ç–Ω–æ–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ:**\n",
    "- ‚ùå **RNN –ù–ï –≤—Å–µ–≥–¥–∞ –ª—É—á—à–µ** –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç–æ–¥–æ–≤ –¥–ª—è univariate TS\n",
    "- ‚ùå **–¢—Ä–µ–±—É—é—Ç –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö** (–Ω–∞—à –¥–∞—Ç–∞—Å–µ—Ç –º–∞–ª–µ–Ω—å–∫–∏–π - 144 —Ç–æ—á–∫–∏)\n",
    "- ‚ùå **–î–æ–ª—å—à–µ –æ–±—É—á–∞—é—Ç—Å—è** –∏ —Å–ª–æ–∂–Ω–µ–µ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å\n",
    "- ‚úÖ **–¶–µ–ª—å:** –ø–æ–Ω—è—Ç—å –æ—Å–Ω–æ–≤—ã –¥–ª—è –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö –ß–∞—Å—Ç—å 1: –¢–µ–æ—Ä–∏—è —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö —Å–µ—Ç–µ–π\n",
    "\n",
    "### 1.1 Vanilla RNN (–ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–∞—è —Å–µ—Ç—å)\n",
    "\n",
    "**–ò–¥–µ—è:** –ù–∞ –∫–∞–∂–¥–æ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–º —à–∞–≥–µ —Å–µ—Ç—å –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Ö–æ–¥ $x_t$ –∏ —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ $h_{t-1}$.\n",
    "\n",
    "#### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞\n",
    "\n",
    "**Forward pass:**\n",
    "\n",
    "$$h_t = \\tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$$\n",
    "\n",
    "$$y_t = W_{hy} h_t + b_y$$\n",
    "\n",
    "–≥–¥–µ:\n",
    "- $x_t$ ‚Äî –≤—Ö–æ–¥ –Ω–∞ —à–∞–≥–µ $t$ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Å—Å–∞–∂–∏—Ä–æ–≤)\n",
    "- $h_t$ ‚Äî —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ (hidden state) –Ω–∞ —à–∞–≥–µ $t$\n",
    "- $h_{t-1}$ ‚Äî —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —à–∞–≥–∞\n",
    "- $W_{xh}$ ‚Äî –≤–µ—Å–∞ –¥–ª—è –≤—Ö–æ–¥–∞\n",
    "- $W_{hh}$ ‚Äî –≤–µ—Å–∞ –¥–ª—è —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è (—Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ)\n",
    "- $W_{hy}$ ‚Äî –≤–µ—Å–∞ –¥–ª—è –≤—ã—Ö–æ–¥–∞\n",
    "- $\\tanh$ ‚Äî —Ñ—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏\n",
    "\n",
    "**–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è (—Ä–∞–∑–≤–µ—Ä–Ω—É—Ç–∞—è –≤–æ –≤—Ä–µ–º–µ–Ω–∏):**\n",
    "\n",
    "```\n",
    "x‚ÇÅ  ‚Üí  [RNN]  ‚Üí  h‚ÇÅ  ‚Üí  y‚ÇÅ\n",
    "         ‚Üì\n",
    "x‚ÇÇ  ‚Üí  [RNN]  ‚Üí  h‚ÇÇ  ‚Üí  y‚ÇÇ\n",
    "         ‚Üì\n",
    "x‚ÇÉ  ‚Üí  [RNN]  ‚Üí  h‚ÇÉ  ‚Üí  y‚ÇÉ\n",
    "         ‚Üì\n",
    "       ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 –ü—Ä–æ–±–ª–µ–º–∞ Vanishing Gradient\n",
    "\n",
    "**Backpropagation Through Time (BPTT):**\n",
    "\n",
    "–ü—Ä–∏ –æ–±—Ä–∞—Ç–Ω–æ–º —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–∏ —á–µ—Ä–µ–∑ –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial h_1} = \\frac{\\partial L}{\\partial h_T} \\prod_{t=2}^{T} \\frac{\\partial h_t}{\\partial h_{t-1}}$$\n",
    "\n",
    "–ì—Ä–∞–¥–∏–µ–Ω—Ç:\n",
    "\n",
    "$$\\frac{\\partial h_t}{\\partial h_{t-1}} = W_{hh}^T \\cdot \\text{diag}(1 - \\tanh^2(z_t))$$\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º–∞:**\n",
    "- –ï—Å–ª–∏ $\\|W_{hh}\\| < 1$ ‚Üí –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã **–∏—Å—á–µ–∑–∞—é—Ç** (vanishing)\n",
    "- –ï—Å–ª–∏ $\\|W_{hh}\\| > 1$ ‚Üí –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã **–≤–∑—Ä—ã–≤–∞—é—Ç—Å—è** (exploding)\n",
    "\n",
    "**–†–µ–∑—É–ª—å—Ç–∞—Ç:** Vanilla RNN **–ø–ª–æ—Ö–æ –æ–±—É—á–∞–µ—Ç—Å—è** –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö (> 10-20 —à–∞–≥–æ–≤).\n",
    "\n",
    "**–†–µ—à–µ–Ω–∏–µ:** LSTM –∏ GRU!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 LSTM (Long Short-Term Memory)\n",
    "\n",
    "**LSTM —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É vanishing gradient** —á–µ—Ä–µ–∑ –º–µ—Ö–∞–Ω–∏–∑–º **gates** (–≤–æ—Ä–æ—Ç).\n",
    "\n",
    "#### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ LSTM\n",
    "\n",
    "LSTM –∏–º–µ–µ—Ç **–¥–≤–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è:**\n",
    "- $c_t$ ‚Äî **cell state** (–¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–∞—è –ø–∞–º—è—Ç—å)\n",
    "- $h_t$ ‚Äî **hidden state** (–∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–∞—è –ø–∞–º—è—Ç—å)\n",
    "\n",
    "**–¢—Ä–∏ gates:**\n",
    "\n",
    "**1. Forget Gate** (—á—Ç–æ –∑–∞–±—ã—Ç—å –∏–∑ cell state):\n",
    "\n",
    "$$f_t = \\sigma(W_f [h_{t-1}, x_t] + b_f)$$\n",
    "\n",
    "**2. Input Gate** (—á—Ç–æ –¥–æ–±–∞–≤–∏—Ç—å –≤ cell state):\n",
    "\n",
    "$$i_t = \\sigma(W_i [h_{t-1}, x_t] + b_i)$$\n",
    "\n",
    "$$\\tilde{c}_t = \\tanh(W_c [h_{t-1}, x_t] + b_c)$$\n",
    "\n",
    "**3. Output Gate** (—á—Ç–æ –≤—ã–≤–µ—Å—Ç–∏):\n",
    "\n",
    "$$o_t = \\sigma(W_o [h_{t-1}, x_t] + b_o)$$\n",
    "\n",
    "---\n",
    "\n",
    "**–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏–π:**\n",
    "\n",
    "$$c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$$\n",
    "\n",
    "$$h_t = o_t \\odot \\tanh(c_t)$$\n",
    "\n",
    "–≥–¥–µ:\n",
    "- $\\sigma$ ‚Äî sigmoid (–≤—ã—Ö–æ–¥ 0-1, —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ gate)\n",
    "- $\\odot$ ‚Äî element-wise —É–º–Ω–æ–∂–µ–Ω–∏–µ\n",
    "- $[h_{t-1}, x_t]$ ‚Äî –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è\n",
    "\n",
    "---\n",
    "\n",
    "**–ò–Ω—Ç—É–∏—Ü–∏—è:**\n",
    "- **Forget gate:** \"–°–∫–æ–ª—å–∫–æ –∑–∞–±—ã—Ç—å –∏–∑ –ø—Ä–æ—à–ª–æ–≥–æ?\"\n",
    "- **Input gate:** \"–°–∫–æ–ª—å–∫–æ –Ω–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–æ–±–∞–≤–∏—Ç—å?\"\n",
    "- **Output gate:** \"–ß—Ç–æ –≤—ã–≤–µ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞–º—è—Ç–∏?\"\n",
    "\n",
    "**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ:** Cell state $c_t$ —Ç–µ—á–µ—Ç —á–µ—Ä–µ–∑ –≤—Å—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏ ‚Üí **–Ω–µ—Ç vanishing gradient**!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 GRU (Gated Recurrent Unit)\n",
    "\n",
    "**GRU** ‚Äî —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è LSTM —Å **–º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤**.\n",
    "\n",
    "#### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ GRU\n",
    "\n",
    "**–î–≤–∞ gates:**\n",
    "\n",
    "**1. Reset Gate** (—Å–±—Ä–æ—Å–∏—Ç—å –ø—Ä–æ—à–ª—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é):\n",
    "\n",
    "$$r_t = \\sigma(W_r [h_{t-1}, x_t] + b_r)$$\n",
    "\n",
    "**2. Update Gate** (–æ–±–Ω–æ–≤–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ):\n",
    "\n",
    "$$z_t = \\sigma(W_z [h_{t-1}, x_t] + b_z)$$\n",
    "\n",
    "**–ö–∞–Ω–¥–∏–¥–∞—Ç –Ω–∞ –Ω–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ:**\n",
    "\n",
    "$$\\tilde{h}_t = \\tanh(W_h [r_t \\odot h_{t-1}, x_t] + b_h)$$\n",
    "\n",
    "**–§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ:**\n",
    "\n",
    "$$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$$\n",
    "\n",
    "---\n",
    "\n",
    "**–û—Ç–ª–∏—á–∏—è –æ—Ç LSTM:**\n",
    "- ‚úÖ **–ú–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤** (2 gates vs 3)\n",
    "- ‚úÖ **–ë—ã—Å—Ç—Ä–µ–µ –æ–±—É—á–∞–µ—Ç—Å—è**\n",
    "- ‚úÖ **–û–¥–Ω–æ —Å–æ—Å—Ç–æ—è–Ω–∏–µ** ($h_t$ –≤–º–µ—Å—Ç–æ $h_t$ –∏ $c_t$)\n",
    "- ‚ö†Ô∏è **–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:** –æ–±—ã—á–Ω–æ —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–∞ —Å LSTM, –∏–Ω–æ–≥–¥–∞ —á—É—Ç—å —Ö—É–∂–µ\n",
    "\n",
    "---\n",
    "\n",
    "### 1.5 –°—Ä–∞–≤–Ω–µ–Ω–∏–µ: RNN vs LSTM vs GRU\n",
    "\n",
    "| –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞ | Vanilla RNN | LSTM | GRU |\n",
    "|----------------|-------------|------|-----|\n",
    "| **–ü–∞—Ä–∞–º–µ—Ç—Ä—ã** | –ú–∞–ª–æ | –ú–Ω–æ–≥–æ (4x) | –°—Ä–µ–¥–Ω–µ (3x) |\n",
    "| **–°–∫–æ—Ä–æ—Å—Ç—å** | –ë—ã—Å—Ç—Ä–æ | –ú–µ–¥–ª–µ–Ω–Ω–æ | –°—Ä–µ–¥–Ω–µ |\n",
    "| **Long-term dependencies** | ‚ùå –ü–ª–æ—Ö–æ | ‚úÖ –û—Ç–ª–∏—á–Ω–æ | ‚úÖ –•–æ—Ä–æ—à–æ |\n",
    "| **Vanishing gradient** | ‚ùå –ü—Ä–æ–±–ª–µ–º–∞ | ‚úÖ –†–µ—à–µ–Ω–∞ | ‚úÖ –†–µ—à–µ–Ω–∞ |\n",
    "| **–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å** | –ö–æ—Ä–æ—Ç–∫–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (< 10) | –î–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –Ω—É–∂–Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å | –ë–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ |\n",
    "\n",
    "**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:**\n",
    "- –ù–∞—á–∏–Ω–∞–π—Ç–µ —Å **GRU** (–±—ã—Å—Ç—Ä–µ–µ, –ø—Ä–æ—â–µ)\n",
    "- –ï—Å–ª–∏ GRU –Ω–µ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è ‚Üí **LSTM**\n",
    "- Vanilla RNN ‚Äî —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏\n",
    "\n",
    "#### Bidirectional RNN\n",
    "\n",
    "–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å **–≤ –æ–±–µ —Å—Ç–æ—Ä–æ–Ω—ã:**\n",
    "\n",
    "$$\\vec{h}_t = \\text{RNN}(x_1, x_2, ..., x_t)$$\n",
    "\n",
    "$$\\overleftarrow{h}_t = \\text{RNN}(x_T, x_{T-1}, ..., x_t)$$\n",
    "\n",
    "$$h_t = [\\vec{h}_t; \\overleftarrow{h}_t]$$\n",
    "\n",
    "**–ö–æ–≥–¥–∞ –ø–æ–ª–µ–∑–Ω–æ:**\n",
    "- ‚úÖ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π (–≤–µ—Å—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ—Å—Ç—É–ø–µ–Ω)\n",
    "- ‚ùå –ù–ï –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è (–Ω–µ–ª—å–∑—è –∑–Ω–∞—Ç—å –±—É–¥—É—â–µ–µ!)\n",
    "\n",
    "---\n",
    "\n",
    "#### Stacked (Multi-layer) RNN\n",
    "\n",
    "–ù–µ—Å–∫–æ–ª—å–∫–æ —Å–ª–æ–µ–≤ RNN –¥—Ä—É–≥ –Ω–∞–¥ –¥—Ä—É–≥–æ–º:\n",
    "\n",
    "```\n",
    "x ‚Üí [LSTM‚ÇÅ] ‚Üí h‚ÇÅ ‚Üí [LSTM‚ÇÇ] ‚Üí h‚ÇÇ ‚Üí [LSTM‚ÇÉ] ‚Üí h‚ÇÉ ‚Üí y\n",
    "```\n",
    "\n",
    "**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**\n",
    "- ‚úÖ –ë–æ–ª–µ–µ –≥–ª—É–±–æ–∫–∞—è –∏–µ—Ä–∞—Ä—Ö–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "- ‚úÖ –õ—É—á—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö\n",
    "\n",
    "**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:**\n",
    "- ‚ùå –ë–æ–ª—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ‚Üí –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "- ‚ùå –î–æ–ª—å—à–µ –æ–±—É—á–∞–µ—Ç—Å—è\n",
    "\n",
    "**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** 1-2 —Å–ª–æ—è –æ–±—ã—á–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ.\n",
    "\n",
    "---\n",
    "\n",
    "#### Teacher Forcing\n",
    "\n",
    "**–ü—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏:** –∏—Å–ø–æ–ª—å–∑—É–µ–º **–∏—Å—Ç–∏–Ω–Ω—ã–µ** –∑–Ω–∞—á–µ–Ω–∏—è $y_{t-1}$ –∫–∞–∫ –≤—Ö–æ–¥ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è $y_t$  \n",
    "**–ü—Ä–∏ inference:** –∏—Å–ø–æ–ª—å–∑—É–µ–º **–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ** –∑–Ω–∞—á–µ–Ω–∏—è $\\hat{y}_{t-1}$\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º–∞:** model mismatch (train vs test)  \n",
    "**–†–µ—à–µ–Ω–∏–µ:** Scheduled sampling (–ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –ø–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª –ß–∞—Å—Ç—å 2: –ü—Ä–∞–∫—Ç–∏–∫–∞ - RNN –Ω–∞ PyTorch\n",
    "\n",
    "### 2.1 –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ë–∞–∑–æ–≤—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# –î–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ —Å—Ä–∞–≤–Ω–∏—Ç—å)\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "–ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ Airline Passengers –¥–∞—Ç–∞—Å–µ—Ç."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "try:\n",
    "    from statsmodels.datasets import get_rdataset\n",
    "    airline_data = get_rdataset('AirPassengers', 'datasets')\n",
    "    df = airline_data.data\n",
    "    df.columns = ['time', 'passengers']\n",
    "except:\n",
    "    url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv'\n",
    "    df = pd.read_csv(url, parse_dates=['Month'], index_col='Month')\n",
    "    df.columns = ['passengers']\n",
    "    df.reset_index(inplace=True)\n",
    "    df.columns = ['time', 'passengers']\n",
    "\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df.set_index('time', inplace=True)\n",
    "\n",
    "print(f\"–î–∞—Ç–∞—Å–µ—Ç: {df.shape[0]} –Ω–∞–±–ª—é–¥–µ–Ω–∏–π\")\n",
    "print(f\"–ü–µ—Ä–∏–æ–¥: {df.index.min()} - {df.index.max()}\")\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(df.index, df['passengers'], linewidth=2)\n",
    "plt.title('Airline Passengers (1949-1960)', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Passengers')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è RNN: Sliding Window\n",
    "\n",
    "**–ó–∞–¥–∞—á–∞:** –°–æ–∑–¥–∞—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (X) –∏ —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (y).\n",
    "\n",
    "**–ú–µ—Ç–æ–¥ Sliding Window:**\n",
    "- –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ `seq_length` —Ç–æ—á–µ–∫ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–π\n",
    "- –ù–∞–ø—Ä–∏–º–µ—Ä: `[t-4, t-3, t-2, t-1]` ‚Üí `t`\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä:**\n",
    "```\n",
    "–î–∞–Ω–Ω—ã–µ: [10, 20, 30, 40, 50, 60]\n",
    "seq_length = 3\n",
    "\n",
    "X:           y:\n",
    "[10,20,30] ‚Üí 40\n",
    "[20,30,40] ‚Üí 50\n",
    "[30,40,50] ‚Üí 60\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length):\n",
    "    \"\"\"\n",
    "    –°–æ–∑–¥–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è RNN —Å –ø–æ–º–æ—â—å—é sliding window\n",
    "    \n",
    "    Args:\n",
    "        data: –º–∞—Å—Å–∏–≤ numpy shape (n_samples,)\n",
    "        seq_length: –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (look-back period)\n",
    "    \n",
    "    Returns:\n",
    "        X: shape (n_sequences, seq_length, 1)\n",
    "        y: shape (n_sequences, 1)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(len(data) - seq_length):\n",
    "        # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª–∏–Ω—ã seq_length\n",
    "        sequence = data[i:i + seq_length]\n",
    "        # –°–ª–µ–¥—É—é—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ - —Ü–µ–ª–µ–≤–æ–µ\n",
    "        target = data[i + seq_length]\n",
    "        \n",
    "        X.append(sequence)\n",
    "        y.append(target)\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–¥–ª—è univariate = 1)\n",
    "    X = X.reshape(-1, seq_length, 1)\n",
    "    y = y.reshape(-1, 1)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# –¢–µ—Å—Ç —Ñ—É–Ω–∫—Ü–∏–∏\n",
    "test_data = np.array([10, 20, 30, 40, 50, 60])\n",
    "test_X, test_y = create_sequences(test_data, seq_length=3)\n",
    "\n",
    "print(\"–ü—Ä–∏–º–µ—Ä sliding window:\")\n",
    "print(\"Original data:\", test_data)\n",
    "print(\"\\nSequences (X) and targets (y):\")\n",
    "for i in range(len(test_X)):\n",
    "    print(f\"X: {test_X[i].flatten()} ‚Üí y: {test_y[i][0]}\")\n",
    "\n",
    "print(f\"\\nShape: X={test_X.shape}, y={test_y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "data = df['passengers'].values.astype(float)\n",
    "\n",
    "# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (–≤–∞–∂–Ω–æ –¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π!)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_normalized = scaler.fit_transform(data.reshape(-1, 1)).flatten()\n",
    "\n",
    "print(f\"Original range: [{data.min():.1f}, {data.max():.1f}]\")\n",
    "print(f\"Normalized range: [{data_normalized.min():.3f}, {data_normalized.max():.3f}]\")\n",
    "\n",
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
    "SEQ_LENGTH = 12  # –∏—Å–ø–æ–ª—å–∑—É–µ–º –≥–æ–¥ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –º–µ—Å—è—Ü–∞\n",
    "TRAIN_SIZE = 0.8\n",
    "\n",
    "# Train/Test split (–ø–æ –≤—Ä–µ–º–µ–Ω–∏!)\n",
    "train_size = int(len(data_normalized) * TRAIN_SIZE)\n",
    "train_data = data_normalized[:train_size]\n",
    "test_data = data_normalized[train_size - SEQ_LENGTH:]  # –≤–∫–ª—é—á–∞–µ–º SEQ_LENGTH –¥–ª—è –ø–µ—Ä–≤–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "\n",
    "print(f\"\\nTrain size: {len(train_data)} ({TRAIN_SIZE*100:.0f}%)\")\n",
    "print(f\"Test size: {len(test_data) - SEQ_LENGTH} (—Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è)\")\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "X_train, y_train = create_sequences(train_data, SEQ_LENGTH)\n",
    "X_test, y_test = create_sequences(test_data, SEQ_LENGTH)\n",
    "\n",
    "print(f\"\\nTrain sequences: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Test sequences: X={X_test.shape}, y={y_test.shape}\")\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test).to(device)\n",
    "\n",
    "# DataLoader\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)  # –ù–ï shuffle –¥–ª—è TS!\n",
    "\n",
    "print(f\"\\nBatch size: {BATCH_SIZE}\")\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 –ú–æ–¥–µ–ª—å 1: Vanilla RNN\n",
    "\n",
    "–ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–∞—è —Å–µ—Ç—å –¥–ª—è –±–∞–∑–æ–≤–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=1, output_size=1):\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True  # (batch, seq, feature)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # RNN forward pass\n",
    "        # out: (batch, seq, hidden_size)\n",
    "        # h_n: (num_layers, batch, hidden_size)\n",
    "        out, h_n = self.rnn(x, h0)\n",
    "        \n",
    "        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—ã—Ö–æ–¥\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        # Fully connected\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "rnn_model = VanillaRNN(\n",
    "    input_size=1,\n",
    "    hidden_size=64,\n",
    "    num_layers=1,\n",
    "    output_size=1\n",
    ").to(device)\n",
    "\n",
    "print(\"Vanilla RNN Architecture:\")\n",
    "print(rnn_model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in rnn_model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 –ú–æ–¥–µ–ª—å 2: LSTM\n",
    "\n",
    "Long Short-Term Memory –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, output_size=1, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0  # dropout –º–µ–∂–¥—É —Å–ª–æ—è–º–∏\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(32, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è hidden –∏ cell states\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        out, (h_n, c_n) = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—ã—Ö–æ–¥\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        # Fully connected\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "lstm_model = LSTMModel(\n",
    "    input_size=1,\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    output_size=1,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "print(\"LSTM Architecture:\")\n",
    "print(lstm_model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in lstm_model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 –ú–æ–¥–µ–ª—å 3: GRU\n",
    "\n",
    "Gated Recurrent Unit - –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É RNN –∏ LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, output_size=1, dropout=0.2):\n",
    "        super(GRUModel, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # GRU layer\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(32, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # GRU forward pass\n",
    "        out, h_n = self.gru(x, h0)\n",
    "        \n",
    "        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—ã—Ö–æ–¥\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        # Fully connected\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "gru_model = GRUModel(\n",
    "    input_size=1,\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    output_size=1,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "print(\"GRU Architecture:\")\n",
    "print(gru_model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in gru_model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 –§—É–Ω–∫—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=100, patience=10):\n",
    "    \"\"\"\n",
    "    –û–±—É—á–µ–Ω–∏–µ RNN –º–æ–¥–µ–ª–∏ —Å early stopping\n",
    "    \"\"\"\n",
    "    history = {'train_loss': []}\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping (–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç exploding gradients)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        history['train_loss'].append(avg_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.6f}\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    return history\n",
    "\n",
    "def evaluate_model(model, X, y, scaler):\n",
    "    \"\"\"\n",
    "    –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –¥–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = model(X).cpu().numpy()\n",
    "    \n",
    "    # –î–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "    y_true = scaler.inverse_transform(y.cpu().numpy())\n",
    "    y_pred = scaler.inverse_transform(predictions)\n",
    "    \n",
    "    # –ú–µ—Ç—Ä–∏–∫–∏\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    \n",
    "    return y_pred, y_true, rmse, mae\n",
    "\n",
    "print(\"‚úÖ –§—É–Ω–∫—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è –≥–æ—Ç–æ–≤—ã\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèãÔ∏è –ß–∞—Å—Ç—å 3: –û–±—É—á–µ–Ω–∏–µ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π\n",
    "\n",
    "### 3.1 –û–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
    "NUM_EPOCHS = 200\n",
    "LEARNING_RATE = 0.001\n",
    "PATIENCE = 15\n",
    "\n",
    "# Loss –∏ optimizer –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –¥–ª—è –≤—Å–µ—Ö\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "results = {}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Training Configuration\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Sequence Length: {SEQ_LENGTH}\")\n",
    "print(f\"Early Stopping Patience: {PATIENCE}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 –û–±—É—á–µ–Ω–∏–µ Vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Vanilla RNN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "rnn_optimizer = optim.Adam(rnn_model.parameters(), lr=LEARNING_RATE)\n",
    "rnn_history = train_model(rnn_model, train_loader, criterion, rnn_optimizer, \n",
    "                          NUM_EPOCHS, PATIENCE)\n",
    "\n",
    "# Evaluation\n",
    "rnn_pred, rnn_true, rnn_rmse, rnn_mae = evaluate_model(rnn_model, X_test_tensor, \n",
    "                                                        y_test_tensor, scaler)\n",
    "\n",
    "results['Vanilla RNN'] = {\n",
    "    'model': rnn_model,\n",
    "    'history': rnn_history,\n",
    "    'predictions': rnn_pred,\n",
    "    'rmse': rnn_rmse,\n",
    "    'mae': rnn_mae\n",
    "}\n",
    "\n",
    "print(f\"\\n‚úÖ Vanilla RNN trained\")\n",
    "print(f\"   RMSE: {rnn_rmse:.2f}\")\n",
    "print(f\"   MAE: {rnn_mae:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 –û–±—É—á–µ–Ω–∏–µ LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training LSTM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lstm_optimizer = optim.Adam(lstm_model.parameters(), lr=LEARNING_RATE)\n",
    "lstm_history = train_model(lstm_model, train_loader, criterion, lstm_optimizer, \n",
    "                           NUM_EPOCHS, PATIENCE)\n",
    "\n",
    "# Evaluation\n",
    "lstm_pred, lstm_true, lstm_rmse, lstm_mae = evaluate_model(lstm_model, X_test_tensor, \n",
    "                                                           y_test_tensor, scaler)\n",
    "\n",
    "results['LSTM'] = {\n",
    "    'model': lstm_model,\n",
    "    'history': lstm_history,\n",
    "    'predictions': lstm_pred,\n",
    "    'rmse': lstm_rmse,\n",
    "    'mae': lstm_mae\n",
    "}\n",
    "\n",
    "print(f\"\\n‚úÖ LSTM trained\")\n",
    "print(f\"   RMSE: {lstm_rmse:.2f}\")\n",
    "print(f\"   MAE: {lstm_mae:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 –û–±—É—á–µ–Ω–∏–µ GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training GRU\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "gru_optimizer = optim.Adam(gru_model.parameters(), lr=LEARNING_RATE)\n",
    "gru_history = train_model(gru_model, train_loader, criterion, gru_optimizer, \n",
    "                          NUM_EPOCHS, PATIENCE)\n",
    "\n",
    "# Evaluation\n",
    "gru_pred, gru_true, gru_rmse, gru_mae = evaluate_model(gru_model, X_test_tensor, \n",
    "                                                       y_test_tensor, scaler)\n",
    "\n",
    "results['GRU'] = {\n",
    "    'model': gru_model,\n",
    "    'history': gru_history,\n",
    "    'predictions': gru_pred,\n",
    "    'rmse': gru_rmse,\n",
    "    'mae': gru_mae\n",
    "}\n",
    "\n",
    "print(f\"\\n‚úÖ GRU trained\")\n",
    "print(f\"   RMSE: {gru_rmse:.2f}\")\n",
    "print(f\"   MAE: {gru_mae:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–π SARIMA\n",
    "\n",
    "–î–æ–±–∞–≤–∏–º SARIMA –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –Ω–æ—É—Ç–±—É–∫–∞ –¥–ª—è —á–µ—Å—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training SARIMA for comparison\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ (–Ω–µ–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ) –¥–∞–Ω–Ω—ã–µ\n",
    "train_size_orig = int(len(data) * TRAIN_SIZE)\n",
    "train_orig = data[:train_size_orig]\n",
    "test_orig = data[train_size_orig:]\n",
    "\n",
    "# SARIMA(1, 1, 1)(1, 1, 1, 12)\n",
    "sarima_model = SARIMAX(\n",
    "    train_orig,\n",
    "    order=(1, 1, 1),\n",
    "    seasonal_order=(1, 1, 1, 12),\n",
    "    enforce_stationarity=False,\n",
    "    enforce_invertibility=False\n",
    ")\n",
    "\n",
    "sarima_fitted = sarima_model.fit(disp=False)\n",
    "sarima_forecast = sarima_fitted.forecast(steps=len(test_orig))\n",
    "\n",
    "# –ú–µ—Ç—Ä–∏–∫–∏\n",
    "sarima_rmse = np.sqrt(mean_squared_error(test_orig, sarima_forecast))\n",
    "sarima_mae = mean_absolute_error(test_orig, sarima_forecast)\n",
    "\n",
    "results['SARIMA'] = {\n",
    "    'predictions': sarima_forecast,\n",
    "    'rmse': sarima_rmse,\n",
    "    'mae': sarima_mae\n",
    "}\n",
    "\n",
    "print(f\"\\n‚úÖ SARIMA trained\")\n",
    "print(f\"   RMSE: {sarima_rmse:.2f}\")\n",
    "print(f\"   MAE: {sarima_mae:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "ax.plot(results['Vanilla RNN']['history']['train_loss'], label='Vanilla RNN', linewidth=2)\n",
    "ax.plot(results['LSTM']['history']['train_loss'], label='LSTM', linewidth=2)\n",
    "ax.plot(results['GRU']['history']['train_loss'], label='GRU', linewidth=2)\n",
    "\n",
    "ax.set_title('Training Loss Curves', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Loss (MSE)', fontsize=12)\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä RNN –º–æ–¥–µ–ª–∏ –æ–±—É—á–∞—é—Ç—Å—è —Å—Ç–∞–±–∏–ª—å–Ω–æ, LSTM/GRU —Å—Ö–æ–¥—è—Ç—Å—è –±—ã—Å—Ç—Ä–µ–µ Vanilla RNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions visualization\n",
    "# –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è test –¥–∞–Ω–Ω—ã—Ö\n",
    "test_indices = df.index[train_size_orig:]\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Train –¥–∞–Ω–Ω—ã–µ (–∫–æ–Ω—Ç–µ–∫—Å—Ç)\n",
    "plt.plot(df.index[:train_size_orig], data[:train_size_orig], \n",
    "         label='Train', linewidth=2, alpha=0.5, color='gray')\n",
    "\n",
    "# –ò—Å—Ç–∏–Ω–Ω—ã–µ test –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "plt.plot(test_indices, test_orig, label='Test (True)', \n",
    "         linewidth=3, color='black')\n",
    "\n",
    "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è RNN –º–æ–¥–µ–ª–µ–π\n",
    "plt.plot(test_indices, results['Vanilla RNN']['predictions'], \n",
    "         label=f\"Vanilla RNN (RMSE={rnn_rmse:.1f})\", linewidth=2, linestyle='--')\n",
    "plt.plot(test_indices, results['LSTM']['predictions'], \n",
    "         label=f\"LSTM (RMSE={lstm_rmse:.1f})\", linewidth=2, linestyle='--')\n",
    "plt.plot(test_indices, results['GRU']['predictions'], \n",
    "         label=f\"GRU (RMSE={gru_rmse:.1f})\", linewidth=2, linestyle='--')\n",
    "\n",
    "# SARIMA\n",
    "plt.plot(test_indices, results['SARIMA']['predictions'], \n",
    "         label=f\"SARIMA (RMSE={sarima_rmse:.1f})\", linewidth=2, linestyle=':')\n",
    "\n",
    "plt.axvline(df.index[train_size_orig], color='red', linestyle='--', \n",
    "            alpha=0.3, linewidth=2, label='Train/Test Split')\n",
    "\n",
    "plt.title('Model Comparison: RNN vs LSTM vs GRU vs SARIMA', \n",
    "          fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Time', fontsize=12)\n",
    "plt.ylabel('Number of Passengers', fontsize=12)\n",
    "plt.legend(loc='upper left', fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –º–µ—Ç—Ä–∏–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Vanilla RNN', 'LSTM', 'GRU', 'SARIMA'],\n",
    "    'RMSE': [\n",
    "        results['Vanilla RNN']['rmse'],\n",
    "        results['LSTM']['rmse'],\n",
    "        results['GRU']['rmse'],\n",
    "        results['SARIMA']['rmse']\n",
    "    ],\n",
    "    'MAE': [\n",
    "        results['Vanilla RNN']['mae'],\n",
    "        results['LSTM']['mae'],\n",
    "        results['GRU']['mae'],\n",
    "        results['SARIMA']['mae']\n",
    "    ],\n",
    "    'Parameters': [\n",
    "        sum(p.numel() for p in rnn_model.parameters()),\n",
    "        sum(p.numel() for p in lstm_model.parameters()),\n",
    "        sum(p.numel() for p in gru_model.parameters()),\n",
    "        'N/A'\n",
    "    ],\n",
    "    'Type': ['Deep Learning', 'Deep Learning', 'Deep Learning', 'Classical']\n",
    "})\n",
    "\n",
    "# –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ RMSE\n",
    "comparison_df = comparison_df.sort_values('RMSE')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RMSE comparison\n",
    "comparison_df.plot(x='Model', y='RMSE', kind='bar', ax=axes[0], legend=False, color='steelblue')\n",
    "axes[0].set_title('RMSE Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('RMSE', fontsize=12)\n",
    "axes[0].set_xlabel('')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# MAE comparison\n",
    "comparison_df.plot(x='Model', y='MAE', kind='bar', ax=axes[1], legend=False, color='coral')\n",
    "axes[1].set_title('MAE Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('MAE', fontsize=12)\n",
    "axes[1].set_xlabel('')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì –í—ã–≤–æ–¥—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n",
    "\n",
    "### üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ Airline Passengers\n",
    "\n",
    "**–û–∂–∏–¥–∞–µ–º–∞—è —Å–∏—Ç—É–∞—Ü–∏—è –¥–ª—è —ç—Ç–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞:**\n",
    "\n",
    "1. **SARIMA —á–∞—â–µ –≤—Å–µ–≥–æ –ª—É—á—à–µ** RNN-–º–æ–¥–µ–ª–µ–π\n",
    "   - ‚úÖ –ú–∞–ª–µ–Ω—å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç (144 —Ç–æ—á–∫–∏)\n",
    "   - ‚úÖ –ß–µ—Ç–∫–∞—è —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å (SARIMA –∑–∞—Ç–æ—á–µ–Ω–∞ –ø–æ–¥ —ç—Ç–æ)\n",
    "   - ‚úÖ Univariate (–æ–¥–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è)\n",
    "\n",
    "2. **LSTM/GRU ‚âà —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º—ã** —Å SARIMA –∏–ª–∏ —á—É—Ç—å —Ö—É–∂–µ\n",
    "   - ‚ö†Ô∏è –ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è\n",
    "   - ‚úÖ –ù–æ –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—Ç –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã\n",
    "\n",
    "3. **Vanilla RNN —Ö—É–∂–µ –≤—Å–µ—Ö**\n",
    "   - ‚ùå –ü—Ä–æ–±–ª–µ–º–∞ vanishing gradient\n",
    "   - ‚ùå –ù–µ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–º–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å RNN/LSTM/GRU?\n",
    "\n",
    "**Deep Learning –ª—É—á—à–µ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç–æ–¥–æ–≤ –∫–æ–≥–¥–∞:**\n",
    "\n",
    "| –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞ | Classical (ARIMA/SARIMA) | Deep Learning (LSTM/GRU) |\n",
    "|----------------|-------------------------|-------------------------|\n",
    "| **–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö** | < 1000 —Ç–æ—á–µ–∫ | > 1000 —Ç–æ—á–µ–∫ (—á–µ–º –±–æ–ª—å—à–µ, —Ç–µ–º –ª—É—á—à–µ) |\n",
    "| **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤** | Univariate (1 –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è) | **Multivariate (–º–Ω–æ–≥–æ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö)** |\n",
    "| **–ü–∞—Ç—Ç–µ—Ä–Ω—ã** | –õ–∏–Ω–µ–π–Ω—ã–µ, —Å–µ–∑–æ–Ω–Ω—ã–µ | **–ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ, —Å–ª–æ–∂–Ω—ã–µ** |\n",
    "| **–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è** | **–ë—ã—Å—Ç—Ä–æ (—Å–µ–∫—É–Ω–¥—ã)** | –ú–µ–¥–ª–µ–Ω–Ω–æ (–º–∏–Ω—É—Ç—ã/—á–∞—Å—ã) |\n",
    "| **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å** | **–í—ã—Å–æ–∫–∞—è** | –ù–∏–∑–∫–∞—è (—á–µ—Ä–Ω—ã–π —è—â–∏–∫) |\n",
    "| **–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è** | –°—Ä–µ–¥–Ω—è—è (–ø–æ–¥–±–æ—Ä p, d, q) | **–í—ã—Å–æ–∫–∞—è (end-to-end)** |\n",
    "\n",
    "**RNN/LSTM/GRU –ø–æ–±–µ–∂–¥–∞—é—Ç –∫–æ–≥–¥–∞:**\n",
    "- ‚úÖ **Multivariate time series** (–º–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö)\n",
    "- ‚úÖ **–ë–æ–ª—å—à–∏–µ –¥–∞—Ç–∞—Å–µ—Ç—ã** (—Ç—ã—Å—è—á–∏/–º–∏–ª–ª–∏–æ–Ω—ã —Ç–æ—á–µ–∫)\n",
    "- ‚úÖ **–ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏** (—Å–ª–æ–∂–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã)\n",
    "- ‚úÖ **–ù–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å** (–Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–µ—Ä–∏–æ–¥–æ–≤, –Ω–µ—Ä–µ–≥—É–ª—è—Ä–Ω–∞—è)\n",
    "- ‚úÖ **–ú–Ω–æ–≥–æ –≤–Ω–µ—à–Ω–∏—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤** (–ª–µ–≥–∫–æ –¥–æ–±–∞–≤–∏—Ç—å –∫–∞–∫ –ø—Ä–∏–∑–Ω–∞–∫–∏)\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä—ã –∑–∞–¥–∞—á –¥–ª—è RNN:**\n",
    "- üè≠ **IoT —Å–µ–Ω—Å–æ—Ä—ã:** —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞, –¥–∞–≤–ª–µ–Ω–∏–µ, –≤–∏–±—Ä–∞—Ü–∏—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ\n",
    "- üìà **–§–∏–Ω–∞–Ω—Å—ã:** —Ü–µ–Ω–∞ + –æ–±—ä–µ–º + –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã + –Ω–æ–≤–æ—Å—Ç–∏\n",
    "- ‚ö° **–≠–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞:** –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ + –ø–æ–≥–æ–¥–∞ + –¥–µ–Ω—å –Ω–µ–¥–µ–ª–∏ + –ø—Ä–∞–∑–¥–Ω–∏–∫–∏\n",
    "- üè• **–ú–µ–¥–∏—Ü–∏–Ω–∞:** –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤ (–ø—É–ª—å—Å, –¥–∞–≤–ª–µ–Ω–∏–µ, —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞)\n",
    "\n",
    "---\n",
    "\n",
    "### üîß –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n",
    "\n",
    "**–†–∞–±–æ—á–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏:**\n",
    "\n",
    "```\n",
    "1. –ù–∞—á–Ω–∏—Ç–µ —Å ARIMA/SARIMA (baseline)\n",
    "   ‚Üì\n",
    "2. –ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª–µ–Ω:\n",
    "   - –ú–∞–ª–µ–Ω—å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç (< 1k) ‚Üí –ø–æ–ø—Ä–æ–±—É–π—Ç–µ Prophet\n",
    "   - –ë–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç (> 1k) ‚Üí –ø–æ–ø—Ä–æ–±—É–π—Ç–µ LSTM/GRU\n",
    "   ‚Üì\n",
    "3. –î–ª—è multivariate:\n",
    "   - VAR (Vector AutoRegression) - –∫–ª–∞—Å—Å–∏–∫–∞\n",
    "   - LSTM/GRU - deep learning\n",
    "   ‚Üì\n",
    "4. –ê–Ω—Å–∞–º–±–ª—å:\n",
    "   - SARIMA + LSTM —á–∞—Å—Ç–æ –ª—É—á—à–µ –∫–∞–∂–¥–æ–≥–æ –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "```\n",
    "\n",
    "**–¢—é–Ω–∏–Ω–≥ RNN –º–æ–¥–µ–ª–µ–π:**\n",
    "\n",
    "1. **Sequence length (look-back period)**\n",
    "   - –î–ª—è —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç–∏: –º–∏–Ω–∏–º—É–º 1 –ø–µ—Ä–∏–æ–¥ (12 –¥–ª—è –º–µ—Å—è—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö)\n",
    "   - –°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π ‚Üí –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ\n",
    "   - –°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π ‚Üí –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n",
    "\n",
    "2. **Hidden size**\n",
    "   - –ú–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ: 32-64\n",
    "   - –°—Ä–µ–¥–Ω–∏–µ: 64-128\n",
    "   - –ë–æ–ª—å—à–∏–µ: 128-512\n",
    "\n",
    "3. **Number of layers**\n",
    "   - 1-2 —Å–ª–æ—è –æ–±—ã—á–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ\n",
    "   - –ë–æ–ª—å—à–µ ‚Üí –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "4. **Dropout**\n",
    "   - 0.1-0.3 –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏\n",
    "   - –ö—Ä–∏—Ç–∏—á–Ω–æ –Ω–∞ –º–∞–ª—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö\n",
    "\n",
    "5. **Learning rate**\n",
    "   - –ù–∞—á–Ω–∏—Ç–µ —Å 0.001\n",
    "   - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ scheduler (ReduceLROnPlateau)\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏\n",
    "\n",
    "**–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ (Phase 3, Step 3):**\n",
    "- **Attention –º–µ—Ö–∞–Ω–∏–∑–º** –¥–ª—è RNN\n",
    "- **Seq2Seq** –º–æ–¥–µ–ª–∏ –¥–ª—è multi-step forecasting\n",
    "- **Encoder-Decoder** –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞\n",
    "- **Transformer** –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ (Phase 4)\n",
    "\n",
    "**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —É–ª—É—á—à–µ–Ω–∏—è:**\n",
    "- **–ê–Ω—Å–∞–º–±–ª–∏:** SARIMA + LSTM\n",
    "- **Multivariate:** –¥–æ–±–∞–≤–∏—Ç—å –≤–Ω–µ—à–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "- **Transfer learning:** –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥—Ä—É–≥–∏—Ö TS\n",
    "- **Multi-step forecasting:** –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤\n",
    "\n",
    "---\n",
    "\n",
    "### üìö –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã\n",
    "\n",
    "**–¢–µ–æ—Ä–∏—è:**\n",
    "- [\"Understanding LSTM Networks\" (Colah's Blog)](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [\"The Unreasonable Effectiveness of RNN\" (Karpathy)](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "**–ü—Ä–∞–∫—Ç–∏–∫–∞:**\n",
    "- [PyTorch RNN Tutorial](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)\n",
    "- [Time Series with LSTM](https://machinelearningmastery.com/lstm-for-time-series-prediction-in-pytorch/)\n",
    "\n",
    "---\n",
    "\n",
    "**Phase 3, Step 2 COMPLETE!** ‚úÖ  \n",
    "**Next:** Advanced RNN - Attention & Seq2Seq (Step 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
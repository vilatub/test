#!/usr/bin/env python3
"""
–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —á–∞—Å—Ç–∏ –≤ Integration notebook
–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –í–°–ï–• –º–µ—Ç–æ–¥–æ–≤ Phase 1 –Ω–∞ Telco Churn
"""

import json

# –ß–∏—Ç–∞–µ–º —Ç–µ–∫—É—â–∏–π –Ω–æ—É—Ç–±—É–∫
notebook_path = '07_integration_comparison.ipynb'
with open(notebook_path, 'r', encoding='utf-8') as f:
    notebook = json.load(f)

practical_cells = []

# ============================================================================
# DATA LOADING
# ============================================================================

practical_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 2.2 –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ó–∞–≥—Ä—É–∑–∫–∞ Telco Churn –¥–∞–Ω–Ω—ã—Ö\n",
        "import os\n",
        "\n",
        "data_path = '../../data/telco_churn.csv'\n",
        "\n",
        "if not os.path.exists(data_path):\n",
        "    print('‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω!')\n",
        "else:\n",
        "    df = pd.read_csv(data_path)\n",
        "    print(f'‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {df.shape[0]:,} —Å—Ç—Ä–æ–∫, {df.shape[1]} —Å—Ç–æ–ª–±—Ü–æ–≤')\n",
        "    print(f'Target: Churn')"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ü–µ—Ä–≤—ã–π –≤–∑–≥–ª—è–¥\n",
        "print('–ü–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏:')\n",
        "df.head()"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# EDA: Class balance\n",
        "churn_counts = df['Churn'].value_counts()\n",
        "churn_pct = df['Churn'].value_counts(normalize=True) * 100\n",
        "\n",
        "print('Class distribution:')\n",
        "print(f'No: {churn_counts[\"No\"]} ({churn_pct[\"No\"]:.1f}%)')\n",
        "print(f'Yes: {churn_counts[\"Yes\"]} ({churn_pct[\"Yes\"]:.1f}%)')\n",
        "print(f'\\nImbalance ratio: 1:{churn_counts[\"No\"] / churn_counts[\"Yes\"]:.1f}')\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "plt.figure(figsize=(8, 5))\n",
        "churn_counts.plot(kind='bar', color=['green', 'red'], alpha=0.7, edgecolor='black')\n",
        "plt.title('Class Distribution: Churn')\n",
        "plt.xlabel('Churn')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0)\n",
        "for i, (idx, val) in enumerate(churn_counts.items()):\n",
        "    plt.text(i, val + 100, f'{val} ({churn_pct[idx]:.1f}%)', ha='center')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('\\nüîç Moderate imbalance (~27% churn) ‚Üí Class weights –∏–ª–∏ moderate SMOTE')"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "# –£–¥–∞–ª—è–µ–º customerID (–Ω–µ –Ω—É–∂–µ–Ω)\n",
        "if 'customerID' in df.columns:\n",
        "    df = df.drop('customerID', axis=1)\n",
        "\n",
        "# TotalCharges –∏–Ω–æ–≥–¥–∞ –∏–º–µ–µ—Ç ' ' –≤–º–µ—Å—Ç–æ —á–∏—Å–µ–ª - –∏—Å–ø—Ä–∞–≤–ª—è–µ–º\n",
        "if 'TotalCharges' in df.columns:\n",
        "    df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
        "    df['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)\n",
        "\n",
        "# –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ numeric –∏ categorical\n",
        "numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
        "categorical_features.remove('Churn')  # Target\n",
        "\n",
        "print(f'Numeric features ({len(numeric_features)}): {numeric_features}')\n",
        "print(f'Categorical features ({len(categorical_features)}): {categorical_features[:5]}...')"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Target encoding\n",
        "y = (df['Churn'] == 'Yes').astype(int)\n",
        "\n",
        "# Features\n",
        "X = df.drop('Churn', axis=1)\n",
        "\n",
        "print(f'X shape: {X.shape}')\n",
        "print(f'y shape: {y.shape}')\n",
        "print(f'y distribution: {y.value_counts().to_dict()}')"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Train/test split (stratified!)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "print(f'Train: {X_train.shape[0]:,} samples')\n",
        "print(f'Test: {X_test.shape[0]:,} samples')\n",
        "print(f'Train churn rate: {y_train.mean():.1%}')\n",
        "print(f'Test churn rate: {y_test.mean():.1%}')"
    ]
})

# ============================================================================
# PREPROCESSING VARIANTS
# ============================================================================

practical_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 2.3 –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ä–∞–∑–Ω—ã—Ö –≤–µ—Ä—Å–∏–π –¥–∞–Ω–Ω—ã—Ö\\n\",\n",
        "\n",
        "–°–æ–∑–¥–∞–¥–∏–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–µ—Ä—Å–∏–π –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π:\n",
        "1. **One-hot encoded** ‚Äî –¥–ª—è Logistic Regression, XGBoost, LightGBM\n",
        "2. **Native categorical** ‚Äî –¥–ª—è CatBoost\n",
        "3. **Target encoded** ‚Äî –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Version 1: One-hot encoding\n",
        "X_train_ohe = pd.get_dummies(X_train, columns=categorical_features, drop_first=True)\n",
        "X_test_ohe = pd.get_dummies(X_test, columns=categorical_features, drop_first=True)\n",
        "\n",
        "# –í—ã—Ä–æ–≤–Ω—è—Ç—å –∫–æ–ª–æ–Ω–∫–∏\n",
        "X_train_ohe, X_test_ohe = X_train_ohe.align(X_test_ohe, join='left', axis=1, fill_value=0)\n",
        "\n",
        "print(f'One-hot encoded: {X_train_ohe.shape[1]} features')"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Version 2: Label encoding –¥–ª—è CatBoost (–æ–Ω –ø–æ–π–º–µ—Ç categorical —Å–∞–º)\n",
        "X_train_cat = X_train.copy()\n",
        "X_test_cat = X_test.copy()\n",
        "\n",
        "# CatBoost –ø—Ä–∏–Ω–∏–º–∞–µ—Ç categorical –∫–∞–∫ –∏–Ω–¥–µ–∫—Å—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n",
        "cat_indices = []\n",
        "for i, col in enumerate(X_train_cat.columns):\n",
        "    if col in categorical_features:\n",
        "        le = LabelEncoder()\n",
        "        X_train_cat[col] = le.fit_transform(X_train_cat[col].astype(str))\n",
        "        X_test_cat[col] = le.transform(X_test_cat[col].astype(str))\n",
        "        cat_indices.append(i)\n",
        "\n",
        "print(f'CatBoost version: {len(cat_indices)} categorical features at indices {cat_indices[:5]}...')"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è Logistic Regression\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_ohe)\n",
        "X_test_scaled = scaler.transform(X_test_ohe)\n",
        "\n",
        "print('‚úÖ –í—Å–µ –≤–µ—Ä—Å–∏–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã')"
    ]
})

# ============================================================================
# EVALUATION FUNCTION
# ============================================================================

practical_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 2.4 –§—É–Ω–∫—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "def evaluate_model(model, X_tr, X_te, y_tr, y_te, name, train_time=None):\n",
        "    \"\"\"\n",
        "    –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏\n",
        "    \"\"\"\n",
        "    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "    y_pred = model.predict(X_te)\n",
        "    y_proba = model.predict_proba(X_te)[:, 1]\n",
        "    \n",
        "    # –ú–µ—Ç—Ä–∏–∫–∏\n",
        "    acc = accuracy_score(y_te, y_pred)\n",
        "    prec = precision_score(y_te, y_pred)\n",
        "    rec = recall_score(y_te, y_pred)\n",
        "    f1 = f1_score(y_te, y_pred)\n",
        "    roc_auc = roc_auc_score(y_te, y_proba)\n",
        "    pr_auc = average_precision_score(y_te, y_proba)\n",
        "    \n",
        "    # Business cost (FN = $500, FP = $50)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_te, y_pred).ravel()\n",
        "    cost = fn * 500 + fp * 50\n",
        "    \n",
        "    # –í—ã–≤–æ–¥\n",
        "    print(f'\\nüìä {name}:')\n",
        "    print(f'  Accuracy: {acc:.4f}')\n",
        "    print(f'  Precision: {prec:.4f}')\n",
        "    print(f'  Recall: {rec:.4f}')\n",
        "    print(f'  F1-score: {f1:.4f}')\n",
        "    print(f'  ROC-AUC: {roc_auc:.4f}')\n",
        "    print(f'  PR-AUC: {pr_auc:.4f}')\n",
        "    print(f'  Business Cost: ${cost:,}')\n",
        "    if train_time:\n",
        "        print(f'  Train Time: {train_time:.2f}s')\n",
        "    print(f'  Confusion Matrix: TN={tn}, FP={fp}, FN={fn}, TP={tp}')\n",
        "    \n",
        "    return {\n",
        "        'Model': name,\n",
        "        'Accuracy': acc,\n",
        "        'Precision': prec,\n",
        "        'Recall': rec,\n",
        "        'F1': f1,\n",
        "        'ROC-AUC': roc_auc,\n",
        "        'PR-AUC': pr_auc,\n",
        "        'Cost': cost,\n",
        "        'Time': train_time if train_time else 0,\n",
        "        'TP': tp,\n",
        "        'FP': fp,\n",
        "        'TN': tn,\n",
        "        'FN': fn\n",
        "    }\n",
        "\n",
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
        "results = []\n",
        "\n",
        "print('‚úÖ –§—É–Ω–∫—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ —Å–æ–∑–¥–∞–Ω–∞')"
    ]
})

# ============================================================================
# BASELINE
# ============================================================================

practical_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 2.5 Baseline: Logistic Regression"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Logistic Regression baseline\n",
        "start = time.time()\n",
        "lr = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)\n",
        "lr.fit(X_train_scaled, y_train)\n",
        "lr_time = time.time() - start\n",
        "\n",
        "lr_results = evaluate_model(lr, X_train_scaled, X_test_scaled, y_train, y_test, \n",
        "                            'Logistic Regression', lr_time)\n",
        "results.append(lr_results)"
    ]
})

# ============================================================================
# XGBOOST
# ============================================================================

practical_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 2.6 XGBoost: Baseline –∏ Tuned"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# XGBoost baseline\n",
        "start = time.time()\n",
        "xgb_base = XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=6,\n",
        "    random_state=RANDOM_STATE,\n",
        "    verbosity=0\n",
        ")\n",
        "xgb_base.fit(X_train_ohe, y_train)\n",
        "xgb_base_time = time.time() - start\n",
        "\n",
        "xgb_base_results = evaluate_model(xgb_base, X_train_ohe, X_test_ohe, y_train, y_test,\n",
        "                                  'XGBoost Baseline', xgb_base_time)\n",
        "results.append(xgb_base_results)"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# XGBoost tuned —Å class weights\n",
        "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "\n",
        "start = time.time()\n",
        "xgb_tuned = XGBClassifier(\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=4,\n",
        "    min_child_weight=3,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_alpha=0.1,\n",
        "    reg_lambda=1.0,\n",
        "    scale_pos_weight=scale_pos_weight,  # Class weights!\n",
        "    random_state=RANDOM_STATE,\n",
        "    verbosity=0\n",
        ")\n",
        "xgb_tuned.fit(X_train_ohe, y_train)\n",
        "xgb_tuned_time = time.time() - start\n",
        "\n",
        "xgb_tuned_results = evaluate_model(xgb_tuned, X_train_ohe, X_test_ohe, y_train, y_test,\n",
        "                                   'XGBoost Tuned + Weights', xgb_tuned_time)\n",
        "results.append(xgb_tuned_results)"
    ]
})

# ============================================================================
# LIGHTGBM
# ============================================================================

practical_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 2.7 LightGBM: Baseline –∏ Tuned"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# LightGBM baseline\n",
        "start = time.time()\n",
        "lgbm_base = LGBMClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    num_leaves=31,\n",
        "    random_state=RANDOM_STATE,\n",
        "    verbose=-1\n",
        ")\n",
        "lgbm_base.fit(X_train_ohe, y_train)\n",
        "lgbm_base_time = time.time() - start\n",
        "\n",
        "lgbm_base_results = evaluate_model(lgbm_base, X_train_ohe, X_test_ohe, y_train, y_test,\n",
        "                                   'LightGBM Baseline', lgbm_base_time)\n",
        "results.append(lgbm_base_results)"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# LightGBM tuned —Å class weights\n",
        "start = time.time()\n",
        "lgbm_tuned = LGBMClassifier(\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.05,\n",
        "    num_leaves=31,\n",
        "    max_depth=6,\n",
        "    min_child_samples=20,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_alpha=0.1,\n",
        "    reg_lambda=1.0,\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    random_state=RANDOM_STATE,\n",
        "    verbose=-1\n",
        ")\n",
        "lgbm_tuned.fit(X_train_ohe, y_train)\n",
        "lgbm_tuned_time = time.time() - start\n",
        "\n",
        "lgbm_tuned_results = evaluate_model(lgbm_tuned, X_train_ohe, X_test_ohe, y_train, y_test,\n",
        "                                    'LightGBM Tuned + Weights', lgbm_tuned_time)\n",
        "results.append(lgbm_tuned_results)"
    ]
})

# ============================================================================
# CATBOOST
# ============================================================================

practical_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 2.8 CatBoost: Native Categorical Features"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# CatBoost baseline (—Å native categorical)\n",
        "start = time.time()\n",
        "cat_base = CatBoostClassifier(\n",
        "    iterations=100,\n",
        "    learning_rate=0.1,\n",
        "    depth=6,\n",
        "    cat_features=cat_indices,  # NATIVE CATEGORICAL!\n",
        "    random_state=RANDOM_STATE,\n",
        "    verbose=0\n",
        ")\n",
        "cat_base.fit(X_train_cat, y_train)\n",
        "cat_base_time = time.time() - start\n",
        "\n",
        "cat_base_results = evaluate_model(cat_base, X_train_cat, X_test_cat, y_train, y_test,\n",
        "                                  'CatBoost Baseline (Native Cat)', cat_base_time)\n",
        "results.append(cat_base_results)"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# CatBoost tuned —Å class weights\n",
        "start = time.time()\n",
        "cat_tuned = CatBoostClassifier(\n",
        "    iterations=200,\n",
        "    learning_rate=0.05,\n",
        "    depth=6,\n",
        "    l2_leaf_reg=3,\n",
        "    cat_features=cat_indices,\n",
        "    class_weights=[1, scale_pos_weight],  # Class weights!\n",
        "    random_state=RANDOM_STATE,\n",
        "    verbose=0\n",
        ")\n",
        "cat_tuned.fit(X_train_cat, y_train)\n",
        "cat_tuned_time = time.time() - start\n",
        "\n",
        "cat_tuned_results = evaluate_model(cat_tuned, X_train_cat, X_test_cat, y_train, y_test,\n",
        "                                   'CatBoost Tuned + Weights', cat_tuned_time)\n",
        "results.append(cat_tuned_results)"
    ]
})

# ============================================================================
# SMOTE
# ============================================================================

practical_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 2.9 Imbalanced Data: SMOTE"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# SMOTE + XGBoost\n",
        "if HAS_IMBLEARN:\n",
        "    smote = SMOTE(sampling_strategy=0.7, random_state=RANDOM_STATE)  # –î–æ 70% minority\n",
        "    X_train_smote, y_train_smote = smote.fit_resample(X_train_ohe, y_train)\n",
        "    \n",
        "    print(f'–ü–æ—Å–ª–µ SMOTE: {X_train_smote.shape[0]} samples')\n",
        "    print(f'Churn rate: {y_train_smote.mean():.1%}')\n",
        "    \n",
        "    start = time.time()\n",
        "    xgb_smote = XGBClassifier(\n",
        "        n_estimators=200,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=4,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=RANDOM_STATE,\n",
        "        verbosity=0\n",
        "    )\n",
        "    xgb_smote.fit(X_train_smote, y_train_smote)\n",
        "    xgb_smote_time = time.time() - start\n",
        "    \n",
        "    xgb_smote_results = evaluate_model(xgb_smote, X_train_smote, X_test_ohe, \n",
        "                                       y_train_smote, y_test,\n",
        "                                       'XGBoost + SMOTE', xgb_smote_time)\n",
        "    results.append(xgb_smote_results)\n",
        "else:\n",
        "    print('‚ö†Ô∏è SMOTE –ø—Ä–æ–ø—É—â–µ–Ω (imbalanced-learn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω)')"
    ]
})

# ============================================================================
# STACKING
# ============================================================================

practical_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 2.10 Stacking Ensemble"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Stacking: XGBoost + LightGBM + CatBoost ‚Üí Logistic Regression\n",
        "print('üöÄ –°–æ–∑–¥–∞–µ–º Stacking Ensemble...')\n",
        "\n",
        "# Out-of-fold predictions –¥–ª—è meta features\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "# –ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (—É–∂–µ –æ–±—É—á–µ–Ω—ã)\n",
        "base_models = [\n",
        "    ('xgb', xgb_tuned, X_train_ohe, X_test_ohe),\n",
        "    ('lgbm', lgbm_tuned, X_train_ohe, X_test_ohe),\n",
        "    ('cat', cat_tuned, X_train_cat, X_test_cat)\n",
        "]\n",
        "\n",
        "# OOF predictions\n",
        "oof_train = np.zeros((X_train.shape[0], len(base_models)))\n",
        "oof_test = np.zeros((X_test.shape[0], len(base_models)))\n",
        "\n",
        "for i, (name, model, X_tr, X_te) in enumerate(base_models):\n",
        "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è test\n",
        "    oof_test[:, i] = model.predict_proba(X_te)[:, 1]\n",
        "    \n",
        "    # OOF –¥–ª—è train —á–µ—Ä–µ–∑ cross-validation\n",
        "    oof_preds = cross_val_predict(model, X_tr, y_train, cv=cv, method='predict_proba')[:, 1]\n",
        "    oof_train[:, i] = oof_preds\n",
        "    \n",
        "    print(f'‚úÖ {name}: OOF –≥–æ—Ç–æ–≤')\n",
        "\n",
        "print(f'Meta features shape: train {oof_train.shape}, test {oof_test.shape}')"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Meta-learner: Logistic Regression\n",
        "start = time.time()\n",
        "meta_learner = LogisticRegression(random_state=RANDOM_STATE)\n",
        "meta_learner.fit(oof_train, y_train)\n",
        "stacking_time = time.time() - start\n",
        "\n",
        "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "stacking_pred_proba = meta_learner.predict_proba(oof_test)[:, 1]\n",
        "stacking_pred = (stacking_pred_proba > 0.5).astype(int)\n",
        "\n",
        "# –û—Ü–µ–Ω–∫–∞\n",
        "acc = accuracy_score(y_test, stacking_pred)\n",
        "prec = precision_score(y_test, stacking_pred)\n",
        "rec = recall_score(y_test, stacking_pred)\n",
        "f1 = f1_score(y_test, stacking_pred)\n",
        "roc_auc = roc_auc_score(y_test, stacking_pred_proba)\n",
        "pr_auc = average_precision_score(y_test, stacking_pred_proba)\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, stacking_pred).ravel()\n",
        "cost = fn * 500 + fp * 50\n",
        "\n",
        "print(f'\\nüìä Stacking Ensemble:')\n",
        "print(f'  Accuracy: {acc:.4f}')\n",
        "print(f'  Precision: {prec:.4f}')\n",
        "print(f'  Recall: {rec:.4f}')\n",
        "print(f'  F1-score: {f1:.4f}')\n",
        "print(f'  ROC-AUC: {roc_auc:.4f}')\n",
        "print(f'  PR-AUC: {pr_auc:.4f}')\n",
        "print(f'  Business Cost: ${cost:,}')\n",
        "print(f'  Meta-learner weights: {meta_learner.coef_[0]}')\n",
        "\n",
        "stacking_results = {\n",
        "    'Model': 'Stacking Ensemble',\n",
        "    'Accuracy': acc,\n",
        "    'Precision': prec,\n",
        "    'Recall': rec,\n",
        "    'F1': f1,\n",
        "    'ROC-AUC': roc_auc,\n",
        "    'PR-AUC': pr_auc,\n",
        "    'Cost': cost,\n",
        "    'Time': stacking_time,\n",
        "    'TP': tp,\n",
        "    'FP': fp,\n",
        "    'TN': tn,\n",
        "    'FN': fn\n",
        "}\n",
        "results.append(stacking_results)"
    ]
})

# ============================================================================
# FINAL COMPARISON
# ============================================================================

practical_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 2.11 –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –°–æ–∑–¥–∞–µ–º —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É\n",
        "comparison_df = pd.DataFrame(results)\n",
        "comparison_df = comparison_df.sort_values('ROC-AUC', ascending=False)\n",
        "\n",
        "print('\\n' + '='*100)\n",
        "print('üèÜ –§–ò–ù–ê–õ–¨–ù–û–ï –°–†–ê–í–ù–ï–ù–ò–ï –í–°–ï–• –ú–ï–¢–û–î–û–í PHASE 1')\n",
        "print('='*100)\n",
        "print(comparison_df[['Model', 'ROC-AUC', 'F1', 'Precision', 'Recall', 'Cost', 'Time']].to_string(index=False))\n",
        "print('='*100)"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "# ROC-AUC\n",
        "ax = axes[0, 0]\n",
        "bars = ax.barh(comparison_df['Model'], comparison_df['ROC-AUC'], color='skyblue', edgecolor='black')\n",
        "ax.set_xlabel('ROC-AUC')\n",
        "ax.set_title('ROC-AUC Score')\n",
        "ax.axvline(comparison_df['ROC-AUC'].max(), color='red', linestyle='--', alpha=0.5)\n",
        "\n",
        "# F1-score\n",
        "ax = axes[0, 1]\n",
        "ax.barh(comparison_df['Model'], comparison_df['F1'], color='lightgreen', edgecolor='black')\n",
        "ax.set_xlabel('F1-score')\n",
        "ax.set_title('F1 Score')\n",
        "\n",
        "# Precision vs Recall\n",
        "ax = axes[0, 2]\n",
        "ax.scatter(comparison_df['Recall'], comparison_df['Precision'], s=200, alpha=0.6, edgecolor='black')\n",
        "for i, model in enumerate(comparison_df['Model']):\n",
        "    ax.annotate(model.split()[0], \n",
        "                (comparison_df['Recall'].iloc[i], comparison_df['Precision'].iloc[i]),\n",
        "                fontsize=8)\n",
        "ax.set_xlabel('Recall')\n",
        "ax.set_ylabel('Precision')\n",
        "ax.set_title('Precision vs Recall')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Business Cost\n",
        "ax = axes[1, 0]\n",
        "comparison_sorted_cost = comparison_df.sort_values('Cost')\n",
        "ax.barh(comparison_sorted_cost['Model'], comparison_sorted_cost['Cost'], \n",
        "        color='lightcoral', edgecolor='black')\n",
        "ax.set_xlabel('Business Cost ($)')\n",
        "ax.set_title('Business Cost (Lower is Better)')\n",
        "\n",
        "# Training Time\n",
        "ax = axes[1, 1]\n",
        "comparison_sorted_time = comparison_df.sort_values('Time')\n",
        "ax.barh(comparison_sorted_time['Model'], comparison_sorted_time['Time'], \n",
        "        color='wheat', edgecolor='black')\n",
        "ax.set_xlabel('Time (seconds)')\n",
        "ax.set_title('Training Time')\n",
        "\n",
        "# Confusion Matrix for best model\n",
        "ax = axes[1, 2]\n",
        "best_model = comparison_df.iloc[0]\n",
        "cm = np.array([[best_model['TN'], best_model['FP']], \n",
        "               [best_model['FN'], best_model['TP']]])\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, cbar=False)\n",
        "ax.set_xlabel('Predicted')\n",
        "ax.set_ylabel('Actual')\n",
        "ax.set_title(f'Best Model: {best_model[\"Model\"]}')\n",
        "ax.set_xticklabels(['No Churn', 'Churn'])\n",
        "ax.set_yticklabels(['No Churn', 'Churn'])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
    ]
})

# ============================================================================
# CONCLUSIONS
# ============================================================================

practical_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "## üéØ –í—ã–≤–æ–¥—ã –∏ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n",
        "\n",
        "### –ß—Ç–æ –º—ã –≤—ã—è—Å–Ω–∏–ª–∏:\n",
        "\n",
        "#### 1. –ö–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π (ROC-AUC)\n",
        "\n",
        "–¢–∏–ø–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç –¥–∞–Ω–Ω—ã—Ö):\n",
        "- **Stacking Ensemble:** –û–±—ã—á–Ω–æ –ª—É—á—à–∏–π (~0.85-0.87)\n",
        "- **CatBoost Tuned:** –ë–ª–∏–∑–∫–æ –∫ stacking (~0.84-0.86)\n",
        "- **XGBoost/LightGBM Tuned:** –°–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ (~0.83-0.85)\n",
        "- **–ë–∞–∑–æ–≤—ã–µ boosting:** –•–æ—Ä–æ—à–æ (~0.81-0.83)\n",
        "- **Logistic Regression:** Baseline (~0.75-0.78)\n",
        "\n",
        "**–í—ã–≤–æ–¥:** –†–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É tuned –º–æ–¥–µ–ª—è–º–∏ —á–∞—Å—Ç–æ <2% ROC-AUC\n",
        "\n",
        "#### 2. –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è\n",
        "\n",
        "- **LightGBM:** –°–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π (–æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö)\n",
        "- **XGBoost:** –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å\n",
        "- **CatBoost:** –ú–µ–¥–ª–µ–Ω–Ω–µ–µ (ordered boosting —Å–ª–æ–∂–Ω–µ–µ)\n",
        "- **Stacking:** –°–∞–º—ã–π –º–µ–¥–ª–µ–Ω–Ω—ã–π (–æ–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π)\n",
        "\n",
        "#### 3. Categorical features\n",
        "\n",
        "**CatBoost —Å native categorical –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ—Ç–ª–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ë–ï–ó:**\n",
        "- One-hot encoding\n",
        "- Target encoding\n",
        "- Feature engineering\n",
        "\n",
        "**–í—ã–≤–æ–¥:** –î–ª—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º–∏ - CatBoost –ª—É—á—à–∏–π –≤—ã–±–æ—Ä –∏–∑ –∫–æ—Ä–æ–±–∫–∏!\n",
        "\n",
        "#### 4. Imbalanced data\n",
        "\n",
        "**Class weights vs SMOTE:**\n",
        "- Class weights: –ü—Ä–æ—â–µ, –±—ã—Å—Ç—Ä–µ–µ, —Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ –¥–ª—è moderate imbalance\n",
        "- SMOTE: –ú–æ–∂–µ—Ç –ø–æ–º–æ—á—å –¥–ª—è extreme imbalance, –Ω–æ —Ä–∏—Å–∫ noise\n",
        "- **Hybrid** (moderate SMOTE + weights): –ß–∞—Å—Ç–æ –ª—É—á—à–∏–π –≤–∞—Ä–∏–∞–Ω—Ç\n",
        "\n",
        "#### 5. Stacking\n",
        "\n",
        "**–ü—Ä–∏—Ä–æ—Å—Ç:** –û–±—ã—á–Ω–æ 0.5-2% –ø–æ–≤–µ—Ä—Ö –ª—É—á—à–µ–π –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏\n",
        "\n",
        "**–°—Ç–æ–∏—Ç –ª–∏?**\n",
        "- ‚úÖ Kaggle, research: –î–∞!\n",
        "- ‚ùå Production —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏: –í–µ—Ä–æ—è—Ç–Ω–æ –Ω–µ—Ç\n",
        "\n",
        "---\n",
        "\n",
        "### –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è Production\n",
        "\n",
        "#### –°—Ü–µ–Ω–∞—Ä–∏–π 1: –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç\n",
        "\n",
        "**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** XGBoost —Å –¥–µ—Ñ–æ–ª—Ç–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n",
        "\n",
        "```python\n",
        "XGBClassifier(n_estimators=100, learning_rate=0.1)\n",
        "```\n",
        "\n",
        "- ‚úÖ –•–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑ –∫–æ—Ä–æ–±–∫–∏\n",
        "- ‚úÖ –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å\n",
        "- ‚úÖ –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π tuning\n",
        "\n",
        "#### –°—Ü–µ–Ω–∞—Ä–∏–π 2: –ú–Ω–æ–≥–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "\n",
        "**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** CatBoost —Å native categorical\n",
        "\n",
        "```python\n",
        "CatBoostClassifier(iterations=100, cat_features=cat_indices)\n",
        "```\n",
        "\n",
        "- ‚úÖ –ù–µ –Ω—É–∂–µ–Ω one-hot encoding\n",
        "- ‚úÖ –û—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é\n",
        "- ‚úÖ Ordered target statistics –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏\n",
        "\n",
        "#### –°—Ü–µ–Ω–∞—Ä–∏–π 3: –ë–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ (>1M —Å—Ç—Ä–æ–∫)\n",
        "\n",
        "**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** LightGBM\n",
        "\n",
        "```python\n",
        "LGBMClassifier(n_estimators=100, num_leaves=31)\n",
        "```\n",
        "\n",
        "- ‚úÖ –í 5-20x –±—ã—Å—Ç—Ä–µ–µ XGBoost\n",
        "- ‚úÖ –ú–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏\n",
        "- ‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç tuning –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è overfitting\n",
        "\n",
        "#### –°—Ü–µ–Ω–∞—Ä–∏–π 4: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ (Kaggle)\n",
        "\n",
        "**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** Stacking Ensemble\n",
        "\n",
        "```python\n",
        "Base: XGBoost + LightGBM + CatBoost + RF\n",
        "Meta: LogisticRegression / XGBoost\n",
        "```\n",
        "\n",
        "- ‚úÖ –í—ã–∂–∏–º–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø—Ä–æ—Ü–µ–Ω—Ç—ã\n",
        "- ‚úÖ Diversity –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π\n",
        "- ‚ùå –°–ª–æ–∂–Ω–æ—Å—Ç—å –∏ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è\n",
        "\n",
        "#### –°—Ü–µ–Ω–∞—Ä–∏–π 5: Imbalanced data\n",
        "\n",
        "**Moderate imbalance (1:10):**\n",
        "```python\n",
        "scale_pos_weight = n_negative / n_positive\n",
        "XGBClassifier(scale_pos_weight=scale_pos_weight)\n",
        "```\n",
        "\n",
        "**Extreme imbalance (1:100+):**\n",
        "```python\n",
        "smote = SMOTE(sampling_strategy=0.1)  # Moderate oversample\n",
        "+ class_weights\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### –û–±—â–∏–µ Best Practices\n",
        "\n",
        "1. **–í—Å–µ–≥–¥–∞ –Ω–∞—á–∏–Ω–∞–π—Ç–µ —Å baseline** (Logistic Regression)\n",
        "2. **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ cross-validation** –¥–ª—è —á–µ—Å—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–∏\n",
        "3. **Class imbalance:** –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ PR-AUC, –Ω–µ accuracy!\n",
        "4. **Feature engineering:** 60-70% –∫–∞—á–µ—Å—Ç–≤–∞, –∏–Ω–≤–µ—Å—Ç–∏—Ä—É–π—Ç–µ –≤—Ä–µ–º—è\n",
        "5. **Hyperparameter tuning:** –¢–æ–ª—å–∫–æ –ø–æ—Å–ª–µ feature engineering\n",
        "6. **Ensemble:** –ü–æ—Å–ª–µ–¥–Ω–∏–π —à–∞–≥, –µ—Å–ª–∏ –Ω—É–∂–Ω—ã –ø—Ä–æ—Ü–µ–Ω—Ç—ã\n",
        "7. **Production:** –ü—Ä–æ—Å—Ç–æ—Ç–∞ > —Å–ª–æ–∂–Ω–æ—Å—Ç—å (XGBoost –∏–ª–∏ CatBoost)\n",
        "\n",
        "---\n",
        "\n",
        "## üéâ Phase 1 Classical ML –∑–∞–≤–µ—Ä—à–µ–Ω!\n",
        "\n",
        "**–ú—ã –∏–∑—É—á–∏–ª–∏:**\n",
        "- ‚úÖ XGBoost, LightGBM, CatBoost –≤ –¥–µ—Ç–∞–ª—è—Ö\n",
        "- ‚úÖ Stacking Ensemble\n",
        "- ‚úÖ Imbalanced data —Ç–µ—Ö–Ω–∏–∫–∏\n",
        "- ‚úÖ Advanced Feature Engineering\n",
        "- ‚úÖ –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "\n",
        "**–°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:**\n",
        "- Phase 2: Deep Learning (MLP, CNN, RNN, Transformers)\n",
        "- Phase 3: Time Series\n",
        "- Phase 4: NLP\n",
        "- Phase 5: Computer Vision\n",
        "\n",
        "**–ü–æ–∑–¥—Ä–∞–≤–ª—è—é! üöÄ**\n"
    ]
})

# –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —è—á–µ–π–∫–∏
for cell in practical_cells:
    notebook['cells'].append(cell)

# –°–æ—Ö—Ä–∞–Ω—è–µ–º
with open(notebook_path, 'w', encoding='utf-8') as f:
    json.dump(notebook, f, ensure_ascii=False, indent=1)

print(f'‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(practical_cells)} –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö —è—á–µ–µ–∫')
print(f'–í—Å–µ–≥–æ —è—á–µ–µ–∫ –≤ –Ω–æ—É—Ç–±—É–∫–µ: {len(notebook["cells"])}')

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Production ML & MLOps: From Notebook to Production\n",
    "\n",
    "**Phase 7: –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º ML-—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –≤ Production-Ready —Å–∏—Å—Ç–µ–º—É**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ The Production Gap\n",
    "\n",
    "### –ü—Ä–æ–±–ª–µ–º–∞: 90% ML-–ø—Ä–æ–µ–∫—Ç–æ–≤ –Ω–µ –¥–æ—Ö–æ–¥—è—Ç –¥–æ production!\n",
    "\n",
    "**–ü–æ—á–µ–º—É?**\n",
    "\n",
    "```python\n",
    "# Jupyter Notebook (10% —Ä–∞–±–æ—Ç—ã)\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "# \"–ì–æ—Ç–æ–≤–æ! –ú–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç!\" üéâ\n",
    "```\n",
    "\n",
    "**–ù–æ –¥–ª—è production –Ω—É–∂–Ω–æ (90% —Ä–∞–±–æ—Ç—ã):**\n",
    "\n",
    "- üì¶ **Packaging:** –ö–∞–∫ —É–ø–∞–∫–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è deployment?\n",
    "- üåê **API:** –ö–∞–∫ –ø–æ–ª—É—á–∞—Ç—å predictions –ø–æ HTTP?\n",
    "- üê≥ **Containerization:** –ö–∞–∫ –æ–±–µ—Å–ø–µ—á–∏—Ç—å reproducibility?\n",
    "- üìä **Monitoring:** –ö–∞–∫ —É–∑–Ω–∞—Ç—å, —á—Ç–æ –º–æ–¥–µ–ª—å degraded?\n",
    "- üîÑ **CI/CD:** –ö–∞–∫ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å deployment?\n",
    "- üìù **Versioning:** –ö–∞–∫ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –∏ –¥–∞–Ω–Ω—ã–µ?\n",
    "- ‚ö° **Scaling:** –ö–∞–∫ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å 1000+ requests/sec?\n",
    "- üîí **Security:** Authentication, rate limiting, input validation?\n",
    "\n",
    "---\n",
    "\n",
    "## üîß MLOps = ML + DevOps + Data Engineering\n",
    "\n",
    "### MLOps Lifecycle:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   DATA      ‚îÇ ‚Üí ‚îÇ   MODEL     ‚îÇ ‚Üí ‚îÇ   DEPLOY    ‚îÇ\n",
    "‚îÇ  PIPELINE   ‚îÇ    ‚îÇ  TRAINING   ‚îÇ    ‚îÇ   & SERVE   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚Üë                                      ‚îÇ\n",
    "       ‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ\n",
    "       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ  MONITOR &  ‚îÇ ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                 ‚îÇ   RETRAIN   ‚îÇ\n",
    "                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### –ö–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:\n",
    "\n",
    "#### 1. **Experiment Tracking**\n",
    "- –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –º–µ—Ç—Ä–∏–∫, –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤\n",
    "- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤\n",
    "- Reproducibility\n",
    "- **Tools:** MLflow, Weights & Biases, Neptune, TensorBoard\n",
    "\n",
    "#### 2. **Model Registry**\n",
    "- –í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π\n",
    "- Staging ‚Üí Production –ø–µ—Ä–µ—Ö–æ–¥—ã\n",
    "- Model lineage\n",
    "- **Tools:** MLflow Model Registry, SageMaker Model Registry\n",
    "\n",
    "#### 3. **Data Versioning**\n",
    "- Version control –¥–ª—è –¥–∞–Ω–Ω—ã—Ö\n",
    "- Reproducibility —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤\n",
    "- Data lineage\n",
    "- **Tools:** DVC, Delta Lake, LakeFS\n",
    "\n",
    "#### 4. **Feature Store**\n",
    "- –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ features\n",
    "- Online/offline serving\n",
    "- Feature sharing –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏\n",
    "- **Tools:** Feast, Tecton, AWS Feature Store\n",
    "\n",
    "#### 5. **Model Serving**\n",
    "- REST API –¥–ª—è predictions\n",
    "- Batch vs Real-time inference\n",
    "- A/B testing\n",
    "- **Tools:** FastAPI, BentoML, Seldon, KServe\n",
    "\n",
    "#### 6. **Monitoring**\n",
    "- Data drift detection\n",
    "- Model performance monitoring\n",
    "- Alerting\n",
    "- **Tools:** Evidently, WhyLabs, Arize, Prometheus/Grafana\n",
    "\n",
    "#### 7. **CI/CD for ML**\n",
    "- Automated testing (data, model, API)\n",
    "- Automated deployment\n",
    "- **Tools:** GitHub Actions, Jenkins, GitLab CI, Kubeflow Pipelines\n",
    "\n",
    "---\n",
    "\n",
    "## üìä –ß—Ç–æ –º—ã —Ä–µ–∞–ª–∏–∑—É–µ–º\n",
    "\n",
    "### Full Production Pipeline:\n",
    "\n",
    "**Part 1: Experiment Tracking**\n",
    "1. MLflow setup –∏ logging\n",
    "2. Tracking parameters, metrics, models\n",
    "3. Model comparison –∏ selection\n",
    "4. Model Registry\n",
    "\n",
    "**Part 2: Model Serialization**\n",
    "1. Pickle, joblib, ONNX\n",
    "2. Custom transformers\n",
    "3. Pipeline serialization\n",
    "\n",
    "**Part 3: API Development (FastAPI)**\n",
    "1. REST API –¥–ª—è ML –º–æ–¥–µ–ª–∏\n",
    "2. Pydantic schemas –¥–ª—è validation\n",
    "3. Async endpoints\n",
    "4. Error handling\n",
    "5. API testing\n",
    "\n",
    "**Part 4: Containerization (Docker)**\n",
    "1. Dockerfile –¥–ª—è ML –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è\n",
    "2. Multi-stage builds\n",
    "3. Docker Compose\n",
    "4. Best practices –¥–ª—è ML containers\n",
    "\n",
    "**Part 5: Monitoring & Drift Detection**\n",
    "1. Data drift (input distribution changes)\n",
    "2. Concept drift (target relationship changes)\n",
    "3. Model performance monitoring\n",
    "4. Alerting strategies\n",
    "\n",
    "**Part 6: CI/CD –¥–ª—è ML**\n",
    "1. GitHub Actions workflow\n",
    "2. Automated testing\n",
    "3. Automated deployment\n",
    "4. Continuous training\n",
    "\n",
    "**Part 7: Data Versioning (DVC)**\n",
    "1. DVC basics\n",
    "2. Tracking data –∏ models\n",
    "3. Pipelines\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª –ß–∞—Å—Ç—å 1: Setup –∏ Experiment Tracking\n",
    "\n",
    "### 1.1 –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ë–∞–∑–æ–≤—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, classification_report\n",
    ")\n",
    "\n",
    "# MLOps Tools\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Serialization\n",
    "import pickle\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Type hints\n",
    "from typing import Dict, List, Any, Tuple, Optional\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"\\n‚úÖ –í—Å–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
    "print(f\"MLflow version: {mlflow.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dataset –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\n",
    "\n",
    "–ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π dataset –¥–ª—è **Customer Churn Prediction** - —Ç–∏–ø–∏—á–Ω–∞—è production –∑–∞–¥–∞—á–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_churn_dataset(n_samples: int = 10000) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create synthetic customer churn dataset.\n",
    "    \n",
    "    Features:\n",
    "    - tenure: months with company\n",
    "    - monthly_charges: monthly bill\n",
    "    - total_charges: total amount paid\n",
    "    - contract_type: month-to-month, one year, two year\n",
    "    - payment_method: credit card, bank transfer, electronic check\n",
    "    - internet_service: DSL, Fiber optic, No\n",
    "    - online_security: Yes, No\n",
    "    - tech_support: Yes, No\n",
    "    - num_tickets: support tickets opened\n",
    "    - senior_citizen: 0 or 1\n",
    "    \n",
    "    Target: churn (0 or 1)\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Numerical features\n",
    "    tenure = np.random.randint(1, 72, n_samples)\n",
    "    monthly_charges = np.random.uniform(20, 100, n_samples)\n",
    "    total_charges = tenure * monthly_charges + np.random.normal(0, 100, n_samples)\n",
    "    total_charges = np.maximum(total_charges, 0)\n",
    "    num_tickets = np.random.poisson(2, n_samples)\n",
    "    \n",
    "    # Categorical features\n",
    "    contract_type = np.random.choice(\n",
    "        ['Month-to-month', 'One year', 'Two year'],\n",
    "        n_samples, p=[0.5, 0.3, 0.2]\n",
    "    )\n",
    "    payment_method = np.random.choice(\n",
    "        ['Credit card', 'Bank transfer', 'Electronic check'],\n",
    "        n_samples, p=[0.4, 0.3, 0.3]\n",
    "    )\n",
    "    internet_service = np.random.choice(\n",
    "        ['DSL', 'Fiber optic', 'No'],\n",
    "        n_samples, p=[0.4, 0.4, 0.2]\n",
    "    )\n",
    "    online_security = np.random.choice(['Yes', 'No'], n_samples, p=[0.4, 0.6])\n",
    "    tech_support = np.random.choice(['Yes', 'No'], n_samples, p=[0.3, 0.7])\n",
    "    senior_citizen = np.random.choice([0, 1], n_samples, p=[0.84, 0.16])\n",
    "    \n",
    "    # Create churn with realistic patterns\n",
    "    churn_prob = (\n",
    "        0.1 +  # baseline\n",
    "        (tenure < 12).astype(float) * 0.15 +  # short tenure = higher churn\n",
    "        (monthly_charges > 70).astype(float) * 0.1 +  # high charges = higher churn\n",
    "        (contract_type == 'Month-to-month').astype(float) * 0.2 +  # month-to-month = higher churn\n",
    "        (payment_method == 'Electronic check').astype(float) * 0.05 +  # electronic check = higher churn\n",
    "        (online_security == 'No').astype(float) * 0.05 +  # no security = higher churn\n",
    "        (tech_support == 'No').astype(float) * 0.05 +  # no support = higher churn\n",
    "        (num_tickets > 3).astype(float) * 0.1  # many tickets = higher churn\n",
    "    )\n",
    "    \n",
    "    # Normalize probabilities\n",
    "    churn_prob = np.clip(churn_prob, 0, 0.9)\n",
    "    churn = (np.random.random(n_samples) < churn_prob).astype(int)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'tenure': tenure,\n",
    "        'monthly_charges': monthly_charges,\n",
    "        'total_charges': total_charges,\n",
    "        'num_tickets': num_tickets,\n",
    "        'contract_type': contract_type,\n",
    "        'payment_method': payment_method,\n",
    "        'internet_service': internet_service,\n",
    "        'online_security': online_security,\n",
    "        'tech_support': tech_support,\n",
    "        'senior_citizen': senior_citizen,\n",
    "        'churn': churn\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# –°–æ–∑–¥–∞—ë–º dataset\n",
    "df = create_churn_dataset(10000)\n",
    "\n",
    "print(\"\\nüìä Customer Churn Dataset\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Churn rate: {df['churn'].mean():.2%}\")\n",
    "print(f\"\\nFeature types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nFirst rows:\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"PREPROCESSING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "numerical_cols = ['tenure', 'monthly_charges', 'total_charges', 'num_tickets']\n",
    "categorical_cols = [\n",
    "    'contract_type', 'payment_method', 'internet_service',\n",
    "    'online_security', 'tech_support', 'senior_citizen'\n",
    "]\n",
    "\n",
    "# senior_citizen —É–∂–µ —á–∏—Å–ª–æ–≤–æ–π, –Ω–æ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π\n",
    "df['senior_citizen'] = df['senior_citizen'].astype(str)\n",
    "\n",
    "# Label encoding –¥–ª—è categorical\n",
    "label_encoders = {}\n",
    "df_processed = df.copy()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_processed[col] = le.fit_transform(df_processed[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Feature matrix –∏ target\n",
    "feature_cols = numerical_cols + categorical_cols\n",
    "X = df_processed[feature_cols]\n",
    "y = df_processed['churn']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "X_train_scaled[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
    "X_test_scaled[numerical_cols] = scaler.transform(X_test[numerical_cols])\n",
    "\n",
    "print(f\"\\n‚úÖ Preprocessing completed\")\n",
    "print(f\"Train size: {X_train.shape[0]}\")\n",
    "print(f\"Test size: {X_test.shape[0]}\")\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"Train churn rate: {y_train.mean():.2%}\")\n",
    "print(f\"Test churn rate: {y_test.mean():.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî¨ –ß–∞—Å—Ç—å 2: Experiment Tracking —Å MLflow\n",
    "\n",
    "### –ß—Ç–æ —Ç–∞–∫–æ–µ MLflow?\n",
    "\n",
    "**MLflow** - open-source –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è ML lifecycle:\n",
    "\n",
    "1. **MLflow Tracking:** –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤\n",
    "2. **MLflow Projects:** Packaging code –¥–ª—è reproducibility\n",
    "3. **MLflow Models:** Packaging models –¥–ª—è deployment\n",
    "4. **MLflow Model Registry:** –í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ staging –º–æ–¥–µ–ª–µ–π\n",
    "\n",
    "### –ü–æ—á–µ–º—É Experiment Tracking –≤–∞–∂–µ–Ω?\n",
    "\n",
    "**–ë–µ–∑ tracking:**\n",
    "```python\n",
    "# model_v1.py, model_v2_final.py, model_v2_final_FINAL.py...\n",
    "# \"–ö–∞–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–∞–ª–∏ –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç?\"\n",
    "# \"–ö–∞–∫—É—é –≤–µ—Ä—Å–∏—é –º—ã –∑–∞–¥–µ–ø–ª–æ–∏–ª–∏?\"\n",
    "# \"–ü–æ—á–µ–º—É accuracy —É–ø–∞–ª–∞?\"\n",
    "```\n",
    "\n",
    "**–° MLflow:**\n",
    "- ‚úÖ –í—Å–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ª–æ–≥–∏—Ä—É—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏\n",
    "- ‚úÖ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã, –º–µ—Ç—Ä–∏–∫–∏, –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã\n",
    "- ‚úÖ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –≤ UI\n",
    "- ‚úÖ Reproducibility –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω\n",
    "- ‚úÖ Model lineage –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç—Å—è\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"MLFLOW EXPERIMENT TRACKING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# –°–æ–∑–¥–∞—ë–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç\n",
    "experiment_name = \"churn_prediction\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"\\n‚úÖ MLflow experiment created: {experiment_name}\")\n",
    "print(f\"Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º experiment info\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "print(f\"Experiment ID: {experiment.experiment_id}\")\n",
    "print(f\"Artifact Location: {experiment.artifact_location}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤\n",
    "\n",
    "–û–±—É—á–∏–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π –∏ –∑–∞–ª–æ–≥–∏—Ä—É–µ–º –≤—Å–µ –¥–µ—Ç–∞–ª–∏ –≤ MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_log_model(\n",
    "    model,\n",
    "    model_name: str,\n",
    "    X_train: pd.DataFrame,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    y_test: pd.Series,\n",
    "    params: Dict[str, Any]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Train model and log everything to MLflow.\n",
    "    \n",
    "    Returns:\n",
    "        run_id: MLflow run ID\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        # Log parameters\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_param(\"model_type\", model_name)\n",
    "        mlflow.log_param(\"train_size\", len(X_train))\n",
    "        mlflow.log_param(\"test_size\", len(X_test))\n",
    "        mlflow.log_param(\"n_features\", X_train.shape[1])\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"precision\": precision_score(y_test, y_pred),\n",
    "            \"recall\": recall_score(y_test, y_pred),\n",
    "            \"f1\": f1_score(y_test, y_pred),\n",
    "            \"roc_auc\": roc_auc_score(y_test, y_pred_proba)\n",
    "        }\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metrics(metrics)\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(\n",
    "            model,\n",
    "            artifact_path=\"model\",\n",
    "            registered_model_name=f\"{model_name}_churn\"\n",
    "        )\n",
    "        \n",
    "        # Log additional artifacts\n",
    "        # Save feature importance (for tree-based models)\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': feature_cols,\n",
    "                'importance': model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            importance_path = f\"/tmp/{model_name}_importance.csv\"\n",
    "            importance_df.to_csv(importance_path, index=False)\n",
    "            mlflow.log_artifact(importance_path)\n",
    "        \n",
    "        # Log confusion matrix as artifact\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        cm_df = pd.DataFrame(\n",
    "            cm,\n",
    "            columns=['Predicted 0', 'Predicted 1'],\n",
    "            index=['Actual 0', 'Actual 1']\n",
    "        )\n",
    "        cm_path = f\"/tmp/{model_name}_confusion_matrix.csv\"\n",
    "        cm_df.to_csv(cm_path)\n",
    "        mlflow.log_artifact(cm_path)\n",
    "        \n",
    "        # Get run ID\n",
    "        run_id = mlflow.active_run().info.run_id\n",
    "        \n",
    "        print(f\"\\n‚úÖ {model_name} logged to MLflow\")\n",
    "        print(f\"   Run ID: {run_id}\")\n",
    "        print(f\"   Metrics:\")\n",
    "        for metric_name, value in metrics.items():\n",
    "            print(f\"      {metric_name}: {value:.4f}\")\n",
    "        \n",
    "        return run_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–±—É—á–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING MODELS WITH MLFLOW TRACKING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "run_ids = {}\n",
    "\n",
    "# 1. Logistic Regression\n",
    "print(\"\\n[1/4] Logistic Regression\")\n",
    "lr_params = {\n",
    "    \"C\": 1.0,\n",
    "    \"max_iter\": 1000,\n",
    "    \"solver\": \"lbfgs\"\n",
    "}\n",
    "lr = LogisticRegression(**lr_params, random_state=42)\n",
    "run_ids['LogisticRegression'] = train_and_log_model(\n",
    "    lr, \"LogisticRegression\",\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test,\n",
    "    lr_params\n",
    ")\n",
    "\n",
    "# 2. Random Forest (default)\n",
    "print(\"\\n[2/4] Random Forest (default)\")\n",
    "rf_params = {\n",
    "    \"n_estimators\": 100,\n",
    "    \"max_depth\": 10,\n",
    "    \"min_samples_split\": 10\n",
    "}\n",
    "rf = RandomForestClassifier(**rf_params, random_state=42, n_jobs=-1)\n",
    "run_ids['RandomForest_v1'] = train_and_log_model(\n",
    "    rf, \"RandomForest_v1\",\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test,\n",
    "    rf_params\n",
    ")\n",
    "\n",
    "# 3. Random Forest (tuned)\n",
    "print(\"\\n[3/4] Random Forest (tuned)\")\n",
    "rf_params_v2 = {\n",
    "    \"n_estimators\": 200,\n",
    "    \"max_depth\": 15,\n",
    "    \"min_samples_split\": 5,\n",
    "    \"min_samples_leaf\": 2\n",
    "}\n",
    "rf_v2 = RandomForestClassifier(**rf_params_v2, random_state=42, n_jobs=-1)\n",
    "run_ids['RandomForest_v2'] = train_and_log_model(\n",
    "    rf_v2, \"RandomForest_v2\",\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test,\n",
    "    rf_params_v2\n",
    ")\n",
    "\n",
    "# 4. Gradient Boosting\n",
    "print(\"\\n[4/4] Gradient Boosting\")\n",
    "gb_params = {\n",
    "    \"n_estimators\": 100,\n",
    "    \"max_depth\": 5,\n",
    "    \"learning_rate\": 0.1\n",
    "}\n",
    "gb = GradientBoostingClassifier(**gb_params, random_state=42)\n",
    "run_ids['GradientBoosting'] = train_and_log_model(\n",
    "    gb, \"GradientBoosting\",\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test,\n",
    "    gb_params\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ ALL MODELS LOGGED TO MLFLOW\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nTotal runs: {len(run_ids)}\")\n",
    "print(\"Run IDs:\")\n",
    "for name, run_id in run_ids.items():\n",
    "    print(f\"  {name}: {run_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤\n",
    "\n",
    "MLflow –ø–æ–∑–≤–æ–ª—è–µ—Ç –ª–µ–≥–∫–æ —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å runs –∏ –≤—ã–±–∏—Ä–∞—Ç—å –ª—É—á—à—É—é –º–æ–¥–µ–ª—å."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ runs –∏–∑ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞\n",
    "client = MlflowClient()\n",
    "\n",
    "# Search runs\n",
    "runs = mlflow.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    order_by=[\"metrics.roc_auc DESC\"]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENT COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º comparison table\n",
    "comparison_cols = [\n",
    "    'run_id', 'tags.mlflow.runName',\n",
    "    'metrics.accuracy', 'metrics.precision', 'metrics.recall',\n",
    "    'metrics.f1', 'metrics.roc_auc'\n",
    "]\n",
    "\n",
    "comparison_df = runs[comparison_cols].copy()\n",
    "comparison_df.columns = ['Run ID', 'Model', 'Accuracy', 'Precision', 'Recall', 'F1', 'ROC AUC']\n",
    "comparison_df['Run ID'] = comparison_df['Run ID'].str[:8] + '...'\n",
    "\n",
    "print(\"\\nAll Runs (sorted by ROC AUC):\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Best model\n",
    "best_run = runs.iloc[0]\n",
    "print(f\"\\nüèÜ Best Model: {best_run['tags.mlflow.runName']}\")\n",
    "print(f\"   ROC AUC: {best_run['metrics.roc_auc']:.4f}\")\n",
    "print(f\"   F1 Score: {best_run['metrics.f1']:.4f}\")\n",
    "print(f\"   Run ID: {best_run['run_id']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Metrics comparison\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1', 'ROC AUC']\n",
    "comparison_df.set_index('Model')[metrics_to_plot].plot(\n",
    "    kind='bar', ax=axes[0], width=0.8\n",
    ")\n",
    "axes[0].set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# ROC AUC comparison\n",
    "colors = ['#e74c3c' if i == 0 else '#3498db' for i in range(len(comparison_df))]\n",
    "axes[1].barh(comparison_df['Model'], comparison_df['ROC AUC'], color=colors)\n",
    "axes[1].set_title('ROC AUC Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('ROC AUC')\n",
    "for i, v in enumerate(comparison_df['ROC AUC']):\n",
    "    axes[1].text(v + 0.005, i, f'{v:.4f}', va='center')\n",
    "axes[1].set_xlim([0.5, 1.0])\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° MLflow UI –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ –∞–¥—Ä–µ—Å—É: mlflow ui\")\n",
    "print(\"–¢–∞–º –º–æ–∂–Ω–æ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã, —Å–º–æ—Ç—Ä–µ—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏, –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì¶ –ß–∞—Å—Ç—å 3: Model Serialization\n",
    "\n",
    "### –ó–∞—á–µ–º —Å–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å?\n",
    "\n",
    "1. **Persistence:** –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–∑–¥–Ω–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "2. **Deployment:** –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –≤ production —Å–µ—Ä–≤–∏—Å\n",
    "3. **Sharing:** –ü–µ—Ä–µ–¥–∞—Ç—å –º–æ–¥–µ–ª—å –¥—Ä—É–≥–æ–π –∫–æ–º–∞–Ω–¥–µ\n",
    "4. **Versioning:** –°–æ—Ö—Ä–∞–Ω—è—Ç—å —Ä–∞–∑–Ω—ã–µ –≤–µ—Ä—Å–∏–∏\n",
    "\n",
    "### –§–æ—Ä–º–∞—Ç—ã —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏:\n",
    "\n",
    "| –§–æ—Ä–º–∞—Ç | Pros | Cons | Use Case |\n",
    "|--------|------|------|----------|\n",
    "| **Pickle** | Simple, native Python | Security risks, version issues | Quick prototypes |\n",
    "| **Joblib** | Efficient for numpy arrays | Python-only | Sklearn models |\n",
    "| **ONNX** | Cross-platform, optimized | Limited model support | Production inference |\n",
    "| **MLflow** | Full lineage, metadata | Heavier | Full MLOps pipeline |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"MODEL SERIALIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# –í—ã–±–µ—Ä–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å (–∏–ª–∏ –ø–æ—Å–ª–µ–¥–Ω—é—é trained)\n",
    "best_model = gb  # Gradient Boosting\n",
    "model_name = \"churn_predictor\"\n",
    "\n",
    "# Create directory for models\n",
    "import os\n",
    "model_dir = \"./models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# ========================================\n",
    "# 1. Pickle\n",
    "# ========================================\n",
    "print(\"\\n[1/3] Pickle serialization\")\n",
    "pickle_path = f\"{model_dir}/{model_name}.pkl\"\n",
    "\n",
    "with open(pickle_path, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "pickle_size = os.path.getsize(pickle_path) / 1024  # KB\n",
    "print(f\"‚úÖ Saved: {pickle_path}\")\n",
    "print(f\"   Size: {pickle_size:.2f} KB\")\n",
    "\n",
    "# Load and verify\n",
    "with open(pickle_path, 'rb') as f:\n",
    "    loaded_pickle = pickle.load(f)\n",
    "    \n",
    "pickle_pred = loaded_pickle.predict(X_test_scaled.iloc[:5])\n",
    "print(f\"   Verification: {pickle_pred}\")\n",
    "\n",
    "# ========================================\n",
    "# 2. Joblib (better for sklearn)\n",
    "# ========================================\n",
    "print(\"\\n[2/3] Joblib serialization\")\n",
    "joblib_path = f\"{model_dir}/{model_name}.joblib\"\n",
    "\n",
    "joblib.dump(best_model, joblib_path)\n",
    "\n",
    "joblib_size = os.path.getsize(joblib_path) / 1024\n",
    "print(f\"‚úÖ Saved: {joblib_path}\")\n",
    "print(f\"   Size: {joblib_size:.2f} KB\")\n",
    "\n",
    "# Load and verify\n",
    "loaded_joblib = joblib.load(joblib_path)\n",
    "joblib_pred = loaded_joblib.predict(X_test_scaled.iloc[:5])\n",
    "print(f\"   Verification: {joblib_pred}\")\n",
    "\n",
    "# ========================================\n",
    "# 3. Save preprocessing artifacts\n",
    "# ========================================\n",
    "print(\"\\n[3/3] Saving preprocessing artifacts\")\n",
    "\n",
    "# Save scaler\n",
    "scaler_path = f\"{model_dir}/scaler.joblib\"\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"‚úÖ Scaler saved: {scaler_path}\")\n",
    "\n",
    "# Save label encoders\n",
    "encoders_path = f\"{model_dir}/label_encoders.joblib\"\n",
    "joblib.dump(label_encoders, encoders_path)\n",
    "print(f\"‚úÖ Label encoders saved: {encoders_path}\")\n",
    "\n",
    "# Save feature columns\n",
    "config = {\n",
    "    'numerical_cols': numerical_cols,\n",
    "    'categorical_cols': categorical_cols,\n",
    "    'feature_cols': feature_cols\n",
    "}\n",
    "config_path = f\"{model_dir}/config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(f\"‚úÖ Config saved: {config_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ ALL ARTIFACTS SAVED\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nDirectory: {model_dir}\")\n",
    "for f in os.listdir(model_dir):\n",
    "    size = os.path.getsize(f\"{model_dir}/{f}\") / 1024\n",
    "    print(f\"  {f}: {size:.2f} KB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üåê –ß–∞—Å—Ç—å 4: API Development —Å FastAPI\n",
    "\n",
    "### –ü–æ—á–µ–º—É FastAPI?\n",
    "\n",
    "**FastAPI** - —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π Python framework –¥–ª—è building APIs:\n",
    "\n",
    "- ‚úÖ **Very fast:** –ù–∞ —É—Ä–æ–≤–Ω–µ Node.js –∏ Go (–±–ª–∞–≥–æ–¥–∞—Ä—è Starlette –∏ Pydantic)\n",
    "- ‚úÖ **Type hints:** Automatic validation –∏ documentation\n",
    "- ‚úÖ **Async support:** –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤\n",
    "- ‚úÖ **Auto documentation:** Swagger UI –∏ ReDoc –∏–∑ –∫–æ—Ä–æ–±–∫–∏\n",
    "- ‚úÖ **Easy testing:** –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π test client\n",
    "\n",
    "### Architecture –¥–ª—è ML API:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   Client    ‚îÇ ‚Üí ‚îÇ   FastAPI   ‚îÇ ‚Üí ‚îÇ    Model    ‚îÇ\n",
    "‚îÇ  (Request)  ‚îÇ     ‚îÇ   Server    ‚îÇ     ‚îÇ  Inference  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ                   ‚îÇ                   ‚îÇ\n",
    "       ‚îÇ                   ‚Üì                   ‚îÇ\n",
    "       ‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ\n",
    "       ‚îÇ         ‚îÇ   Pydantic  ‚îÇ              ‚îÇ\n",
    "       ‚îÇ         ‚îÇ  Validation ‚îÇ              ‚îÇ\n",
    "       ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ\n",
    "       ‚îÇ                                       ‚îÇ\n",
    "       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                    (Response)\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Pydantic Schemas –¥–ª—è Validation\n",
    "\n",
    "**Pydantic** –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç:\n",
    "- Type validation\n",
    "- Data parsing\n",
    "- JSON serialization\n",
    "- Auto-documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List, Optional\n",
    "from enum import Enum\n",
    "\n",
    "# Enum –¥–ª—è categorical fields\n",
    "class ContractType(str, Enum):\n",
    "    month_to_month = \"Month-to-month\"\n",
    "    one_year = \"One year\"\n",
    "    two_year = \"Two year\"\n",
    "\n",
    "class PaymentMethod(str, Enum):\n",
    "    credit_card = \"Credit card\"\n",
    "    bank_transfer = \"Bank transfer\"\n",
    "    electronic_check = \"Electronic check\"\n",
    "\n",
    "class InternetService(str, Enum):\n",
    "    dsl = \"DSL\"\n",
    "    fiber_optic = \"Fiber optic\"\n",
    "    no = \"No\"\n",
    "\n",
    "class YesNo(str, Enum):\n",
    "    yes = \"Yes\"\n",
    "    no = \"No\"\n",
    "\n",
    "# Request schema\n",
    "class CustomerData(BaseModel):\n",
    "    \"\"\"Schema for customer churn prediction request.\"\"\"\n",
    "    \n",
    "    tenure: int = Field(..., ge=0, le=100, description=\"Months with company\")\n",
    "    monthly_charges: float = Field(..., ge=0, le=500, description=\"Monthly bill amount\")\n",
    "    total_charges: float = Field(..., ge=0, description=\"Total amount paid\")\n",
    "    num_tickets: int = Field(..., ge=0, le=50, description=\"Number of support tickets\")\n",
    "    contract_type: ContractType = Field(..., description=\"Contract type\")\n",
    "    payment_method: PaymentMethod = Field(..., description=\"Payment method\")\n",
    "    internet_service: InternetService = Field(..., description=\"Internet service type\")\n",
    "    online_security: YesNo = Field(..., description=\"Online security service\")\n",
    "    tech_support: YesNo = Field(..., description=\"Tech support service\")\n",
    "    senior_citizen: int = Field(..., ge=0, le=1, description=\"Senior citizen (0 or 1)\")\n",
    "    \n",
    "    @validator('total_charges')\n",
    "    def total_charges_must_be_reasonable(cls, v, values):\n",
    "        if 'tenure' in values and 'monthly_charges' in values:\n",
    "            expected = values['tenure'] * values['monthly_charges']\n",
    "            if v < expected * 0.5:\n",
    "                raise ValueError(f'Total charges too low for tenure and monthly charges')\n",
    "        return v\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"tenure\": 24,\n",
    "                \"monthly_charges\": 65.5,\n",
    "                \"total_charges\": 1500.0,\n",
    "                \"num_tickets\": 2,\n",
    "                \"contract_type\": \"One year\",\n",
    "                \"payment_method\": \"Credit card\",\n",
    "                \"internet_service\": \"Fiber optic\",\n",
    "                \"online_security\": \"Yes\",\n",
    "                \"tech_support\": \"No\",\n",
    "                \"senior_citizen\": 0\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Response schema\n",
    "class PredictionResponse(BaseModel):\n",
    "    \"\"\"Schema for churn prediction response.\"\"\"\n",
    "    \n",
    "    customer_id: Optional[str] = Field(None, description=\"Customer ID if provided\")\n",
    "    churn_prediction: int = Field(..., description=\"Churn prediction (0 or 1)\")\n",
    "    churn_probability: float = Field(..., ge=0, le=1, description=\"Probability of churn\")\n",
    "    confidence: str = Field(..., description=\"Confidence level (Low/Medium/High)\")\n",
    "    risk_factors: List[str] = Field(default=[], description=\"Top risk factors\")\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"customer_id\": \"CUST-123\",\n",
    "                \"churn_prediction\": 1,\n",
    "                \"churn_probability\": 0.73,\n",
    "                \"confidence\": \"High\",\n",
    "                \"risk_factors\": [\"Month-to-month contract\", \"No tech support\"]\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Batch request/response\n",
    "class BatchPredictionRequest(BaseModel):\n",
    "    \"\"\"Schema for batch predictions.\"\"\"\n",
    "    customers: List[CustomerData]\n",
    "\n",
    "class BatchPredictionResponse(BaseModel):\n",
    "    \"\"\"Schema for batch prediction response.\"\"\"\n",
    "    predictions: List[PredictionResponse]\n",
    "    total_processed: int\n",
    "    processing_time_ms: float\n",
    "\n",
    "# Health check\n",
    "class HealthResponse(BaseModel):\n",
    "    \"\"\"Health check response.\"\"\"\n",
    "    status: str\n",
    "    model_loaded: bool\n",
    "    model_version: str\n",
    "    timestamp: str\n",
    "\n",
    "print(\"‚úÖ Pydantic schemas defined\")\n",
    "print(\"\\nSchemas:\")\n",
    "print(\"  - CustomerData (request)\")\n",
    "print(\"  - PredictionResponse (response)\")\n",
    "print(\"  - BatchPredictionRequest/Response\")\n",
    "print(\"  - HealthResponse\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 FastAPI Application\n",
    "\n",
    "–°–æ–∑–¥–∞–¥–∏–º –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–µ API –¥–ª—è serving ML –º–æ–¥–µ–ª–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastAPI Application Code\n",
    "# –≠—Ç–æ—Ç –∫–æ–¥ –æ–±—ã—á–Ω–æ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º —Ñ–∞–π–ª–µ app.py\n",
    "\n",
    "fastapi_code = '''\n",
    "\"\"\"FastAPI application for Churn Prediction ML Model.\"\"\"\n",
    "\n",
    "from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import JSONResponse\n",
    "import uvicorn\n",
    "import joblib\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import logging\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from enum import Enum\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"Churn Prediction API\",\n",
    "    description=\"ML API for predicting customer churn\",\n",
    "    version=\"1.0.0\",\n",
    "    docs_url=\"/docs\",  # Swagger UI\n",
    "    redoc_url=\"/redoc\"  # ReDoc\n",
    ")\n",
    "\n",
    "# CORS middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Global variables for model and artifacts\n",
    "model = None\n",
    "scaler = None\n",
    "label_encoders = None\n",
    "config = None\n",
    "model_version = \"1.0.0\"\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def load_model():\n",
    "    \"\"\"Load model and artifacts on startup.\"\"\"\n",
    "    global model, scaler, label_encoders, config\n",
    "    \n",
    "    logger.info(\"Loading model and artifacts...\")\n",
    "    \n",
    "    try:\n",
    "        model = joblib.load(\"models/churn_predictor.joblib\")\n",
    "        scaler = joblib.load(\"models/scaler.joblib\")\n",
    "        label_encoders = joblib.load(\"models/label_encoders.joblib\")\n",
    "        \n",
    "        with open(\"models/config.json\", \"r\") as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        logger.info(\"‚úÖ Model and artifacts loaded successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Failed to load model: {e}\")\n",
    "        raise\n",
    "\n",
    "def preprocess_input(data: dict) -> np.ndarray:\n",
    "    \"\"\"Preprocess input data for prediction.\"\"\"\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame([data])\n",
    "    \n",
    "    # Convert senior_citizen to string for encoding\n",
    "    df[\\'senior_citizen\\'] = df[\\'senior_citizen\\'].astype(str)\n",
    "    \n",
    "    # Apply label encoding\n",
    "    for col in config[\\'categorical_cols\\']:\n",
    "        if col in label_encoders:\n",
    "            df[col] = label_encoders[col].transform(df[col])\n",
    "    \n",
    "    # Apply scaling to numerical columns\n",
    "    df[config[\\'numerical_cols\\']] = scaler.transform(df[config[\\'numerical_cols\\']])\n",
    "    \n",
    "    # Select features in correct order\n",
    "    X = df[config[\\'feature_cols\\']].values\n",
    "    \n",
    "    return X\n",
    "\n",
    "def get_risk_factors(data: dict, probability: float) -> List[str]:\n",
    "    \"\"\"Identify risk factors based on input data.\"\"\"\n",
    "    risk_factors = []\n",
    "    \n",
    "    if data.get(\\'contract_type\\') == \"Month-to-month\":\n",
    "        risk_factors.append(\"Month-to-month contract (high churn risk)\")\n",
    "    if data.get(\\'tenure\\', 0) < 12:\n",
    "        risk_factors.append(\"Short tenure (< 12 months)\")\n",
    "    if data.get(\\'monthly_charges\\', 0) > 70:\n",
    "        risk_factors.append(\"High monthly charges\")\n",
    "    if data.get(\\'tech_support\\') == \"No\":\n",
    "        risk_factors.append(\"No tech support\")\n",
    "    if data.get(\\'online_security\\') == \"No\":\n",
    "        risk_factors.append(\"No online security\")\n",
    "    if data.get(\\'num_tickets\\', 0) > 3:\n",
    "        risk_factors.append(\"Many support tickets\")\n",
    "    \n",
    "    return risk_factors[:5]  # Top 5\n",
    "\n",
    "def get_confidence(probability: float) -> str:\n",
    "    \"\"\"Get confidence level based on probability.\"\"\"\n",
    "    if probability < 0.3 or probability > 0.7:\n",
    "        return \"High\"\n",
    "    elif probability < 0.4 or probability > 0.6:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Low\"\n",
    "\n",
    "# ========================================\n",
    "# ENDPOINTS\n",
    "# ========================================\n",
    "\n",
    "@app.get(\"/\", tags=[\"Root\"])\n",
    "async def root():\n",
    "    \"\"\"Root endpoint.\"\"\"\n",
    "    return {\n",
    "        \"message\": \"Churn Prediction API\",\n",
    "        \"version\": model_version,\n",
    "        \"docs\": \"/docs\"\n",
    "    }\n",
    "\n",
    "@app.get(\"/health\", response_model=HealthResponse, tags=[\"Health\"])\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint.\"\"\"\n",
    "    return {\n",
    "        \"status\": \"healthy\" if model is not None else \"unhealthy\",\n",
    "        \"model_loaded\": model is not None,\n",
    "        \"model_version\": model_version,\n",
    "        \"timestamp\": datetime.utcnow().isoformat()\n",
    "    }\n",
    "\n",
    "@app.post(\"/predict\", response_model=PredictionResponse, tags=[\"Prediction\"])\n",
    "async def predict(customer: CustomerData, customer_id: Optional[str] = None):\n",
    "    \"\"\"Make a single churn prediction.\"\"\"\n",
    "    try:\n",
    "        # Convert to dict\n",
    "        data = {\n",
    "            \"tenure\": customer.tenure,\n",
    "            \"monthly_charges\": customer.monthly_charges,\n",
    "            \"total_charges\": customer.total_charges,\n",
    "            \"num_tickets\": customer.num_tickets,\n",
    "            \"contract_type\": customer.contract_type.value,\n",
    "            \"payment_method\": customer.payment_method.value,\n",
    "            \"internet_service\": customer.internet_service.value,\n",
    "            \"online_security\": customer.online_security.value,\n",
    "            \"tech_support\": customer.tech_support.value,\n",
    "            \"senior_citizen\": customer.senior_citizen\n",
    "        }\n",
    "        \n",
    "        # Preprocess\n",
    "        X = preprocess_input(data)\n",
    "        \n",
    "        # Predict\n",
    "        prediction = int(model.predict(X)[0])\n",
    "        probability = float(model.predict_proba(X)[0, 1])\n",
    "        \n",
    "        # Get risk factors and confidence\n",
    "        risk_factors = get_risk_factors(data, probability)\n",
    "        confidence = get_confidence(probability)\n",
    "        \n",
    "        return {\n",
    "            \"customer_id\": customer_id,\n",
    "            \"churn_prediction\": prediction,\n",
    "            \"churn_probability\": round(probability, 4),\n",
    "            \"confidence\": confidence,\n",
    "            \"risk_factors\": risk_factors\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prediction error: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/predict/batch\", response_model=BatchPredictionResponse, tags=[\"Prediction\"])\n",
    "async def predict_batch(request: BatchPredictionRequest):\n",
    "    \"\"\"Make batch churn predictions.\"\"\"\n",
    "    start_time = time.time()\n",
    "    predictions = []\n",
    "    \n",
    "    for i, customer in enumerate(request.customers):\n",
    "        try:\n",
    "            result = await predict(customer, customer_id=f\"batch_{i}\")\n",
    "            predictions.append(result)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Batch prediction error for customer {i}: {e}\")\n",
    "            predictions.append({\n",
    "                \"customer_id\": f\"batch_{i}\",\n",
    "                \"churn_prediction\": -1,\n",
    "                \"churn_probability\": 0.0,\n",
    "                \"confidence\": \"Error\",\n",
    "                \"risk_factors\": [str(e)]\n",
    "            })\n",
    "    \n",
    "    processing_time = (time.time() - start_time) * 1000\n",
    "    \n",
    "    return {\n",
    "        \"predictions\": predictions,\n",
    "        \"total_processed\": len(predictions),\n",
    "        \"processing_time_ms\": round(processing_time, 2)\n",
    "    }\n",
    "\n",
    "@app.get(\"/model/info\", tags=[\"Model\"])\n",
    "async def model_info():\n",
    "    \"\"\"Get model information.\"\"\"\n",
    "    return {\n",
    "        \"model_type\": type(model).__name__,\n",
    "        \"model_version\": model_version,\n",
    "        \"features\": config[\\'feature_cols\\'],\n",
    "        \"n_features\": len(config[\\'feature_cols\\']),\n",
    "        \"numerical_features\": config[\\'numerical_cols\\'],\n",
    "        \"categorical_features\": config[\\'categorical_cols\\']\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(\"app:app\", host=\"0.0.0.0\", port=8000, reload=True)\n",
    "'''\n",
    "\n",
    "# Save FastAPI code\n",
    "api_path = \"./app.py\"\n",
    "with open(api_path, 'w') as f:\n",
    "    f.write(fastapi_code)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FASTAPI APPLICATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n‚úÖ FastAPI app saved to: {api_path}\")\n",
    "print(\"\\nEndpoints:\")\n",
    "print(\"  GET  /              - Root endpoint\")\n",
    "print(\"  GET  /health        - Health check\")\n",
    "print(\"  POST /predict       - Single prediction\")\n",
    "print(\"  POST /predict/batch - Batch predictions\")\n",
    "print(\"  GET  /model/info    - Model information\")\n",
    "print(\"\\nRun with: uvicorn app:app --reload\")\n",
    "print(\"Docs available at: http://localhost:8000/docs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 API Testing\n",
    "\n",
    "–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–µ–º API –ª–æ–∫–∞–ª—å–Ω–æ —Å –ø–æ–º–æ—â—å—é Python requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–∏–º—É–ª—è—Ü–∏—è API –∑–∞–ø—Ä–æ—Å–æ–≤ (–±–µ–∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ —Å–µ—Ä–≤–µ—Ä–∞)\n",
    "# –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –±—ã requests.post()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"API TESTING (SIMULATION)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test data\n",
    "test_customer = {\n",
    "    \"tenure\": 24,\n",
    "    \"monthly_charges\": 65.5,\n",
    "    \"total_charges\": 1500.0,\n",
    "    \"num_tickets\": 2,\n",
    "    \"contract_type\": \"One year\",\n",
    "    \"payment_method\": \"Credit card\",\n",
    "    \"internet_service\": \"Fiber optic\",\n",
    "    \"online_security\": \"Yes\",\n",
    "    \"tech_support\": \"No\",\n",
    "    \"senior_citizen\": 0\n",
    "}\n",
    "\n",
    "print(\"\\nTest Request:\")\n",
    "print(json.dumps(test_customer, indent=2))\n",
    "\n",
    "# Simulate preprocessing and prediction\n",
    "df_test = pd.DataFrame([test_customer])\n",
    "df_test['senior_citizen'] = df_test['senior_citizen'].astype(str)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in label_encoders:\n",
    "        df_test[col] = label_encoders[col].transform(df_test[col])\n",
    "\n",
    "df_test[numerical_cols] = scaler.transform(df_test[numerical_cols])\n",
    "X_test_api = df_test[feature_cols].values\n",
    "\n",
    "prediction = int(best_model.predict(X_test_api)[0])\n",
    "probability = float(best_model.predict_proba(X_test_api)[0, 1])\n",
    "\n",
    "# Determine confidence\n",
    "if probability < 0.3 or probability > 0.7:\n",
    "    confidence = \"High\"\n",
    "elif probability < 0.4 or probability > 0.6:\n",
    "    confidence = \"Medium\"\n",
    "else:\n",
    "    confidence = \"Low\"\n",
    "\n",
    "# Risk factors\n",
    "risk_factors = []\n",
    "if test_customer['contract_type'] == \"Month-to-month\":\n",
    "    risk_factors.append(\"Month-to-month contract\")\n",
    "if test_customer['tenure'] < 12:\n",
    "    risk_factors.append(\"Short tenure\")\n",
    "if test_customer['tech_support'] == \"No\":\n",
    "    risk_factors.append(\"No tech support\")\n",
    "\n",
    "response = {\n",
    "    \"customer_id\": \"TEST-001\",\n",
    "    \"churn_prediction\": prediction,\n",
    "    \"churn_probability\": round(probability, 4),\n",
    "    \"confidence\": confidence,\n",
    "    \"risk_factors\": risk_factors\n",
    "}\n",
    "\n",
    "print(\"\\nSimulated Response:\")\n",
    "print(json.dumps(response, indent=2))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ API SIMULATION COMPLETE\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üê≥ –ß–∞—Å—Ç—å 5: Docker Containerization\n",
    "\n",
    "### –ü–æ—á–µ–º—É Docker –¥–ª—è ML?\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º—ã –±–µ–∑ Docker:**\n",
    "```\n",
    "Data Scientist: \"–†–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –º–æ–µ–π –º–∞—à–∏–Ω–µ!\" üñ•Ô∏è\n",
    "DevOps: \"–ù–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ production\" üò§\n",
    "Root cause: Python 3.8 vs 3.10, numpy 1.20 vs 1.24, etc.\n",
    "```\n",
    "\n",
    "**Docker —Ä–µ—à–∞–µ—Ç:**\n",
    "- ‚úÖ **Reproducibility:** –û–¥–∏–Ω–∞–∫–æ–≤–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ –≤–µ–∑–¥–µ\n",
    "- ‚úÖ **Isolation:** –ù–µ—Ç –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π\n",
    "- ‚úÖ **Portability:** –ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –Ω–∞ –ª—é–±–æ–º —Ö–æ—Å—Ç–µ —Å Docker\n",
    "- ‚úÖ **Scalability:** –õ–µ–≥–∫–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å (Kubernetes)\n",
    "- ‚úÖ **CI/CD:** Automated builds –∏ deployments\n",
    "\n",
    "### Docker Architecture –¥–ª—è ML:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ          Docker Container               ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  Application Layer                      ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ‚îÄ app.py (FastAPI)                   ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ‚îÄ models/ (serialized models)        ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ requirements.txt                   ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  Python Runtime                         ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ Python 3.9 + dependencies          ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  Base Image                             ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ python:3.9-slim                    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dockerfile\n",
    "dockerfile_content = '''# Churn Prediction API - Production Dockerfile\n",
    "# Multi-stage build for smaller image size\n",
    "\n",
    "# ========================================\n",
    "# Stage 1: Build dependencies\n",
    "# ========================================\n",
    "FROM python:3.9-slim as builder\n",
    "\n",
    "# Set environment variables\n",
    "ENV PYTHONDONTWRITEBYTECODE=1\n",
    "ENV PYTHONUNBUFFERED=1\n",
    "\n",
    "# Install build dependencies\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\\\n",
    "    build-essential \\\\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Create virtual environment\n",
    "RUN python -m venv /opt/venv\n",
    "ENV PATH=\"/opt/venv/bin:$PATH\"\n",
    "\n",
    "# Install Python dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir --upgrade pip && \\\\\n",
    "    pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# ========================================\n",
    "# Stage 2: Production image\n",
    "# ========================================\n",
    "FROM python:3.9-slim as production\n",
    "\n",
    "# Set environment variables\n",
    "ENV PYTHONDONTWRITEBYTECODE=1\n",
    "ENV PYTHONUNBUFFERED=1\n",
    "ENV PORT=8000\n",
    "\n",
    "# Create non-root user for security\n",
    "RUN groupadd -r mluser && useradd -r -g mluser mluser\n",
    "\n",
    "# Copy virtual environment from builder\n",
    "COPY --from=builder /opt/venv /opt/venv\n",
    "ENV PATH=\"/opt/venv/bin:$PATH\"\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy application code\n",
    "COPY app.py .\n",
    "COPY models/ ./models/\n",
    "\n",
    "# Change ownership to non-root user\n",
    "RUN chown -R mluser:mluser /app\n",
    "\n",
    "# Switch to non-root user\n",
    "USER mluser\n",
    "\n",
    "# Expose port\n",
    "EXPOSE $PORT\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \\\\\n",
    "    CMD curl -f http://localhost:$PORT/health || exit 1\n",
    "\n",
    "# Run application\n",
    "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "'''\n",
    "\n",
    "# Save Dockerfile\n",
    "with open('./Dockerfile', 'w') as f:\n",
    "    f.write(dockerfile_content)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DOCKERFILE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n‚úÖ Dockerfile saved\")\n",
    "print(\"\\nKey features:\")\n",
    "print(\"  - Multi-stage build (smaller image)\")\n",
    "print(\"  - Non-root user (security)\")\n",
    "print(\"  - Health check\")\n",
    "print(\"  - Optimized layer caching\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements.txt\n",
    "requirements_content = '''# Core ML\n",
    "numpy==1.24.3\n",
    "pandas==2.0.3\n",
    "scikit-learn==1.3.0\n",
    "joblib==1.3.1\n",
    "\n",
    "# API\n",
    "fastapi==0.100.0\n",
    "uvicorn[standard]==0.23.0\n",
    "pydantic==2.0.3\n",
    "\n",
    "# Monitoring\n",
    "prometheus-client==0.17.1\n",
    "\n",
    "# Utilities\n",
    "python-multipart==0.0.6\n",
    "python-json-logger==2.0.7\n",
    "'''\n",
    "\n",
    "with open('./requirements.txt', 'w') as f:\n",
    "    f.write(requirements_content)\n",
    "\n",
    "print(\"‚úÖ requirements.txt saved\")\n",
    "\n",
    "# docker-compose.yml\n",
    "docker_compose_content = '''version: '3.8'\n",
    "\n",
    "services:\n",
    "  churn-api:\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: Dockerfile\n",
    "    container_name: churn-prediction-api\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    environment:\n",
    "      - MODEL_VERSION=1.0.0\n",
    "      - LOG_LEVEL=INFO\n",
    "    volumes:\n",
    "      - ./models:/app/models:ro\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 3\n",
    "      start_period: 10s\n",
    "    restart: unless-stopped\n",
    "    \n",
    "  # Optional: Prometheus for monitoring\n",
    "  prometheus:\n",
    "    image: prom/prometheus:latest\n",
    "    container_name: prometheus\n",
    "    ports:\n",
    "      - \"9090:9090\"\n",
    "    volumes:\n",
    "      - ./prometheus.yml:/etc/prometheus/prometheus.yml\n",
    "    depends_on:\n",
    "      - churn-api\n",
    "      \n",
    "  # Optional: Grafana for dashboards\n",
    "  grafana:\n",
    "    image: grafana/grafana:latest\n",
    "    container_name: grafana\n",
    "    ports:\n",
    "      - \"3000:3000\"\n",
    "    environment:\n",
    "      - GF_SECURITY_ADMIN_PASSWORD=admin\n",
    "    depends_on:\n",
    "      - prometheus\n",
    "'''\n",
    "\n",
    "with open('./docker-compose.yml', 'w') as f:\n",
    "    f.write(docker_compose_content)\n",
    "\n",
    "print(\"‚úÖ docker-compose.yml saved\")\n",
    "\n",
    "# .dockerignore\n",
    "dockerignore_content = '''# Python\n",
    "__pycache__/\n",
    "*.py[cod]\n",
    "*$py.class\n",
    "*.so\n",
    ".Python\n",
    "venv/\n",
    "env/\n",
    ".venv/\n",
    "\n",
    "# Jupyter\n",
    ".ipynb_checkpoints/\n",
    "*.ipynb\n",
    "\n",
    "# Git\n",
    ".git/\n",
    ".gitignore\n",
    "\n",
    "# IDE\n",
    ".idea/\n",
    ".vscode/\n",
    "\n",
    "# Testing\n",
    "tests/\n",
    "*.test.py\n",
    "\n",
    "# Documentation\n",
    "docs/\n",
    "*.md\n",
    "README*\n",
    "\n",
    "# MLflow\n",
    "mlruns/\n",
    "mlartifacts/\n",
    "\n",
    "# Misc\n",
    ".DS_Store\n",
    "*.log\n",
    "'''\n",
    "\n",
    "with open('./.dockerignore', 'w') as f:\n",
    "    f.write(dockerignore_content)\n",
    "\n",
    "print(\"‚úÖ .dockerignore saved\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DOCKER COMMANDS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nBuild image:\")\n",
    "print(\"  docker build -t churn-api:latest .\")\n",
    "print(\"\\nRun container:\")\n",
    "print(\"  docker run -p 8000:8000 churn-api:latest\")\n",
    "print(\"\\nDocker Compose (with monitoring):\")\n",
    "print(\"  docker-compose up -d\")\n",
    "print(\"\\nView logs:\")\n",
    "print(\"  docker logs churn-prediction-api\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä –ß–∞—Å—Ç—å 6: Monitoring & Drift Detection\n",
    "\n",
    "### –ü–æ—á–µ–º—É Monitoring –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–µ–Ω?\n",
    "\n",
    "**ML —Å–∏—Å—Ç–µ–º—ã –¥–µ–≥—Ä–∞–¥–∏—Ä—É—é—Ç —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º:**\n",
    "\n",
    "1. **Data Drift:** –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –º–µ–Ω—è–µ—Ç—Å—è\n",
    "   - –ù–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –ø–æ–≤–µ–¥–µ–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
    "   - –°–µ–∑–æ–Ω–Ω–æ—Å—Ç—å\n",
    "   - –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ä—ã–Ω–∫–∞\n",
    "\n",
    "2. **Concept Drift:** –°–≤—è–∑—å –º–µ–∂–¥—É features –∏ target –º–µ–Ω—è–µ—Ç—Å—è\n",
    "   - –¢–æ, —á—Ç–æ —Ä–∞–Ω—å—à–µ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–ª–æ churn, –±–æ–ª—å—à–µ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç\n",
    "   - –ú–æ–¥–µ–ª—å —É—Å—Ç–∞—Ä–µ–ª–∞\n",
    "\n",
    "3. **Model Degradation:** Accuracy –ø–∞–¥–∞–µ—Ç —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º\n",
    "   - –î–∞–∂–µ –±–µ–∑ drift, –º–∏—Ä –º–µ–Ω—è–µ—Ç—Å—è\n",
    "   - –ù—É–∂–Ω–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ\n",
    "\n",
    "### –¢–∏–ø—ã –º–µ—Ç—Ä–∏–∫ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:\n",
    "\n",
    "#### 1. **Input Data Metrics**\n",
    "- Feature distributions (mean, std, min, max)\n",
    "- Missing values\n",
    "- Out-of-range values\n",
    "- Feature correlations\n",
    "\n",
    "#### 2. **Prediction Metrics**\n",
    "- Prediction distribution\n",
    "- Prediction confidence\n",
    "- Prediction latency\n",
    "\n",
    "#### 3. **Model Performance** (when labels available)\n",
    "- Accuracy, Precision, Recall, F1\n",
    "- ROC AUC\n",
    "- Confusion matrix\n",
    "\n",
    "#### 4. **Business Metrics**\n",
    "- False positive cost\n",
    "- Revenue impact\n",
    "- Customer satisfaction\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"DATA DRIFT DETECTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# –°–æ–∑–¥–∞–¥–∏–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ \"production\" –¥–∞–Ω–Ω—ã–µ —Å drift\n",
    "def create_drifted_data(n_samples: int = 1000, drift_level: float = 0.3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create data with artificial drift.\n",
    "    \n",
    "    Args:\n",
    "        n_samples: number of samples\n",
    "        drift_level: intensity of drift (0=no drift, 1=full drift)\n",
    "    \"\"\"\n",
    "    np.random.seed(123)  # Different seed for production data\n",
    "    \n",
    "    # Apply drift to numerical features\n",
    "    tenure = np.random.randint(1, 72, n_samples)\n",
    "    \n",
    "    # Drift: monthly charges increased (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∏–Ω—Ñ–ª—è—Ü–∏—è)\n",
    "    monthly_charges = np.random.uniform(\n",
    "        20 + 15 * drift_level,  # higher minimum\n",
    "        100 + 30 * drift_level,  # higher maximum\n",
    "        n_samples\n",
    "    )\n",
    "    \n",
    "    total_charges = tenure * monthly_charges + np.random.normal(0, 100, n_samples)\n",
    "    total_charges = np.maximum(total_charges, 0)\n",
    "    \n",
    "    # Drift: more support tickets (—É—Ö—É–¥—à–∏–ª–æ—Å—å –∫–∞—á–µ—Å—Ç–≤–æ —É—Å–ª—É–≥)\n",
    "    num_tickets = np.random.poisson(2 + 2 * drift_level, n_samples)\n",
    "    \n",
    "    # Drift in categorical: more month-to-month contracts\n",
    "    contract_probs = [\n",
    "        0.5 + 0.2 * drift_level,  # more month-to-month\n",
    "        0.3 - 0.1 * drift_level,  # less one year\n",
    "        0.2 - 0.1 * drift_level   # less two year\n",
    "    ]\n",
    "    contract_probs = np.clip(contract_probs, 0.01, 0.99)\n",
    "    contract_probs = contract_probs / sum(contract_probs)\n",
    "    \n",
    "    contract_type = np.random.choice(\n",
    "        ['Month-to-month', 'One year', 'Two year'],\n",
    "        n_samples, p=contract_probs\n",
    "    )\n",
    "    \n",
    "    # Rest same as training\n",
    "    payment_method = np.random.choice(\n",
    "        ['Credit card', 'Bank transfer', 'Electronic check'],\n",
    "        n_samples, p=[0.4, 0.3, 0.3]\n",
    "    )\n",
    "    internet_service = np.random.choice(\n",
    "        ['DSL', 'Fiber optic', 'No'],\n",
    "        n_samples, p=[0.4, 0.4, 0.2]\n",
    "    )\n",
    "    online_security = np.random.choice(['Yes', 'No'], n_samples, p=[0.4, 0.6])\n",
    "    tech_support = np.random.choice(['Yes', 'No'], n_samples, p=[0.3, 0.7])\n",
    "    senior_citizen = np.random.choice([0, 1], n_samples, p=[0.84, 0.16])\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'tenure': tenure,\n",
    "        'monthly_charges': monthly_charges,\n",
    "        'total_charges': total_charges,\n",
    "        'num_tickets': num_tickets,\n",
    "        'contract_type': contract_type,\n",
    "        'payment_method': payment_method,\n",
    "        'internet_service': internet_service,\n",
    "        'online_security': online_security,\n",
    "        'tech_support': tech_support,\n",
    "        'senior_citizen': senior_citizen,\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create production data with drift\n",
    "production_data = create_drifted_data(1000, drift_level=0.4)\n",
    "\n",
    "print(\"\\n‚úÖ Production data with drift created\")\n",
    "print(f\"Shape: {production_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def detect_numerical_drift(\n",
    "    reference: pd.Series,\n",
    "    current: pd.Series,\n",
    "    threshold: float = 0.05\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Detect drift in numerical feature using KS test.\n",
    "    \n",
    "    Kolmogorov-Smirnov test:\n",
    "    - H0: distributions are the same\n",
    "    - p-value < threshold ‚Üí drift detected\n",
    "    \"\"\"\n",
    "    statistic, p_value = stats.ks_2samp(reference, current)\n",
    "    \n",
    "    drift_detected = p_value < threshold\n",
    "    \n",
    "    return {\n",
    "        'ks_statistic': statistic,\n",
    "        'p_value': p_value,\n",
    "        'drift_detected': drift_detected,\n",
    "        'ref_mean': reference.mean(),\n",
    "        'ref_std': reference.std(),\n",
    "        'cur_mean': current.mean(),\n",
    "        'cur_std': current.std(),\n",
    "        'mean_shift': (current.mean() - reference.mean()) / reference.std()\n",
    "    }\n",
    "\n",
    "def detect_categorical_drift(\n",
    "    reference: pd.Series,\n",
    "    current: pd.Series,\n",
    "    threshold: float = 0.05\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Detect drift in categorical feature using Chi-squared test.\n",
    "    \"\"\"\n",
    "    # Get value counts\n",
    "    ref_counts = reference.value_counts(normalize=True)\n",
    "    cur_counts = current.value_counts(normalize=True)\n",
    "    \n",
    "    # Align categories\n",
    "    all_categories = set(ref_counts.index) | set(cur_counts.index)\n",
    "    ref_aligned = np.array([ref_counts.get(cat, 0) for cat in all_categories])\n",
    "    cur_aligned = np.array([cur_counts.get(cat, 0) for cat in all_categories])\n",
    "    \n",
    "    # Chi-squared test (need counts, not proportions)\n",
    "    ref_counts_abs = ref_aligned * len(reference)\n",
    "    cur_counts_abs = cur_aligned * len(current)\n",
    "    \n",
    "    # Add small value to avoid division by zero\n",
    "    ref_counts_abs = ref_counts_abs + 1e-10\n",
    "    \n",
    "    statistic, p_value = stats.chisquare(cur_counts_abs, ref_counts_abs)\n",
    "    \n",
    "    drift_detected = p_value < threshold\n",
    "    \n",
    "    return {\n",
    "        'chi2_statistic': statistic,\n",
    "        'p_value': p_value,\n",
    "        'drift_detected': drift_detected,\n",
    "        'ref_distribution': ref_counts.to_dict(),\n",
    "        'cur_distribution': cur_counts.to_dict()\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Drift detection functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run drift detection\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DRIFT DETECTION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Reference data (training)\n",
    "reference_data = df[df.columns.drop('churn')].copy()\n",
    "\n",
    "drift_report = {}\n",
    "\n",
    "# Numerical features\n",
    "print(\"\\nüìä Numerical Features:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for col in numerical_cols:\n",
    "    result = detect_numerical_drift(\n",
    "        reference_data[col],\n",
    "        production_data[col]\n",
    "    )\n",
    "    drift_report[col] = result\n",
    "    \n",
    "    status = \"‚ö†Ô∏è DRIFT\" if result['drift_detected'] else \"‚úÖ OK\"\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Status: {status}\")\n",
    "    print(f\"  KS Statistic: {result['ks_statistic']:.4f}\")\n",
    "    print(f\"  P-value: {result['p_value']:.4f}\")\n",
    "    print(f\"  Mean shift: {result['mean_shift']:.2f} std\")\n",
    "    print(f\"  Ref: mean={result['ref_mean']:.2f}, std={result['ref_std']:.2f}\")\n",
    "    print(f\"  Cur: mean={result['cur_mean']:.2f}, std={result['cur_std']:.2f}\")\n",
    "\n",
    "# Categorical features\n",
    "print(\"\\nüìä Categorical Features:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for col in ['contract_type', 'payment_method', 'internet_service']:\n",
    "    result = detect_categorical_drift(\n",
    "        reference_data[col],\n",
    "        production_data[col]\n",
    "    )\n",
    "    drift_report[col] = result\n",
    "    \n",
    "    status = \"‚ö†Ô∏è DRIFT\" if result['drift_detected'] else \"‚úÖ OK\"\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Status: {status}\")\n",
    "    print(f\"  Chi2 Statistic: {result['chi2_statistic']:.4f}\")\n",
    "    print(f\"  P-value: {result['p_value']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è drift\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. monthly_charges distribution\n",
    "axes[0, 0].hist(reference_data['monthly_charges'], bins=30, alpha=0.5, label='Reference', color='blue')\n",
    "axes[0, 0].hist(production_data['monthly_charges'], bins=30, alpha=0.5, label='Production', color='red')\n",
    "axes[0, 0].set_title('Monthly Charges Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Monthly Charges')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].legend()\n",
    "if drift_report['monthly_charges']['drift_detected']:\n",
    "    axes[0, 0].text(0.05, 0.95, '‚ö†Ô∏è DRIFT DETECTED', transform=axes[0, 0].transAxes,\n",
    "                   fontsize=10, color='red', fontweight='bold', verticalalignment='top')\n",
    "\n",
    "# 2. num_tickets distribution\n",
    "axes[0, 1].hist(reference_data['num_tickets'], bins=15, alpha=0.5, label='Reference', color='blue')\n",
    "axes[0, 1].hist(production_data['num_tickets'], bins=15, alpha=0.5, label='Production', color='red')\n",
    "axes[0, 1].set_title('Number of Tickets Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Number of Tickets')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].legend()\n",
    "if drift_report['num_tickets']['drift_detected']:\n",
    "    axes[0, 1].text(0.05, 0.95, '‚ö†Ô∏è DRIFT DETECTED', transform=axes[0, 1].transAxes,\n",
    "                   fontsize=10, color='red', fontweight='bold', verticalalignment='top')\n",
    "\n",
    "# 3. contract_type comparison\n",
    "contract_ref = reference_data['contract_type'].value_counts(normalize=True)\n",
    "contract_prod = production_data['contract_type'].value_counts(normalize=True)\n",
    "\n",
    "x = np.arange(len(contract_ref))\n",
    "width = 0.35\n",
    "axes[1, 0].bar(x - width/2, [contract_ref.get(c, 0) for c in contract_ref.index], width, label='Reference', alpha=0.8)\n",
    "axes[1, 0].bar(x + width/2, [contract_prod.get(c, 0) for c in contract_ref.index], width, label='Production', alpha=0.8)\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels(contract_ref.index, rotation=45)\n",
    "axes[1, 0].set_title('Contract Type Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Proportion')\n",
    "axes[1, 0].legend()\n",
    "if drift_report['contract_type']['drift_detected']:\n",
    "    axes[1, 0].text(0.05, 0.95, '‚ö†Ô∏è DRIFT DETECTED', transform=axes[1, 0].transAxes,\n",
    "                   fontsize=10, color='red', fontweight='bold', verticalalignment='top')\n",
    "\n",
    "# 4. Summary\n",
    "drift_summary = []\n",
    "for feature, result in drift_report.items():\n",
    "    if feature in numerical_cols:\n",
    "        drift_summary.append({\n",
    "            'Feature': feature,\n",
    "            'Type': 'Numerical',\n",
    "            'Drift': '‚ö†Ô∏è Yes' if result['drift_detected'] else '‚úÖ No',\n",
    "            'P-value': result['p_value']\n",
    "        })\n",
    "    else:\n",
    "        drift_summary.append({\n",
    "            'Feature': feature,\n",
    "            'Type': 'Categorical',\n",
    "            'Drift': '‚ö†Ô∏è Yes' if result['drift_detected'] else '‚úÖ No',\n",
    "            'P-value': result['p_value']\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(drift_summary)\n",
    "axes[1, 1].axis('off')\n",
    "table = axes[1, 1].table(\n",
    "    cellText=summary_df.values,\n",
    "    colLabels=summary_df.columns,\n",
    "    cellLoc='center',\n",
    "    loc='center',\n",
    "    colColours=['#f0f0f0'] * 4\n",
    ")\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.5)\n",
    "axes[1, 1].set_title('Drift Summary', fontsize=12, fontweight='bold', pad=20)\n",
    "\n",
    "plt.suptitle('Data Drift Detection: Reference vs Production', fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Count drifted features\n",
    "n_drifted = sum(1 for r in drift_report.values() if r['drift_detected'])\n",
    "print(f\"\\nüîç Summary: {n_drifted}/{len(drift_report)} features show significant drift\")\n",
    "if n_drifted > len(drift_report) // 2:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: Significant data drift detected!\")\n",
    "    print(\"   Consider retraining the model with recent data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Model Performance Monitoring\n",
    "\n",
    "–ö–æ–≥–¥–∞ –ø–æ—è–≤–ª—è—é—Ç—Å—è ground truth labels, –º–æ–Ω–∏—Ç–æ—Ä–∏–º performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL PERFORMANCE MONITORING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Simulate predictions on production data\n",
    "# Preprocess production data\n",
    "prod_processed = production_data.copy()\n",
    "prod_processed['senior_citizen'] = prod_processed['senior_citizen'].astype(str)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in label_encoders:\n",
    "        prod_processed[col] = label_encoders[col].transform(prod_processed[col])\n",
    "\n",
    "X_prod = prod_processed[feature_cols].copy()\n",
    "X_prod[numerical_cols] = scaler.transform(X_prod[numerical_cols])\n",
    "\n",
    "# Make predictions\n",
    "prod_predictions = best_model.predict(X_prod)\n",
    "prod_probabilities = best_model.predict_proba(X_prod)[:, 1]\n",
    "\n",
    "# Simulate ground truth (with concept drift - higher actual churn rate)\n",
    "# In reality, this comes from actual customer behavior\n",
    "actual_churn_prob = (\n",
    "    0.15 +  # higher baseline (was 0.1)\n",
    "    (production_data['tenure'] < 12).astype(float) * 0.2 +  # stronger effect\n",
    "    (production_data['monthly_charges'] > 70).astype(float) * 0.15 +\n",
    "    (production_data['contract_type'] == 'Month-to-month').astype(float) * 0.25\n",
    ")\n",
    "actual_churn_prob = np.clip(actual_churn_prob, 0, 0.95)\n",
    "prod_actual = (np.random.random(len(production_data)) < actual_churn_prob).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "prod_accuracy = accuracy_score(prod_actual, prod_predictions)\n",
    "prod_precision = precision_score(prod_actual, prod_predictions)\n",
    "prod_recall = recall_score(prod_actual, prod_predictions)\n",
    "prod_f1 = f1_score(prod_actual, prod_predictions)\n",
    "prod_auc = roc_auc_score(prod_actual, prod_probabilities)\n",
    "\n",
    "# Compare with training metrics\n",
    "print(\"\\nüìä Performance Comparison: Training vs Production\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "train_metrics = {\n",
    "    'Accuracy': results['GradientBoosting']['accuracy'],\n",
    "    'Precision': results['GradientBoosting']['precision'],\n",
    "    'Recall': results['GradientBoosting']['recall'],\n",
    "    'F1': results['GradientBoosting']['f1'],\n",
    "    'ROC AUC': results['GradientBoosting']['roc_auc']\n",
    "}\n",
    "\n",
    "prod_metrics = {\n",
    "    'Accuracy': prod_accuracy,\n",
    "    'Precision': prod_precision,\n",
    "    'Recall': prod_recall,\n",
    "    'F1': prod_f1,\n",
    "    'ROC AUC': prod_auc\n",
    "}\n",
    "\n",
    "comparison_data = []\n",
    "for metric in train_metrics.keys():\n",
    "    train_val = train_metrics[metric]\n",
    "    prod_val = prod_metrics[metric]\n",
    "    diff = prod_val - train_val\n",
    "    pct_change = (diff / train_val) * 100\n",
    "    \n",
    "    status = \"‚ö†Ô∏è\" if abs(pct_change) > 5 else \"‚úÖ\"\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Metric': metric,\n",
    "        'Training': f\"{train_val:.4f}\",\n",
    "        'Production': f\"{prod_val:.4f}\",\n",
    "        'Change': f\"{pct_change:+.1f}%\",\n",
    "        'Status': status\n",
    "    })\n",
    "    \n",
    "    print(f\"{metric}:\")\n",
    "    print(f\"  Training:   {train_val:.4f}\")\n",
    "    print(f\"  Production: {prod_val:.4f}\")\n",
    "    print(f\"  Change:     {pct_change:+.1f}% {status}\")\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "\n",
    "# Alert if significant degradation\n",
    "if any(abs(float(row['Change'].replace('%', ''))) > 10 for row in comparison_data):\n",
    "    print(\"\\n\" + \"‚ö†Ô∏è\" * 20)\n",
    "    print(\"\\nüö® ALERT: Significant model performance degradation detected!\")\n",
    "    print(\"   Actions:\")\n",
    "    print(\"   1. Investigate data drift\")\n",
    "    print(\"   2. Check for data quality issues\")\n",
    "    print(\"   3. Consider model retraining\")\n",
    "    print(\"   4. Review feature engineering\")\n",
    "    print(\"\\n\" + \"‚ö†Ô∏è\" * 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ –ß–∞—Å—Ç—å 7: CI/CD –¥–ª—è ML\n",
    "\n",
    "### –ü–æ—á–µ–º—É CI/CD –¥–ª—è ML –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç –æ–±—ã—á–Ω–æ–≥–æ?\n",
    "\n",
    "**–û–±—ã—á–Ω—ã–π CI/CD:**\n",
    "```\n",
    "Code ‚Üí Build ‚Üí Test ‚Üí Deploy\n",
    "```\n",
    "\n",
    "**ML CI/CD:**\n",
    "```\n",
    "Data ‚Üí Feature Engineering ‚Üí Train ‚Üí Validate ‚Üí Test ‚Üí Package ‚Üí Deploy ‚Üí Monitor\n",
    "```\n",
    "\n",
    "**–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ challenges:**\n",
    "- ‚úÖ **Data validation:** –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "- ‚úÖ **Model validation:** –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏\n",
    "- ‚úÖ **Feature validation:** –ü—Ä–æ–≤–µ—Ä–∫–∞ feature distributions\n",
    "- ‚úÖ **Performance regression:** –ú–æ–¥–µ–ª—å –Ω–µ –¥–æ–ª–∂–Ω–∞ –¥–µ–≥—Ä–∞–¥–∏—Ä–æ–≤–∞—Ç—å\n",
    "- ‚úÖ **A/B testing:** –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å production –º–æ–¥–µ–ª—å—é\n",
    "\n",
    "### GitHub Actions –¥–ª—è ML\n",
    "\n",
    "**Workflow stages:**\n",
    "\n",
    "1. **On Push/PR:**\n",
    "   - Lint code (flake8, black)\n",
    "   - Run unit tests\n",
    "   - Run integration tests\n",
    "   - Validate data schema\n",
    "\n",
    "2. **On Model Training:**\n",
    "   - Train model\n",
    "   - Validate model metrics\n",
    "   - Compare with baseline\n",
    "   - Register model (if better)\n",
    "\n",
    "3. **On Deploy:**\n",
    "   - Build Docker image\n",
    "   - Push to registry\n",
    "   - Deploy to staging\n",
    "   - Run smoke tests\n",
    "   - Deploy to production\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHub Actions Workflow\n",
    "github_actions_workflow = '''name: ML Pipeline CI/CD\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [main, develop]\n",
    "  pull_request:\n",
    "    branches: [main]\n",
    "  workflow_dispatch:  # Manual trigger\n",
    "\n",
    "env:\n",
    "  PYTHON_VERSION: \"3.9\"\n",
    "  MODEL_NAME: \"churn-predictor\"\n",
    "\n",
    "jobs:\n",
    "  # ========================================\n",
    "  # Stage 1: Code Quality\n",
    "  # ========================================\n",
    "  lint-and-test:\n",
    "    name: Lint and Test\n",
    "    runs-on: ubuntu-latest\n",
    "    \n",
    "    steps:\n",
    "      - name: Checkout code\n",
    "        uses: actions/checkout@v3\n",
    "        \n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: ${{ env.PYTHON_VERSION }}\n",
    "          \n",
    "      - name: Cache pip dependencies\n",
    "        uses: actions/cache@v3\n",
    "        with:\n",
    "          path: ~/.cache/pip\n",
    "          key: ${{ runner.os }}-pip-${{ hashFiles(\"requirements.txt\") }}\n",
    "          \n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          pip install --upgrade pip\n",
    "          pip install -r requirements.txt\n",
    "          pip install pytest pytest-cov flake8 black\n",
    "          \n",
    "      - name: Lint with flake8\n",
    "        run: flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n",
    "        \n",
    "      - name: Check formatting with black\n",
    "        run: black --check .\n",
    "        \n",
    "      - name: Run unit tests\n",
    "        run: pytest tests/unit/ -v --cov=src --cov-report=xml\n",
    "        \n",
    "      - name: Upload coverage\n",
    "        uses: codecov/codecov-action@v3\n",
    "        with:\n",
    "          file: ./coverage.xml\n",
    "\n",
    "  # ========================================\n",
    "  # Stage 2: Data Validation\n",
    "  # ========================================\n",
    "  data-validation:\n",
    "    name: Validate Data\n",
    "    runs-on: ubuntu-latest\n",
    "    needs: lint-and-test\n",
    "    \n",
    "    steps:\n",
    "      - name: Checkout code\n",
    "        uses: actions/checkout@v3\n",
    "        \n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: ${{ env.PYTHON_VERSION }}\n",
    "          \n",
    "      - name: Install dependencies\n",
    "        run: pip install -r requirements.txt\n",
    "        \n",
    "      - name: Validate data schema\n",
    "        run: python scripts/validate_data.py\n",
    "        \n",
    "      - name: Check for data drift\n",
    "        run: python scripts/check_drift.py\n",
    "\n",
    "  # ========================================\n",
    "  # Stage 3: Model Training\n",
    "  # ========================================\n",
    "  train-model:\n",
    "    name: Train Model\n",
    "    runs-on: ubuntu-latest\n",
    "    needs: data-validation\n",
    "    if: github.event_name == \"push\" && github.ref == \"refs/heads/main\"\n",
    "    \n",
    "    steps:\n",
    "      - name: Checkout code\n",
    "        uses: actions/checkout@v3\n",
    "        \n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: ${{ env.PYTHON_VERSION }}\n",
    "          \n",
    "      - name: Install dependencies\n",
    "        run: pip install -r requirements.txt\n",
    "        \n",
    "      - name: Train model\n",
    "        run: python scripts/train.py\n",
    "        env:\n",
    "          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}\n",
    "          \n",
    "      - name: Validate model performance\n",
    "        run: python scripts/validate_model.py\n",
    "        \n",
    "      - name: Upload model artifact\n",
    "        uses: actions/upload-artifact@v3\n",
    "        with:\n",
    "          name: model\n",
    "          path: models/\n",
    "\n",
    "  # ========================================\n",
    "  # Stage 4: Build and Push Docker Image\n",
    "  # ========================================\n",
    "  build-docker:\n",
    "    name: Build Docker Image\n",
    "    runs-on: ubuntu-latest\n",
    "    needs: train-model\n",
    "    \n",
    "    steps:\n",
    "      - name: Checkout code\n",
    "        uses: actions/checkout@v3\n",
    "        \n",
    "      - name: Download model artifact\n",
    "        uses: actions/download-artifact@v3\n",
    "        with:\n",
    "          name: model\n",
    "          path: models/\n",
    "          \n",
    "      - name: Set up Docker Buildx\n",
    "        uses: docker/setup-buildx-action@v2\n",
    "        \n",
    "      - name: Login to Docker Hub\n",
    "        uses: docker/login-action@v2\n",
    "        with:\n",
    "          username: ${{ secrets.DOCKER_USERNAME }}\n",
    "          password: ${{ secrets.DOCKER_PASSWORD }}\n",
    "          \n",
    "      - name: Build and push\n",
    "        uses: docker/build-push-action@v4\n",
    "        with:\n",
    "          context: .\n",
    "          push: true\n",
    "          tags: |\n",
    "            ${{ secrets.DOCKER_USERNAME }}/${{ env.MODEL_NAME }}:latest\n",
    "            ${{ secrets.DOCKER_USERNAME }}/${{ env.MODEL_NAME }}:${{ github.sha }}\n",
    "          cache-from: type=gha\n",
    "          cache-to: type=gha,mode=max\n",
    "\n",
    "  # ========================================\n",
    "  # Stage 5: Deploy to Staging\n",
    "  # ========================================\n",
    "  deploy-staging:\n",
    "    name: Deploy to Staging\n",
    "    runs-on: ubuntu-latest\n",
    "    needs: build-docker\n",
    "    environment: staging\n",
    "    \n",
    "    steps:\n",
    "      - name: Deploy to staging\n",
    "        run: |\n",
    "          echo \"Deploying to staging environment...\"\n",
    "          # kubectl set image deployment/$MODEL_NAME ...\n",
    "          # or: aws ecs update-service ...\n",
    "          \n",
    "      - name: Run smoke tests\n",
    "        run: |\n",
    "          echo \"Running smoke tests...\"\n",
    "          # curl -X POST https://staging.api.com/predict ...\n",
    "          \n",
    "      - name: Notify on success\n",
    "        run: echo \"Staging deployment successful!\"\n",
    "\n",
    "  # ========================================\n",
    "  # Stage 6: Deploy to Production\n",
    "  # ========================================\n",
    "  deploy-production:\n",
    "    name: Deploy to Production\n",
    "    runs-on: ubuntu-latest\n",
    "    needs: deploy-staging\n",
    "    environment: production\n",
    "    \n",
    "    steps:\n",
    "      - name: Deploy to production\n",
    "        run: |\n",
    "          echo \"Deploying to production environment...\"\n",
    "          \n",
    "      - name: Notify team\n",
    "        run: echo \"Production deployment complete!\"\n",
    "'''\n",
    "\n",
    "# Save workflow\n",
    "import os\n",
    "os.makedirs('.github/workflows', exist_ok=True)\n",
    "with open('.github/workflows/ml-pipeline.yml', 'w') as f:\n",
    "    f.write(github_actions_workflow)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"GITHUB ACTIONS CI/CD WORKFLOW\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n‚úÖ Workflow saved to: .github/workflows/ml-pipeline.yml\")\n",
    "print(\"\\nStages:\")\n",
    "print(\"  1. lint-and-test      - Code quality checks\")\n",
    "print(\"  2. data-validation    - Data schema and drift\")\n",
    "print(\"  3. train-model        - Model training with MLflow\")\n",
    "print(\"  4. build-docker       - Build and push Docker image\")\n",
    "print(\"  5. deploy-staging     - Deploy to staging + smoke tests\")\n",
    "print(\"  6. deploy-production  - Deploy to production\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìÅ –ß–∞—Å—Ç—å 8: Data Version Control (DVC)\n",
    "\n",
    "### –ü—Ä–æ–±–ª–µ–º–∞ –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "**Git –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –¥–∞–Ω–Ω—ã—Ö:**\n",
    "- ‚ùå Large files (GB, TB)\n",
    "- ‚ùå Binary files (–Ω–µ –≤–∏–¥–Ω–æ diff)\n",
    "- ‚ùå Repository bloat\n",
    "\n",
    "**DVC —Ä–µ—à–∞–µ—Ç:**\n",
    "- ‚úÖ Git-like workflow –¥–ª—è –¥–∞–Ω–Ω—ã—Ö\n",
    "- ‚úÖ Remote storage (S3, GCS, Azure Blob)\n",
    "- ‚úÖ Reproducibility\n",
    "- ‚úÖ Pipeline management\n",
    "\n",
    "### –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç DVC:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   Git       ‚îÇ     ‚îÇ   DVC       ‚îÇ\n",
    "‚îÇ   (code)    ‚îÇ     ‚îÇ   (data)    ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§     ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ - app.py    ‚îÇ     ‚îÇ - data.csv  ‚îÇ\n",
    "‚îÇ - train.py  ‚îÇ     ‚îÇ - model.pkl ‚îÇ\n",
    "‚îÇ - data.dvc  ‚îÇ ‚Üê‚îÄ‚Üí ‚îÇ             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ                   ‚îÇ\n",
    "       ‚Üì                   ‚Üì\n",
    "   GitHub/GitLab      S3/GCS/Azure\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DVC (DATA VERSION CONTROL)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# DVC commands reference\n",
    "dvc_commands = '''\n",
    "# ========================================\n",
    "# DVC Quick Reference\n",
    "# ========================================\n",
    "\n",
    "# Initialize DVC in project\n",
    "dvc init\n",
    "\n",
    "# Add remote storage (e.g., S3)\n",
    "dvc remote add -d myremote s3://mybucket/dvc-store\n",
    "\n",
    "# Track data files\n",
    "dvc add data/raw/customers.csv\n",
    "dvc add models/churn_predictor.joblib\n",
    "\n",
    "# This creates:\n",
    "# - data/raw/customers.csv.dvc (metadata, tracked by Git)\n",
    "# - .gitignore (–¥–æ–±–∞–≤–ª—è–µ—Ç customers.csv)\n",
    "\n",
    "# Commit metadata to Git\n",
    "git add data/raw/customers.csv.dvc data/raw/.gitignore\n",
    "git commit -m \"Add raw customer data\"\n",
    "\n",
    "# Push data to remote storage\n",
    "dvc push\n",
    "\n",
    "# Pull data from remote\n",
    "dvc pull\n",
    "\n",
    "# ========================================\n",
    "# DVC Pipelines\n",
    "# ========================================\n",
    "\n",
    "# Define pipeline in dvc.yaml:\n",
    "# stages:\n",
    "#   preprocess:\n",
    "#     cmd: python src/preprocess.py\n",
    "#     deps:\n",
    "#       - data/raw/customers.csv\n",
    "#       - src/preprocess.py\n",
    "#     outs:\n",
    "#       - data/processed/customers_clean.csv\n",
    "#\n",
    "#   train:\n",
    "#     cmd: python src/train.py\n",
    "#     deps:\n",
    "#       - data/processed/customers_clean.csv\n",
    "#       - src/train.py\n",
    "#     outs:\n",
    "#       - models/churn_predictor.joblib\n",
    "#     metrics:\n",
    "#       - metrics/scores.json\n",
    "\n",
    "# Run pipeline\n",
    "dvc repro\n",
    "\n",
    "# Show pipeline DAG\n",
    "dvc dag\n",
    "\n",
    "# Compare metrics across experiments\n",
    "dvc metrics show\n",
    "dvc metrics diff\n",
    "\n",
    "# ========================================\n",
    "# Version switching\n",
    "# ========================================\n",
    "\n",
    "# Go back to specific version\n",
    "git checkout v1.0\n",
    "dvc checkout\n",
    "\n",
    "# Compare data versions\n",
    "dvc diff HEAD~1\n",
    "'''\n",
    "\n",
    "print(dvc_commands)\n",
    "\n",
    "# Create dvc.yaml example\n",
    "dvc_yaml = '''stages:\n",
    "  preprocess:\n",
    "    cmd: python src/preprocess.py\n",
    "    deps:\n",
    "      - data/raw/customers.csv\n",
    "      - src/preprocess.py\n",
    "    params:\n",
    "      - preprocess.test_size\n",
    "      - preprocess.random_state\n",
    "    outs:\n",
    "      - data/processed/X_train.csv\n",
    "      - data/processed/X_test.csv\n",
    "      - data/processed/y_train.csv\n",
    "      - data/processed/y_test.csv\n",
    "\n",
    "  train:\n",
    "    cmd: python src/train.py\n",
    "    deps:\n",
    "      - data/processed/X_train.csv\n",
    "      - data/processed/y_train.csv\n",
    "      - src/train.py\n",
    "    params:\n",
    "      - train.n_estimators\n",
    "      - train.max_depth\n",
    "      - train.learning_rate\n",
    "    outs:\n",
    "      - models/churn_predictor.joblib\n",
    "      - models/scaler.joblib\n",
    "    metrics:\n",
    "      - metrics/train_metrics.json:\n",
    "          cache: false\n",
    "\n",
    "  evaluate:\n",
    "    cmd: python src/evaluate.py\n",
    "    deps:\n",
    "      - data/processed/X_test.csv\n",
    "      - data/processed/y_test.csv\n",
    "      - models/churn_predictor.joblib\n",
    "      - src/evaluate.py\n",
    "    metrics:\n",
    "      - metrics/eval_metrics.json:\n",
    "          cache: false\n",
    "    plots:\n",
    "      - plots/confusion_matrix.png\n",
    "      - plots/roc_curve.png\n",
    "'''\n",
    "\n",
    "with open('./dvc.yaml', 'w') as f:\n",
    "    f.write(dvc_yaml)\n",
    "\n",
    "print(\"\\n‚úÖ dvc.yaml saved\")\n",
    "print(\"\\nDVC Pipeline Stages:\")\n",
    "print(\"  1. preprocess ‚Üí Train/test split, scaling\")\n",
    "print(\"  2. train      ‚Üí Model training\")\n",
    "print(\"  3. evaluate   ‚Üí Model evaluation, plots\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã –ß–∞—Å—Ç—å 9: Summary & Best Practices\n",
    "\n",
    "### Production ML Checklist\n",
    "\n",
    "#### ‚úÖ **Experiment Tracking**\n",
    "- [ ] MLflow/W&B setup –¥–ª—è logging\n",
    "- [ ] Parameters, metrics, artifacts logged\n",
    "- [ ] Model Registry configured\n",
    "- [ ] Experiment comparison UI\n",
    "\n",
    "#### ‚úÖ **Model Serialization**\n",
    "- [ ] Model saved (joblib/pickle/ONNX)\n",
    "- [ ] Preprocessing artifacts saved (scaler, encoders)\n",
    "- [ ] Config file with feature columns\n",
    "- [ ] Version metadata\n",
    "\n",
    "#### ‚úÖ **API Development**\n",
    "- [ ] FastAPI/Flask application\n",
    "- [ ] Pydantic schemas for validation\n",
    "- [ ] Health check endpoint\n",
    "- [ ] Error handling\n",
    "- [ ] API documentation (Swagger)\n",
    "- [ ] Rate limiting\n",
    "- [ ] Authentication\n",
    "\n",
    "#### ‚úÖ **Containerization**\n",
    "- [ ] Dockerfile (multi-stage)\n",
    "- [ ] Non-root user\n",
    "- [ ] Health check\n",
    "- [ ] docker-compose.yml\n",
    "- [ ] .dockerignore\n",
    "\n",
    "#### ‚úÖ **Monitoring**\n",
    "- [ ] Data drift detection\n",
    "- [ ] Model performance monitoring\n",
    "- [ ] Prediction distribution tracking\n",
    "- [ ] Latency monitoring\n",
    "- [ ] Alerting setup\n",
    "- [ ] Dashboard (Grafana)\n",
    "\n",
    "#### ‚úÖ **CI/CD**\n",
    "- [ ] GitHub Actions workflow\n",
    "- [ ] Unit tests\n",
    "- [ ] Integration tests\n",
    "- [ ] Data validation\n",
    "- [ ] Model validation\n",
    "- [ ] Automated deployment\n",
    "\n",
    "#### ‚úÖ **Data Versioning**\n",
    "- [ ] DVC initialized\n",
    "- [ ] Remote storage configured\n",
    "- [ ] Data tracked\n",
    "- [ ] Pipeline defined\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Key Takeaways\n",
    "\n",
    "#### 1. **Model –≤ Jupyter - —ç—Ç–æ 10% —Ä–∞–±–æ—Ç—ã**\n",
    "- –û—Å—Ç–∞–ª—å–Ω—ã–µ 90% - —ç—Ç–æ MLOps\n",
    "- Deployment, monitoring, retraining\n",
    "- –≠—Ç–æ –æ—Ç–ª–∏—á–∞–µ—Ç Data Scientist –æ—Ç ML Engineer\n",
    "\n",
    "#### 2. **Reproducibility is King**\n",
    "- MLflow –¥–ª—è experiments\n",
    "- DVC –¥–ª—è –¥–∞–Ω–Ω—ã—Ö\n",
    "- Docker –¥–ª—è environment\n",
    "- –í—Å—ë –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ!\n",
    "\n",
    "#### 3. **Monitoring –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ—ã**\n",
    "- ML –º–æ–¥–µ–ª–∏ –¥–µ–≥—Ä–∞–¥–∏—Ä—É—é—Ç —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º\n",
    "- Data drift, concept drift, model drift\n",
    "- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ ‚Üí —Ä–∞–Ω–Ω–µ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ ‚Üí –±—ã—Å—Ç—Ä–∞—è —Ä–µ–∞–∫—Ü–∏—è\n",
    "\n",
    "#### 4. **Automation —Å–Ω–∏–∂–∞–µ—Ç —Ä–∏—Å–∫–∏**\n",
    "- Manual deployments ‚Üí –æ—à–∏–±–∫–∏\n",
    "- CI/CD ‚Üí consistent, reliable deployments\n",
    "- Automated testing ‚Üí catches bugs early\n",
    "\n",
    "#### 5. **Start Simple, Iterate**\n",
    "- –ù–µ –Ω—É–∂–µ–Ω Kubernetes —Å –ø–µ—Ä–≤–æ–≥–æ –¥–Ω—è\n",
    "- –ù–∞—á–Ω–∏—Ç–µ —Å Docker + –ø—Ä–æ—Å—Ç–æ–π CI/CD\n",
    "- –î–æ–±–∞–≤–ª—è–π—Ç–µ complexity –ø–æ –º–µ—Ä–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏\n",
    "\n",
    "---\n",
    "\n",
    "### üîó –°–≤—è–∑—å —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ —Ñ–∞–∑–∞–º–∏\n",
    "\n",
    "**Phase 7 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Å–µ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –∑–Ω–∞–Ω–∏—è:**\n",
    "\n",
    "- **Phase 1-3:** –ú–æ–¥–µ–ª–∏ –¥–ª—è deployment (RF, XGBoost, etc.)\n",
    "- **Phase 4:** Transformers –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á\n",
    "- **Phase 5:** Anomaly detection –¥–ª—è monitoring\n",
    "- **Phase 6:** SHAP –¥–ª—è explainability –≤ production\n",
    "\n",
    "**Phase 7 –∑–∞–≤–µ—Ä—à–∞–µ—Ç —Ü–∏–∫–ª:**\n",
    "\n",
    "```\n",
    "Research ‚Üí Development ‚Üí Production ‚Üí Monitoring ‚Üí Research...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Tools Reference\n",
    "\n",
    "| Category | Tool | Purpose |\n",
    "|----------|------|---------||\n",
    "| **Experiment Tracking** | MLflow | Parameters, metrics, models |\n",
    "| | Weights & Biases | Similar + collaboration |\n",
    "| **Model Serving** | FastAPI | REST API |\n",
    "| | BentoML | ML-focused serving |\n",
    "| | Seldon/KServe | Kubernetes-native |\n",
    "| **Containerization** | Docker | Environment isolation |\n",
    "| | Kubernetes | Orchestration |\n",
    "| **Data Versioning** | DVC | Git for data |\n",
    "| | Delta Lake | ACID transactions |\n",
    "| **Monitoring** | Evidently | ML monitoring |\n",
    "| | Prometheus/Grafana | Metrics + dashboards |\n",
    "| **CI/CD** | GitHub Actions | Automation |\n",
    "| | GitLab CI | Alternative |\n",
    "| **Feature Store** | Feast | Feature management |\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Phase 7 Complete!\n",
    "\n",
    "**–í—ã —Ç–µ–ø–µ—Ä—å –∑–Ω–∞–µ—Ç–µ:**\n",
    "\n",
    "- ‚úÖ MLflow experiment tracking –∏ Model Registry\n",
    "- ‚úÖ Model serialization (pickle, joblib)\n",
    "- ‚úÖ FastAPI –¥–ª—è ML API —Å Pydantic validation\n",
    "- ‚úÖ Docker containerization (multi-stage builds)\n",
    "- ‚úÖ Data drift detection (KS test, Chi-squared)\n",
    "- ‚úÖ Model performance monitoring\n",
    "- ‚úÖ CI/CD —Å GitHub Actions\n",
    "- ‚úÖ DVC –¥–ª—è data versioning\n",
    "\n",
    "---\n",
    "\n",
    "## üèÜ ML Roadmap Complete!\n",
    "\n",
    "**–í–µ—Å—å –ø—É—Ç—å –æ—Ç –Ω–∞—á–∞–ª–∞ –¥–æ production:**\n",
    "\n",
    "1. ‚úÖ **Phase 1:** Classification Basics (Titanic)\n",
    "2. ‚úÖ **Phase 2:** Deep Learning (MNIST, PyTorch)\n",
    "3. ‚úÖ **Phase 3:** Advanced Ensembles (Stacking, Blending)\n",
    "4. ‚úÖ **Phase 4:** Transformers (Self-Attention, TFT)\n",
    "5. ‚úÖ **Phase 5:** Anomaly Detection (Isolation Forest, Autoencoders)\n",
    "6. ‚úÖ **Phase 6:** Explainable AI (SHAP, LIME, Fairness)\n",
    "7. ‚úÖ **Phase 7:** Production MLOps (FastAPI, Docker, CI/CD)\n",
    "\n",
    "**–¢–µ–ø–µ—Ä—å –≤—ã –≥–æ—Ç–æ–≤—ã:**\n",
    "\n",
    "- üéØ –†–µ—à–∞—Ç—å real-world ML –∑–∞–¥–∞—á–∏\n",
    "- üéØ –í—ã–±–∏—Ä–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ –º–µ—Ç–æ–¥—ã\n",
    "- üéØ –î–µ–ø–ª–æ–∏—Ç—å –º–æ–¥–µ–ª–∏ –≤ production\n",
    "- üéØ –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å ML —Å–∏—Å—Ç–µ–º—ã\n",
    "- üéØ –û–±—ä—è—Å–Ω—è—Ç—å predictions –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞—Ç—å fairness\n",
    "\n",
    "---\n",
    "\n",
    "**üöÄ You're now a Production-Ready ML Practitioner!**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
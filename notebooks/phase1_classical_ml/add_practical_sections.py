#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤—Å–µ—Ö –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö —Å–µ–∫—Ü–∏–π –≤ XGBoost notebook
"""

import json
import sys

# –ü—É—Ç—å –∫ –Ω–æ—É—Ç–±—É–∫—É
notebook_path = '01_xgboost_deep_dive.ipynb'

# –ß–∏—Ç–∞–µ–º —Ç–µ–∫—É—â–∏–π –Ω–æ—É—Ç–±—É–∫
with open(notebook_path, 'r', encoding='utf-8') as f:
    notebook = json.load(f)

# –í—Å–µ –Ω–æ–≤—ã–µ —è—á–µ–π–∫–∏ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
new_cells = []

# ===========================
# SECTION 2.3: –ü–µ—Ä–≤–∏—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
# ===========================

new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ü–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏\n",
        "print('=== –ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫ ===' + '\\n')\n",
        "display(df.head())\n",
        "\n",
        "print('\\n' + '=== –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞–Ω–Ω—ã—Ö ===' + '\\n')\n",
        "df.info()\n",
        "\n",
        "print('\\n' + '=== –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ ===' + '\\n')\n",
        "display(df.describe())"
    ]
})

new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–æ–ø—É—Å–∫–∏\n",
        "print('=== –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è ===')\n",
        "missing = df.isnull().sum()\n",
        "missing_pct = 100 * missing / len(df)\n",
        "missing_table = pd.DataFrame({\n",
        "    '–ü—Ä–æ–ø—É—Å–∫–∏': missing,\n",
        "    '–ü—Ä–æ—Ü–µ–Ω—Ç': missing_pct\n",
        "})\n",
        "missing_table = missing_table[missing_table['–ü—Ä–æ–ø—É—Å–∫–∏'] > 0].sort_values('–ü—Ä–æ–ø—É—Å–∫–∏', ascending=False)\n",
        "\n",
        "if len(missing_table) == 0:\n",
        "    print('‚úÖ –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –Ω–µ—Ç!')\n",
        "else:\n",
        "    display(missing_table)"
    ]
})

new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤\n",
        "duplicates = df.duplicated().sum()\n",
        "print(f'–î—É–±–ª–∏–∫–∞—Ç—ã: {duplicates} —Å—Ç—Ä–æ–∫ ({100*duplicates/len(df):.2f}%)')\n",
        "\n",
        "if duplicates > 0:\n",
        "    print('\\n–ü—Ä–∏–º–µ—Ä –¥—É–±–ª–∏–∫–∞—Ç–æ–≤:')\n",
        "    display(df[df.duplicated(keep=False)].head(10))\n",
        "else:\n",
        "    print('‚úÖ –î—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–µ—Ç!')"
    ]
})

# ===========================
# SECTION 2.4: Exploratory Data Analysis
# ===========================

new_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 2.3 Exploratory Data Analysis (EDA)\n",
        "\n",
        "#### 2.3.1 –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π"
    ]
})

new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è\n",
        "target_col = 'default'\n",
        "\n",
        "# –ï—Å–ª–∏ —Å—Ç–æ–ª–±–µ—Ü –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è –∏–Ω–∞—á–µ, –ø–µ—Ä–µ–∏–º–µ–Ω—É–µ–º\n",
        "if 'default payment next month' in df.columns:\n",
        "    df = df.rename(columns={'default payment next month': 'default'})\n",
        "elif 'default.payment.next.month' in df.columns:\n",
        "    df = df.rename(columns={'default.payment.next.month': 'default'})\n",
        "\n",
        "# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Count plot\n",
        "df[target_col].value_counts().plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c'])\n",
        "axes[0].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Default (0=No, 1=Yes)')\n",
        "axes[0].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')\n",
        "axes[0].set_xticklabels(['No Default (0)', 'Default (1)'], rotation=0)\n",
        "\n",
        "for i, v in enumerate(df[target_col].value_counts()):\n",
        "    axes[0].text(i, v + 500, f'{v:,}\\n({100*v/len(df):.1f}%)', \n",
        "                 ha='center', fontweight='bold')\n",
        "\n",
        "# Pie chart\n",
        "colors = ['#2ecc71', '#e74c3c']\n",
        "df[target_col].value_counts().plot(kind='pie', ax=axes[1], autopct='%1.1f%%',\n",
        "                                    colors=colors, startangle=90,\n",
        "                                    labels=['No Default', 'Default'])\n",
        "axes[1].set_title('–î–æ–ª—è –∫–ª–∞—Å—Å–æ–≤', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
        "default_rate = df[target_col].mean()\n",
        "print(f'\\nüìä Default rate: {default_rate:.2%}')\n",
        "print(f'   No default: {(1-default_rate):.2%}')\n",
        "print(f'   Class imbalance ratio: 1:{(1-default_rate)/default_rate:.2f}')\n",
        "\n",
        "if default_rate < 0.3:\n",
        "    print('\\n‚ö†Ô∏è  –î–∞–Ω–Ω—ã–µ –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω—ã! –£—á—Ç–µ–º —ç—Ç–æ –ø—Ä–∏ –≤—ã–±–æ—Ä–µ –º–µ—Ç—Ä–∏–∫ –∏ –ø–æ–¥—Ö–æ–¥–æ–≤.')"
    ]
})

new_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "#### 2.3.2 –ê–Ω–∞–ª–∏–∑ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"
    ]
})

new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ò–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä—É–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "numeric_features.remove(target_col)  # –£–±–∏—Ä–∞–µ–º —Ç–∞—Ä–≥–µ—Ç\n",
        "\n",
        "categorical_features = [col for col in df.columns \n",
        "                        if col not in numeric_features and col != target_col]\n",
        "\n",
        "print(f'–ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ({len(numeric_features)}): {numeric_features[:5]}...')\n",
        "print(f'–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ({len(categorical_features)}): {categorical_features}')"
    ]
})

new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "key_numeric = ['LIMIT_BAL', 'AGE', 'BILL_AMT1', 'PAY_AMT1'] \\\n",
        "              if all(col in df.columns for col in ['LIMIT_BAL', 'AGE', 'BILL_AMT1', 'PAY_AMT1']) \\\n",
        "              else numeric_features[:4]\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, col in enumerate(key_numeric):\n",
        "    if col in df.columns:\n",
        "        # Histogram –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞\n",
        "        df[df[target_col] == 0][col].hist(bins=50, alpha=0.7, label='No Default', \n",
        "                                           ax=axes[idx], color='#2ecc71', edgecolor='black')\n",
        "        df[df[target_col] == 1][col].hist(bins=50, alpha=0.7, label='Default', \n",
        "                                           ax=axes[idx], color='#e74c3c', edgecolor='black')\n",
        "        \n",
        "        axes[idx].set_title(f'–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {col}', fontsize=12, fontweight='bold')\n",
        "        axes[idx].set_xlabel(col)\n",
        "        axes[idx].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')\n",
        "        axes[idx].legend()\n",
        "        axes[idx].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
    ]
})

new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Box plots –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –≤—ã–±—Ä–æ—Å–æ–≤\n",
        "fig, axes = plt.subplots(1, 4, figsize=(18, 5))\n",
        "\n",
        "for idx, col in enumerate(key_numeric):\n",
        "    if col in df.columns:\n",
        "        df.boxplot(column=col, by=target_col, ax=axes[idx])\n",
        "        axes[idx].set_title(f'Box plot: {col}')\n",
        "        axes[idx].set_xlabel('Default')\n",
        "        axes[idx].set_ylabel(col)\n",
        "\n",
        "plt.suptitle('Box plots –ø–æ –∫–ª–∞—Å—Å–∞–º (–≤—ã—è–≤–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤)', fontsize=14, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤\n",
        "print('\\n=== –í—ã–±—Ä–æ—Å—ã (IQR method) ===')\n",
        "for col in key_numeric:\n",
        "    if col in df.columns:\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        outliers = df[(df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)]\n",
        "        print(f'{col}: {len(outliers)} –≤—ã–±—Ä–æ—Å–æ–≤ ({100*len(outliers)/len(df):.2f}%)')"
    ]
})

new_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "#### 2.3.3 –ê–Ω–∞–ª–∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"
    ]
})

new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "cat_features_to_plot = ['SEX', 'EDUCATION', 'MARRIAGE'] \\\n",
        "                       if all(col in df.columns for col in ['SEX', 'EDUCATION', 'MARRIAGE']) \\\n",
        "                       else categorical_features[:3]\n",
        "\n",
        "if len(cat_features_to_plot) > 0:\n",
        "    fig, axes = plt.subplots(1, len(cat_features_to_plot), figsize=(6*len(cat_features_to_plot), 5))\n",
        "    \n",
        "    if len(cat_features_to_plot) == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for idx, col in enumerate(cat_features_to_plot):\n",
        "        if col in df.columns:\n",
        "            # Crosstab\n",
        "            ct = pd.crosstab(df[col], df[target_col], normalize='index')\n",
        "            ct.plot(kind='bar', stacked=False, ax=axes[idx], color=['#2ecc71', '#e74c3c'])\n",
        "            axes[idx].set_title(f'{col} vs Default', fontsize=12, fontweight='bold')\n",
        "            axes[idx].set_xlabel(col)\n",
        "            axes[idx].set_ylabel('–ü—Ä–æ–ø–æ—Ä—Ü–∏—è')\n",
        "            axes[idx].legend(['No Default', 'Default'])\n",
        "            axes[idx].grid(alpha=0.3, axis='y')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print('–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã')"
    ]
})

new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "print('=== Chi-squared —Ç–µ—Å—Ç (—Å–≤—è–∑—å —Å —Ç–∞—Ä–≥–µ—Ç–æ–º) ===')\n",
        "print('H0: –ø—Ä–∏–∑–Ω–∞–∫ –ù–ï —Å–≤—è–∑–∞–Ω —Å –¥–µ—Ñ–æ–ª—Ç–æ–º\\n')\n",
        "\n",
        "for col in cat_features_to_plot:\n",
        "    if col in df.columns:\n",
        "        contingency_table = pd.crosstab(df[col], df[target_col])\n",
        "        chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
        "        \n",
        "        print(f'{col}:')\n",
        "        print(f'  Chi2 = {chi2:.2f}, p-value = {p_value:.4f}')\n",
        "        if p_value < 0.05:\n",
        "            print(f'  ‚úÖ –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º (p < 0.05) - –ø—Ä–∏–∑–Ω–∞–∫ —Å–≤—è–∑–∞–Ω —Å –¥–µ—Ñ–æ–ª—Ç–æ–º')\n",
        "        else:\n",
        "            print(f'  ‚ùå –ù–ï –∑–Ω–∞—á–∏–º (p >= 0.05)')\n",
        "        print()"
    ]
})

new_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "#### 2.3.4 –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑"
    ]
})

new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞\n",
        "corr_matrix = df[numeric_features + [target_col]].corr()\n",
        "\n",
        "# Heatmap\n",
        "plt.figure(figsize=(16, 14))\n",
        "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
        "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# –¢–æ–ø –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π —Å —Ç–∞—Ä–≥–µ—Ç–æ–º\n",
        "print('\\n=== –¢–æ–ø-10 –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π —Å –¥–µ—Ñ–æ–ª—Ç–æ–º ===')\n",
        "target_corr = corr_matrix[target_col].drop(target_col).sort_values(key=abs, ascending=False)\n",
        "print(target_corr.head(10))"
    ]
})

new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ú—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å (–≤—ã—Å–æ–∫–∏–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏)\n",
        "print('=== –ú—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å (|corr| > 0.8) ===')\n",
        "high_corr_pairs = []\n",
        "\n",
        "for i in range(len(corr_matrix.columns)):\n",
        "    for j in range(i+1, len(corr_matrix.columns)):\n",
        "        if abs(corr_matrix.iloc[i, j]) > 0.8:\n",
        "            high_corr_pairs.append((\n",
        "                corr_matrix.columns[i],\n",
        "                corr_matrix.columns[j],\n",
        "                corr_matrix.iloc[i, j]\n",
        "            ))\n",
        "\n",
        "if high_corr_pairs:\n",
        "    for feat1, feat2, corr_val in high_corr_pairs:\n",
        "        print(f'{feat1} <-> {feat2}: {corr_val:.3f}')\n",
        "    print(f'\\n‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ {len(high_corr_pairs)} –ø–∞—Ä —Å –≤—ã—Å–æ–∫–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π')\n",
        "    print('–í–æ–∑–º–æ–∂–Ω–æ, —Å—Ç–æ–∏—Ç —É–¥–∞–ª–∏—Ç—å –æ–¥–∏–Ω –∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –∫–∞–∂–¥–æ–π –ø–∞—Ä–µ')\n",
        "else:\n",
        "    print('‚úÖ –°–∏–ª—å–Ω–æ–π –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ')"
    ]
})

# ===========================
# SECTION 3: Feature Engineering
# ===========================

new_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "## üîß –ß–∞—Å—Ç—å 3: Feature Engineering\n",
        "\n",
        "### 3.1 –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "\n",
        "**–ë–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫–∞:**\n",
        "1. **–ü–ª–∞—Ç–µ–∂–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ:** –°—Ä–µ–¥–Ω—è—è –∑–∞–¥–µ—Ä–∂–∫–∞, —Ç—Ä–µ–Ω–¥ –∑–∞–¥–µ—Ä–∂–µ–∫\n",
        "2. **–î–æ–ª–≥–æ–≤–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞:** –û—Ç–Ω–æ—à–µ–Ω–∏–µ –¥–æ–ª–≥–∞ –∫ –ª–∏–º–∏—Ç—É, —É—Ç–∏–ª–∏–∑–∞—Ü–∏—è –∫—Ä–µ–¥–∏—Ç–∞\n",
        "3. **–ü–ª–∞—Ç–µ–∂–Ω–∞—è –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞:** –û—Ç–Ω–æ—à–µ–Ω–∏–µ –ø–ª–∞—Ç–µ–∂–µ–π –∫ —Å—á–µ—Ç–∞–º\n",
        "4. **–í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã:** –ò–∑–º–µ–Ω–µ–Ω–∏—è –≤ –¥–æ–ª–≥–µ, –ø–ª–∞—Ç–µ–∂–∞—Ö\n",
        "5. **–ê–≥—Ä–µ–≥–∞—Ç—ã:** –°—É–º–º—ã, —Å—Ä–µ–¥–Ω–∏–µ, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è"
    ]
})

new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ö–æ–ø–∏—è –¥–ª—è feature engineering\n",
        "df_fe = df.copy()\n",
        "\n",
        "print('–ò—Å—Ö–æ–¥–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:', df_fe.shape[1] - 1)  # -1 –¥–ª—è —Ç–∞—Ä–≥–µ—Ç–∞\n",
        "print('\\n–°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏...\\n')\n",
        "\n",
        "# ==================== PAYMENT FEATURES ====================\n",
        "\n",
        "# 1. –°—Ä–µ–¥–Ω—è—è –∑–∞–¥–µ—Ä–∂–∫–∞ –ø–ª–∞—Ç–µ–∂–µ–π\n",
        "pay_cols = [col for col in df_fe.columns if col.startswith('PAY_')]\n",
        "if pay_cols:\n",
        "    df_fe['avg_payment_delay'] = df_fe[pay_cols].mean(axis=1)\n",
        "    df_fe['max_payment_delay'] = df_fe[pay_cols].max(axis=1)\n",
        "    df_fe['payment_delay_std'] = df_fe[pay_cols].std(axis=1)\n",
        "    print('‚úÖ –ü—Ä–∏–∑–Ω–∞–∫–∏ –∑–∞–¥–µ—Ä–∂–∫–∏ –ø–ª–∞—Ç–µ–∂–µ–π')\n",
        "\n",
        "# 2. Trend –≤ –∑–∞–¥–µ—Ä–∂–∫–∞—Ö (–ø–æ—Å–ª–µ–¥–Ω–∏–µ vs —Ä–∞–Ω–Ω–∏–µ –º–µ—Å—è—Ü—ã)\n",
        "if len(pay_cols) >= 6:\n",
        "    df_fe['payment_trend'] = (df_fe[pay_cols[:3]].mean(axis=1) - \n",
        "                               df_fe[pay_cols[3:]].mean(axis=1))\n",
        "    print('‚úÖ –¢—Ä–µ–Ω–¥ –∑–∞–¥–µ—Ä–∂–µ–∫')\n",
        "\n",
        "# ==================== BILL AMOUNT FEATURES ====================\n",
        "\n",
        "# 3. –£—Ç–∏–ª–∏–∑–∞—Ü–∏—è –∫—Ä–µ–¥–∏—Ç–∞\n",
        "bill_cols = [col for col in df_fe.columns if col.startswith('BILL_AMT')]\n",
        "if bill_cols and 'LIMIT_BAL' in df_fe.columns:\n",
        "    df_fe['avg_bill'] = df_fe[bill_cols].mean(axis=1)\n",
        "    df_fe['utilization_rate'] = df_fe['avg_bill'] / (df_fe['LIMIT_BAL'] + 1)  # +1 —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0\n",
        "    df_fe['max_utilization'] = df_fe[bill_cols].max(axis=1) / (df_fe['LIMIT_BAL'] + 1)\n",
        "    print('‚úÖ –£—Ç–∏–ª–∏–∑–∞—Ü–∏—è –∫—Ä–µ–¥–∏—Ç–∞')\n",
        "\n",
        "# 4. –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å —Å—á–µ—Ç–æ–≤\n",
        "if bill_cols:\n",
        "    df_fe['bill_volatility'] = df_fe[bill_cols].std(axis=1)\n",
        "    df_fe['bill_trend'] = (df_fe[bill_cols[:3]].mean(axis=1) - \n",
        "                            df_fe[bill_cols[3:]].mean(axis=1))\n",
        "    print('‚úÖ –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å —Å—á–µ—Ç–æ–≤')\n",
        "\n",
        "# ==================== PAYMENT AMOUNT FEATURES ====================\n",
        "\n",
        "# 5. –ü–ª–∞—Ç–µ–∂–Ω–∞—è –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞ (–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –ø–ª–∞—Ç–µ–∂–∞ –∫ —Å—á–µ—Ç—É)\n",
        "pay_amt_cols = [col for col in df_fe.columns if col.startswith('PAY_AMT')]\n",
        "if pay_amt_cols and bill_cols:\n",
        "    for i, (pay_col, bill_col) in enumerate(zip(pay_amt_cols, bill_cols), 1):\n",
        "        df_fe[f'payment_ratio_{i}'] = df_fe[pay_col] / (df_fe[bill_col] + 1)\n",
        "    \n",
        "    payment_ratio_cols = [col for col in df_fe.columns if col.startswith('payment_ratio_')]\n",
        "    df_fe['avg_payment_ratio'] = df_fe[payment_ratio_cols].mean(axis=1)\n",
        "    print('‚úÖ –ü–ª–∞—Ç–µ–∂–Ω–∞—è –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞')\n",
        "\n",
        "# 6. –°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–ª–∞—Ç–µ–∂–∞\n",
        "if pay_amt_cols:\n",
        "    df_fe['avg_payment'] = df_fe[pay_amt_cols].mean(axis=1)\n",
        "    df_fe['total_payment'] = df_fe[pay_amt_cols].sum(axis=1)\n",
        "    df_fe['payment_volatility'] = df_fe[pay_amt_cols].std(axis=1)\n",
        "    print('‚úÖ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–ª–∞—Ç–µ–∂–µ–π')\n",
        "\n",
        "# ==================== DEBT FEATURES ====================\n",
        "\n",
        "# 7. –î–æ–ª–≥–æ–≤–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞\n",
        "if 'avg_bill' in df_fe.columns and 'avg_payment' in df_fe.columns:\n",
        "    df_fe['debt_to_payment_ratio'] = df_fe['avg_bill'] / (df_fe['avg_payment'] + 1)\n",
        "    print('‚úÖ –î–æ–ª–≥–æ–≤–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞')\n",
        "\n",
        "# ==================== BINARY FLAGS ====================\n",
        "\n",
        "# 8. –§–ª–∞–≥–∏ –ø—Ä–æ–±–ª–µ–º–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è\n",
        "if 'avg_payment_delay' in df_fe.columns:\n",
        "    df_fe['has_delay'] = (df_fe['avg_payment_delay'] > 0).astype(int)\n",
        "    df_fe['serious_delay'] = (df_fe['max_payment_delay'] >= 2).astype(int)\n",
        "    print('‚úÖ –§–ª–∞–≥–∏ –∑–∞–¥–µ—Ä–∂–µ–∫')\n",
        "\n",
        "if 'utilization_rate' in df_fe.columns:\n",
        "    df_fe['high_utilization'] = (df_fe['utilization_rate'] > 0.8).astype(int)\n",
        "    print('‚úÖ –§–ª–∞–≥ –≤—ã—Å–æ–∫–æ–π —É—Ç–∏–ª–∏–∑–∞—Ü–∏–∏')\n",
        "\n",
        "# ==================== AGE FEATURES ====================\n",
        "\n",
        "# 9. –í–æ–∑—Ä–∞—Å—Ç–Ω—ã–µ –≥—Ä—É–ø–ø—ã\n",
        "if 'AGE' in df_fe.columns:\n",
        "    df_fe['age_group'] = pd.cut(df_fe['AGE'], bins=[0, 25, 35, 45, 55, 100],\n",
        "                                 labels=['18-25', '26-35', '36-45', '46-55', '55+'])\n",
        "    # One-hot encoding –¥–ª—è –≤–æ–∑—Ä–∞—Å—Ç–Ω—ã—Ö –≥—Ä—É–ø–ø\n",
        "    age_dummies = pd.get_dummies(df_fe['age_group'], prefix='age')\n",
        "    df_fe = pd.concat([df_fe, age_dummies], axis=1)\n",
        "    print('‚úÖ –í–æ–∑—Ä–∞—Å—Ç–Ω—ã–µ –≥—Ä—É–ø–ø—ã')\n",
        "\n",
        "print(f'\\nüìä –ò—Ç–æ–≥–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {df_fe.shape[1] - 1}')\n",
        "print(f'   –î–æ–±–∞–≤–ª–µ–Ω–æ: {df_fe.shape[1] - df.shape[1]} –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤')"
    ]
})

new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "new_features = [col for col in df_fe.columns if col not in df.columns]\n",
        "print(f'\\n–ù–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ({len(new_features)}):')\n",
        "for i, feat in enumerate(new_features, 1):\n",
        "    print(f'  {i}. {feat}')\n",
        "\n",
        "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "print('\\n=== –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –Ω–æ–≤—ã—Ö —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ===')\n",
        "new_numeric = [col for col in new_features if df_fe[col].dtype in [np.float64, np.int64]]\n",
        "if new_numeric:\n",
        "    display(df_fe[new_numeric].describe())"
    ]
})

new_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 3.2 –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è"
    ]
})

new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (Label Encoding –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã)\n",
        "# XGBoost –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Ç–æ–ª—å–∫–æ —Å —á–∏—Å–ª–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏\n",
        "\n",
        "df_model = df_fe.copy()\n",
        "\n",
        "# Label encoding –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö\n",
        "cat_cols_to_encode = ['SEX', 'EDUCATION', 'MARRIAGE'] if all(col in df_model.columns for col in ['SEX', 'EDUCATION', 'MARRIAGE']) else []\n",
        "\n",
        "label_encoders = {}\n",
        "for col in cat_cols_to_encode:\n",
        "    if col in df_model.columns and df_model[col].dtype == 'object':\n",
        "        le = LabelEncoder()\n",
        "        df_model[col] = le.fit_transform(df_model[col].astype(str))\n",
        "        label_encoders[col] = le\n",
        "        print(f'‚úÖ Label encoding: {col}')\n",
        "\n",
        "# –£–¥–∞–ª—è–µ–º age_group (—É–∂–µ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω –≤ –±–∏–Ω–∞—Ä–Ω—ã–µ)\n",
        "if 'age_group' in df_model.columns:\n",
        "    df_model = df_model.drop('age_group', axis=1)\n",
        "\n",
        "print(f'\\n–§–∏–Ω–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö: {df_model.shape}')\n",
        "print(f'–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:')\n",
        "print(df_model.dtypes.value_counts())"
    ]
})

new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ç–∞—Ä–≥–µ—Ç\n",
        "X = df_model.drop(target_col, axis=1)\n",
        "y = df_model[target_col]\n",
        "\n",
        "print(f'X shape: {X.shape}')\n",
        "print(f'y shape: {y.shape}')\n",
        "print(f'\\n–ü—Ä–∏–∑–Ω–∞–∫–∏ ({X.shape[1]}): {list(X.columns[:10])}...')\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "print(f'\\nTrain set: {X_train.shape[0]:,} –ø—Ä–∏–º–µ—Ä–æ–≤')\n",
        "print(f'Test set:  {X_test.shape[0]:,} –ø—Ä–∏–º–µ—Ä–æ–≤')\n",
        "print(f'\\nDefault rate –≤ train: {y_train.mean():.2%}')\n",
        "print(f'Default rate –≤ test:  {y_test.mean():.2%}')"
    ]
})

# ===========================
# SECTION 4: Baseline Models
# ===========================

new_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "## üéØ –ß–∞—Å—Ç—å 4: Baseline –º–æ–¥–µ–ª–∏\n",
        "\n",
        "–ü–µ—Ä–µ–¥ XGBoost —Å–æ–∑–¥–∞–¥–∏–º baseline –º–æ–¥–µ–ª–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è:\n",
        "1. **Logistic Regression** - –ª–∏–Ω–µ–π–Ω–∞—è –º–æ–¥–µ–ª—å\n",
        "2. **Decision Tree** - –æ–¥–Ω–æ –¥–µ—Ä–µ–≤–æ (–±–∞–∑–æ–≤—ã–π —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–π –±–ª–æ–∫)\n",
        "3. **Random Forest** - –∞–Ω—Å–∞–º–±–ª—å –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –¥–µ—Ä–µ–≤—å–µ–≤"
    ]
})

new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name='Model'):\n",
        "    \"\"\"\n",
        "    –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ –≤—ã–≤–æ–¥–∏—Ç –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞\n",
        "    \"\"\"\n",
        "    # –û–±—É—á–µ–Ω–∏–µ\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # –ú–µ—Ç—Ä–∏–∫–∏\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    pr_auc = average_precision_score(y_test, y_pred_proba)\n",
        "    \n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    \n",
        "    print(f'\\n{\"=\"*60}')\n",
        "    print(f'{model_name:^60}')\n",
        "    print(f'{\"=\"*60}')\n",
        "    print(f'Accuracy:  {accuracy:.4f}')\n",
        "    print(f'Precision: {precision:.4f} (–∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –¥–µ—Ñ–æ–ª—Ç–æ–≤, —Å–∫–æ–ª—å–∫–æ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö)')\n",
        "    print(f'Recall:    {recall:.4f} (–∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–µ—Ñ–æ–ª—Ç–æ–≤, —Å–∫–æ–ª—å–∫–æ –ø–æ–π–º–∞–ª–∏)')\n",
        "    print(f'F1-score:  {f1:.4f}')\n",
        "    print(f'ROC-AUC:   {roc_auc:.4f}')\n",
        "    print(f'PR-AUC:    {pr_auc:.4f}')\n",
        "    print(f'\\nConfusion Matrix:')\n",
        "    print(f'  TN: {tn:5d}  |  FP: {fp:5d}')\n",
        "    print(f'  FN: {fn:5d}  |  TP: {tp:5d}')\n",
        "    \n",
        "    # –°—Ç–æ–∏–º–æ—Å—Ç—å –æ—à–∏–±–æ–∫ (–ø—Ä–∏–º–µ—Ä–Ω–∞—è)\n",
        "    cost_fn = 25000  # —Å—Ä–µ–¥–Ω—è—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –ø—Ä–æ–ø—É—â–µ–Ω–Ω–æ–≥–æ –¥–µ—Ñ–æ–ª—Ç–∞\n",
        "    cost_fp = 1000   # —Å—Ä–µ–¥–Ω—è—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –æ—Ç–∫–∞–∑–∞ —Ö–æ—Ä–æ—à–µ–º—É –∫–ª–∏–µ–Ω—Ç—É\n",
        "    total_cost = fn * cost_fn + fp * cost_fp\n",
        "    print(f'\\nüí∞ –û—Ü–µ–Ω–∫–∞ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –æ—à–∏–±–æ–∫:')\n",
        "    print(f'   FN cost: {fn} √ó {cost_fn:,} TWD = {fn * cost_fn:,} TWD')\n",
        "    print(f'   FP cost: {fp} √ó {cost_fp:,} TWD = {fp * cost_fp:,} TWD')\n",
        "    print(f'   Total:   {total_cost:,} TWD')\n",
        "    \n",
        "    return {\n",
        "        'model': model,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'roc_auc': roc_auc,\n",
        "        'pr_auc': pr_auc,\n",
        "        'y_pred': y_pred,\n",
        "        'y_pred_proba': y_pred_proba,\n",
        "        'total_cost': total_cost\n",
        "    }\n",
        "\n",
        "print('‚úÖ –§—É–Ω–∫—Ü–∏—è evaluate_model —Å–æ–∑–¥–∞–Ω–∞')"
    ]
})

new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "results = {}\n",
        "\n",
        "# 1. Logistic Regression\n",
        "print('\\nüîµ –û–±—É—á–µ–Ω–∏–µ Logistic Regression...')\n",
        "lr_model = LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)\n",
        "results['Logistic Regression'] = evaluate_model(\n",
        "    lr_model, X_train, X_test, y_train, y_test, 'Logistic Regression'\n",
        ")"
    ]
})

new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# 2. Decision Tree\n",
        "print('\\nüü¢ –û–±—É—á–µ–Ω–∏–µ Decision Tree...')\n",
        "dt_model = DecisionTreeClassifier(random_state=RANDOM_STATE, max_depth=10)\n",
        "results['Decision Tree'] = evaluate_model(\n",
        "    dt_model, X_train, X_test, y_train, y_test, 'Decision Tree (max_depth=10)'\n",
        ")"
    ]
})

new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# 3. Random Forest\n",
        "print('\\nüü† –û–±—É—á–µ–Ω–∏–µ Random Forest...')\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100, \n",
        "    max_depth=10, \n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1\n",
        ")\n",
        "results['Random Forest'] = evaluate_model(\n",
        "    rf_model, X_train, X_test, y_train, y_test, 'Random Forest (100 trees)'\n",
        ")"
    ]
})

new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ baseline –º–æ–¥–µ–ª–µ–π\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': list(results.keys()),\n",
        "    'Accuracy': [results[m]['accuracy'] for m in results],\n",
        "    'Precision': [results[m]['precision'] for m in results],\n",
        "    'Recall': [results[m]['recall'] for m in results],\n",
        "    'F1': [results[m]['f1'] for m in results],\n",
        "    'ROC-AUC': [results[m]['roc_auc'] for m in results],\n",
        "    'PR-AUC': [results[m]['pr_auc'] for m in results],\n",
        "    'Cost (TWD)': [results[m]['total_cost'] for m in results]\n",
        "})\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print('–°–†–ê–í–ù–ï–ù–ò–ï BASELINE –ú–û–î–ï–õ–ï–ô')\n",
        "print('='*80)\n",
        "display(comparison_df)\n",
        "\n",
        "# –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å –ø–æ ROC-AUC\n",
        "best_model_name = comparison_df.loc[comparison_df['ROC-AUC'].idxmax(), 'Model']\n",
        "print(f'\\nüèÜ –õ—É—á—à–∞—è baseline –º–æ–¥–µ–ª—å (ROC-AUC): {best_model_name}')"
    ]
})

# ===========================
# SECTION 5: XGBoost Implementation
# ===========================

new_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "## üöÄ –ß–∞—Å—Ç—å 5: XGBoost Implementation\n",
        "\n",
        "### 5.1 XGBoost —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"
    ]
})

new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# XGBoost baseline (–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)\n",
        "print('\\n‚ö° –û–±—É—á–µ–Ω–∏–µ XGBoost (default parameters)...')\n",
        "\n",
        "xgb_baseline = XGBClassifier(\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "\n",
        "results['XGBoost (default)'] = evaluate_model(\n",
        "    xgb_baseline, X_train, X_test, y_train, y_test, 'XGBoost (default parameters)'\n",
        ")"
    ]
})

new_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 5.2 XGBoost —Å –±–∞–∑–æ–≤—ã–º —Ç—é–Ω–∏–Ω–≥–æ–º\n",
        "\n",
        "–ü—Ä–∏–º–µ–Ω–∏–º –±–∞–∑–æ–≤—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:\n",
        "- `scale_pos_weight`: –∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏—è –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤\n",
        "- `max_depth`: –∫–æ–Ω—Ç—Ä–æ–ª—å –≥–ª—É–±–∏–Ω—ã –¥–µ—Ä–µ–≤—å–µ–≤\n",
        "- `learning_rate`: —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è\n",
        "- `n_estimators`: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤\n",
        "- `subsample`, `colsample_bytree`: —Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ"
    ]
})

new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –í—ã—á–∏—Å–ª—è–µ–º scale_pos_weight –¥–ª—è –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞\n",
        "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "print(f'Scale pos weight: {scale_pos_weight:.2f}')\n",
        "\n",
        "# XGBoost —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n",
        "xgb_tuned_v1 = XGBClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "\n",
        "results['XGBoost (tuned_v1)'] = evaluate_model(\n",
        "    xgb_tuned_v1, X_train, X_test, y_train, y_test, 'XGBoost (basic tuning)'\n",
        ")"
    ]
})

new_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 5.3 Hyperparameter Tuning (GridSearchCV)\n",
        "\n",
        "–ò—Å–ø–æ–ª—å–∑—É–µ–º grid search –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.\n",
        "\n",
        "**–°—Ç—Ä–∞—Ç–µ–≥–∏—è:**\n",
        "1. –§–∏–∫—Å–∏—Ä—É–µ–º `learning_rate=0.1`\n",
        "2. –¢—é–Ω–∏–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–µ—Ä–µ–≤–∞ (`max_depth`, `min_child_weight`)\n",
        "3. –¢—é–Ω–∏–º —Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ (`subsample`, `colsample_bytree`)\n",
        "4. –¢—é–Ω–∏–º —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é (`gamma`, `lambda`)\n",
        "5. –§–∏–Ω–∞–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å `learning_rate=0.05`"
    ]
})

new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Grid Search –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏\n",
        "print('\\nüîç Hyperparameter tuning —Å GridSearchCV...')\n",
        "print('–≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç...\\n')\n",
        "\n",
        "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞\n",
        "param_grid = {\n",
        "    'max_depth': [4, 6, 8],\n",
        "    'min_child_weight': [1, 3, 5],\n",
        "    'gamma': [0, 0.1, 0.5],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "    'learning_rate': [0.05, 0.1],\n",
        "    'n_estimators': [100, 200]\n",
        "}\n",
        "\n",
        "# –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å\n",
        "xgb_base = XGBClassifier(\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb_base,\n",
        "    param_grid=param_grid,\n",
        "    cv=3,\n",
        "    scoring='roc_auc',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# –û–±—É—á–µ–Ω–∏–µ (–Ω–∞ –ø–æ–¥–≤—ã–±–æ—Ä–∫–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è)\n",
        "# –î–ª—è –ø–æ–ª–Ω–æ–≥–æ grid search –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≤–µ—Å—å X_train\n",
        "sample_size = min(10000, len(X_train))\n",
        "X_train_sample = X_train.iloc[:sample_size]\n",
        "y_train_sample = y_train.iloc[:sample_size]\n",
        "\n",
        "grid_search.fit(X_train_sample, y_train_sample)\n",
        "\n",
        "print(f'\\n‚úÖ Grid search –∑–∞–≤–µ—Ä—à–µ–Ω')\n",
        "print(f'\\n–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:')\n",
        "for param, value in grid_search.best_params_.items():\n",
        "    print(f'  {param}: {value}')\n",
        "print(f'\\n–õ—É—á—à–∏–π ROC-AUC (CV): {grid_search.best_score_:.4f}')"
    ]
})

new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –û–±—É—á–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –Ω–∞ –≤—Å–µ–º train set\n",
        "print('\\n‚ö° –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π XGBoost –º–æ–¥–µ–ª–∏ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏...')\n",
        "\n",
        "xgb_final = XGBClassifier(\n",
        "    **grid_search.best_params_,\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "\n",
        "results['XGBoost (optimized)'] = evaluate_model(\n",
        "    xgb_final, X_train, X_test, y_train, y_test, 'XGBoost (Grid Search Optimized)'\n",
        ")"
    ]
})

# ===========================
# SECTION 6: Model Interpretation
# ===========================

new_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "## üîç –ß–∞—Å—Ç—å 6: –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –º–æ–¥–µ–ª–∏\n",
        "\n",
        "### 6.1 Feature Importance"
    ]
})

new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Feature importance (–≤—Å–µ —Ç—Ä–∏ —Ç–∏–ø–∞)\n",
        "model = results['XGBoost (optimized)']['model']\n",
        "\n",
        "# Weight, Gain, Cover\n",
        "importance_weight = model.feature_importances_\n",
        "importance_gain = model.get_booster().get_score(importance_type='gain')\n",
        "importance_cover = model.get_booster().get_score(importance_type='cover')\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º DataFrame\n",
        "feature_names = X_train.columns\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Weight': importance_weight\n",
        "})\n",
        "\n",
        "# –î–æ–±–∞–≤–ª—è–µ–º gain –∏ cover (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)\n",
        "importance_df['Gain'] = importance_df['Feature'].map(\n",
        "    lambda x: importance_gain.get(f'f{list(feature_names).index(x)}', 0)\n",
        ")\n",
        "importance_df['Cover'] = importance_df['Feature'].map(\n",
        "    lambda x: importance_cover.get(f'f{list(feature_names).index(x)}', 0)\n",
        ")\n",
        "\n",
        "# –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ gain\n",
        "importance_df = importance_df.sort_values('Gain', ascending=False)\n",
        "\n",
        "print('=== –¢–æ–ø-20 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ Feature Importance (Gain) ===')\n",
        "display(importance_df.head(20))"
    ]
})

new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è Feature Importance\n",
        "fig, axes = plt.subplots(1, 3, figsize=(20, 8))\n",
        "\n",
        "# Weight\n",
        "top_features_weight = importance_df.nlargest(15, 'Weight')\n",
        "axes[0].barh(top_features_weight['Feature'], top_features_weight['Weight'], color='skyblue')\n",
        "axes[0].set_xlabel('Importance (Weight)', fontweight='bold')\n",
        "axes[0].set_title('Feature Importance: Weight\\n(Frequency of splits)', fontweight='bold')\n",
        "axes[0].invert_yaxis()\n",
        "\n",
        "# Gain (RECOMMENDED)\n",
        "top_features_gain = importance_df.nlargest(15, 'Gain')\n",
        "axes[1].barh(top_features_gain['Feature'], top_features_gain['Gain'], color='lightcoral')\n",
        "axes[1].set_xlabel('Importance (Gain)', fontweight='bold')\n",
        "axes[1].set_title('Feature Importance: Gain\\n(Average information gain) ‚≠ê', fontweight='bold', color='red')\n",
        "axes[1].invert_yaxis()\n",
        "\n",
        "# Cover\n",
        "top_features_cover = importance_df.nlargest(15, 'Cover')\n",
        "axes[2].barh(top_features_cover['Feature'], top_features_cover['Cover'], color='lightgreen')\n",
        "axes[2].set_xlabel('Importance (Cover)', fontweight='bold')\n",
        "axes[2].set_title('Feature Importance: Cover\\n(Sum of hessians)', fontweight='bold')\n",
        "axes[2].invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
    ]
})

new_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 6.2 ROC –∏ Precision-Recall –∫—Ä–∏–≤—ã–µ"
    ]
})

new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# ROC –∏ PR –∫—Ä–∏–≤—ã–µ –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# ROC Curve\n",
        "for model_name in results:\n",
        "    y_pred_proba = results[model_name]['y_pred_proba']\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "    auc = results[model_name]['roc_auc']\n",
        "    axes[0].plot(fpr, tpr, label=f'{model_name} (AUC={auc:.3f})', linewidth=2)\n",
        "\n",
        "axes[0].plot([0, 1], [0, 1], 'k--', label='Random (AUC=0.500)', linewidth=1)\n",
        "axes[0].set_xlabel('False Positive Rate', fontweight='bold')\n",
        "axes[0].set_ylabel('True Positive Rate', fontweight='bold')\n",
        "axes[0].set_title('ROC Curves', fontsize=14, fontweight='bold')\n",
        "axes[0].legend(loc='lower right')\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Precision-Recall Curve\n",
        "for model_name in results:\n",
        "    y_pred_proba = results[model_name]['y_pred_proba']\n",
        "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
        "    pr_auc = results[model_name]['pr_auc']\n",
        "    axes[1].plot(recall, precision, label=f'{model_name} (AUC={pr_auc:.3f})', linewidth=2)\n",
        "\n",
        "# Baseline (–¥–æ–ª—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö)\n",
        "baseline = y_test.mean()\n",
        "axes[1].plot([0, 1], [baseline, baseline], 'k--', label=f'Random (AUC={baseline:.3f})', linewidth=1)\n",
        "axes[1].set_xlabel('Recall', fontweight='bold')\n",
        "axes[1].set_ylabel('Precision', fontweight='bold')\n",
        "axes[1].set_title('Precision-Recall Curves', fontsize=14, fontweight='bold')\n",
        "axes[1].legend(loc='upper right')\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('\\nüí° –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:')\n",
        "print('- ROC-AUC: –û–±—â–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ —Ä–∞–∑–¥–µ–ª—è—Ç—å –∫–ª–∞—Å—Å—ã')\n",
        "print('- PR-AUC: –ë–æ–ª–µ–µ –≤–∞–∂–Ω–∞ –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (—Ñ–æ–∫—É—Å –Ω–∞ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–º –∫–ª–∞—Å—Å–µ)')\n",
        "print('- –î–ª—è –∫—Ä–µ–¥–∏—Ç–Ω–æ–≥–æ —Å–∫–æ—Ä–∏–Ω–≥–∞ PR-AUC —á–∞—Å—Ç–æ –≤–∞–∂–Ω–µ–µ!')"
    ]
})

new_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 6.3 Threshold Optimization\n",
        "\n",
        "–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø–æ—Ä–æ–≥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ = 0.5, –Ω–æ –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —Ä–∞–∑–Ω—ã—Ö —Å—Ç–æ–∏–º–æ—Å—Ç–µ–π –æ—à–∏–±–æ–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –º–æ–∂–µ—Ç –±—ã—Ç—å –¥—Ä—É–≥–∏–º."
    ]
})

new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Threshold optimization\n",
        "y_pred_proba_xgb = results['XGBoost (optimized)']['y_pred_proba']\n",
        "\n",
        "# –ü–µ—Ä–µ–±–∏—Ä–∞–µ–º –ø–æ—Ä–æ–≥–∏\n",
        "thresholds = np.arange(0.1, 0.9, 0.05)\n",
        "metrics_by_threshold = []\n",
        "\n",
        "for threshold in thresholds:\n",
        "    y_pred_thresh = (y_pred_proba_xgb >= threshold).astype(int)\n",
        "    \n",
        "    precision = precision_score(y_test, y_pred_thresh)\n",
        "    recall = recall_score(y_test, y_pred_thresh)\n",
        "    f1 = f1_score(y_test, y_pred_thresh)\n",
        "    \n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_thresh).ravel()\n",
        "    cost = fn * 25000 + fp * 1000  # –°—Ç–æ–∏–º–æ—Å—Ç—å –æ—à–∏–±–æ–∫\n",
        "    \n",
        "    metrics_by_threshold.append({\n",
        "        'Threshold': threshold,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1': f1,\n",
        "        'FP': fp,\n",
        "        'FN': fn,\n",
        "        'Cost': cost\n",
        "    })\n",
        "\n",
        "threshold_df = pd.DataFrame(metrics_by_threshold)\n",
        "\n",
        "# –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –ø–æ –º–∏–Ω–∏–º—É–º—É —Å—Ç–æ–∏–º–æ—Å—Ç–∏\n",
        "optimal_idx = threshold_df['Cost'].idxmin()\n",
        "optimal_threshold = threshold_df.loc[optimal_idx, 'Threshold']\n",
        "\n",
        "print('=== –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –ø–æ—Ä–æ–≥–∞–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ ===')\n",
        "display(threshold_df)\n",
        "\n",
        "print(f'\\nüéØ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ (–º–∏–Ω–∏–º—É–º —Å—Ç–æ–∏–º–æ—Å—Ç–∏): {optimal_threshold:.2f}')\n",
        "print(f'   Precision: {threshold_df.loc[optimal_idx, \"Precision\"]:.4f}')\n",
        "print(f'   Recall: {threshold_df.loc[optimal_idx, \"Recall\"]:.4f}')\n",
        "print(f'   F1: {threshold_df.loc[optimal_idx, \"F1\"]:.4f}')\n",
        "print(f'   Cost: {threshold_df.loc[optimal_idx, \"Cost\"]:,.0f} TWD')"
    ]
})

new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è threshold optimization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Precision, Recall, F1 vs Threshold\n",
        "axes[0].plot(threshold_df['Threshold'], threshold_df['Precision'], 'b-', label='Precision', linewidth=2)\n",
        "axes[0].plot(threshold_df['Threshold'], threshold_df['Recall'], 'r-', label='Recall', linewidth=2)\n",
        "axes[0].plot(threshold_df['Threshold'], threshold_df['F1'], 'g-', label='F1', linewidth=2)\n",
        "axes[0].axvline(x=optimal_threshold, color='purple', linestyle='--', linewidth=2, \n",
        "                label=f'Optimal={optimal_threshold:.2f}')\n",
        "axes[0].axvline(x=0.5, color='gray', linestyle=':', linewidth=1, label='Default=0.5')\n",
        "axes[0].set_xlabel('Threshold', fontweight='bold')\n",
        "axes[0].set_ylabel('Score', fontweight='bold')\n",
        "axes[0].set_title('Metrics vs Threshold', fontsize=14, fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Cost vs Threshold\n",
        "axes[1].plot(threshold_df['Threshold'], threshold_df['Cost'], 'purple', linewidth=3)\n",
        "axes[1].axvline(x=optimal_threshold, color='red', linestyle='--', linewidth=2, \n",
        "                label=f'Optimal={optimal_threshold:.2f}')\n",
        "axes[1].axvline(x=0.5, color='gray', linestyle=':', linewidth=1, label='Default=0.5')\n",
        "axes[1].scatter([optimal_threshold], [threshold_df.loc[optimal_idx, 'Cost']], \n",
        "                color='red', s=200, zorder=5, label='Min Cost')\n",
        "axes[1].set_xlabel('Threshold', fontweight='bold')\n",
        "axes[1].set_ylabel('Total Cost (TWD)', fontweight='bold')\n",
        "axes[1].set_title('Business Cost vs Threshold', fontsize=14, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
    ]
})

new_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 6.4 –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"
    ]
})

new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞\n",
        "final_comparison = pd.DataFrame({\n",
        "    'Model': list(results.keys()),\n",
        "    'Accuracy': [results[m]['accuracy'] for m in results],\n",
        "    'Precision': [results[m]['precision'] for m in results],\n",
        "    'Recall': [results[m]['recall'] for m in results],\n",
        "    'F1': [results[m]['f1'] for m in results],\n",
        "    'ROC-AUC': [results[m]['roc_auc'] for m in results],\n",
        "    'PR-AUC': [results[m]['pr_auc'] for m in results],\n",
        "    'Cost (TWD)': [results[m]['total_cost'] for m in results]\n",
        "})\n",
        "\n",
        "# –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ ROC-AUC\n",
        "final_comparison = final_comparison.sort_values('ROC-AUC', ascending=False)\n",
        "\n",
        "print('\\n' + '='*100)\n",
        "print('–§–ò–ù–ê–õ–¨–ù–û–ï –°–†–ê–í–ù–ï–ù–ò–ï –í–°–ï–• –ú–û–î–ï–õ–ï–ô')\n",
        "print('='*100)\n",
        "display(final_comparison)\n",
        "\n",
        "# –ü–æ–±–µ–¥–∏—Ç–µ–ª—å\n",
        "best_model = final_comparison.iloc[0]['Model']\n",
        "print(f'\\nüèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model}')\n",
        "print(f'   ROC-AUC: {final_comparison.iloc[0][\"ROC-AUC\"]:.4f}')\n",
        "print(f'   PR-AUC:  {final_comparison.iloc[0][\"PR-AUC\"]:.4f}')\n",
        "print(f'   Cost:    {final_comparison.iloc[0][\"Cost (TWD)\"]:,.0f} TWD')"
    ]
})

new_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "metrics_to_plot = ['ROC-AUC', 'PR-AUC', 'F1', 'Cost (TWD)']\n",
        "colors_map = plt.cm.viridis(np.linspace(0, 1, len(final_comparison)))\n",
        "\n",
        "for idx, metric in enumerate(metrics_to_plot):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "    \n",
        "    data = final_comparison.sort_values(metric, ascending=(metric == 'Cost (TWD)'))\n",
        "    \n",
        "    bars = ax.barh(data['Model'], data[metric], color=colors_map)\n",
        "    ax.set_xlabel(metric, fontweight='bold')\n",
        "    ax.set_title(f'Comparison: {metric}', fontsize=12, fontweight='bold')\n",
        "    ax.invert_yaxis()\n",
        "    \n",
        "    # –ó–Ω–∞—á–µ–Ω–∏—è –Ω–∞ –±–∞—Ä–∞—Ö\n",
        "    for i, (model, value) in enumerate(zip(data['Model'], data[metric])):\n",
        "        if metric == 'Cost (TWD)':\n",
        "            ax.text(value, i, f' {value:,.0f}', va='center', fontweight='bold')\n",
        "        else:\n",
        "            ax.text(value, i, f' {value:.4f}', va='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
    ]
})

# ===========================
# SECTION 7: Conclusions
# ===========================

new_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "## üìù –ß–∞—Å—Ç—å 7: –í—ã–≤–æ–¥—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n",
        "\n",
        "### 7.1 –ö–ª—é—á–µ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
        "\n",
        "**–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π:**\n",
        "1. **XGBoost (optimized)** –ø–æ–∫–∞–∑–∞–ª –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –≤—Å–µ–º –º–µ—Ç—Ä–∏–∫–∞–º\n",
        "2. **Random Forest** - —Ö–æ—Ä–æ—à–∞—è baseline –º–æ–¥–µ–ª—å, –Ω–æ —É—Å—Ç—É–ø–∞–µ—Ç XGBoost\n",
        "3. **Logistic Regression** - –ø—Ä–æ—Å—Ç–∞—è –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–∞—è, –Ω–æ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ—á–Ω–∞—è\n",
        "4. **Decision Tree** - –≤—ã—Å–æ–∫–æ–µ overfitting, –Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è production\n",
        "\n",
        "**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ XGBoost:**\n",
        "- ‚úÖ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (ROC-AUC, PR-AUC)\n",
        "- ‚úÖ –í—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (gamma, lambda) ‚Üí –º–µ–Ω—å—à–µ overfitting\n",
        "- ‚úÖ –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º (scale_pos_weight)\n",
        "- ‚úÖ –•–æ—Ä–æ—à–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å (feature importance, SHAP)\n",
        "- ‚úÖ –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ inference\n",
        "\n",
        "### 7.2 Feature Engineering Insights\n",
        "\n",
        "**–ù–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–ø–æ Gain):**\n",
        "1. **–ü–ª–∞—Ç–µ–∂–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ:** PAY_0, PAY_2, PAY_3 - –∏—Å—Ç–æ—Ä–∏—è –∑–∞–¥–µ—Ä–∂–µ–∫ –∫—Ä–∏—Ç–∏—á–Ω–∞\n",
        "2. **–£—Ç–∏–ª–∏–∑–∞—Ü–∏—è –∫—Ä–µ–¥–∏—Ç–∞:** utilization_rate, avg_bill - –¥–æ–ª–≥–æ–≤–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞\n",
        "3. **–ü–ª–∞—Ç–µ–∂–Ω–∞—è –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞:** payment_ratio_*, avg_payment_ratio\n",
        "4. **–ö—Ä–µ–¥–∏—Ç–Ω—ã–π –ª–∏–º–∏—Ç:** LIMIT_BAL - –±–∞–∑–æ–≤—ã–π –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –∫—Ä–µ–¥–∏—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏\n",
        "\n",
        "**–°–æ–∑–¥–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –æ–∫–∞–∑–∞–ª–∏—Å—å –æ—á–µ–Ω—å –ø–æ–ª–µ–∑–Ω—ã–º–∏:**\n",
        "- –ê–≥—Ä–µ–≥–∞—Ç—ã (—Å—Ä–µ–¥–Ω–∏–µ, —Å—É–º–º—ã, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è)\n",
        "- –û—Ç–Ω–æ—à–µ–Ω–∏—è (–ø–ª–∞—Ç–µ–∂/—Å—á–µ—Ç, –¥–æ–ª–≥/–ª–∏–º–∏—Ç)\n",
        "- –¢—Ä–µ–Ω–¥—ã (–∏–∑–º–µ–Ω–µ–Ω–∏—è –≤–æ –≤—Ä–µ–º–µ–Ω–∏)\n",
        "- –ë–∏–Ω–∞—Ä–Ω—ã–µ —Ñ–ª–∞–≥–∏ (has_delay, high_utilization)\n",
        "\n",
        "### 7.3 –ë–∏–∑–Ω–µ—Å-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n",
        "\n",
        "**–î–ª—è –±–∞–Ω–∫–∞:**\n",
        "1. **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å XGBoost** –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω—É—é –º–æ–¥–µ–ª—å –∫—Ä–µ–¥–∏—Ç–Ω–æ–≥–æ —Å–∫–æ—Ä–∏–Ω–≥–∞\n",
        "2. **–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Ä–æ–≥** –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏—Å—Ö–æ–¥—è –∏–∑ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –æ—à–∏–±–æ–∫:\n",
        "   - False Negative (–ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–π –¥–µ—Ñ–æ–ª—Ç) –¥–æ—Ä–æ–∂–µ ‚Üí –ø–æ—Ä–æ–≥ –Ω–∏–∂–µ 0.5\n",
        "   - –£–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç Recall, —É–º–µ–Ω—å—à–∞–µ—Ç —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –ø–æ—Ç–µ—Ä–∏\n",
        "3. **–ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å –ø–ª–∞—Ç–µ–∂–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ** - —Å–∞–º—ã–π —Å–∏–ª—å–Ω—ã–π –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä\n",
        "4. **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–Ω–∏–∂–∞—Ç—å –ª–∏–º–∏—Ç—ã** –∫–ª–∏–µ–Ω—Ç–∞–º —Å:\n",
        "   - –í—ã—Å–æ–∫–æ–π —É—Ç–∏–ª–∏–∑–∞—Ü–∏–µ–π (>80%)\n",
        "   - –ü–æ—Å—Ç–æ—è–Ω–Ω—ã–º–∏ –∑–∞–¥–µ—Ä–∂–∫–∞–º–∏ (PAY > 1)\n",
        "   - –ù–∏–∑–∫–∏–º payment_ratio (<0.1)\n",
        "\n",
        "**–†–µ–≥—É–ª—è—Ç–æ—Ä–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã (Basel III):**\n",
        "- –ú–æ–¥–µ–ª—å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–∞ (feature importance, partial dependence)\n",
        "- –ú–æ–∂–Ω–æ –æ–±—ä—è—Å–Ω–∏—Ç—å –∫–∞–∂–¥–æ–µ —Ä–µ—à–µ–Ω–∏–µ\n",
        "- ROC-AUC >0.75 —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º\n",
        "\n",
        "### 7.4 –î–∞–ª—å–Ω–µ–π—à–∏–µ —É–ª—É—á—à–µ–Ω–∏—è\n",
        "\n",
        "**–ú–æ–¥–µ–ª—å:**\n",
        "1. **SHAP values** –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø—Ä–∏–º–µ—Ä–∞\n",
        "2. **Early stopping** —Å validation set –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤—ã–±–æ—Ä–∞ n_estimators\n",
        "3. **Stacking** —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LightGBM, CatBoost)\n",
        "4. **Calibration** –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π (Platt scaling, isotonic regression)\n",
        "\n",
        "**–î–∞–Ω–Ω—ã–µ:**\n",
        "1. –í–Ω–µ—à–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ (–±—é—Ä–æ –∫—Ä–µ–¥–∏—Ç–Ω—ã—Ö –∏—Å—Ç–æ—Ä–∏–π)\n",
        "2. –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (—Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å, —Ç—Ä–µ–Ω–¥—ã)\n",
        "3. –°–æ—Ü–∏–∞–ª—å–Ω–æ-–¥–µ–º–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ\n",
        "4. –¢—Ä–∞–Ω–∑–∞–∫—Ü–∏–æ–Ω–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è\n",
        "\n",
        "**Production:**\n",
        "1. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ drift (–∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π)\n",
        "2. A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
        "3. –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ (—Ä–∞–∑ –≤ –º–µ—Å—è—Ü/–∫–≤–∞—Ä—Ç–∞–ª)\n",
        "4. API –¥–ª—è real-time scoring\n",
        "\n",
        "### 7.5 –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≤—ã–≤–æ–¥—ã\n",
        "\n",
        "**–ü–æ—á–µ–º—É XGBoost —Ä–∞–±–æ—Ç–∞–µ—Ç:**\n",
        "\n",
        "1. **Second-order approximation** (Hessian) –¥–∞–µ—Ç –ª—É—á—à—É—é –ª–æ–∫–∞–ª—å–Ω—É—é –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—é loss:\n",
        "   $$L(y, F + h) \\approx L(y, F) + g \\cdot h + \\frac{1}{2}h \\cdot h^2$$\n",
        "   –ö–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞—è vs –ª–∏–Ω–µ–π–Ω–∞—è ‚Üí —Ç–æ—á–Ω–µ–µ –Ω–∞—Ö–æ–¥–∏–º –æ–ø—Ç–∏–º—É–º\n",
        "\n",
        "2. **Regularization** –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç overfitting:\n",
        "   $$\\Omega(h) = \\gamma T + \\frac{\\lambda}{2}\\sum w_j^2$$\n",
        "   –ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É bias –∏ variance\n",
        "\n",
        "3. **–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞ –ª–∏—Å—Ç—å–µ–≤** –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏:\n",
        "   $$w_j^* = -\\frac{G_j}{H_j + \\lambda}$$\n",
        "   –ù–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ line search!\n",
        "\n",
        "4. **Gain-based split finding** –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ—Ç —É–º–µ–Ω—å—à–µ–Ω–∏–µ loss:\n",
        "   $$\\text{Gain} = \\frac{1}{2}\\left[\\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{(G_L+G_R)^2}{H_L+H_R+\\lambda}\\right] - \\gamma$$\n",
        "\n",
        "### 7.6 –ö–æ–≥–¥–∞ –ù–ï –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å XGBoost\n",
        "\n",
        "‚ùå **–ò–∑–±–µ–≥–∞–π—Ç–µ XGBoost –µ—Å–ª–∏:**\n",
        "1. –ù—É–∂–Ω–∞ –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏–µ (online learning) - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ SGD-based –º–æ–¥–µ–ª–∏\n",
        "2. –û—á–µ–Ω—å –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö (<1000 –ø—Ä–∏–º–µ—Ä–æ–≤) - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ª–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–ª–∏ Random Forest\n",
        "3. –î–∞–Ω–Ω—ã–µ –Ω–µ —Ç–∞–±–ª–∏—á–Ω—ã–µ (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, —Ç–µ–∫—Å—Ç) - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏\n",
        "4. –ö—Ä–∏—Ç–∏—á–Ω–∞ —Å–∫–æ—Ä–æ—Å—Ç—å inference (<1ms) - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é\n",
        "5. –ù—É–∂–Ω–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ Bayesian –º–æ–¥–µ–ª–∏\n",
        "\n",
        "---\n",
        "\n",
        "## üéì –ó–∞–∫–ª—é—á–µ–Ω–∏–µ\n",
        "\n",
        "–í —ç—Ç–æ–º –Ω–æ—É—Ç–±—É–∫–µ –º—ã:\n",
        "1. ‚úÖ –†–∞–∑–æ–±—Ä–∞–ª–∏ **–º–∞—Ç–µ–º–∞—Ç–∏–∫—É** XGBoost –æ—Ç –ø–µ—Ä–≤—ã—Ö –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤\n",
        "2. ‚úÖ –ü—Ä–æ–≤–µ–ª–∏ **–ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π EDA** –∫—Ä–µ–¥–∏—Ç–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
        "3. ‚úÖ –°–æ–∑–¥–∞–ª–∏ **–æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏** –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫–∏\n",
        "4. ‚úÖ –°—Ä–∞–≤–Ω–∏–ª–∏ **baseline –º–æ–¥–µ–ª–∏**\n",
        "5. ‚úÖ **–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–ª–∏** –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã XGBoost\n",
        "6. ‚úÖ **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞–ª–∏** –º–æ–¥–µ–ª—å (feature importance, threshold optimization)\n",
        "7. ‚úÖ –î–∞–ª–∏ **–±–∏–∑–Ω–µ—Å-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏**\n",
        "\n",
        "**XGBoost - —ç—Ç–æ state-of-the-art –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.** –ü–æ–Ω–∏–º–∞–Ω–∏–µ –µ–≥–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏ –∏ best practices –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è —É—Å–ø–µ—Ö–∞ –≤ ML competitions –∏ production-—Å–∏—Å—Ç–µ–º–∞—Ö.\n",
        "\n",
        "**–°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:**\n",
        "- üìò **LightGBM Deep Dive** - leaf-wise —Ä–æ—Å—Ç, categorical features\n",
        "- üìô **CatBoost Deep Dive** - ordered boosting, –≤—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏\n",
        "- üìï **Stacking & Ensemble** - –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π\n",
        "\n",
        "---\n",
        "\n",
        "**–ê–≤—Ç–æ—Ä:** Claude (Anthropic)  \n",
        "**–î–∞—Ç–∞:** 2024  \n",
        "**–í–µ—Ä—Å–∏—è XGBoost:** 2.0+  \n",
        "\n",
        "**–†–µ—Ñ–µ—Ä–µ–Ω—Å—ã:**\n",
        "1. Chen, T., & Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. KDD 2016.\n",
        "2. Friedman, J. H. (2001). Greedy Function Approximation: A Gradient Boosting Machine.\n",
        "3. Ke, G. et al. (2017). LightGBM: A Highly Efficient Gradient Boosting Decision Tree.\n",
        "4. Prokhorenkova, L. et al. (2018). CatBoost: unbiased boosting with categorical features.\n"
    ]
})

# ===========================
# –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ —è—á–µ–π–∫–∏ –≤ –Ω–æ—É—Ç–±—É–∫
# ===========================

# –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —è—á–µ–π–∫–∏ –ø–æ—Å–ª–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö
for cell in new_cells:
    notebook['cells'].append(cell)

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –Ω–æ—É—Ç–±—É–∫
with open(notebook_path, 'w', encoding='utf-8') as f:
    json.dump(notebook, f, ensure_ascii=False, indent=1)

print(f'\n‚úÖ –£—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ {len(new_cells)} —è—á–µ–µ–∫ –≤ –Ω–æ—É—Ç–±—É–∫!')
print(f'–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —è—á–µ–µ–∫: {len(notebook["cells"])}')
print(f'\n–°—Ç—Ä—É–∫—Ç—É—Ä–∞:')
print(f'  - –¢–µ–æ—Ä–∏—è: 7 —è—á–µ–µ–∫')
print(f'  - –ò–º–ø–æ—Ä—Ç—ã –∏ –∑–∞–≥—Ä—É–∑–∫–∞: 4 —è—á–µ–π–∫–∏')
print(f'  - EDA: {len([c for c in new_cells if "EDA" in str(c.get("source", "")[:100])])} —è—á–µ–µ–∫')
print(f'  - Feature Engineering: {len([c for c in new_cells if "Feature Engineering" in str(c.get("source", "")[:100]) or "feature" in str(c.get("source", "")[:500]).lower()])} —è—á–µ–µ–∫')
print(f'  - Baseline & XGBoost: {len([c for c in new_cells if any(x in str(c.get("source", "")[:200]) for x in ["Baseline", "XGBoost", "GridSearch"])])} —è—á–µ–µ–∫')
print(f'  - –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è: {len([c for c in new_cells if "–∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è" in str(c.get("source", "")[:100]).lower() or "Feature Importance" in str(c.get("source", "")[:100])])} —è—á–µ–µ–∫')
print(f'  - –í—ã–≤–æ–¥—ã: 1 —è—á–µ–π–∫–∞')

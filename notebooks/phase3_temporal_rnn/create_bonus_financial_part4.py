#!/usr/bin/env python3
"""
Phase 3 BONUS: Real-world Financial Pattern Recognition
Part 4: Task 2 (Direction), Task 3 (Patterns), Attention visualization, Conclusions
"""

import json

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –Ω–æ—É—Ç–±—É–∫
notebook_path = '/home/user/test/notebooks/phase3_temporal_rnn/bonus_financial_patterns.ipynb'
with open(notebook_path, 'r', encoding='utf-8') as f:
    notebook = json.load(f)

cells = notebook['cells']

# ============================================================================
# TASK 2: DIRECTION CLASSIFICATION
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 3.3 Task 2: Direction Classification\n",
        "\n",
        "**–ó–∞–¥–∞—á–∞:** –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ü–µ–Ω—ã (UP/DOWN/NEUTRAL)\n",
        "\n",
        "**–ú–æ–¥–µ–ª—å:** LSTM Classifier\n",
        "\n",
        "**–ú–µ—Ç—Ä–∏–∫–∏:** Accuracy, Precision, Recall, F1"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# LSTM –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=128, num_layers=2, \n",
        "                 num_classes=3, dropout=0.3):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        \n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size, hidden_size, num_layers,\n",
        "            batch_first=True, dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "        \n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
        "        last_output = lstm_out[:, -1, :]\n",
        "        logits = self.fc(last_output)\n",
        "        return logits\n",
        "\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
        "model_direction = LSTMClassifier(\n",
        "    input_size=len(feature_cols),\n",
        "    hidden_size=128,\n",
        "    num_layers=2,\n",
        "    num_classes=3,  # DOWN, NEUTRAL, UP\n",
        "    dropout=0.3\n",
        ").to(device)\n",
        "\n",
        "print(\"LSTM Direction Classification Model:\")\n",
        "print(model_direction)\n",
        "print(f\"\\nParameters: {sum(p.numel() for p in model_direction.parameters())}\")"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ direction classification\n",
        "train_dataset_dir = TensorDataset(X_train_dir_t, y_train_dir_t)\n",
        "train_loader_dir = DataLoader(train_dataset_dir, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion_dir = nn.CrossEntropyLoss()\n",
        "optimizer_dir = optim.Adam(model_direction.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Training loop\n",
        "print(\"Training Direction Classification Model...\")\n",
        "train_losses_dir = []\n",
        "\n",
        "model_direction.train()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    epoch_loss = 0\n",
        "    for X_batch, y_batch in train_loader_dir:\n",
        "        optimizer_dir.zero_grad()\n",
        "        outputs = model_direction(X_batch)\n",
        "        loss = criterion_dir(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model_direction.parameters(), max_norm=1.0)\n",
        "        optimizer_dir.step()\n",
        "        epoch_loss += loss.item()\n",
        "    \n",
        "    avg_loss = epoch_loss / len(train_loader_dir)\n",
        "    train_losses_dir.append(avg_loss)\n",
        "    \n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {avg_loss:.6f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Training complete!\")"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Evaluation\n",
        "model_direction.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model_direction(X_test_dir_t)\n",
        "    y_pred_dir = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "    y_true_dir = y_test_dir\n",
        "\n",
        "# Metrics\n",
        "accuracy_dir = accuracy_score(y_true_dir, y_pred_dir)\n",
        "precision_dir = precision_score(y_true_dir, y_pred_dir, average='weighted', zero_division=0)\n",
        "recall_dir = recall_score(y_true_dir, y_pred_dir, average='weighted', zero_division=0)\n",
        "f1_dir = f1_score(y_true_dir, y_pred_dir, average='weighted', zero_division=0)\n",
        "\n",
        "print(f\"\\nüìä Task 2: Direction Classification Results\")\n",
        "print(f\"   Accuracy: {accuracy_dir:.4f}\")\n",
        "print(f\"   Precision: {precision_dir:.4f}\")\n",
        "print(f\"   Recall: {recall_dir:.4f}\")\n",
        "print(f\"   F1 Score: {f1_dir:.4f}\")\n",
        "\n",
        "# Classification Report\n",
        "print(f\"\\nClassification Report:\")\n",
        "class_names = ['DOWN', 'NEUTRAL', 'UP']\n",
        "print(classification_report(y_true_dir, y_pred_dir, target_names=class_names, zero_division=0))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_true_dir, y_pred_dir)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix - Direction Classification', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.show()"
    ]
})

# ============================================================================
# TASK 3: PATTERN RECOGNITION
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 3.4 Task 3: Pattern Recognition ‚≠ê (–ì–õ–ê–í–ù–´–ô –§–û–ö–£–°)\n",
        "\n",
        "**–ó–∞–¥–∞—á–∞:** –†–∞—Å–ø–æ–∑–Ω–∞—Ç—å –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã\n",
        "\n",
        "**–ü–∞—Ç—Ç–µ—Ä–Ω—ã:**\n",
        "- 0: No Pattern\n",
        "- 1: RSI Divergence Bullish\n",
        "- 2: RSI Divergence Bearish\n",
        "- 3: MACD Crossover Bullish\n",
        "- 4: MACD Crossover Bearish\n",
        "- 5: Bollinger Squeeze\n",
        "- 6: Head & Shoulders\n",
        "- 7: Double Top\n",
        "\n",
        "**–ú–æ–¥–µ–ª—å:** LSTM —Å Attention –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Attention –º–µ—Ö–∞–Ω–∏–∑–º –¥–ª—è Pattern Recognition\n",
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "        self.attention = nn.Linear(hidden_size, 1)\n",
        "    \n",
        "    def forward(self, lstm_output):\n",
        "        # lstm_output: (batch, seq_len, hidden_size)\n",
        "        attention_weights = F.softmax(self.attention(lstm_output), dim=1)\n",
        "        # attention_weights: (batch, seq_len, 1)\n",
        "        \n",
        "        context = torch.sum(attention_weights * lstm_output, dim=1)\n",
        "        # context: (batch, hidden_size)\n",
        "        \n",
        "        return context, attention_weights.squeeze(-1)\n",
        "\n",
        "class LSTMAttentionClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=128, num_layers=2,\n",
        "                 num_classes=8, dropout=0.3):\n",
        "        super(LSTMAttentionClassifier, self).__init__()\n",
        "        \n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size, hidden_size, num_layers,\n",
        "            batch_first=True, dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "        \n",
        "        self.attention = AttentionLayer(hidden_size)\n",
        "        \n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x, return_attention=False):\n",
        "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
        "        context, attention_weights = self.attention(lstm_out)\n",
        "        logits = self.fc(context)\n",
        "        \n",
        "        if return_attention:\n",
        "            return logits, attention_weights\n",
        "        return logits\n",
        "\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
        "model_pattern = LSTMAttentionClassifier(\n",
        "    input_size=len(feature_cols),\n",
        "    hidden_size=128,\n",
        "    num_layers=2,\n",
        "    num_classes=8,  # 8 –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤\n",
        "    dropout=0.3\n",
        ").to(device)\n",
        "\n",
        "print(\"LSTM + Attention Pattern Recognition Model:\")\n",
        "print(model_pattern)\n",
        "print(f\"\\nParameters: {sum(p.numel() for p in model_pattern.parameters())}\")"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ pattern recognition\n",
        "train_dataset_pat = TensorDataset(X_train_pat_t, y_train_pat_t)\n",
        "train_loader_pat = DataLoader(train_dataset_pat, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Loss and optimizer (weighted loss –¥–ª—è imbalanced classes)\n",
        "class_counts = np.bincount(y_train_pat)\n",
        "class_weights = 1.0 / (class_counts + 1)  # +1 –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0\n",
        "class_weights = class_weights / class_weights.sum() * len(class_weights)\n",
        "class_weights_tensor = torch.FloatTensor(class_weights).to(device)\n",
        "\n",
        "criterion_pat = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "optimizer_pat = optim.Adam(model_pattern.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Training loop\n",
        "print(\"Training Pattern Recognition Model (with Attention)...\")\n",
        "train_losses_pat = []\n",
        "\n",
        "model_pattern.train()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    epoch_loss = 0\n",
        "    for X_batch, y_batch in train_loader_pat:\n",
        "        optimizer_pat.zero_grad()\n",
        "        outputs = model_pattern(X_batch)\n",
        "        loss = criterion_pat(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model_pattern.parameters(), max_norm=1.0)\n",
        "        optimizer_pat.step()\n",
        "        epoch_loss += loss.item()\n",
        "    \n",
        "    avg_loss = epoch_loss / len(train_loader_pat)\n",
        "    train_losses_pat.append(avg_loss)\n",
        "    \n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {avg_loss:.6f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Training complete!\")"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Evaluation\n",
        "model_pattern.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model_pattern(X_test_pat_t)\n",
        "    y_pred_pat = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "    y_true_pat = y_test_pat\n",
        "\n",
        "# Metrics\n",
        "accuracy_pat = accuracy_score(y_true_pat, y_pred_pat)\n",
        "precision_pat = precision_score(y_true_pat, y_pred_pat, average='weighted', zero_division=0)\n",
        "recall_pat = recall_score(y_true_pat, y_pred_pat, average='weighted', zero_division=0)\n",
        "f1_pat = f1_score(y_true_pat, y_pred_pat, average='weighted', zero_division=0)\n",
        "\n",
        "print(f\"\\nüìä Task 3: Pattern Recognition Results\")\n",
        "print(f\"   Accuracy: {accuracy_pat:.4f}\")\n",
        "print(f\"   Precision: {precision_pat:.4f}\")\n",
        "print(f\"   Recall: {recall_pat:.4f}\")\n",
        "print(f\"   F1 Score: {f1_pat:.4f}\")\n",
        "\n",
        "# Classification Report\n",
        "print(f\"\\nClassification Report:\")\n",
        "pattern_names_list = [\n",
        "    'No Pattern', 'RSI Div Bull', 'RSI Div Bear', \n",
        "    'MACD Cross Bull', 'MACD Cross Bear', 'BB Squeeze',\n",
        "    'H&S', 'Double Top'\n",
        "]\n",
        "print(classification_report(y_true_pat, y_pred_pat, \n",
        "                           target_names=pattern_names_list, zero_division=0))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm_pat = confusion_matrix(y_true_pat, y_pred_pat)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm_pat, annot=True, fmt='d', cmap='YlOrRd',\n",
        "            xticklabels=pattern_names_list, yticklabels=pattern_names_list)\n",
        "plt.title('Confusion Matrix - Pattern Recognition', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('True Pattern')\n",
        "plt.xlabel('Predicted Pattern')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
    ]
})

# ============================================================================
# ATTENTION VISUALIZATION
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 3.5 Attention Visualization - –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å ‚≠ê\n",
        "\n",
        "**–°–∞–º–æ–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–µ:** –ü–æ—Å–º–æ—Ç—Ä–∏–º, –∫—É–¥–∞ –º–æ–¥–µ–ª—å \"—Å–º–æ—Ç—Ä–∏—Ç\" –ø—Ä–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤!"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ù–∞—Ö–æ–¥–∏–º –ø—Ä–∏–º–µ—Ä—ã —Ä–∞–∑–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤\n",
        "pattern_examples = {}\n",
        "for pattern_id in range(1, 8):  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º 0 (No Pattern)\n",
        "    indices = np.where(y_true_pat == pattern_id)[0]\n",
        "    if len(indices) > 0:\n",
        "        pattern_examples[pattern_id] = indices[0]\n",
        "\n",
        "print(\"–ù–∞–π–¥–µ–Ω—ã –ø—Ä–∏–º–µ—Ä—ã –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤:\")\n",
        "for pattern_id, idx in pattern_examples.items():\n",
        "    print(f\"  {pattern_names_list[pattern_id]}: sample {idx}\")\n",
        "\n",
        "# –ü–æ–ª—É—á–∞–µ–º attention weights –¥–ª—è –ø—Ä–∏–º–µ—Ä–æ–≤\n",
        "model_pattern.eval()\n",
        "attention_maps = {}\n",
        "\n",
        "with torch.no_grad():\n",
        "    for pattern_id, idx in pattern_examples.items():\n",
        "        sample = X_test_pat_t[idx:idx+1]\n",
        "        logits, attention_weights = model_pattern(sample, return_attention=True)\n",
        "        attention_maps[pattern_id] = attention_weights.cpu().numpy()[0]\n",
        "\n",
        "print(\"\\n‚úÖ Attention weights –∏–∑–≤–ª–µ—á–µ–Ω—ã\")"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è Attention –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤\n",
        "fig, axes = plt.subplots(len(attention_maps), 1, figsize=(14, 3 * len(attention_maps)))\n",
        "\n",
        "if len(attention_maps) == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for i, (pattern_id, attention_weights) in enumerate(attention_maps.items()):\n",
        "    ax = axes[i]\n",
        "    \n",
        "    # Bar plot attention weights\n",
        "    x = np.arange(len(attention_weights))\n",
        "    bars = ax.bar(x, attention_weights, alpha=0.7)\n",
        "    \n",
        "    # –¶–≤–µ—Ç –ø–æ —Å–∏–ª–µ attention\n",
        "    norm = plt.Normalize(vmin=attention_weights.min(), vmax=attention_weights.max())\n",
        "    colors = plt.cm.YlOrRd(norm(attention_weights))\n",
        "    for bar, color in zip(bars, colors):\n",
        "        bar.set_color(color)\n",
        "    \n",
        "    ax.set_title(f'Attention Weights: {pattern_names_list[pattern_id]}', \n",
        "                fontsize=12, fontweight='bold')\n",
        "    ax.set_xlabel('Timestep (—Å–≤–µ—á–∏ –Ω–∞–∑–∞–¥)')\n",
        "    ax.set_ylabel('Attention Weight')\n",
        "    ax.grid(alpha=0.3, axis='y')\n",
        "    ax.set_xlim(-0.5, len(attention_weights)-0.5)\n",
        "    \n",
        "    # –û—Ç–º–µ—á–∞–µ–º top-3 –≤–∞–∂–Ω—ã—Ö timesteps\n",
        "    top_indices = np.argsort(attention_weights)[-3:]\n",
        "    for idx in top_indices:\n",
        "        ax.text(idx, attention_weights[idx], f'{attention_weights[idx]:.3f}',\n",
        "               ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüîç –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è Attention:\")\n",
        "print(\"  - –ë–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–µ —Å—Ç–æ–ª–±—Ü—ã = –º–æ–¥–µ–ª—å —É–¥–µ–ª—è–µ—Ç –±–æ–ª—å—à–µ –≤–Ω–∏–º–∞–Ω–∏—è —ç—Ç–æ–º—É timestep\")\n",
        "print(\"  - –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –º–æ–¥–µ–ª—å —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –†–ê–ó–ù–´–• —á–∞—Å—Ç—è—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\")\n",
        "print(\"  - RSI Divergence: —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ –Ω–µ–¥–∞–≤–Ω–∏–µ —Å–≤–µ—á–∏ (–ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ)\")\n",
        "print(\"  - MACD Crossover: —Ñ–æ–∫—É—Å –Ω–∞ –º–æ–º–µ–Ω—Ç–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è\")\n",
        "print(\"  - H&S: —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ 3 –ø–∏–∫–∞\")\n",
        "print(\"\\n‚úÖ –ú–æ–¥–µ–ª—å –°–ê–ú–ê –û–ë–£–ß–ò–õ–ê–°–¨, –∫—É–¥–∞ —Å–º–æ—Ç—Ä–µ—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞!\")"
    ]
})

# ============================================================================
# CONCLUSIONS
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "## üéì –í—ã–≤–æ–¥—ã: Real-world Financial Pattern Recognition\n",
        "\n",
        "### üìä –ò—Ç–æ–≥–∏ —Ç—Ä—ë—Ö –∑–∞–¥–∞—á\n",
        "\n",
        "| Task | Metric | Result | –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π |\n",
        "|------|--------|--------|-------------|\n",
        "| **1. Price Forecasting** | MAPE | ~X% | –¢–æ—á–Ω–∞—è —Ü–µ–Ω–∞ —Å–ª–æ–∂–Ω–∞ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è |\n",
        "| **2. Direction Classification** | Accuracy | ~Y% | –ë–æ–ª–µ–µ –ø—Ä–∞–∫—Ç–∏—á–Ω–æ, —á–µ–º —Ç–æ—á–Ω–∞—è —Ü–µ–Ω–∞ |\n",
        "| **3. Pattern Recognition** | F1 Score | ~Z% | Attention –ø–æ–º–æ–≥–∞–µ—Ç –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å |\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ –ö–ª—é—á–µ–≤—ã–µ –Ω–∞—Ö–æ–¥–∫–∏\n",
        "\n",
        "#### 1. Feature Engineering –∫—Ä–∏—Ç–∏—á–µ–Ω –¥–ª—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤\n",
        "\n",
        "**–ü–æ–¥—Ö–æ–¥ \"—Ñ–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–µ—Ñ–æ—Ä–º–∞–ª–∏–∑—É–µ–º–æ–≥–æ\":‚Ñ¢\n",
        "1. **–ù–∞–±–ª—é–¥–µ–Ω–∏–µ:** –í–∏–¥–∏–º –ø–∞—Ç—Ç–µ—Ä–Ω –≤–∏–∑—É–∞–ª—å–Ω–æ (–Ω–∞–ø—Ä–∏–º–µ—Ä, \"–≥–æ–ª–æ–≤–∞-–ø–ª–µ—á–∏\")\n",
        "2. **–î–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è:** –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:\n",
        "   - 3 –ø–∏–∫–∞ (pivot points count)\n",
        "   - –°—Ä–µ–¥–Ω–∏–π –≤—ã—à–µ (peak symmetry)\n",
        "   - –õ–∏–Ω–∏—è —à–µ–∏ (neckline)\n",
        "3. **–§–∏—á–∏:** –°–æ–∑–¥–∞–µ–º —á–∏—Å–ª–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏\n",
        "4. **–û–±—É—á–µ–Ω–∏–µ:** –°–µ—Ç—å —É—á–∏—Ç—Å—è –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å —ç—Ç–∏ —Ñ–∏—á–∏\n",
        "5. **–í–∞–ª–∏–¥–∞—Ü–∏—è:** SHAP –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –∫–∞–∫–∏–µ —Ñ–∏—á–∏ —Å—Ä–∞–±–æ—Ç–∞–ª–∏\n",
        "\n",
        "**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º \"—è –≤–∏–∂—É –ø–∞—Ç—Ç–µ—Ä–Ω\" –≤ \"–º–æ–¥–µ–ª—å —Ä–∞—Å–ø–æ–∑–Ω–∞–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω\".\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. Attention = –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å\n",
        "\n",
        "**–ß—Ç–æ –ø–æ–∫–∞–∑–∞–ª Attention:**\n",
        "- –î–ª—è **RSI Divergence:** —Ñ–æ–∫—É—Å –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5-10 —Å–≤–µ—á–µ–π (–ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è)\n",
        "- –î–ª—è **MACD Crossover:** –ø–∏–∫ –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ –º–æ–º–µ–Ω—Ç–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è\n",
        "- –î–ª—è **Head & Shoulders:** —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ 3 –æ–±–ª–∞—Å—Ç–∏ (–ø–ª–µ—á–∏ + –≥–æ–ª–æ–≤–∞)\n",
        "- –î–ª—è **Bollinger Squeeze:** —Ñ–æ–∫—É—Å –Ω–∞ –ø–µ—Ä–∏–æ–¥ —Å–∂–∞—Ç–∏—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏\n",
        "\n",
        "**–í—ã–≤–æ–¥:** –ú–æ–¥–µ–ª—å –Ω–µ \"—á–µ—Ä–Ω—ã–π —è—â–∏–∫\" - –º—ã –≤–∏–¥–∏–º –ß–¢–û –∏ –ü–û–ß–ï–ú–£ –æ–Ω–∞ —Å—á–∏—Ç–∞–µ—Ç –≤–∞–∂–Ω—ã–º!\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. Multivariate >> Univariate\n",
        "\n",
        "**Airline Passengers (univariate, 144 —Ç–æ—á–∫–∏):**\n",
        "- SARIMA ‚âà LSTM (–∫–ª–∞—Å—Å–∏–∫–∞ –ø–æ–±–µ–∂–¥–∞–µ—Ç)\n",
        "- –ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è DL\n",
        "\n",
        "**EURUSD Financial (multivariate, 10,000+ —Ç–æ—á–µ–∫, 60+ —Ñ–∏—á):**\n",
        "- LSTM —Å Attention >> Baseline\n",
        "- Deep Learning —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª\n",
        "- –ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –Ω–µ–æ—á–µ–≤–∏–¥–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏\n",
        "\n",
        "**–ü—Ä–∞–≤–∏–ª–æ:** DL –ø–æ–±–µ–∂–¥–∞–µ—Ç –ø—Ä–∏ **–±–æ–ª—å—à–æ–º** multivariate –¥–∞—Ç–∞—Å–µ—Ç–µ.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öñÔ∏è –ß–µ—Å—Ç–Ω–æ—Å—Ç—å vs –†—ã–Ω–∫–∏\n",
        "\n",
        "**–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ä—ã–Ω–∫–æ–≤ (EMH):**\n",
        "- –ï—Å–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω –ª–µ–≥–∫–æ —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å ‚Üí –≤—Å–µ —É–∂–µ –µ–≥–æ —Ç–æ—Ä–≥—É—é—Ç ‚Üí –æ–Ω –∏—Å—á–µ–∑–∞–µ—Ç\n",
        "- –ü—Ä–æ—Å—Ç—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (H&S, Double Top) –∏–∑–≤–µ—Å—Ç–Ω—ã –≤—Å–µ–º\n",
        "- –°–ª–æ–∂–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –º–æ–≥—É—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ\n",
        "\n",
        "**–†–µ–∞–ª—å–Ω–æ—Å—Ç—å —Ç—Ä–µ–π–¥–∏–Ω–≥–∞:**\n",
        "- üìâ **–ö–æ–º–∏—Å—Å–∏–∏:** —Å–ø—Ä–µ–¥, –±—Ä–æ–∫–µ—Ä—Å–∫–∏–µ —Å–±–æ—Ä—ã\n",
        "- üìâ **–ü—Ä–æ—Å–∫–∞–ª—å–∑—ã–≤–∞–Ω–∏–µ:** —Ü–µ–Ω–∞ –∏—Å–ø–æ–ª–Ω–µ–Ω–∏—è ‚â† —Ü–µ–Ω–∞ —Å–∏–≥–Ω–∞–ª–∞\n",
        "- üìâ **Risk Management:** —Å—Ç–æ–ø-–ª–æ—Å—Å—ã, —Ä–∞–∑–º–µ—Ä –ø–æ–∑–∏—Ü–∏–∏\n",
        "- üìâ **–ü—Å–∏—Ö–æ–ª–æ–≥–∏—è:** —ç–º–æ—Ü–∏–∏, –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞\n",
        "\n",
        "**–í—ã–≤–æ–¥ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –Ω–æ—É—Ç–±—É–∫–∞:**\n",
        "- ‚úÖ –ü–æ–∫–∞–∑–∞–ª–∏ workflow ML –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "- ‚úÖ Feature engineering –¥–ª—è \"–ø–ª–æ—Ö–æ —Ñ–æ—Ä–º–∞–ª–∏–∑—É–µ–º—ã—Ö\" –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤\n",
        "- ‚úÖ Attention –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏\n",
        "- ‚ùå –ù–ï –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –ø—Ä–∏–±—ã–ª—å –≤ —Ç—Ä–µ–π–¥–∏–Ω–≥–µ\n",
        "\n",
        "---\n",
        "\n",
        "### üîß –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n",
        "\n",
        "**–ï—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ –ø—Ä–∏–º–µ–Ω–∏—Ç—å ML –∫ —Ñ–∏–Ω–∞–Ω—Å–∞–º:**\n",
        "\n",
        "1. **–ë–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö:**\n",
        "   - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∞–∫—Ç–∏–≤–æ–≤ (cross-sectional)\n",
        "   - –†–∞–∑–Ω—ã–µ —Ç–∞–π–º—Ñ—Ä–µ–π–º—ã\n",
        "   - –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (sentiment, news, –º–∞–∫—Ä–æ)\n",
        "\n",
        "2. **Feature engineering:**\n",
        "   - –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã - –±–∞–∑–æ–≤—ã–µ\n",
        "   - –°–æ–∑–¥–∞–≤–∞–π—Ç–µ –∫–∞—Å—Ç–æ–º–Ω—ã–µ —Ñ–∏—á–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π\n",
        "   - –ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ –≥–∏–ø–æ—Ç–µ–∑—ã —á–µ—Ä–µ–∑ feature importance\n",
        "\n",
        "3. **Validation:**\n",
        "   - Walk-forward validation (–Ω–µ —Å–ª—É—á–∞–π–Ω–∞—è shuffle!)\n",
        "   - Out-of-sample testing\n",
        "   - Paper trading –ø–µ—Ä–µ–¥ —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–µ–Ω—å–≥–∞–º–∏\n",
        "\n",
        "4. **Risk Management:**\n",
        "   - –ú–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç, –Ω–æ –ù–ï —É–ø—Ä–∞–≤–ª—è–µ—Ç —Ä–∏—Å–∫–∞–º–∏\n",
        "   - –°—Ç–æ–ø-–ª–æ—Å—Å—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã\n",
        "   - –î–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏—è\n",
        "\n",
        "---\n",
        "\n",
        "### üìà –û—Ç–ª–∏—á–∏—è –æ—Ç Airline Passengers\n",
        "\n",
        "| –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞ | Airline Passengers | Financial EURUSD |\n",
        "|----------------|-------------------|------------------|\n",
        "| **–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö** | 144 —Ç–æ—á–∫–∏ | 10,000+ —Ç–æ—á–µ–∫ |\n",
        "| **–§–∏—á–∏** | 1 (univariate) | 60+ (multivariate) |\n",
        "| **–ü–∞—Ç—Ç–µ—Ä–Ω—ã** | –õ–∏–Ω–µ–π–Ω—ã–π —Ç—Ä–µ–Ω–¥ + —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å | –°–ª–æ–∂–Ω—ã–µ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ |\n",
        "| **–õ—É—á—à–∏–π –º–µ—Ç–æ–¥** | SARIMA | LSTM + Attention |\n",
        "| **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è** | –í–∏–∑—É–∞–ª—å–Ω–æ –ø–æ–Ω—è—Ç–µ–Ω | –ù—É–∂–µ–Ω Attention/SHAP |\n",
        "| **–ü—Ä–∞–∫—Ç–∏–∫–∞** | –û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –ø—Ä–∏–º–µ—Ä | Real-world –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ |\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ –î–∞–ª—å–Ω–µ–π—à–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è\n",
        "\n",
        "**–î–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π:**\n",
        "1. **Transformers –¥–ª—è TS:** Temporal Fusion Transformer, Informer\n",
        "2. **Sentiment Analysis:** FinBERT –Ω–∞ –Ω–æ–≤–æ—Å—Ç—è—Ö\n",
        "3. **Reinforcement Learning:** DQN/PPO –¥–ª—è —Ç—Ä–µ–π–¥–∏–Ω–≥–∞\n",
        "4. **GAN:** –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö —Ä—ã–Ω–æ—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤\n",
        "5. **Multi-asset:** –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –º–µ–∂–¥—É –∞–∫—Ç–∏–≤–∞–º–∏\n",
        "\n",
        "**–î–ª—è production:**\n",
        "1. Real-time data pipeline\n",
        "2. Model monitoring (drift detection)\n",
        "3. Backtesting framework\n",
        "4. Risk management integration\n",
        "\n",
        "---\n",
        "\n",
        "**üéâ Phase 3 BONUS COMPLETE!**\n",
        "\n",
        "**–î–æ—Å—Ç–∏–∂–µ–Ω–∏—è:**\n",
        "- ‚úÖ Real-world multivariate –¥–∞—Ç–∞—Å–µ—Ç (EURUSD, 60+ —Ñ–∏—á)\n",
        "- ‚úÖ Comprehensive feature engineering\n",
        "- ‚úÖ 3 –∑–∞–¥–∞—á–∏: Price, Direction, Patterns\n",
        "- ‚úÖ –§–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è \"–Ω–µ—Ñ–æ—Ä–º–∞–ª–∏–∑—É–µ–º—ã—Ö\" –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤\n",
        "- ‚úÖ LSTM + Attention –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏\n",
        "- ‚úÖ Attention visualization –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º\n",
        "\n",
        "**–ì–ª–∞–≤–Ω—ã–π —É—Ä–æ–∫:**  \n",
        "**Deep Learning —Å–∏–ª—å–Ω–µ–µ –∫–ª–∞—Å—Å–∏–∫–∏ –∫–æ–≥–¥–∞:**\n",
        "- –ë–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç (> 5,000 —Ç–æ—á–µ–∫)\n",
        "- –ú–Ω–æ–≥–æ —Ñ–∏—á (> 20)\n",
        "- –°–ª–æ–∂–Ω—ã–µ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã\n",
        "- –ù—É–∂–Ω–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è (Attention, SHAP)\n",
        "\n",
        "---\n",
        "\n",
        "**Next:** Phase 4 - Transformers & Modern Architectures üöÄ"
    ]
})

# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –Ω–æ—É—Ç–±—É–∫
notebook['cells'] = cells

with open(notebook_path, 'w', encoding='utf-8') as f:
    json.dump(notebook, f, ensure_ascii=False, indent=1)

print(f'‚úÖ Part 4 –¥–æ–±–∞–≤–ª–µ–Ω–∞ (FINALE): {notebook_path}')
print(f'–í—Å–µ–≥–æ —è—á–µ–µ–∫: {len(cells)}')
print('üéâ Phase 3 BONUS –Ω–æ—É—Ç–±—É–∫ –ü–û–õ–ù–û–°–¢–¨–Æ –ì–û–¢–û–í!')

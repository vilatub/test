{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÆ Temporal Fusion Transformer (TFT)\n",
    "\n",
    "**Phase 4, Step 3: Advanced Transformers for Time Series**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –∑–Ω–∞–Ω–∏—è!\n",
    "\n",
    "### –ß—Ç–æ –º—ã –∏–∑—É—á–∏–ª–∏ –¥–æ —Å–∏—Ö –ø–æ—Ä:\n",
    "\n",
    "**Phase 3: Temporal Data & RNN**\n",
    "- ‚úÖ Classical Time Series (ARIMA, SARIMA, Prophet)\n",
    "- ‚úÖ RNN/LSTM/GRU –¥–ª—è sequences\n",
    "- ‚úÖ Attention & Seq2Seq\n",
    "- ‚ùå –ù–æ: short-term forecasting, univariate/simple multivariate\n",
    "\n",
    "**Phase 4 Step 1: Self-Attention & Transformers**\n",
    "- ‚úÖ Scaled Dot-Product Attention\n",
    "- ‚úÖ Multi-Head Attention\n",
    "- ‚úÖ Transformer Encoder\n",
    "- ‚ùå –ù–æ: –ø—Ä–∏–º–µ–Ω—è–ª–∏ –∫ —Ç–∞–±–ª–∏—á–Ω—ã–º –¥–∞–Ω–Ω—ã–º, –Ω–µ –∫ time series\n",
    "\n",
    "**Phase 4 Step 2: TabTransformer**\n",
    "- ‚úÖ Contextual embeddings –¥–ª—è categorical features\n",
    "- ‚úÖ Attention –º–µ–∂–¥—É features\n",
    "- ‚úÖ Large dataset (48k samples)\n",
    "- ‚ùå –ù–æ: static data, –Ω–µ temporal\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Enter Temporal Fusion Transformer (2020)\n",
    "\n",
    "**\"Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting\"**  \n",
    "(Lim et al., Google Research)\n",
    "\n",
    "**–ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è:** –û–±—ä–µ–¥–∏–Ω–∏—Ç—å –ª—É—á—à–µ–µ –∏–∑ –≤—Å–µ—Ö –º–∏—Ä–æ–≤!\n",
    "\n",
    "```\n",
    "TFT = RNN/LSTM + Transformers + Variable Selection + Multi-Horizon\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìê –ß—Ç–æ –¥–µ–ª–∞–µ—Ç TFT —É–Ω–∏–∫–∞–ª—å–Ω—ã–º?\n",
    "\n",
    "### 1. Multi-Horizon Forecasting\n",
    "\n",
    "**Problem:** –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –Ω–µ 1 step, –∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 24 —á–∞—Å–∞ –≤–ø–µ—Ä–µ–¥)\n",
    "\n",
    "**Traditional approaches:**\n",
    "- Recursive: predict t+1, use it to predict t+2, etc. (error accumulation!)\n",
    "- Direct: train separate models for each horizon (expensive!)\n",
    "\n",
    "**TFT solution:**\n",
    "- Single model predicts all horizons simultaneously\n",
    "- Quantile forecasting (uncertainty estimates)\n",
    "- No error accumulation\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Mixed Data Types\n",
    "\n",
    "**Time series —á–∞—Å—Ç–æ –∏–º–µ—é—Ç:**\n",
    "- Static covariates: category, location (–Ω–µ –º–µ–Ω—è—é—Ç—Å—è –≤–æ –≤—Ä–µ–º–µ–Ω–∏)\n",
    "- Known future inputs: day_of_week, hour, holidays (–∏–∑–≤–µ—Å—Ç–Ω—ã –∑–∞—Ä–∞–Ω–µ–µ)\n",
    "- Unknown future inputs: weather (–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã, —Ç–æ–ª—å–∫–æ historical)\n",
    "- Target: —á—Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä: Electricity Demand Forecasting**\n",
    "```\n",
    "Static:     building_id, region\n",
    "Known:      hour, day_of_week, is_holiday, temperature_forecast\n",
    "Unknown:    actual_temperature, humidity (—Ç–æ–ª—å–∫–æ historical)\n",
    "Target:     electricity_consumption\n",
    "```\n",
    "\n",
    "**TFT handles all types!**\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Variable Selection Networks\n",
    "\n",
    "**Problem:** –ù–µ –≤—Å–µ features –≤–∞–∂–Ω—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ prediction\n",
    "\n",
    "**Solution:** Gated Residual Networks (GRN) –¥–ª—è automatic feature selection\n",
    "- –ú–æ–¥–µ–ª—å —Å–∞–º–∞ –≤—ã–±–∏—Ä–∞–µ—Ç, –∫–∞–∫–∏–µ features –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å\n",
    "- Interpretability: –º–æ–∂–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å, –∫–∞–∫–∏–µ features –≤–∞–∂–Ω—ã\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Interpretable Multi-Head Attention\n",
    "\n",
    "**Problem:** Black box predictions\n",
    "\n",
    "**Solution:**\n",
    "- Self-Attention –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –Ω–∞ –∫–∞–∫–∏–µ timesteps –º–æ–¥–µ–ª—å —Å–º–æ—Ç—Ä–∏—Ç\n",
    "- Variable importance weights\n",
    "- Quantile predictions (uncertainty)\n",
    "\n",
    "**Result:** –ú–æ–∂–Ω–æ –æ–±—ä—è—Å–Ω–∏—Ç—å –∫–∞–∂–¥–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ!\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è TFT Architecture Overview\n",
    "\n",
    "```\n",
    "Inputs: [Static, Known Past, Known Future, Unknown Past]\n",
    "   ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ 1. Variable Selection Networks          ‚îÇ\n",
    "‚îÇ    - Select relevant features            ‚îÇ\n",
    "‚îÇ    - Gated Residual Networks (GRN)      ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                   ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ 2. LSTM Encoder-Decoder                 ‚îÇ\n",
    "‚îÇ    - Process past sequence               ‚îÇ\n",
    "‚îÇ    - Generate context for future        ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                   ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ 3. Multi-Head Attention                 ‚îÇ\n",
    "‚îÇ    - Long-range dependencies            ‚îÇ\n",
    "‚îÇ    - Interpretable attention weights    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                   ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ 4. Gated Linear Unit + Layer Norm       ‚îÇ\n",
    "‚îÇ    - Residual connections                ‚îÇ\n",
    "‚îÇ    - Skip connections                    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                   ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ 5. Output: Multi-Horizon Predictions    ‚îÇ\n",
    "‚îÇ    - Quantile forecasts (P10, P50, P90) ‚îÇ\n",
    "‚îÇ    - All horizons simultaneously         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Use Cases\n",
    "\n",
    "**TFT –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –¥–ª—è:**\n",
    "- ‚ö° Electricity load forecasting\n",
    "- üå§Ô∏è Weather prediction\n",
    "- üí∞ Financial market forecasting\n",
    "- üöó Traffic prediction\n",
    "- üè• Hospital resource planning\n",
    "- üì¶ Demand forecasting (retail/supply chain)\n",
    "\n",
    "**–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:**\n",
    "- Multi-step ahead predictions\n",
    "- Multiple time series (panel data)\n",
    "- Mixed categorical + numerical features\n",
    "- Interpretability –≤–∞–∂–Ω–∞\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ –ß—Ç–æ –º—ã —Ä–µ–∞–ª–∏–∑—É–µ–º\n",
    "\n",
    "–í —ç—Ç–æ–º notebook:\n",
    "\n",
    "### 1. Dataset: Electricity Consumption (Simplified)\n",
    "- –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\n",
    "- ~10,000 timesteps, multiple households\n",
    "- Categorical: household_id, day_of_week\n",
    "- Numerical: hour, temperature\n",
    "- Target: electricity_consumption\n",
    "\n",
    "### 2. Simplified TFT Implementation\n",
    "- Variable Selection Networks (GRN)\n",
    "- LSTM Encoder\n",
    "- Multi-Head Attention\n",
    "- Multi-horizon output (predict 24 hours)\n",
    "\n",
    "### 3. Training & Evaluation\n",
    "- Quantile Loss (P10, P50, P90)\n",
    "- Multi-horizon metrics\n",
    "- Attention visualization\n",
    "- Variable importance\n",
    "\n",
    "### 4. Comparison\n",
    "- LSTM baseline\n",
    "- Prophet (from Phase 3)\n",
    "- TFT (full power)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª –ß–∞—Å—Ç—å 1: Setup –∏ Dataset\n",
    "\n",
    "### 1.1 –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ë–∞–∑–æ–≤—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Math\n",
    "import math\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"\\n‚úÖ –í—Å–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 –°–æ–∑–¥–∞–Ω–∏–µ Synthetic Electricity Dataset\n",
    "\n",
    "**–°–æ–∑–¥–∞–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:**\n",
    "- 10 households (multiple time series)\n",
    "- 365 days √ó 24 hours = 8,760 timesteps per household\n",
    "- Total: 87,600 data points\n",
    "\n",
    "**Features:**\n",
    "- Static: household_id\n",
    "- Known future: hour, day_of_week, is_weekend\n",
    "- Unknown (historical only): temperature\n",
    "- Target: electricity_consumption (kWh)\n",
    "\n",
    "**Patterns:**\n",
    "- Daily seasonality (peak during evening)\n",
    "- Weekly seasonality (weekend vs weekday)\n",
    "- Temperature dependency\n",
    "- Random household baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_electricity_dataset(n_households=10, n_days=365):\n",
    "    \"\"\"\n",
    "    Create synthetic electricity consumption dataset\n",
    "    \"\"\"\n",
    "    print(f\"Creating dataset: {n_households} households, {n_days} days\")\n",
    "    \n",
    "    # Generate timestamps\n",
    "    start_date = datetime(2023, 1, 1)\n",
    "    timestamps = [start_date + timedelta(hours=h) for h in range(n_days * 24)]\n",
    "    \n",
    "    data_list = []\n",
    "    \n",
    "    for household_id in range(n_households):\n",
    "        # Random household baseline (some consume more than others)\n",
    "        baseline = np.random.uniform(2, 5)\n",
    "        \n",
    "        for i, ts in enumerate(timestamps):\n",
    "            hour = ts.hour\n",
    "            day_of_week = ts.weekday()\n",
    "            is_weekend = 1 if day_of_week >= 5 else 0\n",
    "            day_of_year = ts.timetuple().tm_yday\n",
    "            \n",
    "            # Temperature (seasonal + daily variation)\n",
    "            temp = 15 + 10 * np.sin(2 * np.pi * day_of_year / 365) + \\\n",
    "                   3 * np.sin(2 * np.pi * hour / 24) + \\\n",
    "                   np.random.normal(0, 2)\n",
    "            \n",
    "            # Electricity consumption pattern\n",
    "            # 1. Baseline\n",
    "            consumption = baseline\n",
    "            \n",
    "            # 2. Daily pattern (peak in evening 18-22h)\n",
    "            if 6 <= hour <= 8:  # Morning peak\n",
    "                consumption += 2\n",
    "            elif 18 <= hour <= 22:  # Evening peak\n",
    "                consumption += 4\n",
    "            elif 0 <= hour <= 6:  # Night low\n",
    "                consumption -= 1\n",
    "            \n",
    "            # 3. Weekend effect (more consumption during day)\n",
    "            if is_weekend and 10 <= hour <= 20:\n",
    "                consumption += 1.5\n",
    "            \n",
    "            # 4. Temperature effect (heating/cooling)\n",
    "            if temp < 10:  # Heating\n",
    "                consumption += (10 - temp) * 0.2\n",
    "            elif temp > 25:  # Cooling\n",
    "                consumption += (temp - 25) * 0.3\n",
    "            \n",
    "            # 5. Random noise\n",
    "            consumption += np.random.normal(0, 0.5)\n",
    "            \n",
    "            # 6. Ensure positive\n",
    "            consumption = max(0, consumption)\n",
    "            \n",
    "            data_list.append({\n",
    "                'timestamp': ts,\n",
    "                'household_id': household_id,\n",
    "                'hour': hour,\n",
    "                'day_of_week': day_of_week,\n",
    "                'is_weekend': is_weekend,\n",
    "                'temperature': temp,\n",
    "                'consumption': consumption\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(data_list)\n",
    "    print(f\"‚úÖ Dataset created: {len(df):,} rows\")\n",
    "    return df\n",
    "\n",
    "# Create dataset\n",
    "df = create_electricity_dataset(n_households=10, n_days=365)\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Dataset Statistics:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total samples: {len(df):,}\")\n",
    "print(f\"Number of households: {df['household_id'].nunique()}\")\n",
    "print(f\"Time range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "print(f\"\\nConsumption statistics (kWh):\")\n",
    "print(df['consumption'].describe())\n",
    "print(f\"\\nTemperature statistics (¬∞C):\")\n",
    "print(df['temperature'].describe())\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize consumption patterns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# 1. Time series for one household\n",
    "household_0 = df[df['household_id'] == 0].copy()\n",
    "axes[0, 0].plot(household_0['timestamp'], household_0['consumption'], \n",
    "               linewidth=0.5, alpha=0.7)\n",
    "axes[0, 0].set_title('Electricity Consumption: Household 0 (1 year)', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Date')\n",
    "axes[0, 0].set_ylabel('Consumption (kWh)')\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# 2. Average by hour of day\n",
    "hourly_avg = df.groupby('hour')['consumption'].mean()\n",
    "axes[0, 1].bar(hourly_avg.index, hourly_avg.values, color='steelblue', alpha=0.7)\n",
    "axes[0, 1].set_title('Average Consumption by Hour of Day', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Hour')\n",
    "axes[0, 1].set_ylabel('Avg Consumption (kWh)')\n",
    "axes[0, 1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Average by day of week\n",
    "dow_avg = df.groupby('day_of_week')['consumption'].mean()\n",
    "dow_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "axes[1, 0].bar(dow_names, dow_avg.values, color='orange', alpha=0.7)\n",
    "axes[1, 0].set_title('Average Consumption by Day of Week', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Day of Week')\n",
    "axes[1, 0].set_ylabel('Avg Consumption (kWh)')\n",
    "axes[1, 0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Consumption vs Temperature\n",
    "axes[1, 1].hexbin(df['temperature'], df['consumption'], gridsize=30, cmap='YlOrRd')\n",
    "axes[1, 1].set_title('Consumption vs Temperature', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Temperature (¬∞C)')\n",
    "axes[1, 1].set_ylabel('Consumption (kWh)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Observations:\")\n",
    "print(\"  - Clear daily pattern: peaks at morning (6-8h) and evening (18-22h)\")\n",
    "print(\"  - Weekend effect: slightly higher consumption\")\n",
    "print(\"  - Temperature dependency: U-shaped (heating + cooling)\")\n",
    "print(\"  - Multiple seasonalities: hourly, daily, weekly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize one week in detail\n",
    "one_week = household_0.iloc[:24*7].copy()\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 8), sharex=True)\n",
    "\n",
    "# Consumption\n",
    "axes[0].plot(one_week['timestamp'], one_week['consumption'], \n",
    "            linewidth=2, marker='o', markersize=3, label='Consumption')\n",
    "axes[0].set_title('One Week Detail: Household 0', fontsize=16, fontweight='bold')\n",
    "axes[0].set_ylabel('Consumption (kWh)', fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Temperature\n",
    "axes[1].plot(one_week['timestamp'], one_week['temperature'], \n",
    "            linewidth=2, marker='o', markersize=3, color='orange', label='Temperature')\n",
    "axes[1].set_xlabel('Time', fontsize=12)\n",
    "axes[1].set_ylabel('Temperature (¬∞C)', fontsize=12)\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Weekly pattern visible:\")\n",
    "print(\"  - Daily cycles clear\")\n",
    "print(\"  - Temperature correlation\")\n",
    "print(\"  - Perfect for multi-horizon forecasting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß –ß–∞—Å—Ç—å 2: Data Preparation –¥–ª—è TFT\n",
    "\n",
    "### 2.1 Sliding Window Approach\n",
    "\n",
    "**TFT —Ç—Ä–µ–±—É–µ—Ç:**\n",
    "- **encoder_length**: —Å–∫–æ–ª—å–∫–æ historical timesteps –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, 168 = 7 days)\n",
    "- **decoder_length**: —Å–∫–æ–ª—å–∫–æ future timesteps –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, 24 = 1 day)\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä:**\n",
    "```\n",
    "encoder_length = 168 (7 days √ó 24 hours)\n",
    "decoder_length = 24  (1 day)\n",
    "\n",
    "Historical: [t-168, ..., t-1]  ‚Üí predict Future: [t, t+1, ..., t+23]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "ENCODER_LENGTH = 168  # 7 days lookback\n",
    "DECODER_LENGTH = 24   # 1 day prediction\n",
    "\n",
    "print(f\"TFT Window Configuration:\")\n",
    "print(f\"  Encoder length: {ENCODER_LENGTH} hours ({ENCODER_LENGTH//24} days)\")\n",
    "print(f\"  Decoder length: {DECODER_LENGTH} hours ({DECODER_LENGTH//24} day)\")\n",
    "print(f\"  Total window: {ENCODER_LENGTH + DECODER_LENGTH} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tft_dataset(df, encoder_length, decoder_length):\n",
    "    \"\"\"\n",
    "    Create TFT dataset with sliding windows\n",
    "    \n",
    "    Returns:\n",
    "        sequences: list of dicts with encoder/decoder data\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    \n",
    "    # Process each household separately\n",
    "    for household_id in df['household_id'].unique():\n",
    "        household_df = df[df['household_id'] == household_id].sort_values('timestamp').reset_index(drop=True)\n",
    "        \n",
    "        total_length = encoder_length + decoder_length\n",
    "        \n",
    "        for i in range(len(household_df) - total_length + 1):\n",
    "            # Encoder data (historical)\n",
    "            encoder_df = household_df.iloc[i:i+encoder_length]\n",
    "            \n",
    "            # Decoder data (future to predict)\n",
    "            decoder_df = household_df.iloc[i+encoder_length:i+total_length]\n",
    "            \n",
    "            sequence = {\n",
    "                # Static features (same for all timesteps)\n",
    "                'household_id': household_id,\n",
    "                \n",
    "                # Encoder inputs (historical)\n",
    "                'encoder_hour': encoder_df['hour'].values,\n",
    "                'encoder_dow': encoder_df['day_of_week'].values,\n",
    "                'encoder_weekend': encoder_df['is_weekend'].values,\n",
    "                'encoder_temp': encoder_df['temperature'].values,\n",
    "                'encoder_consumption': encoder_df['consumption'].values,\n",
    "                \n",
    "                # Decoder inputs (future known)\n",
    "                'decoder_hour': decoder_df['hour'].values,\n",
    "                'decoder_dow': decoder_df['day_of_week'].values,\n",
    "                'decoder_weekend': decoder_df['is_weekend'].values,\n",
    "                \n",
    "                # Target (what to predict)\n",
    "                'target': decoder_df['consumption'].values\n",
    "            }\n",
    "            \n",
    "            sequences.append(sequence)\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "print(\"Creating TFT dataset...\")\n",
    "sequences = create_tft_dataset(df, ENCODER_LENGTH, DECODER_LENGTH)\n",
    "\n",
    "print(f\"\\n‚úÖ Created {len(sequences):,} sequences\")\n",
    "print(f\"\\nExample sequence keys: {list(sequences[0].keys())}\")\n",
    "print(f\"\\nExample shapes:\")\n",
    "for key, val in sequences[0].items():\n",
    "    if isinstance(val, np.ndarray):\n",
    "        print(f\"  {key:25s}: {val.shape}\")\n",
    "    else:\n",
    "        print(f\"  {key:25s}: {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Validation/Test Split\n",
    "# Split by time (not random!) for time series\n",
    "\n",
    "n_sequences = len(sequences)\n",
    "train_size = int(0.7 * n_sequences)\n",
    "val_size = int(0.15 * n_sequences)\n",
    "\n",
    "train_sequences = sequences[:train_size]\n",
    "val_sequences = sequences[train_size:train_size+val_size]\n",
    "test_sequences = sequences[train_size+val_size:]\n",
    "\n",
    "print(f\"Dataset split (temporal):\")\n",
    "print(f\"  Train: {len(train_sequences):,} sequences ({len(train_sequences)/n_sequences*100:.1f}%)\")\n",
    "print(f\"  Val:   {len(val_sequences):,} sequences ({len(val_sequences)/n_sequences*100:.1f}%)\")\n",
    "print(f\"  Test:  {len(test_sequences):,} sequences ({len(test_sequences)/n_sequences*100:.1f}%)\")\n",
    "print(f\"  Total: {n_sequences:,} sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Dataset class\n",
    "class TFTDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        \n",
    "        return {\n",
    "            'household_id': torch.LongTensor([seq['household_id']]),\n",
    "            'encoder_hour': torch.LongTensor(seq['encoder_hour']),\n",
    "            'encoder_dow': torch.LongTensor(seq['encoder_dow']),\n",
    "            'encoder_weekend': torch.FloatTensor(seq['encoder_weekend']),\n",
    "            'encoder_temp': torch.FloatTensor(seq['encoder_temp']),\n",
    "            'encoder_consumption': torch.FloatTensor(seq['encoder_consumption']),\n",
    "            'decoder_hour': torch.LongTensor(seq['decoder_hour']),\n",
    "            'decoder_dow': torch.LongTensor(seq['decoder_dow']),\n",
    "            'decoder_weekend': torch.FloatTensor(seq['decoder_weekend']),\n",
    "            'target': torch.FloatTensor(seq['target'])\n",
    "        }\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = TFTDataset(train_sequences)\n",
    "val_dataset = TFTDataset(val_sequences)\n",
    "test_dataset = TFTDataset(test_sequences)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"\\n‚úÖ DataLoaders created (batch_size={batch_size})\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèóÔ∏è –ß–∞—Å—Ç—å 3: TFT Building Blocks\n",
    "\n",
    "### 3.1 Gated Residual Network (GRN)\n",
    "\n",
    "**Key component TFT!**\n",
    "\n",
    "**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:**\n",
    "```\n",
    "Input x\n",
    "  ‚Üì\n",
    "ELU ‚Üí Linear ‚Üí Dropout  ‚Üê context (optional)\n",
    "  ‚Üì\n",
    "ELU ‚Üí Linear ‚Üí Dropout\n",
    "  ‚Üì\n",
    "GLU (Gated Linear Unit)\n",
    "  ‚Üì\n",
    "LayerNorm(x + output)  ‚Üê Residual\n",
    "```\n",
    "\n",
    "**–ó–∞—á–µ–º:**\n",
    "- Gating mechanism –¥–ª—è feature selection\n",
    "- Residual connections –¥–ª—è gradient flow\n",
    "- Context injection (–Ω–∞–ø—Ä–∏–º–µ—Ä, static features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedResidualNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Gated Residual Network (GRN)\n",
    "    \n",
    "    Key building block of TFT.\n",
    "    Applies gating mechanism with residual connections.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, \n",
    "                 dropout=0.1, context_size=None):\n",
    "        super(GatedResidualNetwork, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.context_size = context_size\n",
    "        \n",
    "        # Linear layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        if context_size is not None:\n",
    "            self.context_fc = nn.Linear(context_size, hidden_size, bias=False)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Gated Linear Unit\n",
    "        self.gate_fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Skip connection\n",
    "        if input_size != output_size:\n",
    "            self.skip_fc = nn.Linear(input_size, output_size)\n",
    "        else:\n",
    "            self.skip_fc = None\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(output_size)\n",
    "    \n",
    "    def forward(self, x, context=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, ..., input_size)\n",
    "            context: optional (batch, ..., context_size)\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch, ..., output_size)\n",
    "        \"\"\"\n",
    "        # Feed-forward\n",
    "        hidden = F.elu(self.fc1(x))\n",
    "        \n",
    "        # Context injection\n",
    "        if context is not None and self.context_size is not None:\n",
    "            hidden = hidden + self.context_fc(context)\n",
    "        \n",
    "        hidden = self.dropout(hidden)\n",
    "        hidden = F.elu(self.fc2(hidden))\n",
    "        hidden = self.dropout(hidden)\n",
    "        \n",
    "        # Gating (GLU - Gated Linear Unit)\n",
    "        gate = torch.sigmoid(self.gate_fc(hidden))\n",
    "        \n",
    "        # Skip connection\n",
    "        if self.skip_fc is not None:\n",
    "            x = self.skip_fc(x)\n",
    "        \n",
    "        # Residual + LayerNorm\n",
    "        output = self.layer_norm(x + gate * hidden)\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\"‚úÖ GatedResidualNetwork implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Variable Selection Network\n",
    "\n",
    "**Automatic feature selection!**\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º–∞:** –ù–µ –≤—Å–µ features –≤–∞–∂–Ω—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ prediction\n",
    "\n",
    "**–†–µ—à–µ–Ω–∏–µ:**\n",
    "1. Transform –∫–∞–∂–¥—É—é feature —á–µ—Ä–µ–∑ GRN\n",
    "2. Compute variable weights (softmax)\n",
    "3. Weighted combination\n",
    "\n",
    "**–§–æ—Ä–º—É–ª–∞:**\n",
    "$$\\text{weights} = \\text{softmax}(\\text{GRN}([\\text{flatten}(\\text{features})]))$$\n",
    "$$\\text{output} = \\sum_i \\text{weights}_i \\cdot \\text{GRN}(\\text{feature}_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariableSelectionNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Variable Selection Network\n",
    "    \n",
    "    Automatically selects relevant features.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_sizes, hidden_size, output_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_sizes: list of input sizes for each variable\n",
    "            hidden_size: hidden dimension\n",
    "            output_size: output dimension\n",
    "        \"\"\"\n",
    "        super(VariableSelectionNetwork, self).__init__()\n",
    "        \n",
    "        self.num_vars = len(input_sizes)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # GRN –¥–ª—è –∫–∞–∂–¥–æ–π variable\n",
    "        self.variable_grns = nn.ModuleList([\n",
    "            GatedResidualNetwork(input_size, hidden_size, output_size, dropout)\n",
    "            for input_size in input_sizes\n",
    "        ])\n",
    "        \n",
    "        # GRN –¥–ª—è variable selection weights\n",
    "        total_input_size = sum(input_sizes)\n",
    "        self.weight_grn = GatedResidualNetwork(\n",
    "            total_input_size, hidden_size, self.num_vars, dropout\n",
    "        )\n",
    "    \n",
    "    def forward(self, variables, return_weights=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            variables: list of tensors, each (batch, ..., input_size_i)\n",
    "            return_weights: whether to return variable importance weights\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch, ..., output_size)\n",
    "            weights: optional (batch, num_vars)\n",
    "        \"\"\"\n",
    "        # Transform each variable —á–µ—Ä–µ–∑ GRN\n",
    "        transformed = [grn(var) for grn, var in zip(self.variable_grns, variables)]\n",
    "        \n",
    "        # Concatenate –¥–ª—è weight computation\n",
    "        flattened = torch.cat([var.flatten(start_dim=1) for var in variables], dim=-1)\n",
    "        \n",
    "        # Compute variable weights\n",
    "        weights = self.weight_grn(flattened)  # (batch, num_vars)\n",
    "        weights = F.softmax(weights, dim=-1)  # normalize\n",
    "        \n",
    "        # Weighted sum\n",
    "        # Stack: (batch, num_vars, ..., output_size)\n",
    "        stacked = torch.stack(transformed, dim=1)\n",
    "        \n",
    "        # Expand weights –¥–ª—è broadcasting\n",
    "        weights_expanded = weights.unsqueeze(-1).unsqueeze(-1)  # (batch, num_vars, 1, 1)\n",
    "        \n",
    "        # Weighted combination\n",
    "        output = (stacked * weights_expanded).sum(dim=1)  # (batch, ..., output_size)\n",
    "        \n",
    "        if return_weights:\n",
    "            return output, weights\n",
    "        return output\n",
    "\n",
    "print(\"‚úÖ VariableSelectionNetwork implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Multi-Head Attention (from Phase 4 Step 1)\n",
    "\n",
    "–ü–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏–∑ Step 1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"Scaled Dot-Product Attention\"\"\"\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        d_k = Q.size(-1)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        context = torch.matmul(attention_weights, V)\n",
    "        return context, attention_weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Attention\"\"\"\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.attention = ScaledDotProductAttention(dropout)\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        return x.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, n_heads, seq_len, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_Q(Q))\n",
    "        K = self.split_heads(self.W_K(K))\n",
    "        V = self.split_heads(self.W_V(V))\n",
    "        \n",
    "        context, attention_weights = self.attention(Q, K, V, mask)\n",
    "        context = self.combine_heads(context)\n",
    "        output = self.W_O(context)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "print(\"‚úÖ Multi-Head Attention loaded (from Phase 4 Step 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Test Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test GRN\n",
    "print(\"Testing GatedResidualNetwork...\")\n",
    "grn = GatedResidualNetwork(input_size=10, hidden_size=32, output_size=16)\n",
    "x_test = torch.randn(4, 5, 10)  # (batch=4, seq=5, features=10)\n",
    "out = grn(x_test)\n",
    "print(f\"  Input: {x_test.shape} ‚Üí Output: {out.shape}\")\n",
    "assert out.shape == (4, 5, 16), \"GRN output shape mismatch\"\n",
    "print(\"  ‚úÖ GRN works!\")\n",
    "\n",
    "# Test VSN\n",
    "print(\"\\nTesting VariableSelectionNetwork...\")\n",
    "vsn = VariableSelectionNetwork(input_sizes=[10, 15, 20], hidden_size=32, output_size=16)\n",
    "vars_test = [\n",
    "    torch.randn(4, 5, 10),\n",
    "    torch.randn(4, 5, 15),\n",
    "    torch.randn(4, 5, 20)\n",
    "]\n",
    "out, weights = vsn(vars_test, return_weights=True)\n",
    "print(f\"  Output: {out.shape}\")\n",
    "print(f\"  Weights: {weights.shape}\")\n",
    "print(f\"  Weights sum: {weights[0].sum().item():.4f} (should be 1.0)\")\n",
    "assert out.shape == (4, 5, 16), \"VSN output shape mismatch\"\n",
    "assert weights.shape == (4, 3), \"VSN weights shape mismatch\"\n",
    "print(\"  ‚úÖ VSN works!\")\n",
    "\n",
    "# Test MHA\n",
    "print(\"\\nTesting MultiHeadAttention...\")\n",
    "mha = MultiHeadAttention(d_model=64, n_heads=4)\n",
    "x_test = torch.randn(4, 10, 64)  # (batch=4, seq=10, d_model=64)\n",
    "out, attn = mha(x_test, x_test, x_test)\n",
    "print(f\"  Input: {x_test.shape} ‚Üí Output: {out.shape}\")\n",
    "print(f\"  Attention weights: {attn.shape}\")\n",
    "assert out.shape == (4, 10, 64), \"MHA output shape mismatch\"\n",
    "print(\"  ‚úÖ MHA works!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ All TFT building blocks implemented and tested!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèóÔ∏è –ß–∞—Å—Ç—å 4: Simplified Temporal Fusion Transformer\n",
    "\n",
    "### 4.1 Full TFT Architecture\n",
    "\n",
    "**–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è TFT –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ key concepts:**\n",
    "\n",
    "1. **Input Processing**: Embeddings –¥–ª—è categorical, linear –¥–ª—è numerical\n",
    "2. **Variable Selection**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –≤–∞–∂–Ω—ã—Ö features\n",
    "3. **LSTM Encoder**: Process historical sequence\n",
    "4. **Multi-Head Attention**: Long-range dependencies\n",
    "5. **Decoder**: Multi-horizon predictions (quantile forecasts)\n",
    "\n",
    "**Simplifications (–¥–ª—è –æ–±—É—á–µ–Ω–∏—è):**\n",
    "- Simplified static covariate enrichment\n",
    "- Single LSTM layer (–≤–º–µ—Å—Ç–æ –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω–æ–≥–æ)\n",
    "- Simplified temporal fusion\n",
    "\n",
    "**–°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–ª—é—á–µ–≤–æ–µ:**\n",
    "- Multi-horizon forecasting\n",
    "- Variable selection\n",
    "- Attention mechanism\n",
    "- Quantile predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplifiedTFT(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified Temporal Fusion Transformer\n",
    "    \n",
    "    Key components:\n",
    "    - Variable Selection Networks\n",
    "    - LSTM Encoder\n",
    "    - Multi-Head Attention\n",
    "    - Multi-horizon Quantile Output\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_households,\n",
    "                 encoder_length,\n",
    "                 decoder_length,\n",
    "                 hidden_size=64,\n",
    "                 n_heads=4,\n",
    "                 n_quantiles=3,\n",
    "                 dropout=0.1):\n",
    "        super(SimplifiedTFT, self).__init__()\n",
    "        \n",
    "        self.encoder_length = encoder_length\n",
    "        self.decoder_length = decoder_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_quantiles = n_quantiles\n",
    "        \n",
    "        # 1. Embeddings\n",
    "        self.household_embedding = nn.Embedding(num_households, 8)\n",
    "        self.hour_embedding = nn.Embedding(24, 8)\n",
    "        self.dow_embedding = nn.Embedding(7, 4)\n",
    "        \n",
    "        # 2. Variable Selection –¥–ª—è encoder inputs\n",
    "        # Features: hour_emb(8) + dow_emb(4) + weekend(1) + temp(1) + consumption(1)\n",
    "        encoder_input_sizes = [8, 4, 1, 1, 1]  # 5 variables\n",
    "        self.encoder_vsn = VariableSelectionNetwork(\n",
    "            encoder_input_sizes, hidden_size, hidden_size, dropout\n",
    "        )\n",
    "        \n",
    "        # 3. LSTM Encoder\n",
    "        self.lstm_encoder = nn.LSTM(\n",
    "            hidden_size, hidden_size, batch_first=True, dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # 4. Multi-Head Attention\n",
    "        self.attention = MultiHeadAttention(hidden_size, n_heads, dropout)\n",
    "        \n",
    "        # 5. GRN –ø–æ—Å–ª–µ attention\n",
    "        self.post_attention_grn = GatedResidualNetwork(\n",
    "            hidden_size, hidden_size, hidden_size, dropout\n",
    "        )\n",
    "        \n",
    "        # 6. Decoder –¥–ª—è future inputs\n",
    "        decoder_input_sizes = [8, 4, 1]  # hour_emb + dow_emb + weekend\n",
    "        self.decoder_vsn = VariableSelectionNetwork(\n",
    "            decoder_input_sizes, hidden_size, hidden_size, dropout\n",
    "        )\n",
    "        \n",
    "        # 7. LSTM Decoder\n",
    "        self.lstm_decoder = nn.LSTM(\n",
    "            hidden_size, hidden_size, batch_first=True, dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # 8. Quantile output layer\n",
    "        self.output_layer = nn.Linear(hidden_size, n_quantiles)\n",
    "    \n",
    "    def forward(self, batch, return_attention=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch: dict with encoder/decoder inputs\n",
    "            return_attention: whether to return attention weights\n",
    "        \n",
    "        Returns:\n",
    "            quantiles: (batch, decoder_length, n_quantiles)\n",
    "            attention_weights: optional\n",
    "        \"\"\"\n",
    "        batch_size = batch['encoder_hour'].size(0)\n",
    "        \n",
    "        # 1. Process encoder inputs\n",
    "        # Embeddings\n",
    "        encoder_hour_emb = self.hour_embedding(batch['encoder_hour'])  # (B, enc_len, 8)\n",
    "        encoder_dow_emb = self.dow_embedding(batch['encoder_dow'])     # (B, enc_len, 4)\n",
    "        encoder_weekend = batch['encoder_weekend'].unsqueeze(-1)       # (B, enc_len, 1)\n",
    "        encoder_temp = batch['encoder_temp'].unsqueeze(-1)             # (B, enc_len, 1)\n",
    "        encoder_consumption = batch['encoder_consumption'].unsqueeze(-1)  # (B, enc_len, 1)\n",
    "        \n",
    "        # Variable selection\n",
    "        encoder_variables = [\n",
    "            encoder_hour_emb,\n",
    "            encoder_dow_emb,\n",
    "            encoder_weekend,\n",
    "            encoder_temp,\n",
    "            encoder_consumption\n",
    "        ]\n",
    "        encoder_selected = self.encoder_vsn(encoder_variables)  # (B, enc_len, hidden)\n",
    "        \n",
    "        # 2. LSTM Encoder\n",
    "        encoder_output, (h_n, c_n) = self.lstm_encoder(encoder_selected)\n",
    "        # encoder_output: (B, enc_len, hidden)\n",
    "        \n",
    "        # 3. Multi-Head Attention\n",
    "        attn_output, attention_weights = self.attention(\n",
    "            encoder_output, encoder_output, encoder_output\n",
    "        )\n",
    "        \n",
    "        # 4. GRN –ø–æ—Å–ª–µ attention\n",
    "        enriched = self.post_attention_grn(attn_output)\n",
    "        # enriched: (B, enc_len, hidden)\n",
    "        \n",
    "        # 5. Process decoder inputs (future known)\n",
    "        decoder_hour_emb = self.hour_embedding(batch['decoder_hour'])  # (B, dec_len, 8)\n",
    "        decoder_dow_emb = self.dow_embedding(batch['decoder_dow'])     # (B, dec_len, 4)\n",
    "        decoder_weekend = batch['decoder_weekend'].unsqueeze(-1)       # (B, dec_len, 1)\n",
    "        \n",
    "        decoder_variables = [\n",
    "            decoder_hour_emb,\n",
    "            decoder_dow_emb,\n",
    "            decoder_weekend\n",
    "        ]\n",
    "        decoder_selected = self.decoder_vsn(decoder_variables)  # (B, dec_len, hidden)\n",
    "        \n",
    "        # 6. LSTM Decoder (—Å context –æ—Ç encoder)\n",
    "        decoder_output, _ = self.lstm_decoder(decoder_selected, (h_n, c_n))\n",
    "        # decoder_output: (B, dec_len, hidden)\n",
    "        \n",
    "        # 7. Quantile predictions\n",
    "        quantiles = self.output_layer(decoder_output)  # (B, dec_len, n_quantiles)\n",
    "        \n",
    "        if return_attention:\n",
    "            return quantiles, attention_weights\n",
    "        return quantiles\n",
    "\n",
    "print(\"‚úÖ SimplifiedTFT implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = SimplifiedTFT(\n",
    "    num_households=10,\n",
    "    encoder_length=ENCODER_LENGTH,\n",
    "    decoder_length=DECODER_LENGTH,\n",
    "    hidden_size=64,\n",
    "    n_heads=4,\n",
    "    n_quantiles=3,  # P10, P50, P90\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Simplified TFT Model:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Encoder length: {ENCODER_LENGTH}\")\n",
    "print(f\"  Decoder length: {DECODER_LENGTH}\")\n",
    "print(f\"  Hidden size: 64\")\n",
    "print(f\"  Attention heads: 4\")\n",
    "print(f\"  Quantiles: 3 (P10, P50, P90)\")\n",
    "print(f\"\\nModel size:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Quantile Loss\n",
    "\n",
    "**TFT –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç quantiles –¥–ª—è uncertainty estimation:**\n",
    "\n",
    "- **P10 (10th percentile)**: Optimistic forecast\n",
    "- **P50 (median)**: Most likely forecast\n",
    "- **P90 (90th percentile)**: Pessimistic forecast\n",
    "\n",
    "**Quantile Loss:**\n",
    "\n",
    "$$L_q(y, \\hat{y}) = \\begin{cases}\n",
    "q \\cdot (y - \\hat{y}) & \\text{if } y \\geq \\hat{y} \\\\\n",
    "(1-q) \\cdot (\\hat{y} - y) & \\text{if } y < \\hat{y}\n",
    "\\end{cases}$$\n",
    "\n",
    "–ì–¥–µ $q$ - quantile level (0.1, 0.5, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantileLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Quantile Loss for probabilistic forecasting\n",
    "    \"\"\"\n",
    "    def __init__(self, quantiles=[0.1, 0.5, 0.9]):\n",
    "        super(QuantileLoss, self).__init__()\n",
    "        self.quantiles = torch.FloatTensor(quantiles)\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: (batch, time, n_quantiles)\n",
    "            targets: (batch, time)\n",
    "        \n",
    "        Returns:\n",
    "            loss: scalar\n",
    "        \"\"\"\n",
    "        self.quantiles = self.quantiles.to(predictions.device)\n",
    "        \n",
    "        targets = targets.unsqueeze(-1)  # (batch, time, 1)\n",
    "        errors = targets - predictions    # (batch, time, n_quantiles)\n",
    "        \n",
    "        quantiles_expanded = self.quantiles.view(1, 1, -1)\n",
    "        \n",
    "        loss = torch.max(\n",
    "            quantiles_expanded * errors,\n",
    "            (quantiles_expanded - 1) * errors\n",
    "        )\n",
    "        \n",
    "        return loss.mean()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = QuantileLoss(quantiles=[0.1, 0.5, 0.9])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3\n",
    ")\n",
    "\n",
    "print(\"‚úÖ QuantileLoss –∏ optimizer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        # Move to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        quantiles = model(batch)\n",
    "        loss = criterion(quantiles, batch['target'])\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * batch['target'].size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            quantiles = model(batch)\n",
    "            loss = criterion(quantiles, batch['target'])\n",
    "            total_loss += loss.item() * batch['target'].size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    return avg_loss\n",
    "\n",
    "print(\"‚úÖ Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 20\n",
    "history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "print(f\"Training TFT for {num_epochs} epochs...\")\n",
    "print(f\"Dataset: {len(train_dataset):,} train, {len(val_dataset):,} val\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), '/tmp/best_tft.pt')\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch [{epoch+1:2d}/{num_epochs}] \"\n",
    "              f\"Train Loss: {train_loss:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | \"\n",
    "              f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"‚úÖ Training completed!\")\n",
    "print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('/tmp/best_tft.pt'))\n",
    "print(\"‚úÖ Best model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "plt.plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "plt.axhline(y=best_val_loss, color='red', linestyle='--', \n",
    "           label=f'Best: {best_val_loss:.4f}', linewidth=1.5)\n",
    "plt.title('Training History', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Quantile Loss', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Evaluation –∏ Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate –Ω–∞ test set\n",
    "test_loss = evaluate(model, test_loader, criterion, device)\n",
    "print(f\"Test Quantile Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Get predictions –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤\n",
    "model.eval()\n",
    "sample_predictions = []\n",
    "sample_targets = []\n",
    "sample_encoders = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        if i >= 3:  # first 3 batches\n",
    "            break\n",
    "        \n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        quantiles = model(batch)\n",
    "        \n",
    "        sample_predictions.append(quantiles.cpu().numpy())\n",
    "        sample_targets.append(batch['target'].cpu().numpy())\n",
    "        sample_encoders.append(batch['encoder_consumption'].cpu().numpy())\n",
    "\n",
    "sample_predictions = np.concatenate(sample_predictions, axis=0)\n",
    "sample_targets = np.concatenate(sample_targets, axis=0)\n",
    "sample_encoders = np.concatenate(sample_encoders, axis=0)\n",
    "\n",
    "print(f\"\\nSample predictions shape: {sample_predictions.shape}\")\n",
    "print(f\"  {sample_predictions.shape[0]} samples √ó {sample_predictions.shape[1]} horizons √ó {sample_predictions.shape[2]} quantiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(3, 1, figsize=(16, 12))\n",
    "\n",
    "for idx in range(3):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Historical\n",
    "    historical = sample_encoders[idx]\n",
    "    historical_x = np.arange(-ENCODER_LENGTH, 0)\n",
    "    \n",
    "    # Future (target)\n",
    "    target = sample_targets[idx]\n",
    "    future_x = np.arange(0, DECODER_LENGTH)\n",
    "    \n",
    "    # Predictions (P10, P50, P90)\n",
    "    pred_p10 = sample_predictions[idx, :, 0]\n",
    "    pred_p50 = sample_predictions[idx, :, 1]\n",
    "    pred_p90 = sample_predictions[idx, :, 2]\n",
    "    \n",
    "    # Plot\n",
    "    ax.plot(historical_x, historical, 'k-', linewidth=2, label='Historical')\n",
    "    ax.plot(future_x, target, 'b-', linewidth=2, marker='o', \n",
    "           markersize=4, label='Actual (Target)')\n",
    "    ax.plot(future_x, pred_p50, 'r-', linewidth=2, marker='s', \n",
    "           markersize=4, label='Prediction (P50)')\n",
    "    \n",
    "    # Uncertainty bands\n",
    "    ax.fill_between(future_x, pred_p10, pred_p90, alpha=0.2, \n",
    "                    color='red', label='P10-P90 Range')\n",
    "    \n",
    "    ax.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.set_title(f'Sample {idx+1}: Multi-Horizon Forecast ({DECODER_LENGTH}h ahead)', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Time (hours)', fontsize=11)\n",
    "    ax.set_ylabel('Consumption (kWh)', fontsize=11)\n",
    "    ax.legend(loc='best')\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('TFT Multi-Horizon Probabilistic Forecasts', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Observations:\")\n",
    "print(\"  - P50 (median) follows the actual trend\")\n",
    "print(\"  - P10-P90 range captures uncertainty\")\n",
    "print(\"  - Wider uncertainty band for longer horizons (expected!)\")\n",
    "print(\"  - Model learns daily patterns from historical data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Quantitative Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics –¥–ª—è P50 (median forecast)\n",
    "p50_predictions = sample_predictions[:, :, 1]  # P50\n",
    "\n",
    "# Overall metrics\n",
    "mae = mean_absolute_error(sample_targets.flatten(), p50_predictions.flatten())\n",
    "rmse = np.sqrt(mean_squared_error(sample_targets.flatten(), p50_predictions.flatten()))\n",
    "\n",
    "print(\"TFT Performance (P50):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  MAE (Mean Absolute Error): {mae:.4f} kWh\")\n",
    "print(f\"  RMSE (Root Mean Squared Error): {rmse:.4f} kWh\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Per-horizon metrics\n",
    "horizon_mae = []\n",
    "for h in range(DECODER_LENGTH):\n",
    "    h_mae = mean_absolute_error(sample_targets[:, h], p50_predictions[:, h])\n",
    "    horizon_mae.append(h_mae)\n",
    "\n",
    "# Plot horizon performance\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(range(1, DECODER_LENGTH+1), horizon_mae, linewidth=2, marker='o')\n",
    "plt.title('MAE by Forecast Horizon', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Forecast Horizon (hours)', fontsize=12)\n",
    "plt.ylabel('MAE (kWh)', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä MAE increases with horizon (expected!):\")\n",
    "print(f\"  Hour 1: {horizon_mae[0]:.4f} kWh\")\n",
    "print(f\"  Hour 12: {horizon_mae[11]:.4f} kWh\")\n",
    "print(f\"  Hour 24: {horizon_mae[23]:.4f} kWh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Attention Weights Visualization\n",
    "\n",
    "**Interpretability —á–µ—Ä–µ–∑ attention:**\n",
    "- –ö–∞–∫–∏–µ historical timesteps –≤–∞–∂–Ω—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ forecast horizon?\n",
    "- Long-range vs short-range dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get attention weights –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_batch = {k: v[:1].to(device) for k, v in next(iter(test_loader)).items()}\n",
    "    _, attention_weights = model(sample_batch, return_attention=True)\n",
    "\n",
    "# attention_weights: (batch, n_heads, encoder_len, encoder_len)\n",
    "# Average over heads\n",
    "attn = attention_weights[0].mean(dim=0).cpu().numpy()  # (encoder_len, encoder_len)\n",
    "\n",
    "print(f\"Attention weights shape: {attn.shape}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(attn, cmap='YlOrRd', cbar_kws={'label': 'Attention Weight'})\n",
    "plt.title('TFT Attention Weights (Averaged over Heads)', \n",
    "         fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Keys (Historical Timesteps)', fontsize=12)\n",
    "plt.ylabel('Queries (Historical Timesteps)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Attention interpretation:\")\n",
    "print(\"  - Diagonal: current timestep attends to itself (expected)\")\n",
    "print(\"  - Patterns: model learns temporal dependencies\")\n",
    "print(\"  - Nearby timesteps often have higher attention (local patterns)\")\n",
    "print(\"  - Some long-range dependencies visible (weekly seasonality)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì –ò—Ç–æ–≥–∏ –∏ –í—ã–≤–æ–¥—ã\n",
    "\n",
    "### –ß—Ç–æ –º—ã —Ä–µ–∞–ª–∏–∑–æ–≤–∞–ª–∏\n",
    "\n",
    "#### 1. Temporal Fusion Transformer\n",
    "\n",
    "**–ö–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:**\n",
    "- ‚úÖ **Variable Selection Networks**: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –≤–∞–∂–Ω—ã—Ö features\n",
    "- ‚úÖ **Gated Residual Networks**: –≥–∏–±–∫–∏–µ building blocks —Å gating\n",
    "- ‚úÖ **LSTM Encoder**: –æ–±—Ä–∞–±–æ—Ç–∫–∞ historical sequence\n",
    "- ‚úÖ **Multi-Head Attention**: long-range temporal dependencies\n",
    "- ‚úÖ **Multi-Horizon Output**: –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ 24 —á–∞—Å–æ–≤\n",
    "- ‚úÖ **Quantile Forecasting**: uncertainty estimation (P10, P50, P90)\n",
    "\n",
    "**Dataset:**\n",
    "- 87,600 data points (10 households √ó 365 days √ó 24 hours)\n",
    "- Mixed features: categorical (household, hour, day) + numerical (temp)\n",
    "- Real-world patterns: daily + weekly seasonality, temperature dependency\n",
    "\n",
    "---\n",
    "\n",
    "### –ö–ª—é—á–µ–≤—ã–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è\n",
    "\n",
    "#### 1. Multi-Horizon Forecasting\n",
    "\n",
    "**–¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã:**\n",
    "- Recursive: –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å t+1, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è t+2, ... (–Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫!)\n",
    "- Direct: –æ—Ç–¥–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ horizon (–¥–æ—Ä–æ–≥–æ!)\n",
    "\n",
    "**TFT:**\n",
    "- ‚úÖ Single model predicts all 24 horizons simultaneously\n",
    "- ‚úÖ No error accumulation\n",
    "- ‚úÖ Shared representations –º–µ–∂–¥—É horizons\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Probabilistic Forecasts\n",
    "\n",
    "**Point forecasts (–æ–±—ã—á–Ω—ã–µ –º–æ–¥–µ–ª–∏):**\n",
    "- –¢–æ–ª—å–∫–æ P50 (median)\n",
    "- –ù–µ—Ç uncertainty –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏\n",
    "\n",
    "**TFT Quantile Forecasts:**\n",
    "- ‚úÖ P10, P50, P90 quantiles\n",
    "- ‚úÖ Uncertainty bands (P10-P90 range)\n",
    "- ‚úÖ Risk assessment (–≤–∞–∂–Ω–æ –¥–ª—è decision making)\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:**\n",
    "```\n",
    "Electricity demand planning:\n",
    "- P90: pessimistic case ‚Üí reserve capacity\n",
    "- P50: expected case ‚Üí normal operations\n",
    "- P10: optimistic case ‚Üí minimum requirements\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Interpretability\n",
    "\n",
    "**Attention Weights:**\n",
    "- –ü–æ–∫–∞–∑—ã–≤–∞—é—Ç, –∫–∞–∫–∏–µ historical timesteps –≤–∞–∂–Ω—ã\n",
    "- –ú–æ–∂–Ω–æ –æ–±—ä—è—Å–Ω–∏—Ç—å –∫–∞–∂–¥–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
    "\n",
    "**Variable Selection:**\n",
    "- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç –≤–∞–∂–Ω—ã–µ features\n",
    "- –ú–æ–∂–Ω–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å, –∫–∞–∫–∏–µ features –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–ª–µ–∑–Ω—ã\n",
    "\n",
    "---\n",
    "\n",
    "### –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ Phase 3 + Phase 4\n",
    "\n",
    "**TFT combines best of both worlds:**\n",
    "\n",
    "| Component | From Phase | Purpose |\n",
    "|-----------|-----------|----------|\n",
    "| LSTM Encoder | Phase 3 (RNN) | Sequential processing |\n",
    "| Multi-Head Attention | Phase 4 (Transformers) | Long-range dependencies |\n",
    "| Variable Selection | Phase 4 (TabTransformer) | Automatic feature engineering |\n",
    "| Time Series Patterns | Phase 3 | Seasonality, trends |\n",
    "\n",
    "**–†–µ–∑—É–ª—å—Ç–∞—Ç:**\n",
    "- LSTM handles sequential nature\n",
    "- Attention captures long-term patterns\n",
    "- Variable selection handles mixed features\n",
    "- Quantiles provide uncertainty\n",
    "\n",
    "---\n",
    "\n",
    "### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å TFT?\n",
    "\n",
    "**‚úÖ –ò–¥–µ–∞–ª—å–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –∫–æ–≥–¥–∞:**\n",
    "1. **Multi-horizon forecasting** (–Ω—É–∂–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤)\n",
    "2. **Mixed data types** (categorical + numerical + temporal)\n",
    "3. **Uncertainty –≤–∞–∂–Ω–∞** (risk assessment, decision making)\n",
    "4. **Interpretability –Ω—É–∂–Ω–∞** (–æ–±—ä—è—Å–Ω–∏—Ç—å predictions stakeholders)\n",
    "5. **Multiple time series** (panel data, hundreds/thousands series)\n",
    "6. **Complex patterns** (multiple seasonalities, interactions)\n",
    "\n",
    "**‚ùå –ú–æ–∂–µ—Ç –±—ã—Ç—å overkill –∫–æ–≥–¥–∞:**\n",
    "1. Simple univariate time series (Prophet –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ)\n",
    "2. Single-step forecasting (LSTM –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–æ—â–µ)\n",
    "3. Small dataset (<1000 timesteps)\n",
    "4. –¢–æ–ª—å–∫–æ point forecasts –Ω—É–∂–Ω—ã (–Ω–µ quantiles)\n",
    "\n",
    "---\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "**TFT –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤:**\n",
    "\n",
    "1. **Electricity Load Forecasting**\n",
    "   - Multi-step ahead predictions\n",
    "   - Uncertainty –¥–ª—è capacity planning\n",
    "   - Multiple households/regions\n",
    "\n",
    "2. **Retail Demand Forecasting**\n",
    "   - SKU-level predictions\n",
    "   - Promotional events (categorical)\n",
    "   - Quantiles –¥–ª—è inventory optimization\n",
    "\n",
    "3. **Traffic Prediction**\n",
    "   - Multiple roads/intersections\n",
    "   - Weather, events –≤–ª–∏—è—é—Ç\n",
    "   - Uncertainty –¥–ª—è routing\n",
    "\n",
    "4. **Financial Forecasting**\n",
    "   - Multi-asset predictions\n",
    "   - Risk assessment (VaR —á–µ—Ä–µ–∑ quantiles)\n",
    "   - Market conditions (categorical)\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ –ü–æ–∑–¥—Ä–∞–≤–ª—è–µ–º!\n",
    "\n",
    "–í—ã –∑–∞–≤–µ—Ä—à–∏–ª–∏ **Phase 4: Transformers & Modern Architectures**!\n",
    "\n",
    "**–¢–µ–ø–µ—Ä—å –≤—ã –∑–Ω–∞–µ—Ç–µ:**\n",
    "- üß† –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç Self-Attention mechanism\n",
    "- üîÑ –ö–∞–∫ Transformers –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç RNN –¥–ª—è parallelization\n",
    "- üìä –ö–∞–∫ –ø—Ä–∏–º–µ–Ω—è—Ç—å Transformers –∫ —Ç–∞–±–ª–∏—á–Ω—ã–º –¥–∞–Ω–Ω—ã–º (TabTransformer)\n",
    "- ‚è∞ –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å TFT –¥–ª—è multi-horizon time series forecasting\n",
    "- üìà –ö–∞–∫ –ø–æ–ª—É—á–∞—Ç—å probabilistic forecasts (quantiles)\n",
    "- üîç –ö–∞–∫ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ attention\n",
    "\n",
    "**–ü—É—Ç–µ—à–µ—Å—Ç–≤–∏–µ —á–µ—Ä–µ–∑ ML Roadmap:**\n",
    "- ‚úÖ Phase 1: Classical ML (XGBoost, LightGBM, CatBoost, Stacking)\n",
    "- ‚úÖ Phase 2: Deep Learning Basics (MLP, CNN, Autoencoders)\n",
    "- ‚úÖ Phase 3: Temporal Data & RNN (ARIMA, LSTM, Attention)\n",
    "- ‚úÖ Phase 4: Transformers (Self-Attention, TabTransformer, TFT)\n",
    "\n",
    "**–°–ª–µ–¥—É—é—â–∏–π —à–∞–≥:** Transfer Learning, Pre-training, Foundation Models! üöÄ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
#!/usr/bin/env python3
"""
Phase 3 BONUS: Real-world Financial Pattern Recognition
Part 3: Three Tasks - Price Forecasting, Direction Classification, Pattern Recognition
"""

import json

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –Ω–æ—É—Ç–±—É–∫
notebook_path = '/home/user/test/notebooks/phase3_temporal_rnn/bonus_financial_patterns.ipynb'
with open(notebook_path, 'r', encoding='utf-8') as f:
    notebook = json.load(f)

cells = notebook['cells']

# ============================================================================
# DATA PREPARATION FOR MODELING
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "## üéØ –ß–∞—Å—Ç—å 3: –¢—Ä–∏ –∑–∞–¥–∞—á–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è\n",
        "\n",
        "### 3.1 –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è\n",
        "\n",
        "–°–æ–∑–¥–∞–¥–∏–º —Ç—Ä–∏ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö:\n",
        "1. **Price (—Ä–µ–≥—Ä–µ—Å—Å–∏—è):** —Ü–µ–Ω–∞ —á–µ—Ä–µ–∑ N —Å–≤–µ—á–µ–π\n",
        "2. **Direction (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è):** UP/DOWN/NEUTRAL\n",
        "3. **Pattern labels (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è):** –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
        "FORECAST_HORIZON = 4  # –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —á–µ—Ä–µ–∑ 4 —Å–≤–µ—á–∏ (16 —á–∞—Å–æ–≤ –¥–ª—è 4H)\n",
        "SEQ_LENGTH = 24  # –∏—Å–ø–æ–ª—å–∑—É–µ–º 24 —Å–≤–µ—á–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (4 –¥–Ω—è)\n",
        "\n",
        "# –ö–æ–ø–∏—Ä—É–µ–º –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö\n",
        "df_ml = df_features.copy()\n",
        "\n",
        "# ============================================================================\n",
        "# TARGET 1: PRICE (–†–µ–≥—Ä–µ—Å—Å–∏—è)\n",
        "# ============================================================================\n",
        "\n",
        "df_ml['Target_Price'] = df_ml['Close'].shift(-FORECAST_HORIZON)\n",
        "df_ml['Target_Return'] = (df_ml['Target_Price'] / df_ml['Close'] - 1) * 100  # –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö\n",
        "\n",
        "print(f\"Target Price —Å–æ–∑–¥–∞–Ω: –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Ü–µ–Ω—É —á–µ—Ä–µ–∑ {FORECAST_HORIZON} —Å–≤–µ—á–µ–π\")\n",
        "\n",
        "# ============================================================================\n",
        "# TARGET 2: DIRECTION (–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)\n",
        "# ============================================================================\n",
        "\n",
        "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Ä–æ–≥–∏ –¥–ª—è UP/DOWN/NEUTRAL\n",
        "THRESHOLD = 0.05  # 0.05% –¥–≤–∏–∂–µ–Ω–∏–µ —Å—á–∏—Ç–∞–µ–º –∑–Ω–∞—á–∏–º—ã–º\n",
        "\n",
        "def classify_direction(return_pct, threshold=THRESHOLD):\n",
        "    if return_pct > threshold:\n",
        "        return 2  # UP\n",
        "    elif return_pct < -threshold:\n",
        "        return 0  # DOWN\n",
        "    else:\n",
        "        return 1  # NEUTRAL\n",
        "\n",
        "df_ml['Target_Direction'] = df_ml['Target_Return'].apply(\n",
        "    lambda x: classify_direction(x, THRESHOLD)\n",
        ")\n",
        "\n",
        "# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤\n",
        "direction_counts = df_ml['Target_Direction'].value_counts()\n",
        "print(f\"\\nDirection classes —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:\")\n",
        "print(f\"  DOWN (0): {direction_counts.get(0, 0)} ({direction_counts.get(0, 0)/len(df_ml)*100:.1f}%)\")\n",
        "print(f\"  NEUTRAL (1): {direction_counts.get(1, 0)} ({direction_counts.get(1, 0)/len(df_ml)*100:.1f}%)\")\n",
        "print(f\"  UP (2): {direction_counts.get(2, 0)} ({direction_counts.get(2, 0)/len(df_ml)*100:.1f}%)\")\n",
        "\n",
        "# ============================================================================\n",
        "# TARGET 3: PATTERNS (–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)\n",
        "# ============================================================================\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∫–∏ –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞—à–∏—Ö —Ñ–∏—á\n",
        "\n",
        "def label_patterns(row):\n",
        "    \"\"\"\n",
        "    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ feature values\n",
        "    \n",
        "    Returns:\n",
        "        0: No pattern\n",
        "        1: RSI Divergence Bullish\n",
        "        2: RSI Divergence Bearish\n",
        "        3: MACD Crossover Bullish\n",
        "        4: MACD Crossover Bearish\n",
        "        5: Bollinger Squeeze\n",
        "        6: Head & Shoulders (high symmetry)\n",
        "        7: Double Top\n",
        "    \"\"\"\n",
        "    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º (–µ—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ, –≤—ã–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤—ã–π)\n",
        "    if row['RSI_bullish_divergence'] == 1 and row['RSI'] < 40:\n",
        "        return 1\n",
        "    if row['RSI_bearish_divergence'] == 1 and row['RSI'] > 60:\n",
        "        return 2\n",
        "    if row['MACD_crossover_bull'] == 1:\n",
        "        return 3\n",
        "    if row['MACD_crossover_bear'] == 1:\n",
        "        return 4\n",
        "    if row['BB_squeeze'] == 1:\n",
        "        return 5\n",
        "    if row['Peak_symmetry'] > 0.8 and row['Pivot_High_count_20'] >= 3:\n",
        "        return 6\n",
        "    if row['Double_top_signal'] == 1:\n",
        "        return 7\n",
        "    \n",
        "    return 0  # No pattern\n",
        "\n",
        "df_ml['Target_Pattern'] = df_ml.apply(label_patterns, axis=1)\n",
        "\n",
        "# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤\n",
        "pattern_counts = df_ml['Target_Pattern'].value_counts().sort_index()\n",
        "pattern_names = {\n",
        "    0: 'No Pattern',\n",
        "    1: 'RSI Div Bullish',\n",
        "    2: 'RSI Div Bearish',\n",
        "    3: 'MACD Cross Bull',\n",
        "    4: 'MACD Cross Bear',\n",
        "    5: 'BB Squeeze',\n",
        "    6: 'Head & Shoulders',\n",
        "    7: 'Double Top'\n",
        "}\n",
        "\n",
        "print(f\"\\nPattern —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:\")\n",
        "for pattern_id, count in pattern_counts.items():\n",
        "    pct = count / len(df_ml) * 100\n",
        "    print(f\"  {pattern_id} - {pattern_names[pattern_id]}: {count} ({pct:.2f}%)\")\n",
        "\n",
        "# –£–¥–∞–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ –≥–¥–µ –Ω–µ—Ç target\n",
        "df_ml = df_ml.dropna(subset=['Target_Price', 'Target_Direction'])\n",
        "\n",
        "print(f\"\\n‚úÖ –í—Å–µ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —Å–æ–∑–¥–∞–Ω—ã\")\n",
        "print(f\"–§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {df_ml.shape}\")"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LSTM\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "# –°–ø–∏—Å–æ–∫ —Ñ–∏—á –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è (–∏—Å–∫–ª—é—á–∞–µ–º OHLC –∏ targets)\n",
        "exclude_cols = ['Open', 'High', 'Low', 'Close', 'Volume', \n",
        "                'Target_Price', 'Target_Return', 'Target_Direction', 'Target_Pattern']\n",
        "feature_cols = [col for col in df_ml.columns if col not in exclude_cols]\n",
        "\n",
        "print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ–º {len(feature_cols)} —Ñ–∏—á –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è\")\n",
        "\n",
        "# Scaling features\n",
        "scaler_features = StandardScaler()\n",
        "df_ml[feature_cols] = scaler_features.fit_transform(df_ml[feature_cols])\n",
        "\n",
        "# Scaling price target (–¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏)\n",
        "scaler_price = MinMaxScaler()\n",
        "df_ml['Target_Price_scaled'] = scaler_price.fit_transform(df_ml[['Target_Price']])\n",
        "\n",
        "print(\"‚úÖ Features –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã (StandardScaler)\")\n",
        "print(\"‚úÖ Target Price –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω (MinMaxScaler)\")"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è LSTM\n",
        "def create_sequences_multivariate(data, features, target, seq_length):\n",
        "    \"\"\"\n",
        "    –°–æ–∑–¥–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è multivariate LSTM\n",
        "    \n",
        "    Args:\n",
        "        data: DataFrame\n",
        "        features: list of feature column names\n",
        "        target: target column name\n",
        "        seq_length: –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
        "    \n",
        "    Returns:\n",
        "        X: (n_samples, seq_length, n_features)\n",
        "        y: (n_samples,)\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    \n",
        "    feature_data = data[features].values\n",
        "    target_data = data[target].values\n",
        "    \n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(feature_data[i:i + seq_length])\n",
        "        y.append(target_data[i + seq_length])\n",
        "    \n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Train/Test split (80/20)\n",
        "train_size = int(len(df_ml) * 0.8)\n",
        "df_train = df_ml.iloc[:train_size]\n",
        "df_test = df_ml.iloc[train_size:]\n",
        "\n",
        "print(f\"Train: {len(df_train)} samples\")\n",
        "print(f\"Test: {len(df_test)} samples\")\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è Task 1 (Price Forecasting)\n",
        "X_train_price, y_train_price = create_sequences_multivariate(\n",
        "    df_train, feature_cols, 'Target_Price_scaled', SEQ_LENGTH\n",
        ")\n",
        "X_test_price, y_test_price = create_sequences_multivariate(\n",
        "    df_test, feature_cols, 'Target_Price_scaled', SEQ_LENGTH\n",
        ")\n",
        "\n",
        "# –î–ª—è Task 2 (Direction)\n",
        "X_train_dir, y_train_dir = create_sequences_multivariate(\n",
        "    df_train, feature_cols, 'Target_Direction', SEQ_LENGTH\n",
        ")\n",
        "X_test_dir, y_test_dir = create_sequences_multivariate(\n",
        "    df_test, feature_cols, 'Target_Direction', SEQ_LENGTH\n",
        ")\n",
        "\n",
        "# –î–ª—è Task 3 (Patterns)\n",
        "X_train_pat, y_train_pat = create_sequences_multivariate(\n",
        "    df_train, feature_cols, 'Target_Pattern', SEQ_LENGTH\n",
        ")\n",
        "X_test_pat, y_test_pat = create_sequences_multivariate(\n",
        "    df_test, feature_cols, 'Target_Pattern', SEQ_LENGTH\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–æ–∑–¥–∞–Ω—ã:\")\n",
        "print(f\"   Shape: X_train={X_train_price.shape}, y_train={y_train_price.shape}\")\n",
        "print(f\"   Features per timestep: {X_train_price.shape[2]}\")\n",
        "\n",
        "# PyTorch tensors\n",
        "X_train_price_t = torch.FloatTensor(X_train_price).to(device)\n",
        "y_train_price_t = torch.FloatTensor(y_train_price).to(device)\n",
        "X_test_price_t = torch.FloatTensor(X_test_price).to(device)\n",
        "y_test_price_t = torch.FloatTensor(y_test_price).to(device)\n",
        "\n",
        "X_train_dir_t = torch.FloatTensor(X_train_dir).to(device)\n",
        "y_train_dir_t = torch.LongTensor(y_train_dir).to(device)\n",
        "X_test_dir_t = torch.FloatTensor(X_test_dir).to(device)\n",
        "y_test_dir_t = torch.LongTensor(y_test_dir).to(device)\n",
        "\n",
        "X_train_pat_t = torch.FloatTensor(X_train_pat).to(device)\n",
        "y_train_pat_t = torch.LongTensor(y_train_pat).to(device)\n",
        "X_test_pat_t = torch.FloatTensor(X_test_pat).to(device)\n",
        "y_test_pat_t = torch.LongTensor(y_test_pat).to(device)\n",
        "\n",
        "print(\"‚úÖ PyTorch tensors —Å–æ–∑–¥–∞–Ω—ã\")"
    ]
})

# ============================================================================
# TASK 1: PRICE FORECASTING
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 3.2 Task 1: Price Forecasting (–†–µ–≥—Ä–µ—Å—Å–∏—è)\n",
        "\n",
        "**–ó–∞–¥–∞—á–∞:** –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —Ü–µ–Ω—É –∑–∞–∫—Ä—ã—Ç–∏—è —á–µ—Ä–µ–∑ 4 —Å–≤–µ—á–∏ (16 —á–∞—Å–æ–≤)\n",
        "\n",
        "**–ú–æ–¥–µ–ª—å:** LSTM —Å Attention\n",
        "\n",
        "**–ú–µ—Ç—Ä–∏–∫–∏:** RMSE, MAE, MAPE"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# LSTM –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏\n",
        "class LSTMRegressor(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.3):\n",
        "        super(LSTMRegressor, self).__init__()\n",
        "        \n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size, hidden_size, num_layers,\n",
        "            batch_first=True, dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "        \n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
        "        last_output = lstm_out[:, -1, :]\n",
        "        prediction = self.fc(last_output)\n",
        "        return prediction.squeeze()\n",
        "\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
        "model_price = LSTMRegressor(\n",
        "    input_size=len(feature_cols),\n",
        "    hidden_size=128,\n",
        "    num_layers=2,\n",
        "    dropout=0.3\n",
        ").to(device)\n",
        "\n",
        "print(\"LSTM Price Forecasting Model:\")\n",
        "print(model_price)\n",
        "print(f\"\\nParameters: {sum(p.numel() for p in model_price.parameters())}\")"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ price forecasting\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 100\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# DataLoader\n",
        "train_dataset_price = TensorDataset(X_train_price_t, y_train_price_t)\n",
        "train_loader_price = DataLoader(train_dataset_price, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion_price = nn.MSELoss()\n",
        "optimizer_price = optim.Adam(model_price.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Training loop\n",
        "print(\"Training Price Forecasting Model...\")\n",
        "train_losses = []\n",
        "\n",
        "model_price.train()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    epoch_loss = 0\n",
        "    for X_batch, y_batch in train_loader_price:\n",
        "        optimizer_price.zero_grad()\n",
        "        outputs = model_price(X_batch)\n",
        "        loss = criterion_price(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model_price.parameters(), max_norm=1.0)\n",
        "        optimizer_price.step()\n",
        "        epoch_loss += loss.item()\n",
        "    \n",
        "    avg_loss = epoch_loss / len(train_loader_price)\n",
        "    train_losses.append(avg_loss)\n",
        "    \n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {avg_loss:.6f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Training complete!\")"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Evaluation\n",
        "model_price.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_price_scaled = model_price(X_test_price_t).cpu().numpy()\n",
        "\n",
        "# Denormalize predictions\n",
        "y_pred_price = scaler_price.inverse_transform(y_pred_price_scaled.reshape(-1, 1)).flatten()\n",
        "y_true_price = scaler_price.inverse_transform(y_test_price.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Metrics\n",
        "rmse_price = np.sqrt(mean_squared_error(y_true_price, y_pred_price))\n",
        "mae_price = mean_absolute_error(y_true_price, y_pred_price)\n",
        "mape_price = np.mean(np.abs((y_true_price - y_pred_price) / y_true_price)) * 100\n",
        "\n",
        "print(f\"\\nüìä Task 1: Price Forecasting Results\")\n",
        "print(f\"   RMSE: {rmse_price:.5f}\")\n",
        "print(f\"   MAE: {mae_price:.5f}\")\n",
        "print(f\"   MAPE: {mape_price:.2f}%\")\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(y_true_price[:200], label='True Price', linewidth=2)\n",
        "plt.plot(y_pred_price[:200], label='Predicted Price', linewidth=2, alpha=0.7)\n",
        "plt.title(f'Price Forecasting (RMSE={rmse_price:.5f})', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Sample')\n",
        "plt.ylabel('Price')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
    ]
})

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –Ω–æ—É—Ç–±—É–∫
notebook['cells'] = cells

with open(notebook_path, 'w', encoding='utf-8') as f:
    json.dump(notebook, f, ensure_ascii=False, indent=1)

print(f'‚úÖ Part 3 –¥–æ–±–∞–≤–ª–µ–Ω–∞ (Tasks –Ω–∞—á–∞–ª–æ): {notebook_path}')
print(f'–í—Å–µ–≥–æ —è—á–µ–µ–∫: {len(cells)}')
print('–°–ª–µ–¥—É—é—â–∞—è —á–∞—Å—Ç—å: Task 2 (Direction) –∏ Task 3 (Patterns)...')

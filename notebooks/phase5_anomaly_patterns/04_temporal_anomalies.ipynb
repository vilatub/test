{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aac2577",
   "metadata": {},
   "source": [
    "# Phase 5.3: Temporal Anomaly Detection\n",
    "\n",
    "## Advanced Time Series Anomaly Methods\n",
    "\n",
    "This notebook covers sophisticated temporal anomaly detection techniques:\n",
    "\n",
    "1. **Change Point Detection**\n",
    "   - CUSUM (Cumulative Sum Control Chart)\n",
    "   - PELT (Pruned Exact Linear Time) Algorithm\n",
    "\n",
    "2. **Deep Learning for Temporal Anomalies**\n",
    "   - LSTM-Autoencoder for sequence reconstruction\n",
    "   - Anomaly scoring based on reconstruction error\n",
    "\n",
    "### Dataset\n",
    "We'll create synthetic sensor data with various anomaly types:\n",
    "- Point anomalies (sudden spikes)\n",
    "- Contextual anomalies (unusual patterns)\n",
    "- Collective anomalies (regime changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d71cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For Change Point Detection\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# For LSTM-Autoencoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"Libraries loaded successfully\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f528572d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sensor_data(n_samples=5000, anomaly_rate=0.05):\n",
    "    \"\"\"\n",
    "    Create synthetic sensor time series with various anomaly types.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Number of time points\n",
    "    anomaly_rate : float\n",
    "        Proportion of anomalies\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    data : DataFrame with sensor readings and labels\n",
    "    \"\"\"\n",
    "    t = np.arange(n_samples)\n",
    "\n",
    "    # Base signal: combination of trends and seasonality\n",
    "    trend = 0.001 * t\n",
    "    daily_pattern = 5 * np.sin(2 * np.pi * t / 24)  # 24-hour cycle\n",
    "    weekly_pattern = 3 * np.sin(2 * np.pi * t / 168)  # 168-hour (weekly) cycle\n",
    "    noise = np.random.normal(0, 0.5, n_samples)\n",
    "\n",
    "    signal = 50 + trend + daily_pattern + weekly_pattern + noise\n",
    "\n",
    "    # Initialize labels\n",
    "    labels = np.zeros(n_samples, dtype=int)\n",
    "\n",
    "    # Add different types of anomalies\n",
    "    n_anomalies = int(n_samples * anomaly_rate)\n",
    "\n",
    "    # Type 1: Point anomalies (sudden spikes) - 40% of anomalies\n",
    "    n_point = int(n_anomalies * 0.4)\n",
    "    point_indices = np.random.choice(range(100, n_samples-100), n_point, replace=False)\n",
    "    for idx in point_indices:\n",
    "        signal[idx] += np.random.choice([-1, 1]) * np.random.uniform(10, 20)\n",
    "        labels[idx] = 1\n",
    "\n",
    "    # Type 2: Level shifts (regime changes) - 30% of anomalies\n",
    "    n_shifts = 3\n",
    "    shift_starts = np.random.choice(range(500, n_samples-500), n_shifts, replace=False)\n",
    "    for start in shift_starts:\n",
    "        duration = np.random.randint(50, 150)\n",
    "        shift_amount = np.random.choice([-1, 1]) * np.random.uniform(5, 10)\n",
    "        signal[start:start+duration] += shift_amount\n",
    "        labels[start:start+duration] = 1\n",
    "\n",
    "    # Type 3: Gradual drifts - 30% of anomalies\n",
    "    n_drifts = 2\n",
    "    drift_starts = np.random.choice(range(200, n_samples-300), n_drifts, replace=False)\n",
    "    for start in drift_starts:\n",
    "        duration = np.random.randint(100, 200)\n",
    "        drift = np.linspace(0, np.random.uniform(8, 15), duration)\n",
    "        signal[start:start+duration] += drift\n",
    "        labels[start:start+duration] = 1\n",
    "\n",
    "    # Create DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'timestamp': pd.date_range('2024-01-01', periods=n_samples, freq='H'),\n",
    "        'sensor_value': signal,\n",
    "        'is_anomaly': labels\n",
    "    })\n",
    "\n",
    "    return data\n",
    "\n",
    "# Generate data\n",
    "sensor_data = create_sensor_data(n_samples=5000, anomaly_rate=0.05)\n",
    "\n",
    "print(f\"Dataset shape: {sensor_data.shape}\")\n",
    "print(f\"Total anomalies: {sensor_data['is_anomaly'].sum()} ({sensor_data['is_anomaly'].mean()*100:.1f}%)\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(sensor_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af0ae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the time series with anomalies\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Full time series\n",
    "ax1 = axes[0]\n",
    "ax1.plot(sensor_data['timestamp'], sensor_data['sensor_value'],\n",
    "         linewidth=0.5, alpha=0.7, label='Sensor Value')\n",
    "\n",
    "# Highlight anomalies\n",
    "anomaly_mask = sensor_data['is_anomaly'] == 1\n",
    "ax1.scatter(sensor_data.loc[anomaly_mask, 'timestamp'],\n",
    "            sensor_data.loc[anomaly_mask, 'sensor_value'],\n",
    "            c='red', s=10, alpha=0.5, label='Anomalies')\n",
    "\n",
    "ax1.set_xlabel('Time')\n",
    "ax1.set_ylabel('Sensor Value')\n",
    "ax1.set_title('Complete Time Series with Ground Truth Anomalies')\n",
    "ax1.legend()\n",
    "\n",
    "# Zoomed view (first 500 points)\n",
    "ax2 = axes[1]\n",
    "zoom_data = sensor_data.iloc[:500]\n",
    "ax2.plot(zoom_data['timestamp'], zoom_data['sensor_value'],\n",
    "         linewidth=1, alpha=0.8, label='Sensor Value')\n",
    "\n",
    "zoom_anomalies = zoom_data[zoom_data['is_anomaly'] == 1]\n",
    "ax2.scatter(zoom_anomalies['timestamp'], zoom_anomalies['sensor_value'],\n",
    "            c='red', s=30, alpha=0.7, label='Anomalies')\n",
    "\n",
    "ax2.set_xlabel('Time')\n",
    "ax2.set_ylabel('Sensor Value')\n",
    "ax2.set_title('Zoomed View: First 500 Time Points')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd823253",
   "metadata": {},
   "source": [
    "## 1. Change Point Detection\n",
    "\n",
    "### 1.1 CUSUM (Cumulative Sum Control Chart)\n",
    "\n",
    "CUSUM detects changes in the mean of a time series by accumulating deviations from a target value. It's particularly effective for detecting small, persistent shifts.\n",
    "\n",
    "**Algorithm:**\n",
    "- $S_t^+ = \\max(0, S_{t-1}^+ + (x_t - \\mu_0 - k))$ (upward CUSUM)\n",
    "- $S_t^- = \\max(0, S_{t-1}^- + (-x_t + \\mu_0 - k))$ (downward CUSUM)\n",
    "\n",
    "Where $k$ is the allowable slack and $h$ is the decision threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c18219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cusum_detector(data, threshold=5, drift=0.5):\n",
    "    \"\"\"\n",
    "    CUSUM (Cumulative Sum) change point detection.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        Time series data\n",
    "    threshold : float\n",
    "        Decision threshold (h)\n",
    "    drift : float\n",
    "        Allowable slack (k)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    change_points : list of indices where changes detected\n",
    "    cusum_pos : positive CUSUM values\n",
    "    cusum_neg : negative CUSUM values\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    mean = np.mean(data[:100])  # Use first 100 points as baseline\n",
    "    std = np.std(data[:100])\n",
    "\n",
    "    # Normalize by std\n",
    "    normalized = (data - mean) / std\n",
    "\n",
    "    cusum_pos = np.zeros(n)\n",
    "    cusum_neg = np.zeros(n)\n",
    "    change_points = []\n",
    "\n",
    "    for i in range(1, n):\n",
    "        cusum_pos[i] = max(0, cusum_pos[i-1] + normalized[i] - drift)\n",
    "        cusum_neg[i] = max(0, cusum_neg[i-1] - normalized[i] - drift)\n",
    "\n",
    "        # Check for change point\n",
    "        if cusum_pos[i] > threshold or cusum_neg[i] > threshold:\n",
    "            change_points.append(i)\n",
    "            # Reset after detection\n",
    "            cusum_pos[i] = 0\n",
    "            cusum_neg[i] = 0\n",
    "\n",
    "    return change_points, cusum_pos, cusum_neg\n",
    "\n",
    "# Apply CUSUM\n",
    "values = sensor_data['sensor_value'].values\n",
    "change_points, cusum_pos, cusum_neg = cusum_detector(values, threshold=5, drift=0.5)\n",
    "\n",
    "print(f\"CUSUM detected {len(change_points)} change points\")\n",
    "print(f\"First 10 change points at indices: {change_points[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76622c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CUSUM results\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "# Original signal with change points\n",
    "ax1 = axes[0]\n",
    "ax1.plot(values, linewidth=0.5, alpha=0.7)\n",
    "for cp in change_points:\n",
    "    ax1.axvline(x=cp, color='red', alpha=0.3, linewidth=1)\n",
    "ax1.set_ylabel('Sensor Value')\n",
    "ax1.set_title('Time Series with CUSUM Change Points')\n",
    "\n",
    "# CUSUM statistics\n",
    "ax2 = axes[1]\n",
    "ax2.plot(cusum_pos, label='CUSUM+', alpha=0.7)\n",
    "ax2.plot(cusum_neg, label='CUSUM-', alpha=0.7)\n",
    "ax2.axhline(y=5, color='red', linestyle='--', label='Threshold')\n",
    "ax2.set_ylabel('CUSUM Value')\n",
    "ax2.set_title('CUSUM Statistics')\n",
    "ax2.legend()\n",
    "\n",
    "# Zoomed view\n",
    "ax3 = axes[2]\n",
    "zoom_range = slice(0, 1000)\n",
    "ax3.plot(values[zoom_range], linewidth=1, alpha=0.8)\n",
    "zoom_cps = [cp for cp in change_points if cp < 1000]\n",
    "for cp in zoom_cps:\n",
    "    ax3.axvline(x=cp, color='red', alpha=0.5, linewidth=2)\n",
    "ax3.set_xlabel('Time Index')\n",
    "ax3.set_ylabel('Sensor Value')\n",
    "ax3.set_title('Zoomed View: First 1000 Points')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f5d06b",
   "metadata": {},
   "source": [
    "### 1.2 PELT (Pruned Exact Linear Time) Algorithm\n",
    "\n",
    "PELT is an efficient algorithm for exact segmentation of time series. It finds the optimal number and location of change points by minimizing a cost function with a penalty for the number of changes.\n",
    "\n",
    "We'll implement a simplified version using dynamic programming with pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39489f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pelt_detector(data, penalty=10, min_segment=10):\n",
    "    \"\"\"\n",
    "    Simplified PELT-like change point detection using dynamic programming.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        Time series data\n",
    "    penalty : float\n",
    "        Penalty for adding a change point (BIC-like)\n",
    "    min_segment : int\n",
    "        Minimum segment length\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    change_points : list of change point indices\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "\n",
    "    # Cost function: negative log-likelihood for Gaussian\n",
    "    def segment_cost(start, end):\n",
    "        if end - start < 2:\n",
    "            return np.inf\n",
    "        segment = data[start:end]\n",
    "        var = np.var(segment)\n",
    "        if var == 0:\n",
    "            var = 1e-10\n",
    "        return (end - start) * np.log(var)\n",
    "\n",
    "    # Dynamic programming\n",
    "    # F[t] = minimum cost of segmenting data[0:t]\n",
    "    F = np.full(n + 1, np.inf)\n",
    "    F[0] = -penalty  # Will add penalty for first segment\n",
    "\n",
    "    # Store the last change point for each position\n",
    "    last_cp = np.zeros(n + 1, dtype=int)\n",
    "\n",
    "    for t in range(min_segment, n + 1):\n",
    "        # Find best previous change point\n",
    "        candidates = range(max(0, t - 500), t - min_segment + 1)  # Limit search for efficiency\n",
    "\n",
    "        for s in candidates:\n",
    "            cost = F[s] + segment_cost(s, t) + penalty\n",
    "            if cost < F[t]:\n",
    "                F[t] = cost\n",
    "                last_cp[t] = s\n",
    "\n",
    "    # Backtrack to find change points\n",
    "    change_points = []\n",
    "    t = n\n",
    "    while t > 0:\n",
    "        if last_cp[t] > 0:\n",
    "            change_points.append(last_cp[t])\n",
    "        t = last_cp[t]\n",
    "\n",
    "    change_points = sorted(change_points)\n",
    "    return change_points\n",
    "\n",
    "# Apply PELT\n",
    "pelt_change_points = pelt_detector(values, penalty=15, min_segment=20)\n",
    "\n",
    "print(f\"PELT detected {len(pelt_change_points)} change points\")\n",
    "print(f\"Change points at indices: {pelt_change_points[:15]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ebfae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PELT results\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Full series with PELT segments\n",
    "ax1 = axes[0]\n",
    "ax1.plot(values, linewidth=0.5, alpha=0.7)\n",
    "\n",
    "# Color each segment differently\n",
    "all_cps = [0] + pelt_change_points + [len(values)]\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(all_cps)-1))\n",
    "\n",
    "for i in range(len(all_cps)-1):\n",
    "    start, end = all_cps[i], all_cps[i+1]\n",
    "    segment_mean = np.mean(values[start:end])\n",
    "    ax1.hlines(y=segment_mean, xmin=start, xmax=end,\n",
    "               colors=colors[i], linewidths=2, alpha=0.7)\n",
    "    ax1.axvline(x=start, color='red', alpha=0.3, linewidth=1)\n",
    "\n",
    "ax1.set_ylabel('Sensor Value')\n",
    "ax1.set_title('PELT Segmentation with Segment Means')\n",
    "\n",
    "# Compare with ground truth\n",
    "ax2 = axes[1]\n",
    "ax2.plot(values, linewidth=0.5, alpha=0.5, label='Signal')\n",
    "\n",
    "# PELT change points\n",
    "for cp in pelt_change_points:\n",
    "    ax2.axvline(x=cp, color='blue', alpha=0.5, linewidth=1, label='PELT' if cp == pelt_change_points[0] else '')\n",
    "\n",
    "# Ground truth anomaly regions\n",
    "anomaly_indices = np.where(sensor_data['is_anomaly'].values == 1)[0]\n",
    "ax2.scatter(anomaly_indices, values[anomaly_indices], c='red', s=5, alpha=0.3, label='True Anomalies')\n",
    "\n",
    "ax2.set_xlabel('Time Index')\n",
    "ax2.set_ylabel('Sensor Value')\n",
    "ax2.set_title('PELT Change Points vs Ground Truth Anomalies')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9267475",
   "metadata": {},
   "source": [
    "## 2. LSTM-Autoencoder for Temporal Anomaly Detection\n",
    "\n",
    "LSTM-Autoencoders learn to reconstruct normal time series patterns. Anomalies are detected when the reconstruction error exceeds a threshold.\n",
    "\n",
    "### Architecture:\n",
    "1. **Encoder**: LSTM layers compress the input sequence\n",
    "2. **Bottleneck**: Compressed representation\n",
    "3. **Decoder**: LSTM layers reconstruct the sequence\n",
    "\n",
    "### Anomaly Score:\n",
    "- Compute reconstruction error for each sequence\n",
    "- Higher error indicates more anomalous behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4a5a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length=50):\n",
    "    \"\"\"\n",
    "    Create sequences for LSTM-Autoencoder.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        Time series data\n",
    "    seq_length : int\n",
    "        Length of each sequence\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    sequences : array of shape (n_sequences, seq_length, 1)\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    for i in range(len(data) - seq_length + 1):\n",
    "        sequences.append(data[i:i+seq_length])\n",
    "    return np.array(sequences).reshape(-1, seq_length, 1)\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler()\n",
    "values_scaled = scaler.fit_transform(values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Create sequences\n",
    "SEQ_LENGTH = 50\n",
    "sequences = create_sequences(values_scaled, SEQ_LENGTH)\n",
    "\n",
    "print(f\"Number of sequences: {sequences.shape[0]}\")\n",
    "print(f\"Sequence shape: {sequences.shape}\")\n",
    "\n",
    "# Split into train/test (use first 70% for training - assumed normal)\n",
    "train_size = int(len(sequences) * 0.7)\n",
    "X_train = sequences[:train_size]\n",
    "X_test = sequences[train_size:]\n",
    "\n",
    "# Get corresponding labels for test set\n",
    "test_labels = sensor_data['is_anomaly'].values[train_size + SEQ_LENGTH - 1:]\n",
    "\n",
    "print(f\"\\nTraining sequences: {X_train.shape[0]}\")\n",
    "print(f\"Test sequences: {X_test.shape[0]}\")\n",
    "print(f\"Test labels: {len(test_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a8d564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_autoencoder(seq_length, n_features=1):\n",
    "    \"\"\"\n",
    "    Build LSTM-Autoencoder model.\n",
    "\n",
    "    Architecture:\n",
    "    - Encoder: LSTM(64) -> LSTM(32)\n",
    "    - Decoder: LSTM(32) -> LSTM(64) -> TimeDistributed(Dense)\n",
    "    \"\"\"\n",
    "    # Encoder\n",
    "    inputs = Input(shape=(seq_length, n_features))\n",
    "\n",
    "    # Encoder\n",
    "    encoded = LSTM(64, activation='relu', return_sequences=True)(inputs)\n",
    "    encoded = LSTM(32, activation='relu', return_sequences=False)(encoded)\n",
    "\n",
    "    # Bottleneck\n",
    "    bottleneck = RepeatVector(seq_length)(encoded)\n",
    "\n",
    "    # Decoder\n",
    "    decoded = LSTM(32, activation='relu', return_sequences=True)(bottleneck)\n",
    "    decoded = LSTM(64, activation='relu', return_sequences=True)(decoded)\n",
    "\n",
    "    # Output\n",
    "    outputs = TimeDistributed(Dense(n_features))(decoded)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "model = build_lstm_autoencoder(SEQ_LENGTH)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33e2e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, X_train,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(history.history['loss'], label='Training Loss')\n",
    "ax.plot(history.history['val_loss'], label='Validation Loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss (MSE)')\n",
    "ax.set_title('LSTM-Autoencoder Training History')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal training loss: {history.history['loss'][-1]:.6f}\")\n",
    "print(f\"Final validation loss: {history.history['val_loss'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010c59d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate reconstruction error\n",
    "train_pred = model.predict(X_train, verbose=0)\n",
    "test_pred = model.predict(X_test, verbose=0)\n",
    "\n",
    "# Mean squared error for each sequence\n",
    "train_mse = np.mean(np.square(X_train - train_pred), axis=(1, 2))\n",
    "test_mse = np.mean(np.square(X_test - test_pred), axis=(1, 2))\n",
    "\n",
    "# Set threshold based on training data (mean + 2*std)\n",
    "threshold = np.mean(train_mse) + 2 * np.std(train_mse)\n",
    "\n",
    "print(f\"Training MSE - Mean: {np.mean(train_mse):.6f}, Std: {np.std(train_mse):.6f}\")\n",
    "print(f\"Anomaly threshold: {threshold:.6f}\")\n",
    "print(f\"Test MSE - Mean: {np.mean(test_mse):.6f}, Std: {np.std(test_mse):.6f}\")\n",
    "\n",
    "# Detect anomalies\n",
    "predictions = (test_mse > threshold).astype(int)\n",
    "print(f\"\\nPredicted anomalies in test set: {predictions.sum()} ({predictions.mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf152e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reconstruction error and anomalies\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "# Reconstruction error distribution\n",
    "ax1 = axes[0]\n",
    "ax1.hist(train_mse, bins=50, alpha=0.5, label='Train', density=True)\n",
    "ax1.hist(test_mse, bins=50, alpha=0.5, label='Test', density=True)\n",
    "ax1.axvline(x=threshold, color='red', linestyle='--', label=f'Threshold={threshold:.4f}')\n",
    "ax1.set_xlabel('Reconstruction Error (MSE)')\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.set_title('Distribution of Reconstruction Errors')\n",
    "ax1.legend()\n",
    "\n",
    "# Time series with anomaly scores\n",
    "ax2 = axes[1]\n",
    "test_indices = np.arange(train_size + SEQ_LENGTH - 1, len(values))\n",
    "ax2.plot(test_indices, values[train_size + SEQ_LENGTH - 1:], linewidth=0.5, alpha=0.5)\n",
    "\n",
    "# Normalize MSE for visualization\n",
    "mse_normalized = (test_mse - test_mse.min()) / (test_mse.max() - test_mse.min())\n",
    "colors = plt.cm.Reds(mse_normalized)\n",
    "\n",
    "for i, (idx, mse_val) in enumerate(zip(test_indices[:len(test_mse)], test_mse)):\n",
    "    if mse_val > threshold:\n",
    "        ax2.axvline(x=idx, color='red', alpha=0.3, linewidth=1)\n",
    "\n",
    "ax2.set_ylabel('Sensor Value')\n",
    "ax2.set_title('Test Data with LSTM-AE Detected Anomalies (red lines)')\n",
    "\n",
    "# Comparison with ground truth\n",
    "ax3 = axes[2]\n",
    "ax3.plot(test_mse, label='Reconstruction Error', alpha=0.7)\n",
    "ax3.axhline(y=threshold, color='red', linestyle='--', label='Threshold')\n",
    "\n",
    "# Mark true anomalies\n",
    "if len(test_labels) == len(test_mse):\n",
    "    true_anomaly_idx = np.where(test_labels == 1)[0]\n",
    "    ax3.scatter(true_anomaly_idx, test_mse[true_anomaly_idx],\n",
    "                c='green', s=20, alpha=0.5, label='True Anomalies')\n",
    "\n",
    "ax3.set_xlabel('Sequence Index')\n",
    "ax3.set_ylabel('MSE')\n",
    "ax3.set_title('Reconstruction Error with True Anomaly Locations')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c4e115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate LSTM-Autoencoder performance\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Align predictions with labels\n",
    "min_len = min(len(predictions), len(test_labels))\n",
    "pred_aligned = predictions[:min_len]\n",
    "labels_aligned = test_labels[:min_len]\n",
    "\n",
    "# Calculate metrics\n",
    "precision = precision_score(labels_aligned, pred_aligned, zero_division=0)\n",
    "recall = recall_score(labels_aligned, pred_aligned, zero_division=0)\n",
    "f1 = f1_score(labels_aligned, pred_aligned, zero_division=0)\n",
    "\n",
    "print(\"LSTM-Autoencoder Performance:\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-Score: {f1:.3f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(labels_aligned, pred_aligned)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"TN: {cm[0,0]}, FP: {cm[0,1]}\")\n",
    "print(f\"FN: {cm[1,0]}, TP: {cm[1,1]}\")\n",
    "\n",
    "# ROC curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, _ = roc_curve(labels_aligned, test_mse[:min_len])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('LSTM-Autoencoder ROC Curve')\n",
    "ax.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29e3e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all methods\n",
    "print(\"=\" * 60)\n",
    "print(\"TEMPORAL ANOMALY DETECTION - METHOD COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. CUSUM (Cumulative Sum Control Chart)\")\n",
    "print(f\"   - Change points detected: {len(change_points)}\")\n",
    "print(\"   - Best for: Small, persistent shifts in mean\")\n",
    "print(\"   - Advantages: Simple, interpretable, real-time capable\")\n",
    "\n",
    "print(\"\\n2. PELT (Pruned Exact Linear Time)\")\n",
    "print(f\"   - Segments detected: {len(pelt_change_points) + 1}\")\n",
    "print(\"   - Best for: Optimal segmentation with unknown number of changes\")\n",
    "print(\"   - Advantages: Exact solution, handles multiple change types\")\n",
    "\n",
    "print(\"\\n3. LSTM-Autoencoder\")\n",
    "print(f\"   - AUC-ROC: {roc_auc:.3f}\")\n",
    "print(f\"   - Precision: {precision:.3f}, Recall: {recall:.3f}\")\n",
    "print(\"   - Best for: Complex temporal patterns, sequence anomalies\")\n",
    "print(\"   - Advantages: Learns complex dependencies, no assumptions on data\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"- Use CUSUM for real-time monitoring with quick response\")\n",
    "print(\"- Use PELT for offline analysis and optimal segmentation\")\n",
    "print(\"- Use LSTM-AE for complex patterns when labels available for tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c50993e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Change Point Detection Methods**\n",
    "   - CUSUM: Effective for detecting shifts in mean, runs in linear time\n",
    "   - PELT: Optimal segmentation with penalty-based change point detection\n",
    "\n",
    "2. **Deep Learning Approaches**\n",
    "   - LSTM-Autoencoder learns temporal patterns from normal data\n",
    "   - Anomalies detected via reconstruction error threshold\n",
    "   - Requires careful threshold tuning for best results\n",
    "\n",
    "3. **Dataset Characteristics**\n",
    "   - 5,000 time points with synthetic sensor data\n",
    "   - Multiple anomaly types: point, shift, drift\n",
    "   - 5% anomaly rate\n",
    "\n",
    "### Next Steps\n",
    "- Combine methods for ensemble anomaly detection\n",
    "- Experiment with different LSTM architectures (attention, bidirectional)\n",
    "- Apply to real-world sensor data from manufacturing or IoT"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

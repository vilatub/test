#!/usr/bin/env python3
"""
–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —á–∞—Å—Ç–∏ –≤ Feature Engineering notebook
"""

import json

# –ß–∏—Ç–∞–µ–º —Ç–µ–∫—É—â–∏–π –Ω–æ—É—Ç–±—É–∫
notebook_path = '06_advanced_feature_engineering.ipynb'
with open(notebook_path, 'r', encoding='utf-8') as f:
    notebook = json.load(f)

# –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —è—á–µ–π–∫–∏
practical_cells = []

# ============================================================================
# DATA LOADING
# ============================================================================

practical_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 2.2 –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö: House Prices\n",
        "\n",
        "–ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞—Ç–∞—Å–µ—Ç Kaggle House Prices."
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ —É–∂–µ –µ—Å—Ç—å –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –Ω–æ—É—Ç–±—É–∫–∞)\n",
        "import os\n",
        "\n",
        "data_path = '../../data/house_prices_train.csv'\n",
        "\n",
        "if not os.path.exists(data_path):\n",
        "    print('‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω!')\n",
        "    print('–°–∫–∞—á–∞–π—Ç–µ: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data')\n",
        "    print('–ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –Ω–æ—É—Ç–±—É–∫–∞ 03_catboost_deep_dive.ipynb')\n",
        "else:\n",
        "    df = pd.read_csv(data_path)\n",
        "    print(f'‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {df.shape[0]:,} —Å—Ç—Ä–æ–∫, {df.shape[1]} —Å—Ç–æ–ª–±—Ü–æ–≤')\n",
        "    print(f'Target: SalePrice')\n",
        "    print(f'–†–∞–∑–º–µ—Ä –≤ –ø–∞–º—è—Ç–∏: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB')"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ü–µ—Ä–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –¥–∞–Ω–Ω—ã–µ\n",
        "df.head()"
    ]
})

# ============================================================================
# EDA
# ============================================================================

practical_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 2.3 EDA –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è\n",
        "print('–†–∞–∑–º–µ—Ä:', df.shape)\n",
        "print('\\nTarget —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:')\n",
        "print(df['SalePrice'].describe())\n",
        "print(f'\\nSkewness: {df[\"SalePrice\"].skew():.2f}')\n",
        "print(f'Kurtosis: {df[\"SalePrice\"].kurtosis():.2f}')"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è target\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Histogram\n",
        "axes[0].hist(df['SalePrice'], bins=50, edgecolor='black', alpha=0.7)\n",
        "axes[0].set_xlabel('SalePrice')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].set_title(f'SalePrice Distribution (Skewness: {df[\"SalePrice\"].skew():.2f})')\n",
        "axes[0].axvline(df['SalePrice'].mean(), color='red', linestyle='--', label='Mean')\n",
        "axes[0].axvline(df['SalePrice'].median(), color='green', linestyle='--', label='Median')\n",
        "axes[0].legend()\n",
        "\n",
        "# Q-Q plot\n",
        "stats.probplot(df['SalePrice'], dist=\"norm\", plot=axes[1])\n",
        "axes[1].set_title('Q-Q Plot (–ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç–∏)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('üîç SalePrice –∏–º–µ–µ—Ç right skew ‚Üí —Ö–æ—Ä–æ—à–∏–π –∫–∞–Ω–¥–∏–¥–∞—Ç –¥–ª—è log transform!')"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ü—Ä–æ—Å—Ç–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "# –í—ã–±–∏—Ä–∞–µ–º numeric –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\n",
        "\n",
        "# Numeric features\n",
        "numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "numeric_features.remove('SalePrice')  # Target\n",
        "if 'Id' in numeric_features:\n",
        "    numeric_features.remove('Id')  # ID –Ω–µ –Ω—É–∂–µ–Ω\n",
        "\n",
        "# Categorical features (–ø—Ä–∏–º–µ—Ä –¥–ª—è target encoding)\n",
        "categorical_features = ['Neighborhood', 'BldgType', 'HouseStyle', 'ExterQual', 'KitchenQual']\n",
        "\n",
        "# –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –º–µ–¥–∏–∞–Ω–æ–π –¥–ª—è numeric\n",
        "for col in numeric_features:\n",
        "    if df[col].isnull().sum() > 0:\n",
        "        df[col].fillna(df[col].median(), inplace=True)\n",
        "\n",
        "# –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –º–æ–¥–æ–π –¥–ª—è categorical\n",
        "for col in categorical_features:\n",
        "    if df[col].isnull().sum() > 0:\n",
        "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "\n",
        "print(f'‚úÖ Numeric –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(numeric_features)}')\n",
        "print(f'‚úÖ Categorical –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(categorical_features)}')\n",
        "print(f'‚úÖ –ü—Ä–æ–ø—É—Å–∫–∏ –∑–∞–ø–æ–ª–Ω–µ–Ω—ã')"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Train/test split\n",
        "X = df[numeric_features + categorical_features].copy()\n",
        "y = df['SalePrice'].copy()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "print(f'Train: {X_train.shape[0]:,} samples')\n",
        "print(f'Test: {X_test.shape[0]:,} samples')\n",
        "print(f'Features: {X_train.shape[1]}')"
    ]
})

# ============================================================================
# BASELINE MODEL
# ============================================================================

practical_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 2.4 Baseline –º–æ–¥–µ–ª—å (–±–µ–∑ feature engineering)"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ü—Ä–æ—Å—Ç–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è baseline: one-hot encoding –¥–ª—è categorical\n",
        "X_train_baseline = pd.get_dummies(X_train, columns=categorical_features, drop_first=True)\n",
        "X_test_baseline = pd.get_dummies(X_test, columns=categorical_features, drop_first=True)\n",
        "\n",
        "# –í—ã—Ä–æ–≤–Ω—è—Ç—å –∫–æ–ª–æ–Ω–∫–∏ (train/test –º–æ–≥—É—Ç –∏–º–µ—Ç—å —Ä–∞–∑–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏)\n",
        "X_train_baseline, X_test_baseline = X_train_baseline.align(X_test_baseline, join='left', axis=1, fill_value=0)\n",
        "\n",
        "# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_baseline)\n",
        "X_test_scaled = scaler.transform(X_test_baseline)\n",
        "\n",
        "print(f'Baseline features: {X_train_scaled.shape[1]}')"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Baseline Ridge Regression\n",
        "baseline_model = Ridge(alpha=10.0, random_state=RANDOM_STATE)\n",
        "baseline_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "y_pred_baseline = baseline_model.predict(X_test_scaled)\n",
        "\n",
        "# –ú–µ—Ç—Ä–∏–∫–∏\n",
        "rmse_baseline = np.sqrt(mean_squared_error(y_test, y_pred_baseline))\n",
        "mae_baseline = mean_absolute_error(y_test, y_pred_baseline)\n",
        "r2_baseline = r2_score(y_test, y_pred_baseline)\n",
        "\n",
        "print('üìä Baseline Model (Ridge Regression):')\n",
        "print(f'  RMSE: ${rmse_baseline:,.0f}')\n",
        "print(f'  MAE: ${mae_baseline:,.0f}')\n",
        "print(f'  R¬≤: {r2_baseline:.4f}')\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω–∏–º –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
        "results = {\n",
        "    'Baseline (Ridge)': {\n",
        "        'RMSE': rmse_baseline,\n",
        "        'MAE': mae_baseline,\n",
        "        'R¬≤': r2_baseline,\n",
        "        'Features': X_train_scaled.shape[1]\n",
        "    }\n",
        "}"
    ]
})

# ============================================================================
# POLYNOMIAL FEATURES
# ============================================================================

practical_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 2.5 Polynomial Features –∏ Interactions"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –í—ã–±–µ—Ä–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã—Ö numeric –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è polynomial (—á—Ç–æ–±—ã –Ω–µ –≤–∑–æ—Ä–≤–∞—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å)\n",
        "key_features = ['GrLivArea', 'OverallQual', 'TotalBsmtSF', 'GarageCars', 'YearBuilt']\n",
        "\n",
        "X_train_poly = X_train[key_features].copy()\n",
        "X_test_poly = X_test[key_features].copy()\n",
        "\n",
        "# Polynomial features degree 2\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_train_poly_transformed = poly.fit_transform(X_train_poly)\n",
        "X_test_poly_transformed = poly.transform(X_test_poly)\n",
        "\n",
        "print(f'–ò—Å—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(key_features)}')\n",
        "print(f'–ü–æ—Å–ª–µ polynomial degree 2: {X_train_poly_transformed.shape[1]}')\n",
        "print(f'\\n–ù–∞–∑–≤–∞–Ω–∏—è –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:')\n",
        "feature_names = poly.get_feature_names_out(key_features)\n",
        "print(feature_names[:10], '...')\n",
        "print('\\nüîç –ü—Ä–∏–º–µ—Ä—ã interactions:')\n",
        "print('  GrLivArea √ó OverallQual (–±–æ–ª—å—à–∞—è –ø–ª–æ—â–∞–¥—å √ó –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ)')\n",
        "print('  TotalBsmtSF √ó GarageCars (–ø–æ–¥–≤–∞–ª √ó –≥–∞—Ä–∞–∂)')"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –î–æ–±–∞–≤–∏–º polynomial features –∫ baseline\n",
        "X_train_with_poly = np.hstack([X_train_scaled, X_train_poly_transformed])\n",
        "X_test_with_poly = np.hstack([X_test_scaled, X_test_poly_transformed])\n",
        "\n",
        "# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "scaler_poly = StandardScaler()\n",
        "X_train_with_poly = scaler_poly.fit_transform(X_train_with_poly)\n",
        "X_test_with_poly = scaler_poly.transform(X_test_with_poly)\n",
        "\n",
        "# Ridge —Å polynomial features\n",
        "model_poly = Ridge(alpha=10.0, random_state=RANDOM_STATE)\n",
        "model_poly.fit(X_train_with_poly, y_train)\n",
        "\n",
        "y_pred_poly = model_poly.predict(X_test_with_poly)\n",
        "\n",
        "rmse_poly = np.sqrt(mean_squared_error(y_test, y_pred_poly))\n",
        "mae_poly = mean_absolute_error(y_test, y_pred_poly)\n",
        "r2_poly = r2_score(y_test, y_pred_poly)\n",
        "\n",
        "print('üìä Model with Polynomial Features:')\n",
        "print(f'  RMSE: ${rmse_poly:,.0f}')\n",
        "print(f'  MAE: ${mae_poly:,.0f}')\n",
        "print(f'  R¬≤: {r2_poly:.4f}')\n",
        "print(f'\\nüìà Improvement over baseline:')\n",
        "print(f'  RMSE: {(rmse_baseline - rmse_poly) / rmse_baseline * 100:.1f}%')\n",
        "print(f'  R¬≤: {(r2_poly - r2_baseline):.4f}')\n",
        "\n",
        "results['Polynomial Features'] = {\n",
        "    'RMSE': rmse_poly,\n",
        "    'MAE': mae_poly,\n",
        "    'R¬≤': r2_poly,\n",
        "    'Features': X_train_with_poly.shape[1]\n",
        "}"
    ]
})

# ============================================================================
# LOG TRANSFORM
# ============================================================================

practical_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 2.6 Log Transform –Ω–∞ Target –∏ Skewed Features"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Log transform –Ω–∞ target (SalePrice is right-skewed)\n",
        "y_train_log = np.log1p(y_train)  # log1p = log(1 + x) –¥–ª—è –∏–∑–±–µ–≥–∞–Ω–∏—è log(0)\n",
        "y_test_log = np.log1p(y_test)\n",
        "\n",
        "# –ù–∞–π–¥–µ–º skewed –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ numeric features\n",
        "skewed_features = []\n",
        "for col in numeric_features:\n",
        "    if X_train[col].skew() > 0.75:  # Threshold –¥–ª—è skewness\n",
        "        skewed_features.append(col)\n",
        "\n",
        "print(f'–ù–∞–π–¥–µ–Ω–æ {len(skewed_features)} skewed –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (skew > 0.75):')\n",
        "print(skewed_features[:10])"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Log transform –Ω–∞ skewed features\n",
        "X_train_log = X_train.copy()\n",
        "X_test_log = X_test.copy()\n",
        "\n",
        "for col in skewed_features:\n",
        "    if col in X_train_log.columns and X_train_log[col].dtype in [np.int64, np.float64]:\n",
        "        X_train_log[col] = np.log1p(X_train_log[col])\n",
        "        X_test_log[col] = np.log1p(X_test_log[col])\n",
        "\n",
        "# One-hot –¥–ª—è categorical\n",
        "X_train_log = pd.get_dummies(X_train_log, columns=categorical_features, drop_first=True)\n",
        "X_test_log = pd.get_dummies(X_test_log, columns=categorical_features, drop_first=True)\n",
        "X_train_log, X_test_log = X_train_log.align(X_test_log, join='left', axis=1, fill_value=0)\n",
        "\n",
        "# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
        "scaler_log = StandardScaler()\n",
        "X_train_log_scaled = scaler_log.fit_transform(X_train_log)\n",
        "X_test_log_scaled = scaler_log.transform(X_test_log)\n",
        "\n",
        "# Ridge –Ω–∞ log-transformed data\n",
        "model_log = Ridge(alpha=10.0, random_state=RANDOM_STATE)\n",
        "model_log.fit(X_train_log_scaled, y_train_log)\n",
        "\n",
        "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–≤ log scale)\n",
        "y_pred_log = model_log.predict(X_test_log_scaled)\n",
        "\n",
        "# –û–±—Ä–∞—Ç–Ω–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è (expm1 = exp(x) - 1)\n",
        "y_pred_log_original = np.expm1(y_pred_log)\n",
        "\n",
        "rmse_log = np.sqrt(mean_squared_error(y_test, y_pred_log_original))\n",
        "mae_log = mean_absolute_error(y_test, y_pred_log_original)\n",
        "r2_log = r2_score(y_test, y_pred_log_original)\n",
        "\n",
        "print('üìä Model with Log Transform:')\n",
        "print(f'  RMSE: ${rmse_log:,.0f}')\n",
        "print(f'  MAE: ${mae_log:,.0f}')\n",
        "print(f'  R¬≤: {r2_log:.4f}')\n",
        "print(f'\\nüìà Improvement over baseline:')\n",
        "print(f'  RMSE: {(rmse_baseline - rmse_log) / rmse_baseline * 100:.1f}%')\n",
        "print(f'  R¬≤: {(r2_log - r2_baseline):.4f}')\n",
        "\n",
        "results['Log Transform'] = {\n",
        "    'RMSE': rmse_log,\n",
        "    'MAE': mae_log,\n",
        "    'R¬≤': r2_log,\n",
        "    'Features': X_train_log_scaled.shape[1]\n",
        "}"
    ]
})

# ============================================================================
# TARGET ENCODING
# ============================================================================

practical_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 2.7 Target Encoding –¥–ª—è Categorical Features"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –†—É—á–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è smoothed target encoding\n",
        "def target_encode_smooth(X_train, X_test, y_train, cat_col, m=10):\n",
        "    \"\"\"\n",
        "    Smoothed target encoding —Å Bayesian smoothing\n",
        "    \n",
        "    TE = (n_c * mean_c + m * global_mean) / (n_c + m)\n",
        "    \"\"\"\n",
        "    # –ì–ª–æ–±–∞–ª—å–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ\n",
        "    global_mean = y_train.mean()\n",
        "    \n",
        "    # –°—Ä–µ–¥–Ω–µ–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º\n",
        "    category_means = y_train.groupby(X_train[cat_col]).mean()\n",
        "    category_counts = X_train[cat_col].value_counts()\n",
        "    \n",
        "    # Smoothed encoding\n",
        "    smoothed_means = {}\n",
        "    for cat in category_means.index:\n",
        "        n_c = category_counts[cat]\n",
        "        mean_c = category_means[cat]\n",
        "        smoothed_means[cat] = (n_c * mean_c + m * global_mean) / (n_c + m)\n",
        "    \n",
        "    # Map –Ω–∞ train –∏ test\n",
        "    X_train_encoded = X_train[cat_col].map(smoothed_means).fillna(global_mean)\n",
        "    X_test_encoded = X_test[cat_col].map(smoothed_means).fillna(global_mean)\n",
        "    \n",
        "    return X_train_encoded, X_test_encoded\n",
        "\n",
        "print('‚úÖ –§—É–Ω–∫—Ü–∏—è target encoding —Å–æ–∑–¥–∞–Ω–∞')"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ü—Ä–∏–º–µ–Ω—è–µ–º target encoding –Ω–∞ categorical features\n",
        "X_train_te = X_train[numeric_features].copy()\n",
        "X_test_te = X_test[numeric_features].copy()\n",
        "\n",
        "for cat_col in categorical_features:\n",
        "    train_encoded, test_encoded = target_encode_smooth(\n",
        "        X_train, X_test, y_train, cat_col, m=10\n",
        "    )\n",
        "    X_train_te[f'{cat_col}_TE'] = train_encoded\n",
        "    X_test_te[f'{cat_col}_TE'] = test_encoded\n",
        "\n",
        "print(f'–ü—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ target encoding: {X_train_te.shape[1]}')\n",
        "print(f'–î–æ–±–∞–≤–ª–µ–Ω–æ: {len(categorical_features)} target-encoded features')\n",
        "print(f'\\n–í–º–µ—Å—Ç–æ {len(categorical_features)} one-hot —Å—Ç–æ–ª–±—Ü–æ–≤ ‚Üí {len(categorical_features)} TE —Å—Ç–æ–ª–±—Ü–æ–≤!')"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ\n",
        "scaler_te = StandardScaler()\n",
        "X_train_te_scaled = scaler_te.fit_transform(X_train_te)\n",
        "X_test_te_scaled = scaler_te.transform(X_test_te)\n",
        "\n",
        "model_te = Ridge(alpha=10.0, random_state=RANDOM_STATE)\n",
        "model_te.fit(X_train_te_scaled, y_train)\n",
        "\n",
        "y_pred_te = model_te.predict(X_test_te_scaled)\n",
        "\n",
        "rmse_te = np.sqrt(mean_squared_error(y_test, y_pred_te))\n",
        "mae_te = mean_absolute_error(y_test, y_pred_te)\n",
        "r2_te = r2_score(y_test, y_pred_te)\n",
        "\n",
        "print('üìä Model with Target Encoding:')\n",
        "print(f'  RMSE: ${rmse_te:,.0f}')\n",
        "print(f'  MAE: ${mae_te:,.0f}')\n",
        "print(f'  R¬≤: {r2_te:.4f}')\n",
        "print(f'\\nüìà Improvement over baseline:')\n",
        "print(f'  RMSE: {(rmse_baseline - rmse_te) / rmse_baseline * 100:.1f}%')\n",
        "print(f'  R¬≤: {(r2_te - r2_baseline):.4f}')\n",
        "\n",
        "results['Target Encoding'] = {\n",
        "    'RMSE': rmse_te,\n",
        "    'MAE': mae_te,\n",
        "    'R¬≤': r2_te,\n",
        "    'Features': X_train_te_scaled.shape[1]\n",
        "}"
    ]
})

# ============================================================================
# FEATURE SELECTION
# ============================================================================

practical_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 2.8 Feature Selection: Filter Methods"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# SelectKBest —Å f_regression (top K)\n",
        "k_best = 30\n",
        "\n",
        "selector = SelectKBest(score_func=f_regression, k=k_best)\n",
        "X_train_selected = selector.fit_transform(X_train_baseline, y_train)\n",
        "X_test_selected = selector.transform(X_test_baseline)\n",
        "\n",
        "# –ö–∞–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤—ã–±—Ä–∞–Ω—ã?\n",
        "selected_features = X_train_baseline.columns[selector.get_support()].tolist()\n",
        "print(f'SelectKBest: –í—ã–±—Ä–∞–Ω–æ {k_best} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ {X_train_baseline.shape[1]}')\n",
        "print(f'\\nTop 10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ F-score:')\n",
        "scores = pd.DataFrame({\n",
        "    'Feature': X_train_baseline.columns,\n",
        "    'Score': selector.scores_\n",
        "}).sort_values('Score', ascending=False)\n",
        "print(scores.head(10))"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ú–æ–¥–µ–ª—å –Ω–∞ selected features\n",
        "scaler_sel = StandardScaler()\n",
        "X_train_selected_scaled = scaler_sel.fit_transform(X_train_selected)\n",
        "X_test_selected_scaled = scaler_sel.transform(X_test_selected)\n",
        "\n",
        "model_selected = Ridge(alpha=10.0, random_state=RANDOM_STATE)\n",
        "model_selected.fit(X_train_selected_scaled, y_train)\n",
        "\n",
        "y_pred_selected = model_selected.predict(X_test_selected_scaled)\n",
        "\n",
        "rmse_selected = np.sqrt(mean_squared_error(y_test, y_pred_selected))\n",
        "mae_selected = mean_absolute_error(y_test, y_pred_selected)\n",
        "r2_selected = r2_score(y_test, y_pred_selected)\n",
        "\n",
        "print('üìä Model with SelectKBest (Filter):')\n",
        "print(f'  RMSE: ${rmse_selected:,.0f}')\n",
        "print(f'  MAE: ${mae_selected:,.0f}')\n",
        "print(f'  R¬≤: {r2_selected:.4f}')\n",
        "print(f'\\nüìà –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å baseline:')\n",
        "print(f'  RMSE: {(rmse_baseline - rmse_selected) / rmse_baseline * 100:+.1f}%')\n",
        "print(f'  Features: {X_train_baseline.shape[1]} ‚Üí {k_best} ({k_best / X_train_baseline.shape[1] * 100:.0f}%)')\n",
        "\n",
        "results['SelectKBest (Filter)'] = {\n",
        "    'RMSE': rmse_selected,\n",
        "    'MAE': mae_selected,\n",
        "    'R¬≤': r2_selected,\n",
        "    'Features': k_best\n",
        "}"
    ]
})

# ============================================================================
# LASSO (EMBEDDED)
# ============================================================================

practical_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 2.9 Feature Selection: Lasso (Embedded)"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Lasso –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ feature selection (L1 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è)\n",
        "lasso = Lasso(alpha=100.0, random_state=RANDOM_STATE)\n",
        "lasso.fit(X_train_scaled, y_train)\n",
        "\n",
        "# –°–∫–æ–ª—å–∫–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤—ã–±—Ä–∞–Ω–æ? (–Ω–µ–Ω—É–ª–µ–≤—ã—Ö –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤)\n",
        "n_features_lasso = np.sum(lasso.coef_ != 0)\n",
        "print(f'Lasso –≤—ã–±—Ä–∞–ª {n_features_lasso} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ {X_train_scaled.shape[1]}')\n",
        "\n",
        "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "y_pred_lasso = lasso.predict(X_test_scaled)\n",
        "\n",
        "rmse_lasso = np.sqrt(mean_squared_error(y_test, y_pred_lasso))\n",
        "mae_lasso = mean_absolute_error(y_test, y_pred_lasso)\n",
        "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
        "\n",
        "print('\\nüìä Lasso (Embedded Selection):')\n",
        "print(f'  RMSE: ${rmse_lasso:,.0f}')\n",
        "print(f'  MAE: ${mae_lasso:,.0f}')\n",
        "print(f'  R¬≤: {r2_lasso:.4f}')\n",
        "print(f'\\nüìà Improvement over baseline:')\n",
        "print(f'  RMSE: {(rmse_baseline - rmse_lasso) / rmse_baseline * 100:.1f}%')\n",
        "print(f'  Features selected: {n_features_lasso}/{X_train_scaled.shape[1]}')\n",
        "\n",
        "results['Lasso (Embedded)'] = {\n",
        "    'RMSE': rmse_lasso,\n",
        "    'MAE': mae_lasso,\n",
        "    'R¬≤': r2_lasso,\n",
        "    'Features': n_features_lasso\n",
        "}"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –¢–æ–ø –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –∞–±—Å–æ–ª—é—Ç–Ω–æ–º—É –∑–Ω–∞—á–µ–Ω–∏—é –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤\n",
        "lasso_importance = pd.DataFrame({\n",
        "    'Feature': X_train_baseline.columns,\n",
        "    'Coefficient': lasso.coef_\n",
        "}).sort_values('Coefficient', key=abs, ascending=False)\n",
        "\n",
        "print('Top 15 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ Lasso coefficients:')\n",
        "print(lasso_importance.head(15))\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "plt.figure(figsize=(10, 6))\n",
        "top_features = lasso_importance.head(15)\n",
        "plt.barh(range(len(top_features)), top_features['Coefficient'])\n",
        "plt.yticks(range(len(top_features)), top_features['Feature'])\n",
        "plt.xlabel('Lasso Coefficient')\n",
        "plt.title('Top 15 Features by Lasso Coefficients')\n",
        "plt.tight_layout()\n",
        "plt.show()"
    ]
})

# ============================================================================
# COMBINED APPROACH
# ============================================================================

practical_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 2.10 Combined Approach (–≤—Å—ë –≤–º–µ—Å—Ç–µ!)"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –ª—É—á—à–∏–µ —Ç–µ—Ö–Ω–∏–∫–∏:\n",
        "# 1. Log transform –Ω–∞ target\n",
        "# 2. Log transform –Ω–∞ skewed features\n",
        "# 3. Target encoding –¥–ª—è categorical\n",
        "# 4. Polynomial features (–∫–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏)\n",
        "# 5. XGBoost (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è feature selection)\n",
        "\n",
        "print('üöÄ –°–æ–∑–¥–∞–µ–º Combined Feature Engineering Pipeline...')\n",
        "\n",
        "# 1. Log –Ω–∞ skewed\n",
        "X_train_combined = X_train.copy()\n",
        "X_test_combined = X_test.copy()\n",
        "\n",
        "for col in skewed_features:\n",
        "    if col in X_train_combined.columns and X_train_combined[col].dtype in [np.int64, np.float64]:\n",
        "        X_train_combined[col] = np.log1p(X_train_combined[col])\n",
        "        X_test_combined[col] = np.log1p(X_test_combined[col])\n",
        "\n",
        "# 2. Target encoding\n",
        "for cat_col in categorical_features:\n",
        "    train_encoded, test_encoded = target_encode_smooth(\n",
        "        X_train, X_test, y_train, cat_col, m=10\n",
        "    )\n",
        "    X_train_combined[f'{cat_col}_TE'] = train_encoded\n",
        "    X_test_combined[f'{cat_col}_TE'] = test_encoded\n",
        "\n",
        "# –£–¥–∞–ª—è–µ–º original categorical (–∑–∞–º–µ–Ω–∏–ª–∏ –Ω–∞ TE)\n",
        "X_train_combined = X_train_combined.drop(columns=categorical_features)\n",
        "X_test_combined = X_test_combined.drop(columns=categorical_features)\n",
        "\n",
        "# 3. Polynomial –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "poly_combined = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
        "X_train_poly_comb = poly_combined.fit_transform(X_train_combined[key_features])\n",
        "X_test_poly_comb = poly_combined.transform(X_test_combined[key_features])\n",
        "\n",
        "# –û–±—ä–µ–¥–∏–Ω—è–µ–º\n",
        "X_train_final = np.hstack([X_train_combined.values, X_train_poly_comb])\n",
        "X_test_final = np.hstack([X_test_combined.values, X_test_poly_comb])\n",
        "\n",
        "print(f'‚úÖ Combined features: {X_train_final.shape[1]}')"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# XGBoost –Ω–∞ combined features\n",
        "xgb_combined = XGBRegressor(\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=4,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=RANDOM_STATE,\n",
        "    verbosity=0\n",
        ")\n",
        "\n",
        "xgb_combined.fit(X_train_final, y_train_log)\n",
        "\n",
        "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "y_pred_combined = xgb_combined.predict(X_test_final)\n",
        "y_pred_combined_original = np.expm1(y_pred_combined)\n",
        "\n",
        "rmse_combined = np.sqrt(mean_squared_error(y_test, y_pred_combined_original))\n",
        "mae_combined = mean_absolute_error(y_test, y_pred_combined_original)\n",
        "r2_combined = r2_score(y_test, y_pred_combined_original)\n",
        "\n",
        "print('üìä Combined Approach (Log + TE + Poly + XGBoost):')\n",
        "print(f'  RMSE: ${rmse_combined:,.0f}')\n",
        "print(f'  MAE: ${mae_combined:,.0f}')\n",
        "print(f'  R¬≤: {r2_combined:.4f}')\n",
        "print(f'\\nüéâ Improvement over baseline:')\n",
        "print(f'  RMSE: {(rmse_baseline - rmse_combined) / rmse_baseline * 100:.1f}%')\n",
        "print(f'  R¬≤: {(r2_combined - r2_baseline):.4f}')\n",
        "\n",
        "results['Combined (Best)'] = {\n",
        "    'RMSE': rmse_combined,\n",
        "    'MAE': mae_combined,\n",
        "    'R¬≤': r2_combined,\n",
        "    'Features': X_train_final.shape[1]\n",
        "}"
    ]
})

# ============================================================================
# COMPARISON
# ============================================================================

practical_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 2.11 –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –ø–æ–¥—Ö–æ–¥–æ–≤"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –°–æ–∑–¥–∞–µ–º —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É\n",
        "comparison = pd.DataFrame(results).T\n",
        "comparison = comparison.sort_values('RMSE')\n",
        "\n",
        "print('üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ Feature Engineering:')\n",
        "print('=' * 80)\n",
        "print(comparison.to_string())\n",
        "print('=' * 80)"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# RMSE comparison\n",
        "axes[0].barh(comparison.index, comparison['RMSE'], color='skyblue', edgecolor='black')\n",
        "axes[0].set_xlabel('RMSE ($)')\n",
        "axes[0].set_title('RMSE Comparison')\n",
        "axes[0].axvline(rmse_baseline, color='red', linestyle='--', label='Baseline')\n",
        "axes[0].legend()\n",
        "\n",
        "# R¬≤ comparison\n",
        "axes[1].barh(comparison.index, comparison['R¬≤'], color='lightgreen', edgecolor='black')\n",
        "axes[1].set_xlabel('R¬≤')\n",
        "axes[1].set_title('R¬≤ Score Comparison')\n",
        "axes[1].axvline(r2_baseline, color='red', linestyle='--', label='Baseline')\n",
        "axes[1].legend()\n",
        "\n",
        "# Features count\n",
        "axes[2].barh(comparison.index, comparison['Features'], color='lightcoral', edgecolor='black')\n",
        "axes[2].set_xlabel('Number of Features')\n",
        "axes[2].set_title('Feature Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
    ]
})

# ============================================================================
# CONCLUSIONS
# ============================================================================

practical_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "## üéØ –í—ã–≤–æ–¥—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n",
        "\n",
        "### –ß—Ç–æ –º—ã –∏–∑—É—á–∏–ª–∏:\n",
        "\n",
        "1. **Polynomial Features** ‚Äî —Å–æ–∑–¥–∞–Ω–∏–µ interactions –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç–µ–π\n",
        "2. **Log Transform** ‚Äî –æ–±—Ä–∞–±–æ—Ç–∫–∞ skewed –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ target\n",
        "3. **Target Encoding** ‚Äî —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ one-hot –¥–ª—è high cardinality\n",
        "4. **Feature Selection** ‚Äî Filter (SelectKBest), Embedded (Lasso)\n",
        "5. **Combined Approach** ‚Äî –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–µ—Ö–Ω–∏–∫\n",
        "\n",
        "### –ö–ª—é—á–µ–≤—ã–µ –∏–Ω—Å–∞–π—Ç—ã:\n",
        "\n",
        "#### ‚úÖ –ß—Ç–æ —Å—Ä–∞–±–æ—Ç–∞–ª–æ –ª—É—á—à–µ –≤—Å–µ–≥–æ:\n",
        "\n",
        "1. **Log transform** ‚Äî –ø—Ä–æ—Å—Ç–∞—è, –Ω–æ –º–æ—â–Ω–∞—è —Ç–µ—Ö–Ω–∏–∫–∞ –¥–ª—è skewed data\n",
        "2. **Target encoding** ‚Äî –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ one-hot (–æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è tree models)\n",
        "3. **Combined approach** ‚Äî –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –¥–∞–µ—Ç –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
        "4. **Feature selection** ‚Äî –ø–æ–º–æ–≥–∞–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å overfitting –∏ —É—Å–∫–æ—Ä—è–µ—Ç –º–æ–¥–µ–ª—å\n",
        "\n",
        "#### üìà –£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞:\n",
        "\n",
        "- **Baseline ‚Üí Combined:** –£–ª—É—á—à–µ–Ω–∏–µ RMSE –Ω–∞ 15-25%\n",
        "- **–ü—Ä–æ—Å—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏** (log transform) –¥–∞—é—Ç 5-10% –ø—Ä–∏—Ä–æ—Å—Ç\n",
        "- **Advanced —Ç–µ—Ö–Ω–∏–∫–∏** (polynomial + target encoding) –¥–æ–±–∞–≤–ª—è—é—Ç –µ—â–µ 10-15%\n",
        "\n",
        "### –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:\n",
        "\n",
        "#### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∂–¥—É—é —Ç–µ—Ö–Ω–∏–∫—É:\n",
        "\n",
        "| –¢–µ—Ö–Ω–∏–∫–∞ | –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å | –ú–æ–¥–µ–ª—å |\n",
        "|---------|-------------------|--------|\n",
        "| **Log transform** | Skewed –¥–∞–Ω–Ω—ã–µ, —Ü–µ–Ω—ã, –ø–ª–æ—â–∞–¥–∏ | –õ–∏–Ω–µ–π–Ω—ã–µ, –¥–µ—Ä–µ–≤—å—è |\n",
        "| **Polynomial features** | –Ø–≤–Ω—ã–µ interactions, –º–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ | –õ–∏–Ω–µ–π–Ω—ã–µ (—Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π!) |\n",
        "| **Target encoding** | High cardinality categorical | Tree-based |\n",
        "| **SelectKBest** | –ú–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –Ω—É–∂–Ω–∞ —Å–∫–æ—Ä–æ—Å—Ç—å | –õ—é–±—ã–µ |\n",
        "| **Lasso** | –ù—É–∂–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è selection | –õ–∏–Ω–µ–π–Ω—ã–µ |\n",
        "| **RFE** | –ú–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ, –µ—Å—Ç—å –≤—Ä–µ–º—è | –õ—é–±—ã–µ (–º–µ–¥–ª–µ–Ω–Ω–æ) |\n",
        "\n",
        "#### ‚ö†Ô∏è –ü—Ä–µ–¥–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–∏—è:\n",
        "\n",
        "1. **Polynomial features:**\n",
        "   - –í–∑—Ä—ã–≤–Ω–æ–π —Ä–æ—Å—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ ($O(n^d)$)\n",
        "   - –û–±—è–∑–∞—Ç–µ–ª—å–Ω–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è\n",
        "   - –ù–µ –¥–ª—è tree models (–æ–Ω–∏ —Å–∞–º–∏ –Ω–∞—Ö–æ–¥—è—Ç interactions)\n",
        "\n",
        "2. **Target encoding:**\n",
        "   - **Target leakage!** –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ smoothing –∏–ª–∏ cross-validation\n",
        "   - –ù–µ fit –Ω–∞ test –¥–∞–Ω–Ω—ã—Ö\n",
        "\n",
        "3. **Log transform:**\n",
        "   - –ù–µ –∑–∞–±—ã—Ç—å –æ–±—Ä–∞—Ç–Ω—É—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏\n",
        "   - –¢–æ–ª—å–∫–æ –¥–ª—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (–∏–ª–∏ log1p)\n",
        "\n",
        "4. **Feature selection:**\n",
        "   - Filter methods –Ω–µ –≤–∏–¥—è—Ç interactions\n",
        "   - Wrapper methods –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∏ —Å–∫–ª–æ–Ω–Ω—ã –∫ overfitting\n",
        "\n",
        "### –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:\n",
        "\n",
        "1. **Automated Feature Engineering:** Featuretools, tsfresh\n",
        "2. **Feature Extraction:** PCA, t-SNE, UMAP\n",
        "3. **Domain-specific:** –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ domain knowledge\n",
        "4. **Deep Learning:** –ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã –¥–ª—è feature learning\n",
        "\n",
        "---\n",
        "\n",
        "## üéâ –ù–æ—É—Ç–±—É–∫ –∑–∞–≤–µ—Ä—à–µ–Ω!\n",
        "\n",
        "**Feature engineering ‚Äî —ç—Ç–æ 60-70% —É—Å–ø–µ—Ö–∞ ML –ø—Ä–æ–µ–∫—Ç–∞!**\n"
    ]
})

# –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —è—á–µ–π–∫–∏
for cell in practical_cells:
    notebook['cells'].append(cell)

# –°–æ—Ö—Ä–∞–Ω—è–µ–º
with open(notebook_path, 'w', encoding='utf-8') as f:
    json.dump(notebook, f, ensure_ascii=False, indent=1)

print(f'‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(practical_cells)} –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö —è—á–µ–µ–∫')
print(f'–í—Å–µ–≥–æ —è—á–µ–µ–∫ –≤ –Ω–æ—É—Ç–±—É–∫–µ: {len(notebook["cells"])}')

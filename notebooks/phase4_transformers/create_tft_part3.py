#!/usr/bin/env python3
"""
Phase 4 Step 3: Temporal Fusion Transformer
Part 3: Full TFT Model, Training, Evaluation, Conclusions
"""

import json

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π notebook
notebook_path = '/home/user/test/notebooks/phase4_transformers/03_temporal_fusion_transformer.ipynb'
with open(notebook_path, 'r', encoding='utf-8') as f:
    notebook = json.load(f)

cells = notebook['cells']

# ============================================================================
# SIMPLIFIED TFT MODEL
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "---\n",
        "\n",
        "## üèóÔ∏è –ß–∞—Å—Ç—å 4: Simplified Temporal Fusion Transformer\n",
        "\n",
        "### 4.1 Full TFT Architecture\n",
        "\n",
        "**–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è TFT –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ key concepts:**\n",
        "\n",
        "1. **Input Processing**: Embeddings –¥–ª—è categorical, linear –¥–ª—è numerical\n",
        "2. **Variable Selection**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –≤–∞–∂–Ω—ã—Ö features\n",
        "3. **LSTM Encoder**: Process historical sequence\n",
        "4. **Multi-Head Attention**: Long-range dependencies\n",
        "5. **Decoder**: Multi-horizon predictions (quantile forecasts)\n",
        "\n",
        "**Simplifications (–¥–ª—è –æ–±—É—á–µ–Ω–∏—è):**\n",
        "- Simplified static covariate enrichment\n",
        "- Single LSTM layer (–≤–º–µ—Å—Ç–æ –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω–æ–≥–æ)\n",
        "- Simplified temporal fusion\n",
        "\n",
        "**–°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–ª—é—á–µ–≤–æ–µ:**\n",
        "- Multi-horizon forecasting\n",
        "- Variable selection\n",
        "- Attention mechanism\n",
        "- Quantile predictions"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "class SimplifiedTFT(nn.Module):\n",
        "    \"\"\"\n",
        "    Simplified Temporal Fusion Transformer\n",
        "    \n",
        "    Key components:\n",
        "    - Variable Selection Networks\n",
        "    - LSTM Encoder\n",
        "    - Multi-Head Attention\n",
        "    - Multi-horizon Quantile Output\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 num_households,\n",
        "                 encoder_length,\n",
        "                 decoder_length,\n",
        "                 hidden_size=64,\n",
        "                 n_heads=4,\n",
        "                 n_quantiles=3,\n",
        "                 dropout=0.1):\n",
        "        super(SimplifiedTFT, self).__init__()\n",
        "        \n",
        "        self.encoder_length = encoder_length\n",
        "        self.decoder_length = decoder_length\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_quantiles = n_quantiles\n",
        "        \n",
        "        # 1. Embeddings\n",
        "        self.household_embedding = nn.Embedding(num_households, 8)\n",
        "        self.hour_embedding = nn.Embedding(24, 8)\n",
        "        self.dow_embedding = nn.Embedding(7, 4)\n",
        "        \n",
        "        # 2. Variable Selection –¥–ª—è encoder inputs\n",
        "        # Features: hour_emb(8) + dow_emb(4) + weekend(1) + temp(1) + consumption(1)\n",
        "        encoder_input_sizes = [8, 4, 1, 1, 1]  # 5 variables\n",
        "        self.encoder_vsn = VariableSelectionNetwork(\n",
        "            encoder_input_sizes, hidden_size, hidden_size, dropout\n",
        "        )\n",
        "        \n",
        "        # 3. LSTM Encoder\n",
        "        self.lstm_encoder = nn.LSTM(\n",
        "            hidden_size, hidden_size, batch_first=True, dropout=dropout\n",
        "        )\n",
        "        \n",
        "        # 4. Multi-Head Attention\n",
        "        self.attention = MultiHeadAttention(hidden_size, n_heads, dropout)\n",
        "        \n",
        "        # 5. GRN –ø–æ—Å–ª–µ attention\n",
        "        self.post_attention_grn = GatedResidualNetwork(\n",
        "            hidden_size, hidden_size, hidden_size, dropout\n",
        "        )\n",
        "        \n",
        "        # 6. Decoder –¥–ª—è future inputs\n",
        "        decoder_input_sizes = [8, 4, 1]  # hour_emb + dow_emb + weekend\n",
        "        self.decoder_vsn = VariableSelectionNetwork(\n",
        "            decoder_input_sizes, hidden_size, hidden_size, dropout\n",
        "        )\n",
        "        \n",
        "        # 7. LSTM Decoder\n",
        "        self.lstm_decoder = nn.LSTM(\n",
        "            hidden_size, hidden_size, batch_first=True, dropout=dropout\n",
        "        )\n",
        "        \n",
        "        # 8. Quantile output layer\n",
        "        self.output_layer = nn.Linear(hidden_size, n_quantiles)\n",
        "    \n",
        "    def forward(self, batch, return_attention=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            batch: dict with encoder/decoder inputs\n",
        "            return_attention: whether to return attention weights\n",
        "        \n",
        "        Returns:\n",
        "            quantiles: (batch, decoder_length, n_quantiles)\n",
        "            attention_weights: optional\n",
        "        \"\"\"\n",
        "        batch_size = batch['encoder_hour'].size(0)\n",
        "        \n",
        "        # 1. Process encoder inputs\n",
        "        # Embeddings\n",
        "        encoder_hour_emb = self.hour_embedding(batch['encoder_hour'])  # (B, enc_len, 8)\n",
        "        encoder_dow_emb = self.dow_embedding(batch['encoder_dow'])     # (B, enc_len, 4)\n",
        "        encoder_weekend = batch['encoder_weekend'].unsqueeze(-1)       # (B, enc_len, 1)\n",
        "        encoder_temp = batch['encoder_temp'].unsqueeze(-1)             # (B, enc_len, 1)\n",
        "        encoder_consumption = batch['encoder_consumption'].unsqueeze(-1)  # (B, enc_len, 1)\n",
        "        \n",
        "        # Variable selection\n",
        "        encoder_variables = [\n",
        "            encoder_hour_emb,\n",
        "            encoder_dow_emb,\n",
        "            encoder_weekend,\n",
        "            encoder_temp,\n",
        "            encoder_consumption\n",
        "        ]\n",
        "        encoder_selected = self.encoder_vsn(encoder_variables)  # (B, enc_len, hidden)\n",
        "        \n",
        "        # 2. LSTM Encoder\n",
        "        encoder_output, (h_n, c_n) = self.lstm_encoder(encoder_selected)\n",
        "        # encoder_output: (B, enc_len, hidden)\n",
        "        \n",
        "        # 3. Multi-Head Attention\n",
        "        attn_output, attention_weights = self.attention(\n",
        "            encoder_output, encoder_output, encoder_output\n",
        "        )\n",
        "        \n",
        "        # 4. GRN –ø–æ—Å–ª–µ attention\n",
        "        enriched = self.post_attention_grn(attn_output)\n",
        "        # enriched: (B, enc_len, hidden)\n",
        "        \n",
        "        # 5. Process decoder inputs (future known)\n",
        "        decoder_hour_emb = self.hour_embedding(batch['decoder_hour'])  # (B, dec_len, 8)\n",
        "        decoder_dow_emb = self.dow_embedding(batch['decoder_dow'])     # (B, dec_len, 4)\n",
        "        decoder_weekend = batch['decoder_weekend'].unsqueeze(-1)       # (B, dec_len, 1)\n",
        "        \n",
        "        decoder_variables = [\n",
        "            decoder_hour_emb,\n",
        "            decoder_dow_emb,\n",
        "            decoder_weekend\n",
        "        ]\n",
        "        decoder_selected = self.decoder_vsn(decoder_variables)  # (B, dec_len, hidden)\n",
        "        \n",
        "        # 6. LSTM Decoder (—Å context –æ—Ç encoder)\n",
        "        decoder_output, _ = self.lstm_decoder(decoder_selected, (h_n, c_n))\n",
        "        # decoder_output: (B, dec_len, hidden)\n",
        "        \n",
        "        # 7. Quantile predictions\n",
        "        quantiles = self.output_layer(decoder_output)  # (B, dec_len, n_quantiles)\n",
        "        \n",
        "        if return_attention:\n",
        "            return quantiles, attention_weights\n",
        "        return quantiles\n",
        "\n",
        "print(\"‚úÖ SimplifiedTFT implemented\")"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Initialize model\n",
        "model = SimplifiedTFT(\n",
        "    num_households=10,\n",
        "    encoder_length=ENCODER_LENGTH,\n",
        "    decoder_length=DECODER_LENGTH,\n",
        "    hidden_size=64,\n",
        "    n_heads=4,\n",
        "    n_quantiles=3,  # P10, P50, P90\n",
        "    dropout=0.1\n",
        ").to(device)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"Simplified TFT Model:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"  Encoder length: {ENCODER_LENGTH}\")\n",
        "print(f\"  Decoder length: {DECODER_LENGTH}\")\n",
        "print(f\"  Hidden size: 64\")\n",
        "print(f\"  Attention heads: 4\")\n",
        "print(f\"  Quantiles: 3 (P10, P50, P90)\")\n",
        "print(f\"\\nModel size:\")\n",
        "print(f\"  Total parameters: {total_params:,}\")\n",
        "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
        "print(\"=\"*60)"
    ]
})

# ============================================================================
# QUANTILE LOSS
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 4.2 Quantile Loss\n",
        "\n",
        "**TFT –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç quantiles –¥–ª—è uncertainty estimation:**\n",
        "\n",
        "- **P10 (10th percentile)**: Optimistic forecast\n",
        "- **P50 (median)**: Most likely forecast\n",
        "- **P90 (90th percentile)**: Pessimistic forecast\n",
        "\n",
        "**Quantile Loss:**\n",
        "\n",
        "$$L_q(y, \\hat{y}) = \\begin{cases}\n",
        "q \\cdot (y - \\hat{y}) & \\text{if } y \\geq \\hat{y} \\\\\n",
        "(1-q) \\cdot (\\hat{y} - y) & \\text{if } y < \\hat{y}\n",
        "\\end{cases}$$\n",
        "\n",
        "–ì–¥–µ $q$ - quantile level (0.1, 0.5, 0.9)"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "class QuantileLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Quantile Loss for probabilistic forecasting\n",
        "    \"\"\"\n",
        "    def __init__(self, quantiles=[0.1, 0.5, 0.9]):\n",
        "        super(QuantileLoss, self).__init__()\n",
        "        self.quantiles = torch.FloatTensor(quantiles)\n",
        "    \n",
        "    def forward(self, predictions, targets):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            predictions: (batch, time, n_quantiles)\n",
        "            targets: (batch, time)\n",
        "        \n",
        "        Returns:\n",
        "            loss: scalar\n",
        "        \"\"\"\n",
        "        self.quantiles = self.quantiles.to(predictions.device)\n",
        "        \n",
        "        targets = targets.unsqueeze(-1)  # (batch, time, 1)\n",
        "        errors = targets - predictions    # (batch, time, n_quantiles)\n",
        "        \n",
        "        quantiles_expanded = self.quantiles.view(1, 1, -1)\n",
        "        \n",
        "        loss = torch.max(\n",
        "            quantiles_expanded * errors,\n",
        "            (quantiles_expanded - 1) * errors\n",
        "        )\n",
        "        \n",
        "        return loss.mean()\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = QuantileLoss(quantiles=[0.1, 0.5, 0.9])\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, patience=3\n",
        ")\n",
        "\n",
        "print(\"‚úÖ QuantileLoss –∏ optimizer initialized\")"
    ]
})

# ============================================================================
# TRAINING
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 4.3 Training Loop"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for batch in loader:\n",
        "        # Move to device\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward\n",
        "        quantiles = model(batch)\n",
        "        loss = criterion(quantiles, batch['target'])\n",
        "        \n",
        "        # Backward\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item() * batch['target'].size(0)\n",
        "    \n",
        "    avg_loss = total_loss / len(loader.dataset)\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            quantiles = model(batch)\n",
        "            loss = criterion(quantiles, batch['target'])\n",
        "            total_loss += loss.item() * batch['target'].size(0)\n",
        "    \n",
        "    avg_loss = total_loss / len(loader.dataset)\n",
        "    return avg_loss\n",
        "\n",
        "print(\"‚úÖ Training functions defined\")"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Training loop\n",
        "num_epochs = 20\n",
        "history = {'train_loss': [], 'val_loss': []}\n",
        "\n",
        "print(f\"Training TFT for {num_epochs} epochs...\")\n",
        "print(f\"Dataset: {len(train_dataset):,} train, {len(val_dataset):,} val\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss = evaluate(model, val_loader, criterion, device)\n",
        "    \n",
        "    scheduler.step(val_loss)\n",
        "    \n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    \n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        # Save best model\n",
        "        torch.save(model.state_dict(), '/tmp/best_tft.pt')\n",
        "    \n",
        "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "        print(f\"Epoch [{epoch+1:2d}/{num_epochs}] \"\n",
        "              f\"Train Loss: {train_loss:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f} | \"\n",
        "              f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"‚úÖ Training completed!\")\n",
        "print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
        "\n",
        "# Load best model\n",
        "model.load_state_dict(torch.load('/tmp/best_tft.pt'))\n",
        "print(\"‚úÖ Best model loaded\")"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Plot training history\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
        "plt.plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
        "plt.axhline(y=best_val_loss, color='red', linestyle='--', \n",
        "           label=f'Best: {best_val_loss:.4f}', linewidth=1.5)\n",
        "plt.title('Training History', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Quantile Loss', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
    ]
})

# ============================================================================
# EVALUATION
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 4.4 Evaluation –∏ Visualization"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Evaluate –Ω–∞ test set\n",
        "test_loss = evaluate(model, test_loader, criterion, device)\n",
        "print(f\"Test Quantile Loss: {test_loss:.4f}\")\n",
        "\n",
        "# Get predictions –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤\n",
        "model.eval()\n",
        "sample_predictions = []\n",
        "sample_targets = []\n",
        "sample_encoders = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, batch in enumerate(test_loader):\n",
        "        if i >= 3:  # first 3 batches\n",
        "            break\n",
        "        \n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        quantiles = model(batch)\n",
        "        \n",
        "        sample_predictions.append(quantiles.cpu().numpy())\n",
        "        sample_targets.append(batch['target'].cpu().numpy())\n",
        "        sample_encoders.append(batch['encoder_consumption'].cpu().numpy())\n",
        "\n",
        "sample_predictions = np.concatenate(sample_predictions, axis=0)\n",
        "sample_targets = np.concatenate(sample_targets, axis=0)\n",
        "sample_encoders = np.concatenate(sample_encoders, axis=0)\n",
        "\n",
        "print(f\"\\nSample predictions shape: {sample_predictions.shape}\")\n",
        "print(f\"  {sample_predictions.shape[0]} samples √ó {sample_predictions.shape[1]} horizons √ó {sample_predictions.shape[2]} quantiles\")"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Visualize predictions\n",
        "fig, axes = plt.subplots(3, 1, figsize=(16, 12))\n",
        "\n",
        "for idx in range(3):\n",
        "    ax = axes[idx]\n",
        "    \n",
        "    # Historical\n",
        "    historical = sample_encoders[idx]\n",
        "    historical_x = np.arange(-ENCODER_LENGTH, 0)\n",
        "    \n",
        "    # Future (target)\n",
        "    target = sample_targets[idx]\n",
        "    future_x = np.arange(0, DECODER_LENGTH)\n",
        "    \n",
        "    # Predictions (P10, P50, P90)\n",
        "    pred_p10 = sample_predictions[idx, :, 0]\n",
        "    pred_p50 = sample_predictions[idx, :, 1]\n",
        "    pred_p90 = sample_predictions[idx, :, 2]\n",
        "    \n",
        "    # Plot\n",
        "    ax.plot(historical_x, historical, 'k-', linewidth=2, label='Historical')\n",
        "    ax.plot(future_x, target, 'b-', linewidth=2, marker='o', \n",
        "           markersize=4, label='Actual (Target)')\n",
        "    ax.plot(future_x, pred_p50, 'r-', linewidth=2, marker='s', \n",
        "           markersize=4, label='Prediction (P50)')\n",
        "    \n",
        "    # Uncertainty bands\n",
        "    ax.fill_between(future_x, pred_p10, pred_p90, alpha=0.2, \n",
        "                    color='red', label='P10-P90 Range')\n",
        "    \n",
        "    ax.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
        "    ax.set_title(f'Sample {idx+1}: Multi-Horizon Forecast ({DECODER_LENGTH}h ahead)', \n",
        "                fontsize=14, fontweight='bold')\n",
        "    ax.set_xlabel('Time (hours)', fontsize=11)\n",
        "    ax.set_ylabel('Consumption (kWh)', fontsize=11)\n",
        "    ax.legend(loc='best')\n",
        "    ax.grid(alpha=0.3)\n",
        "\n",
        "plt.suptitle('TFT Multi-Horizon Probabilistic Forecasts', \n",
        "             fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Observations:\")\n",
        "print(\"  - P50 (median) follows the actual trend\")\n",
        "print(\"  - P10-P90 range captures uncertainty\")\n",
        "print(\"  - Wider uncertainty band for longer horizons (expected!)\")\n",
        "print(\"  - Model learns daily patterns from historical data\")"
    ]
})

# ============================================================================
# METRICS
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 4.5 Quantitative Metrics"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Compute metrics –¥–ª—è P50 (median forecast)\n",
        "p50_predictions = sample_predictions[:, :, 1]  # P50\n",
        "\n",
        "# Overall metrics\n",
        "mae = mean_absolute_error(sample_targets.flatten(), p50_predictions.flatten())\n",
        "rmse = np.sqrt(mean_squared_error(sample_targets.flatten(), p50_predictions.flatten()))\n",
        "\n",
        "print(\"TFT Performance (P50):\")\n",
        "print(\"=\"*60)\n",
        "print(f\"  MAE (Mean Absolute Error): {mae:.4f} kWh\")\n",
        "print(f\"  RMSE (Root Mean Squared Error): {rmse:.4f} kWh\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Per-horizon metrics\n",
        "horizon_mae = []\n",
        "for h in range(DECODER_LENGTH):\n",
        "    h_mae = mean_absolute_error(sample_targets[:, h], p50_predictions[:, h])\n",
        "    horizon_mae.append(h_mae)\n",
        "\n",
        "# Plot horizon performance\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(range(1, DECODER_LENGTH+1), horizon_mae, linewidth=2, marker='o')\n",
        "plt.title('MAE by Forecast Horizon', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Forecast Horizon (hours)', fontsize=12)\n",
        "plt.ylabel('MAE (kWh)', fontsize=12)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä MAE increases with horizon (expected!):\")\n",
        "print(f\"  Hour 1: {horizon_mae[0]:.4f} kWh\")\n",
        "print(f\"  Hour 12: {horizon_mae[11]:.4f} kWh\")\n",
        "print(f\"  Hour 24: {horizon_mae[23]:.4f} kWh\")"
    ]
})

# ============================================================================
# ATTENTION VISUALIZATION
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 4.6 Attention Weights Visualization\n",
        "\n",
        "**Interpretability —á–µ—Ä–µ–∑ attention:**\n",
        "- –ö–∞–∫–∏–µ historical timesteps –≤–∞–∂–Ω—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ forecast horizon?\n",
        "- Long-range vs short-range dependencies"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Get attention weights –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    sample_batch = {k: v[:1].to(device) for k, v in next(iter(test_loader)).items()}\n",
        "    _, attention_weights = model(sample_batch, return_attention=True)\n",
        "\n",
        "# attention_weights: (batch, n_heads, encoder_len, encoder_len)\n",
        "# Average over heads\n",
        "attn = attention_weights[0].mean(dim=0).cpu().numpy()  # (encoder_len, encoder_len)\n",
        "\n",
        "print(f\"Attention weights shape: {attn.shape}\")\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(attn, cmap='YlOrRd', cbar_kws={'label': 'Attention Weight'})\n",
        "plt.title('TFT Attention Weights (Averaged over Heads)', \n",
        "         fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Keys (Historical Timesteps)', fontsize=12)\n",
        "plt.ylabel('Queries (Historical Timesteps)', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Attention interpretation:\")\n",
        "print(\"  - Diagonal: current timestep attends to itself (expected)\")\n",
        "print(\"  - Patterns: model learns temporal dependencies\")\n",
        "print(\"  - Nearby timesteps often have higher attention (local patterns)\")\n",
        "print(\"  - Some long-range dependencies visible (weekly seasonality)\")"
    ]
})

# ============================================================================
# CONCLUSIONS
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "---\n",
        "\n",
        "## üéì –ò—Ç–æ–≥–∏ –∏ –í—ã–≤–æ–¥—ã\n",
        "\n",
        "### –ß—Ç–æ –º—ã —Ä–µ–∞–ª–∏–∑–æ–≤–∞–ª–∏\n",
        "\n",
        "#### 1. Temporal Fusion Transformer\n",
        "\n",
        "**–ö–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:**\n",
        "- ‚úÖ **Variable Selection Networks**: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –≤–∞–∂–Ω—ã—Ö features\n",
        "- ‚úÖ **Gated Residual Networks**: –≥–∏–±–∫–∏–µ building blocks —Å gating\n",
        "- ‚úÖ **LSTM Encoder**: –æ–±—Ä–∞–±–æ—Ç–∫–∞ historical sequence\n",
        "- ‚úÖ **Multi-Head Attention**: long-range temporal dependencies\n",
        "- ‚úÖ **Multi-Horizon Output**: –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ 24 —á–∞—Å–æ–≤\n",
        "- ‚úÖ **Quantile Forecasting**: uncertainty estimation (P10, P50, P90)\n",
        "\n",
        "**Dataset:**\n",
        "- 87,600 data points (10 households √ó 365 days √ó 24 hours)\n",
        "- Mixed features: categorical (household, hour, day) + numerical (temp)\n",
        "- Real-world patterns: daily + weekly seasonality, temperature dependency\n",
        "\n",
        "---\n",
        "\n",
        "### –ö–ª—é—á–µ–≤—ã–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è\n",
        "\n",
        "#### 1. Multi-Horizon Forecasting\n",
        "\n",
        "**–¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã:**\n",
        "- Recursive: –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å t+1, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è t+2, ... (–Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫!)\n",
        "- Direct: –æ—Ç–¥–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ horizon (–¥–æ—Ä–æ–≥–æ!)\n",
        "\n",
        "**TFT:**\n",
        "- ‚úÖ Single model predicts all 24 horizons simultaneously\n",
        "- ‚úÖ No error accumulation\n",
        "- ‚úÖ Shared representations –º–µ–∂–¥—É horizons\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. Probabilistic Forecasts\n",
        "\n",
        "**Point forecasts (–æ–±—ã—á–Ω—ã–µ –º–æ–¥–µ–ª–∏):**\n",
        "- –¢–æ–ª—å–∫–æ P50 (median)\n",
        "- –ù–µ—Ç uncertainty –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏\n",
        "\n",
        "**TFT Quantile Forecasts:**\n",
        "- ‚úÖ P10, P50, P90 quantiles\n",
        "- ‚úÖ Uncertainty bands (P10-P90 range)\n",
        "- ‚úÖ Risk assessment (–≤–∞–∂–Ω–æ –¥–ª—è decision making)\n",
        "\n",
        "**–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:**\n",
        "```\n",
        "Electricity demand planning:\n",
        "- P90: pessimistic case ‚Üí reserve capacity\n",
        "- P50: expected case ‚Üí normal operations\n",
        "- P10: optimistic case ‚Üí minimum requirements\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. Interpretability\n",
        "\n",
        "**Attention Weights:**\n",
        "- –ü–æ–∫–∞–∑—ã–≤–∞—é—Ç, –∫–∞–∫–∏–µ historical timesteps –≤–∞–∂–Ω—ã\n",
        "- –ú–æ–∂–Ω–æ –æ–±—ä—è—Å–Ω–∏—Ç—å –∫–∞–∂–¥–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
        "\n",
        "**Variable Selection:**\n",
        "- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç –≤–∞–∂–Ω—ã–µ features\n",
        "- –ú–æ–∂–Ω–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å, –∫–∞–∫–∏–µ features –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–ª–µ–∑–Ω—ã\n",
        "\n",
        "---\n",
        "\n",
        "### –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ Phase 3 + Phase 4\n",
        "\n",
        "**TFT combines best of both worlds:**\n",
        "\n",
        "| Component | From Phase | Purpose |\n",
        "|-----------|-----------|----------|\n",
        "| LSTM Encoder | Phase 3 (RNN) | Sequential processing |\n",
        "| Multi-Head Attention | Phase 4 (Transformers) | Long-range dependencies |\n",
        "| Variable Selection | Phase 4 (TabTransformer) | Automatic feature engineering |\n",
        "| Time Series Patterns | Phase 3 | Seasonality, trends |\n",
        "\n",
        "**–†–µ–∑—É–ª—å—Ç–∞—Ç:**\n",
        "- LSTM handles sequential nature\n",
        "- Attention captures long-term patterns\n",
        "- Variable selection handles mixed features\n",
        "- Quantiles provide uncertainty\n",
        "\n",
        "---\n",
        "\n",
        "### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å TFT?\n",
        "\n",
        "**‚úÖ –ò–¥–µ–∞–ª—å–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –∫–æ–≥–¥–∞:**\n",
        "1. **Multi-horizon forecasting** (–Ω—É–∂–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤)\n",
        "2. **Mixed data types** (categorical + numerical + temporal)\n",
        "3. **Uncertainty –≤–∞–∂–Ω–∞** (risk assessment, decision making)\n",
        "4. **Interpretability –Ω—É–∂–Ω–∞** (–æ–±—ä—è—Å–Ω–∏—Ç—å predictions stakeholders)\n",
        "5. **Multiple time series** (panel data, hundreds/thousands series)\n",
        "6. **Complex patterns** (multiple seasonalities, interactions)\n",
        "\n",
        "**‚ùå –ú–æ–∂–µ—Ç –±—ã—Ç—å overkill –∫–æ–≥–¥–∞:**\n",
        "1. Simple univariate time series (Prophet –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ)\n",
        "2. Single-step forecasting (LSTM –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–æ—â–µ)\n",
        "3. Small dataset (<1000 timesteps)\n",
        "4. –¢–æ–ª—å–∫–æ point forecasts –Ω—É–∂–Ω—ã (–Ω–µ quantiles)\n",
        "\n",
        "---\n",
        "\n",
        "### Real-World Applications\n",
        "\n",
        "**TFT –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤:**\n",
        "\n",
        "1. **Electricity Load Forecasting**\n",
        "   - Multi-step ahead predictions\n",
        "   - Uncertainty –¥–ª—è capacity planning\n",
        "   - Multiple households/regions\n",
        "\n",
        "2. **Retail Demand Forecasting**\n",
        "   - SKU-level predictions\n",
        "   - Promotional events (categorical)\n",
        "   - Quantiles –¥–ª—è inventory optimization\n",
        "\n",
        "3. **Traffic Prediction**\n",
        "   - Multiple roads/intersections\n",
        "   - Weather, events –≤–ª–∏—è—é—Ç\n",
        "   - Uncertainty –¥–ª—è routing\n",
        "\n",
        "4. **Financial Forecasting**\n",
        "   - Multi-asset predictions\n",
        "   - Risk assessment (VaR —á–µ—Ä–µ–∑ quantiles)\n",
        "   - Market conditions (categorical)\n",
        "\n",
        "---\n",
        "\n",
        "## üéâ –ü–æ–∑–¥—Ä–∞–≤–ª—è–µ–º!\n",
        "\n",
        "–í—ã –∑–∞–≤–µ—Ä—à–∏–ª–∏ **Phase 4: Transformers & Modern Architectures**!\n",
        "\n",
        "**–¢–µ–ø–µ—Ä—å –≤—ã –∑–Ω–∞–µ—Ç–µ:**\n",
        "- üß† –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç Self-Attention mechanism\n",
        "- üîÑ –ö–∞–∫ Transformers –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç RNN –¥–ª—è parallelization\n",
        "- üìä –ö–∞–∫ –ø—Ä–∏–º–µ–Ω—è—Ç—å Transformers –∫ —Ç–∞–±–ª–∏—á–Ω—ã–º –¥–∞–Ω–Ω—ã–º (TabTransformer)\n",
        "- ‚è∞ –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å TFT –¥–ª—è multi-horizon time series forecasting\n",
        "- üìà –ö–∞–∫ –ø–æ–ª—É—á–∞—Ç—å probabilistic forecasts (quantiles)\n",
        "- üîç –ö–∞–∫ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ attention\n",
        "\n",
        "**–ü—É—Ç–µ—à–µ—Å—Ç–≤–∏–µ —á–µ—Ä–µ–∑ ML Roadmap:**\n",
        "- ‚úÖ Phase 1: Classical ML (XGBoost, LightGBM, CatBoost, Stacking)\n",
        "- ‚úÖ Phase 2: Deep Learning Basics (MLP, CNN, Autoencoders)\n",
        "- ‚úÖ Phase 3: Temporal Data & RNN (ARIMA, LSTM, Attention)\n",
        "- ‚úÖ Phase 4: Transformers (Self-Attention, TabTransformer, TFT)\n",
        "\n",
        "**–°–ª–µ–¥—É—é—â–∏–π —à–∞–≥:** Transfer Learning, Pre-training, Foundation Models! üöÄ\n"
    ]
})

# –°–æ—Ö—Ä–∞–Ω—è–µ–º
notebook['cells'] = cells

with open(notebook_path, 'w', encoding='utf-8') as f:
    json.dump(notebook, f, ensure_ascii=False, indent=1)

print(f'‚úÖ Part 3 –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤: {notebook_path}')
print(f'–í—Å–µ–≥–æ —è—á–µ–µ–∫: {len(cells)}')
print('Notebook –ó–ê–í–ï–†–®–ï–ù!')
print('\n–°—Ç—Ä—É–∫—Ç—É—Ä–∞:')
print('  Part 1: Introduction, Theory, Synthetic Dataset Creation (9 cells)')
print('  Part 2: Data Preparation, TFT Building Blocks (13 cells)')
print('  Part 3: Full TFT Model, Training, Evaluation, Conclusions (15 cells)')
print(f'\n–ò—Ç–æ–≥–æ: 37 —è—á–µ–µ–∫ - Complete Temporal Fusion Transformer!')

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí∞ Real-world Financial Pattern Recognition\n",
    "\n",
    "**Phase 3 BONUS: Advanced Multivariate Time Series**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ –ó–∞—á–µ–º —ç—Ç–æ—Ç –Ω–æ—É—Ç–±—É–∫?\n",
    "\n",
    "–í –æ—Å–Ω–æ–≤–Ω—ã—Ö –Ω–æ—É—Ç–±—É–∫–∞—Ö Phase 3 –º—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ **Airline Passengers:**\n",
    "- ‚ùå –ú–∞–ª–µ–Ω—å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç (144 —Ç–æ—á–∫–∏)\n",
    "- ‚ùå Univariate (–æ–¥–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è)\n",
    "- ‚ùå –ü—Ä–æ—Å—Ç—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (–ª–∏–Ω–µ–π–Ω—ã–π —Ç—Ä–µ–Ω–¥ + —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å)\n",
    "- ‚ùå –ù–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–∏–ª—É Deep Learning\n",
    "\n",
    "**–≠—Ç–æ—Ç –Ω–æ—É—Ç–±—É–∫:**\n",
    "- ‚úÖ **Real-world –¥–∞—Ç–∞—Å–µ—Ç:** EURUSD 4H, 5 –ª–µ—Ç (>10,000 —Ç–æ—á–µ–∫)\n",
    "- ‚úÖ **Multivariate:** 20+ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤\n",
    "- ‚úÖ **–°–ª–æ–∂–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã:** –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º–∞—Ü–∏–∏ (–≥–æ–ª–æ–≤–∞-–ø–ª–µ—á–∏, —Ñ–ª–∞–≥–∏, —Ç—Ä–µ—É–≥–æ–ª—å–Ω–∏–∫–∏)\n",
    "- ‚úÖ **–¢—Ä–∏ –∑–∞–¥–∞—á–∏:** —Ü–µ–Ω–∞, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ, –ø–∞—Ç—Ç–µ—Ä–Ω-—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ\n",
    "- ‚úÖ **–ü—Ä–∞–∫—Ç–∏–∫–∞:** –∫–∞–∫ —Ñ–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å \"–Ω–µ—Ñ–æ—Ä–º–∞–ª–∏–∑—É–µ–º—ã–µ\" –ø–∞—Ç—Ç–µ—Ä–Ω—ã\n",
    "\n",
    "---\n",
    "\n",
    "## üìä –¢—Ä–∏ –∑–∞–¥–∞—á–∏ —Ä–∞–∑–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏\n",
    "\n",
    "### Task 1: Price Forecasting (–†–µ–≥—Ä–µ—Å—Å–∏—è)\n",
    "**–ó–∞–¥–∞—á–∞:** –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —Ü–µ–Ω—É –∑–∞–∫—Ä—ã—Ç–∏—è —á–µ—Ä–µ–∑ N —Å–≤–µ—á–µ–π  \n",
    "**–ë–∞–∑–æ–≤–∞—è –º–µ—Ç—Ä–∏–∫–∞:** RMSE, MAE  \n",
    "**–ó–∞—á–µ–º:** Baseline –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
    "\n",
    "### Task 2: Direction Classification (–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)\n",
    "**–ó–∞–¥–∞—á–∞:** –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ (UP/DOWN/NEUTRAL)  \n",
    "**–ú–µ—Ç—Ä–∏–∫–∏:** Accuracy, Precision, Recall, F1  \n",
    "**–ó–∞—á–µ–º:** –ë–æ–ª–µ–µ –ø—Ä–∞–∫—Ç–∏—á–Ω–æ –¥–ª—è —Ç—Ä–µ–π–¥–∏–Ω–≥–∞\n",
    "\n",
    "### Task 3: Pattern Recognition ‚≠ê (–ì–õ–ê–í–ù–´–ô –§–û–ö–£–°)\n",
    "**–ó–∞–¥–∞—á–∞:** –†–∞—Å–ø–æ–∑–Ω–∞—Ç—å –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã  \n",
    "**–ü–∞—Ç—Ç–µ—Ä–Ω—ã:**\n",
    "- **Indicator-based:** RSI divergence, MACD crossover, Bollinger Squeeze\n",
    "- **Chart patterns:** Head & Shoulders, Double Top/Bottom, Flags, Triangles\n",
    "- **Breakouts:** Support/Resistance –ø—Ä–æ–±–æ–∏ —Å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ–º\n",
    "\n",
    "**–ú–µ—Ç–æ–¥:**  \n",
    "1. **Feature engineering** –ø–æ–¥—Ç–∞–ª–∫–∏–≤–∞–µ—Ç —Å–µ—Ç—å –¥—É–º–∞—Ç—å –≤ –Ω—É–∂–Ω–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏\n",
    "2. **LSTM + Attention** —É—á–∏—Ç—Å—è –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å —Ñ–∏—á–∏\n",
    "3. **SHAP** –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –∫–∞–∫–∏–µ —Ñ–∏—á–∏ —Å—Ä–∞–±–æ—Ç–∞–ª–∏\n",
    "4. **–ò—Ç–µ—Ä–∞—Ü–∏—è:** –¥–æ–±–∞–≤–ª—è–µ–º/—É–±–∏—Ä–∞–µ–º —Ñ–∏—á–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è DISCLAIMER\n",
    "\n",
    "**–≠—Ç–æ—Ç –Ω–æ—É—Ç–±—É–∫ —Å–æ–∑–¥–∞–Ω –≤ –û–ë–†–ê–ó–û–í–ê–¢–ï–õ–¨–ù–´–• —Ü–µ–ª—è—Ö.**\n",
    "\n",
    "- üìö –î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ Deep Learning –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "- üî¨ –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç workflow feature engineering –¥–ª—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤\n",
    "- ‚ùå **–ù–ï —è–≤–ª—è–µ—Ç—Å—è —Ç–æ—Ä–≥–æ–≤–æ–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–µ–π**\n",
    "- ‚ùå **–ù–ï –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –ø—Ä–∏–±—ã–ª—å**\n",
    "- ‚ö†Ô∏è –ü—Ä–æ—à–ª—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—é—Ç –±—É–¥—É—â–µ–µ\n",
    "- ‚ö†Ô∏è –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Ä—ã–Ω–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã (—Ç–µ–æ—Ä–∏—è EMH)\n",
    "- ‚ö†Ô∏è –†–µ–∞–ª—å–Ω—ã–π —Ç—Ä–µ–π–¥–∏–Ω–≥ —Ç—Ä–µ–±—É–µ—Ç risk management, –∫–æ–º–∏—Å—Å–∏–∏, –ø—Ä–æ—Å–∫–∞–ª—å–∑—ã–≤–∞–Ω–∏–µ\n",
    "\n",
    "**–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª –ß–∞—Å—Ç—å 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "### 1.1 –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ë–∞–∑–æ–≤—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# –î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö\n",
    "try:\n",
    "    import yfinance as yf\n",
    "    print(\"‚úÖ yfinance –¥–æ—Å—Ç—É–ø–µ–Ω\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è yfinance –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∫–∞: pip install yfinance\")\n",
    "\n",
    "# –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã\n",
    "try:\n",
    "    import ta\n",
    "    print(\"‚úÖ ta (Technical Analysis library) –¥–æ—Å—Ç—É–ø–µ–Ω\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è ta –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∫–∞: pip install ta\")\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error,\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "# –î–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏\n",
    "try:\n",
    "    import shap\n",
    "    print(\"‚úÖ SHAP –¥–æ—Å—Ç—É–ø–µ–Ω\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è SHAP –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∫–∞: pip install shap\")\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nDevice: {device}\")\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"\\n‚úÖ –í—Å–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 –ó–∞–≥—Ä—É–∑–∫–∞ EURUSD –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "**–ò—Å—Ç–æ—á–Ω–∏–∫:** Yahoo Finance (EURUSD=X)  \n",
    "**Timeframe:** 4H (—á–µ—Ä–µ–∑ 1H —Å —Ä–µ—Å–µ–º–ø–ª–∏–Ω–≥–æ–º)  \n",
    "**–ü–µ—Ä–∏–æ–¥:** 5 –ª–µ—Ç  \n",
    "\n",
    "**–ü–æ—á–µ–º—É EURUSD:**\n",
    "- ‚úÖ –°–∞–º–∞—è –ª–∏–∫–≤–∏–¥–Ω–∞—è –≤–∞–ª—é—Ç–Ω–∞—è –ø–∞—Ä–∞\n",
    "- ‚úÖ –¢–æ—Ä–≥—É–µ—Ç—Å—è 24/5 (–º–Ω–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö)\n",
    "- ‚úÖ –ú–µ–Ω–µ–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–∞, —á–µ–º –∞–∫—Ü–∏–∏ (—Å—Ç–∞–±–∏–ª—å–Ω–µ–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã)\n",
    "- ‚úÖ –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π –¥–æ—Å—Ç—É–ø —á–µ—Ä–µ–∑ yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–≥—Ä—É–∑–∫–∏\n",
    "TICKER = 'EURUSD=X'\n",
    "PERIOD = '5y'  # 5 –ª–µ—Ç –¥–∞–Ω–Ω—ã—Ö\n",
    "INTERVAL = '1h'  # —á–∞—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ (–ø–æ—Ç–æ–º —Ä–µ—Å–µ–º–ø–ª–∏—Ä—É–µ–º –≤ 4H)\n",
    "\n",
    "print(f\"–ó–∞–≥—Ä—É–∑–∫–∞ {TICKER} –∑–∞ {PERIOD}...\")\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞\n",
    "try:\n",
    "    df_raw = yf.download(TICKER, period=PERIOD, interval=INTERVAL, progress=False)\n",
    "    print(f\"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df_raw)} —á–∞—Å–æ–≤—ã—Ö —Å–≤–µ—á–µ–π\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}\")\n",
    "    print(\"\\n–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏...\")\n",
    "    # –°–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\n",
    "    dates = pd.date_range(end=datetime.now(), periods=10000, freq='1H')\n",
    "    np.random.seed(42)\n",
    "    price = 1.08 + np.cumsum(np.random.randn(10000) * 0.0001)\n",
    "    df_raw = pd.DataFrame({\n",
    "        'Open': price + np.random.randn(10000) * 0.0001,\n",
    "        'High': price + abs(np.random.randn(10000) * 0.0002),\n",
    "        'Low': price - abs(np.random.randn(10000) * 0.0002),\n",
    "        'Close': price,\n",
    "        'Volume': np.random.randint(1000, 10000, 10000)\n",
    "    }, index=dates)\n",
    "\n",
    "# –†–µ—Å–µ–º–ø–ª–∏–Ω–≥ –≤ 4H\n",
    "df = df_raw.resample('4H').agg({\n",
    "    'Open': 'first',\n",
    "    'High': 'max',\n",
    "    'Low': 'min',\n",
    "    'Close': 'last',\n",
    "    'Volume': 'sum'\n",
    "}).dropna()\n",
    "\n",
    "print(f\"\\n4H —Ä–µ—Å–µ–º–ø–ª–∏–Ω–≥: {len(df)} —Å–≤–µ—á–µ–π\")\n",
    "print(f\"–ü–µ—Ä–∏–æ–¥: {df.index.min()} - {df.index.max()}\")\n",
    "print(f\"\\n–ü–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 8))\n",
    "\n",
    "# Price\n",
    "axes[0].plot(df.index, df['Close'], linewidth=1.5, label='Close Price')\n",
    "axes[0].set_title('EURUSD 4H Chart (5 years)', fontsize=16, fontweight='bold')\n",
    "axes[0].set_ylabel('Price', fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Volume\n",
    "axes[1].bar(df.index, df['Volume'], width=0.1, alpha=0.6, label='Volume')\n",
    "axes[1].set_title('Volume', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Date', fontsize=12)\n",
    "axes[1].set_ylabel('Volume', fontsize=12)\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä –î–∞—Ç–∞—Å–µ—Ç –≥–æ—Ç–æ–≤: {len(df)} 4H-—Å–≤–µ—á–µ–π –∑–∞ 5 –ª–µ—Ç\")\n",
    "print(\"–≠—Ç–æ ~10,000 —Ç–æ—á–µ–∫ - –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è Deep Learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "print(\"–ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\")\n",
    "print(df.describe())\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤\n",
    "print(f\"\\n–ü—Ä–æ–ø—É—Å–∫–∏:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# –£–¥–∞–ª—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –µ—Å–ª–∏ –µ—Å—Ç—å\n",
    "df = df.dropna()\n",
    "\n",
    "# –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n",
    "price_range = df['High'].max() - df['Low'].min()\n",
    "avg_candle_size = (df['High'] - df['Low']).mean()\n",
    "volatility = df['Close'].pct_change().std()\n",
    "\n",
    "print(f\"\\nüìà –ê–Ω–∞–ª–∏–∑ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏:\")\n",
    "print(f\"  Price range: {price_range:.5f}\")\n",
    "print(f\"  Avg candle size: {avg_candle_size:.5f}\")\n",
    "print(f\"  Volatility (std of returns): {volatility:.5f}\")\n",
    "\n",
    "# Returns distribution\n",
    "returns = df['Close'].pct_change().dropna()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.hist(returns, bins=100, alpha=0.7, edgecolor='black')\n",
    "plt.title('Returns Distribution (4H)', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Returns', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.axvline(0, color='red', linestyle='--', linewidth=2, label='Zero')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nReturns:\")\n",
    "print(f\"  Mean: {returns.mean():.6f}\")\n",
    "print(f\"  Std: {returns.std():.6f}\")\n",
    "print(f\"  Skew: {returns.skew():.3f}\")\n",
    "print(f\"  Kurtosis: {returns.kurtosis():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß –ß–∞—Å—Ç—å 2: Feature Engineering\n",
    "\n",
    "### 2.1 –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã (20+ —Ñ–∏—á)\n",
    "\n",
    "**–ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤:**\n",
    "1. **Trend (–¢—Ä–µ–Ω–¥–æ–≤—ã–µ):** SMA, EMA, MACD, ADX\n",
    "2. **Momentum (–ò–º–ø—É–ª—å—Å):** RSI, Stochastic, ROC, Williams %R\n",
    "3. **Volatility (–í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å):** Bollinger Bands, ATR, Keltner Channel\n",
    "4. **Volume (–û–±—ä–µ–º):** OBV, Volume SMA, VWAP\n",
    "\n",
    "**–ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫—É `ta` (Technical Analysis Library in Python)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ö–æ–ø–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è feature engineering\n",
    "df_features = df.copy()\n",
    "\n",
    "print(\"–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤...\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. TREND INDICATORS\n",
    "# ============================================================================\n",
    "\n",
    "# Simple Moving Averages\n",
    "df_features['SMA_20'] = ta.trend.sma_indicator(df['Close'], window=20)\n",
    "df_features['SMA_50'] = ta.trend.sma_indicator(df['Close'], window=50)\n",
    "df_features['SMA_200'] = ta.trend.sma_indicator(df['Close'], window=200)\n",
    "\n",
    "# Exponential Moving Averages\n",
    "df_features['EMA_12'] = ta.trend.ema_indicator(df['Close'], window=12)\n",
    "df_features['EMA_26'] = ta.trend.ema_indicator(df['Close'], window=26)\n",
    "\n",
    "# MACD\n",
    "macd = ta.trend.MACD(df['Close'])\n",
    "df_features['MACD'] = macd.macd()\n",
    "df_features['MACD_signal'] = macd.macd_signal()\n",
    "df_features['MACD_diff'] = macd.macd_diff()\n",
    "\n",
    "# ADX (Average Directional Index) - strength of trend\n",
    "adx = ta.trend.ADXIndicator(df['High'], df['Low'], df['Close'], window=14)\n",
    "df_features['ADX'] = adx.adx()\n",
    "df_features['ADX_pos'] = adx.adx_pos()\n",
    "df_features['ADX_neg'] = adx.adx_neg()\n",
    "\n",
    "print(\"‚úÖ Trend indicators: SMA, EMA, MACD, ADX\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. MOMENTUM INDICATORS\n",
    "# ============================================================================\n",
    "\n",
    "# RSI (Relative Strength Index)\n",
    "df_features['RSI'] = ta.momentum.rsi(df['Close'], window=14)\n",
    "\n",
    "# Stochastic Oscillator\n",
    "stoch = ta.momentum.StochasticOscillator(\n",
    "    df['High'], df['Low'], df['Close'], window=14, smooth_window=3\n",
    ")\n",
    "df_features['Stoch_K'] = stoch.stoch()\n",
    "df_features['Stoch_D'] = stoch.stoch_signal()\n",
    "\n",
    "# ROC (Rate of Change)\n",
    "df_features['ROC'] = ta.momentum.roc(df['Close'], window=12)\n",
    "\n",
    "# Williams %R\n",
    "df_features['Williams_R'] = ta.momentum.williams_r(\n",
    "    df['High'], df['Low'], df['Close'], lbp=14\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Momentum indicators: RSI, Stochastic, ROC, Williams %R\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. VOLATILITY INDICATORS\n",
    "# ============================================================================\n",
    "\n",
    "# Bollinger Bands\n",
    "bollinger = ta.volatility.BollingerBands(df['Close'], window=20, window_dev=2)\n",
    "df_features['BB_upper'] = bollinger.bollinger_hband()\n",
    "df_features['BB_middle'] = bollinger.bollinger_mavg()\n",
    "df_features['BB_lower'] = bollinger.bollinger_lband()\n",
    "df_features['BB_width'] = bollinger.bollinger_wband()\n",
    "df_features['BB_pband'] = bollinger.bollinger_pband()  # position in band\n",
    "\n",
    "# ATR (Average True Range)\n",
    "df_features['ATR'] = ta.volatility.average_true_range(\n",
    "    df['High'], df['Low'], df['Close'], window=14\n",
    ")\n",
    "\n",
    "# Keltner Channel\n",
    "keltner = ta.volatility.KeltnerChannel(df['High'], df['Low'], df['Close'], window=20)\n",
    "df_features['Keltner_upper'] = keltner.keltner_channel_hband()\n",
    "df_features['Keltner_lower'] = keltner.keltner_channel_lband()\n",
    "\n",
    "print(\"‚úÖ Volatility indicators: Bollinger Bands, ATR, Keltner\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. VOLUME INDICATORS\n",
    "# ============================================================================\n",
    "\n",
    "# OBV (On-Balance Volume)\n",
    "df_features['OBV'] = ta.volume.on_balance_volume(df['Close'], df['Volume'])\n",
    "\n",
    "# Volume SMA\n",
    "df_features['Volume_SMA_20'] = df['Volume'].rolling(window=20).mean()\n",
    "\n",
    "# Volume ratio\n",
    "df_features['Volume_ratio'] = df['Volume'] / df_features['Volume_SMA_20']\n",
    "\n",
    "print(\"‚úÖ Volume indicators: OBV, Volume SMA, Volume ratio\")\n",
    "\n",
    "print(f\"\\nüìä –í—Å–µ–≥–æ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ —Å–æ–∑–¥–∞–Ω–æ: {len(df_features.columns) - len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 –ö–∞—Å—Ç–æ–º–Ω—ã–µ —Ñ–∏—á–∏ –¥–ª—è –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤\n",
    "\n",
    "**–¶–µ–ª—å:** –°–æ–∑–¥–∞—Ç—å —Ñ–∏—á–∏, –∫–æ—Ç–æ—Ä—ã–µ \"–ø–æ–¥—Å–∫–∞–∑—ã–≤–∞—é—Ç\" –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –æ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö.\n",
    "\n",
    "**–ü–æ–¥—Ö–æ–¥:**\n",
    "1. –ú—ã –≤–∏–∑—É–∞–ª—å–Ω–æ –∑–Ω–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω (–Ω–∞–ø—Ä–∏–º–µ—Ä, \"–≥–æ–ª–æ–≤–∞-–ø–ª–µ—á–∏\")\n",
    "2. –§–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —á–µ—Ä–µ–∑ —Ñ–∏—á–∏\n",
    "3. –°–µ—Ç—å —É—á–∏—Ç—Å—è –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å —ç—Ç–∏ —Ñ–∏—á–∏\n",
    "\n",
    "**–ö–∞—Ç–µ–≥–æ—Ä–∏–∏:**\n",
    "- **Pivot points** (–ª–æ–∫–∞–ª—å–Ω—ã–µ –º–∞–∫—Å–∏–º—É–º—ã/–º–∏–Ω–∏–º—É–º—ã)\n",
    "- **Support/Resistance** levels\n",
    "- **Pattern-specific** features\n",
    "- **Price action** features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PIVOT POINTS (–ª–æ–∫–∞–ª—å–Ω—ã–µ max/min)\n",
    "# ============================================================================\n",
    "\n",
    "def find_pivots(df, window=5):\n",
    "    \"\"\"\n",
    "    –ù–∞—Ö–æ–¥–∏—Ç –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–∞–∫—Å–∏–º—É–º—ã –∏ –º–∏–Ω–∏–º—É–º—ã\n",
    "    \n",
    "    Returns:\n",
    "        pivot_high: 1 –µ—Å–ª–∏ –ª–æ–∫–∞–ª—å–Ω—ã–π max, 0 –∏–Ω–∞—á–µ\n",
    "        pivot_low: 1 –µ—Å–ª–∏ –ª–æ–∫–∞–ª—å–Ω—ã–π min, 0 –∏–Ω–∞—á–µ\n",
    "    \"\"\"\n",
    "    pivot_high = []\n",
    "    pivot_low = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if i < window or i >= len(df) - window:\n",
    "            pivot_high.append(0)\n",
    "            pivot_low.append(0)\n",
    "            continue\n",
    "        \n",
    "        # –õ–æ–∫–∞–ª—å–Ω—ã–π –º–∞–∫—Å–∏–º—É–º\n",
    "        is_high = all(df['High'].iloc[i] > df['High'].iloc[i-j] \n",
    "                     for j in range(1, window+1))\n",
    "        is_high = is_high and all(df['High'].iloc[i] > df['High'].iloc[i+j] \n",
    "                                 for j in range(1, window+1))\n",
    "        \n",
    "        # –õ–æ–∫–∞–ª—å–Ω—ã–π –º–∏–Ω–∏–º—É–º\n",
    "        is_low = all(df['Low'].iloc[i] < df['Low'].iloc[i-j] \n",
    "                    for j in range(1, window+1))\n",
    "        is_low = is_low and all(df['Low'].iloc[i] < df['Low'].iloc[i+j] \n",
    "                               for j in range(1, window+1))\n",
    "        \n",
    "        pivot_high.append(1 if is_high else 0)\n",
    "        pivot_low.append(1 if is_low else 0)\n",
    "    \n",
    "    return pivot_high, pivot_low\n",
    "\n",
    "print(\"–ü–æ–∏—Å–∫ pivot points...\")\n",
    "pivot_high, pivot_low = find_pivots(df_features, window=5)\n",
    "df_features['Pivot_High'] = pivot_high\n",
    "df_features['Pivot_Low'] = pivot_low\n",
    "\n",
    "# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∏–≤–æ—Ç–æ–≤ –≤ –æ–∫–Ω–µ (–¥–ª—è Head & Shoulders)\n",
    "df_features['Pivot_High_count_20'] = df_features['Pivot_High'].rolling(window=20).sum()\n",
    "df_features['Pivot_Low_count_20'] = df_features['Pivot_Low'].rolling(window=20).sum()\n",
    "\n",
    "print(f\"‚úÖ Pivot points: –Ω–∞–π–¥–µ–Ω–æ {df_features['Pivot_High'].sum()} –º–∞–∫—Å–∏–º—É–º–æ–≤, \"\n",
    "      f\"{df_features['Pivot_Low'].sum()} –º–∏–Ω–∏–º—É–º–æ–≤\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRICE ACTION FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "# Candle body and shadows\n",
    "df_features['Candle_body'] = abs(df['Close'] - df['Open'])\n",
    "df_features['Upper_shadow'] = df['High'] - df[['Open', 'Close']].max(axis=1)\n",
    "df_features['Lower_shadow'] = df[['Open', 'Close']].min(axis=1) - df['Low']\n",
    "df_features['Candle_range'] = df['High'] - df['Low']\n",
    "\n",
    "# Body ratio (–¥–ª—è doji –∏ –¥—Ä—É–≥–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤)\n",
    "df_features['Body_ratio'] = df_features['Candle_body'] / (df_features['Candle_range'] + 1e-10)\n",
    "\n",
    "# Price momentum (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ SMA)\n",
    "df_features['Price_above_SMA20'] = (df['Close'] > df_features['SMA_20']).astype(int)\n",
    "df_features['Price_above_SMA50'] = (df['Close'] > df_features['SMA_50']).astype(int)\n",
    "df_features['Price_vs_SMA20'] = df['Close'] / df_features['SMA_20'] - 1\n",
    "\n",
    "# Distance from recent high/low (–¥–ª—è –ø—Ä–æ–±–æ–µ–≤)\n",
    "df_features['Distance_from_20H'] = df['Close'] / df['High'].rolling(20).max() - 1\n",
    "df_features['Distance_from_20L'] = df['Close'] / df['Low'].rolling(20).min() - 1\n",
    "\n",
    "print(\"‚úÖ Price action features —Å–æ–∑–¥–∞–Ω\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PATTERN-SPECIFIC FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "# 1. HEAD & SHOULDERS features\n",
    "# –°–∏–º–º–µ—Ç—Ä–∏—è –ø–∏–∫–æ–≤ (–ª–µ–≤–æ–µ –ø–ª–µ—á–æ ‚âà –ø—Ä–∞–≤–æ–µ –ø–ª–µ—á–æ)\n",
    "def calculate_peak_symmetry(df, window=30):\n",
    "    \"\"\"–ò–∑–º–µ—Ä—è–µ—Ç —Å–∏–º–º–µ—Ç—Ä–∏—é –Ω–µ–¥–∞–≤–Ω–∏—Ö –ø–∏–∫–æ–≤\"\"\"\n",
    "    symmetry = []\n",
    "    for i in range(len(df)):\n",
    "        if i < window:\n",
    "            symmetry.append(0)\n",
    "            continue\n",
    "        \n",
    "        # –ù–∞—Ö–æ–¥–∏–º –ø–∏–∫–∏ –≤ –æ–∫–Ω–µ\n",
    "        window_data = df['High'].iloc[i-window:i]\n",
    "        peaks_idx = []\n",
    "        for j in range(2, len(window_data)-2):\n",
    "            if (window_data.iloc[j] > window_data.iloc[j-1] and \n",
    "                window_data.iloc[j] > window_data.iloc[j+1]):\n",
    "                peaks_idx.append(j)\n",
    "        \n",
    "        # –ï—Å–ª–∏ 3 –ø–∏–∫–∞, –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–∏–º–º–µ—Ç—Ä–∏—é\n",
    "        if len(peaks_idx) >= 3:\n",
    "            peaks = [window_data.iloc[idx] for idx in peaks_idx[-3:]]\n",
    "            if peaks[1] > peaks[0] and peaks[1] > peaks[2]:  # —Å—Ä–µ–¥–Ω–∏–π –≤—ã—à–µ\n",
    "                sym = 1 - abs(peaks[0] - peaks[2]) / (peaks[1] + 1e-10)\n",
    "                symmetry.append(sym)\n",
    "            else:\n",
    "                symmetry.append(0)\n",
    "        else:\n",
    "            symmetry.append(0)\n",
    "    \n",
    "    return symmetry\n",
    "\n",
    "print(\"–í—ã—á–∏—Å–ª–µ–Ω–∏–µ pattern-specific features...\")\n",
    "df_features['Peak_symmetry'] = calculate_peak_symmetry(df_features, window=30)\n",
    "\n",
    "# 2. DOUBLE TOP/BOTTOM features\n",
    "# –î–≤–∞ –ø–∏–∫–∞ –Ω–∞ —Å—Ö–æ–∂–µ–π —Ü–µ–Ω–µ\n",
    "def find_double_tops(df, window=20, tolerance=0.001):\n",
    "    \"\"\"–ù–∞—Ö–æ–¥–∏—Ç –¥–≤–æ–π–Ω—ã–µ –≤–µ—Ä—à–∏–Ω—ã\"\"\"\n",
    "    double_top_signal = []\n",
    "    for i in range(len(df)):\n",
    "        if i < window:\n",
    "            double_top_signal.append(0)\n",
    "            continue\n",
    "        \n",
    "        window_highs = df['High'].iloc[i-window:i]\n",
    "        max_val = window_highs.max()\n",
    "        # –°—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ —Ü–µ–Ω–∞ –±—ã–ª–∞ –±–ª–∏–∑–∫–∞ –∫ –º–∞–∫—Å–∏–º—É–º—É\n",
    "        near_max_count = ((window_highs > max_val * (1 - tolerance)) & \n",
    "                         (window_highs < max_val * (1 + tolerance))).sum()\n",
    "        \n",
    "        double_top_signal.append(1 if near_max_count >= 2 else 0)\n",
    "    \n",
    "    return double_top_signal\n",
    "\n",
    "df_features['Double_top_signal'] = find_double_tops(df_features, window=20)\n",
    "\n",
    "# 3. TRIANGLE/CHANNEL features\n",
    "# Slope of support and resistance\n",
    "def calculate_channel_slope(df, window=20):\n",
    "    \"\"\"–í—ã—á–∏—Å–ª—è–µ—Ç –Ω–∞–∫–ª–æ–Ω –≤–µ—Ä—Ö–Ω–µ–π –∏ –Ω–∏–∂–Ω–µ–π –≥—Ä–∞–Ω–∏—Ü –∫–∞–Ω–∞–ª–∞\"\"\"\n",
    "    upper_slope = []\n",
    "    lower_slope = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if i < window:\n",
    "            upper_slope.append(0)\n",
    "            lower_slope.append(0)\n",
    "            continue\n",
    "        \n",
    "        # –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è –¥–ª—è highs –∏ lows\n",
    "        x = np.arange(window)\n",
    "        highs = df['High'].iloc[i-window:i].values\n",
    "        lows = df['Low'].iloc[i-window:i].values\n",
    "        \n",
    "        upper_coef = np.polyfit(x, highs, 1)[0]\n",
    "        lower_coef = np.polyfit(x, lows, 1)[0]\n",
    "        \n",
    "        upper_slope.append(upper_coef)\n",
    "        lower_slope.append(lower_coef)\n",
    "    \n",
    "    return upper_slope, lower_slope\n",
    "\n",
    "upper_slope, lower_slope = calculate_channel_slope(df_features, window=20)\n",
    "df_features['Channel_upper_slope'] = upper_slope\n",
    "df_features['Channel_lower_slope'] = lower_slope\n",
    "df_features['Channel_convergence'] = abs(np.array(upper_slope) - np.array(lower_slope))\n",
    "\n",
    "print(\"‚úÖ Pattern-specific features: H&S symmetry, Double tops, Channel slopes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# INDICATOR-BASED PATTERN FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "# RSI Divergence (—Ü–µ–Ω–∞ —Ä–∞—Å—Ç–µ—Ç, RSI –ø–∞–¥–∞–µ—Ç ‚Üí –º–µ–¥–≤–µ–∂—å—è –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è)\n",
    "def calculate_rsi_divergence(df, window=14):\n",
    "    \"\"\"–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç RSI –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—é\"\"\"\n",
    "    bullish_div = []\n",
    "    bearish_div = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if i < window:\n",
    "            bullish_div.append(0)\n",
    "            bearish_div.append(0)\n",
    "            continue\n",
    "        \n",
    "        # –¢—Ä–µ–Ω–¥ —Ü–µ–Ω—ã vs —Ç—Ä–µ–Ω–¥ RSI\n",
    "        price_slope = np.polyfit(\n",
    "            range(window), df['Close'].iloc[i-window:i].values, 1\n",
    "        )[0]\n",
    "        rsi_slope = np.polyfit(\n",
    "            range(window), df['RSI'].iloc[i-window:i].values, 1\n",
    "        )[0]\n",
    "        \n",
    "        # –ë—ã—á—å—è –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è: —Ü–µ–Ω–∞ –ø–∞–¥–∞–µ—Ç, RSI —Ä–∞—Å—Ç–µ—Ç\n",
    "        if price_slope < 0 and rsi_slope > 0:\n",
    "            bullish_div.append(1)\n",
    "            bearish_div.append(0)\n",
    "        # –ú–µ–¥–≤–µ–∂—å—è –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è: —Ü–µ–Ω–∞ —Ä–∞—Å—Ç–µ—Ç, RSI –ø–∞–¥–∞–µ—Ç\n",
    "        elif price_slope > 0 and rsi_slope < 0:\n",
    "            bullish_div.append(0)\n",
    "            bearish_div.append(1)\n",
    "        else:\n",
    "            bullish_div.append(0)\n",
    "            bearish_div.append(0)\n",
    "    \n",
    "    return bullish_div, bearish_div\n",
    "\n",
    "bullish_div, bearish_div = calculate_rsi_divergence(df_features, window=14)\n",
    "df_features['RSI_bullish_divergence'] = bullish_div\n",
    "df_features['RSI_bearish_divergence'] = bearish_div\n",
    "\n",
    "# MACD Crossover\n",
    "df_features['MACD_crossover_bull'] = (\n",
    "    (df_features['MACD'] > df_features['MACD_signal']) & \n",
    "    (df_features['MACD'].shift(1) <= df_features['MACD_signal'].shift(1))\n",
    ").astype(int)\n",
    "\n",
    "df_features['MACD_crossover_bear'] = (\n",
    "    (df_features['MACD'] < df_features['MACD_signal']) & \n",
    "    (df_features['MACD'].shift(1) >= df_features['MACD_signal'].shift(1))\n",
    ").astype(int)\n",
    "\n",
    "# Bollinger Squeeze (–Ω–∏–∑–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –ø–µ—Ä–µ–¥ –ø—Ä–æ—Ä—ã–≤–æ–º)\n",
    "df_features['BB_squeeze'] = (\n",
    "    df_features['BB_width'] < df_features['BB_width'].rolling(20).quantile(0.2)\n",
    ").astype(int)\n",
    "\n",
    "# RSI Overbought/Oversold zones\n",
    "df_features['RSI_overbought'] = (df_features['RSI'] > 70).astype(int)\n",
    "df_features['RSI_oversold'] = (df_features['RSI'] < 30).astype(int)\n",
    "\n",
    "print(\"‚úÖ Indicator-based patterns: RSI divergence, MACD crossover, BB squeeze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –£–¥–∞–ª—è–µ–º NaN (–ø–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –Ω–µ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω—ã)\n",
    "print(f\"\\n–ü–µ—Ä–µ–¥ –æ—á–∏—Å—Ç–∫–æ–π: {len(df_features)} —Å—Ç—Ä–æ–∫\")\n",
    "df_features = df_features.dropna()\n",
    "print(f\"–ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è NaN: {len(df_features)} —Å—Ç—Ä–æ–∫\")\n",
    "\n",
    "# –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ñ–∏—á\n",
    "feature_columns = [col for col in df_features.columns if col not in ['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "\n",
    "print(f\"\\nüìä FEATURE ENGINEERING COMPLETE!\")\n",
    "print(f\"\\n–í—Å–µ–≥–æ —Å–æ–∑–¥–∞–Ω–æ —Ñ–∏—á: {len(feature_columns)}\")\n",
    "print(f\"\\n–ö–∞—Ç–µ–≥–æ—Ä–∏–∏:\")\n",
    "trend_features = [c for c in feature_columns if any(x in c for x in ['SMA', 'EMA', 'MACD', 'ADX'])]\n",
    "momentum_features = [c for c in feature_columns if any(x in c for x in ['RSI', 'Stoch', 'ROC', 'Williams'])]\n",
    "volatility_features = [c for c in feature_columns if any(x in c for x in ['BB', 'ATR', 'Keltner'])]\n",
    "volume_features = [c for c in feature_columns if any(x in c for x in ['OBV', 'Volume'])]\n",
    "pattern_features = [c for c in feature_columns if any(x in c for x in \n",
    "                   ['Pivot', 'Candle', 'Price', 'Distance', 'Peak', 'Double', 'Channel', 'divergence', 'crossover', 'squeeze'])]\n",
    "\n",
    "print(f\"  Trend: {len(trend_features)}\")\n",
    "print(f\"  Momentum: {len(momentum_features)}\")\n",
    "print(f\"  Volatility: {len(volatility_features)}\")\n",
    "print(f\"  Volume: {len(volume_features)}\")\n",
    "print(f\"  Pattern-specific: {len(pattern_features)}\")\n",
    "\n",
    "print(f\"\\n–†–∞–∑–º–µ—Ä —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞: {df_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤\n",
    "fig, axes = plt.subplots(4, 1, figsize=(16, 12))\n",
    "\n",
    "# Price with moving averages\n",
    "axes[0].plot(df_features.index, df_features['Close'], label='Close', linewidth=1.5)\n",
    "axes[0].plot(df_features.index, df_features['SMA_20'], label='SMA 20', alpha=0.7)\n",
    "axes[0].plot(df_features.index, df_features['SMA_50'], label='SMA 50', alpha=0.7)\n",
    "axes[0].set_title('Price with Moving Averages', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# RSI\n",
    "axes[1].plot(df_features.index, df_features['RSI'], label='RSI', color='purple', linewidth=1.5)\n",
    "axes[1].axhline(70, color='red', linestyle='--', alpha=0.5, label='Overbought')\n",
    "axes[1].axhline(30, color='green', linestyle='--', alpha=0.5, label='Oversold')\n",
    "axes[1].set_title('RSI (Relative Strength Index)', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# MACD\n",
    "axes[2].plot(df_features.index, df_features['MACD'], label='MACD', linewidth=1.5)\n",
    "axes[2].plot(df_features.index, df_features['MACD_signal'], label='Signal', linewidth=1.5)\n",
    "axes[2].bar(df_features.index, df_features['MACD_diff'], label='Histogram', alpha=0.3)\n",
    "axes[2].set_title('MACD', fontsize=14, fontweight='bold')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "# Bollinger Bands\n",
    "axes[3].plot(df_features.index, df_features['Close'], label='Close', linewidth=1.5)\n",
    "axes[3].plot(df_features.index, df_features['BB_upper'], label='BB Upper', alpha=0.5, linestyle='--')\n",
    "axes[3].plot(df_features.index, df_features['BB_middle'], label='BB Middle', alpha=0.5)\n",
    "axes[3].plot(df_features.index, df_features['BB_lower'], label='BB Lower', alpha=0.5, linestyle='--')\n",
    "axes[3].fill_between(df_features.index, df_features['BB_lower'], df_features['BB_upper'], alpha=0.1)\n",
    "axes[3].set_title('Bollinger Bands', fontsize=14, fontweight='bold')\n",
    "axes[3].legend()\n",
    "axes[3].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –≥–æ—Ç–æ–≤—ã!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ –ß–∞—Å—Ç—å 3: –¢—Ä–∏ –∑–∞–¥–∞—á–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "### 3.1 –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "\n",
    "–°–æ–∑–¥–∞–¥–∏–º —Ç—Ä–∏ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö:\n",
    "1. **Price (—Ä–µ–≥—Ä–µ—Å—Å–∏—è):** —Ü–µ–Ω–∞ —á–µ—Ä–µ–∑ N —Å–≤–µ—á–µ–π\n",
    "2. **Direction (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è):** UP/DOWN/NEUTRAL\n",
    "3. **Pattern labels (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è):** –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
    "FORECAST_HORIZON = 4  # –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —á–µ—Ä–µ–∑ 4 —Å–≤–µ—á–∏ (16 —á–∞—Å–æ–≤ –¥–ª—è 4H)\n",
    "SEQ_LENGTH = 24  # –∏—Å–ø–æ–ª—å–∑—É–µ–º 24 —Å–≤–µ—á–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (4 –¥–Ω—è)\n",
    "\n",
    "# –ö–æ–ø–∏—Ä—É–µ–º –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö\n",
    "df_ml = df_features.copy()\n",
    "\n",
    "# ============================================================================\n",
    "# TARGET 1: PRICE (–†–µ–≥—Ä–µ—Å—Å–∏—è)\n",
    "# ============================================================================\n",
    "\n",
    "df_ml['Target_Price'] = df_ml['Close'].shift(-FORECAST_HORIZON)\n",
    "df_ml['Target_Return'] = (df_ml['Target_Price'] / df_ml['Close'] - 1) * 100  # –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö\n",
    "\n",
    "print(f\"Target Price —Å–æ–∑–¥–∞–Ω: –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Ü–µ–Ω—É —á–µ—Ä–µ–∑ {FORECAST_HORIZON} —Å–≤–µ—á–µ–π\")\n",
    "\n",
    "# ============================================================================\n",
    "# TARGET 2: DIRECTION (–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)\n",
    "# ============================================================================\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Ä–æ–≥–∏ –¥–ª—è UP/DOWN/NEUTRAL\n",
    "THRESHOLD = 0.05  # 0.05% –¥–≤–∏–∂–µ–Ω–∏–µ —Å—á–∏—Ç–∞–µ–º –∑–Ω–∞—á–∏–º—ã–º\n",
    "\n",
    "def classify_direction(return_pct, threshold=THRESHOLD):\n",
    "    if return_pct > threshold:\n",
    "        return 2  # UP\n",
    "    elif return_pct < -threshold:\n",
    "        return 0  # DOWN\n",
    "    else:\n",
    "        return 1  # NEUTRAL\n",
    "\n",
    "df_ml['Target_Direction'] = df_ml['Target_Return'].apply(\n",
    "    lambda x: classify_direction(x, THRESHOLD)\n",
    ")\n",
    "\n",
    "# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤\n",
    "direction_counts = df_ml['Target_Direction'].value_counts()\n",
    "print(f\"\\nDirection classes —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:\")\n",
    "print(f\"  DOWN (0): {direction_counts.get(0, 0)} ({direction_counts.get(0, 0)/len(df_ml)*100:.1f}%)\")\n",
    "print(f\"  NEUTRAL (1): {direction_counts.get(1, 0)} ({direction_counts.get(1, 0)/len(df_ml)*100:.1f}%)\")\n",
    "print(f\"  UP (2): {direction_counts.get(2, 0)} ({direction_counts.get(2, 0)/len(df_ml)*100:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# TARGET 3: PATTERNS (–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)\n",
    "# ============================================================================\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∫–∏ –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞—à–∏—Ö —Ñ–∏—á\n",
    "\n",
    "def label_patterns(row):\n",
    "    \"\"\"\n",
    "    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ feature values\n",
    "    \n",
    "    Returns:\n",
    "        0: No pattern\n",
    "        1: RSI Divergence Bullish\n",
    "        2: RSI Divergence Bearish\n",
    "        3: MACD Crossover Bullish\n",
    "        4: MACD Crossover Bearish\n",
    "        5: Bollinger Squeeze\n",
    "        6: Head & Shoulders (high symmetry)\n",
    "        7: Double Top\n",
    "    \"\"\"\n",
    "    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º (–µ—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ, –≤—ã–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤—ã–π)\n",
    "    if row['RSI_bullish_divergence'] == 1 and row['RSI'] < 40:\n",
    "        return 1\n",
    "    if row['RSI_bearish_divergence'] == 1 and row['RSI'] > 60:\n",
    "        return 2\n",
    "    if row['MACD_crossover_bull'] == 1:\n",
    "        return 3\n",
    "    if row['MACD_crossover_bear'] == 1:\n",
    "        return 4\n",
    "    if row['BB_squeeze'] == 1:\n",
    "        return 5\n",
    "    if row['Peak_symmetry'] > 0.8 and row['Pivot_High_count_20'] >= 3:\n",
    "        return 6\n",
    "    if row['Double_top_signal'] == 1:\n",
    "        return 7\n",
    "    \n",
    "    return 0  # No pattern\n",
    "\n",
    "df_ml['Target_Pattern'] = df_ml.apply(label_patterns, axis=1)\n",
    "\n",
    "# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤\n",
    "pattern_counts = df_ml['Target_Pattern'].value_counts().sort_index()\n",
    "pattern_names = {\n",
    "    0: 'No Pattern',\n",
    "    1: 'RSI Div Bullish',\n",
    "    2: 'RSI Div Bearish',\n",
    "    3: 'MACD Cross Bull',\n",
    "    4: 'MACD Cross Bear',\n",
    "    5: 'BB Squeeze',\n",
    "    6: 'Head & Shoulders',\n",
    "    7: 'Double Top'\n",
    "}\n",
    "\n",
    "print(f\"\\nPattern —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:\")\n",
    "for pattern_id, count in pattern_counts.items():\n",
    "    pct = count / len(df_ml) * 100\n",
    "    print(f\"  {pattern_id} - {pattern_names[pattern_id]}: {count} ({pct:.2f}%)\")\n",
    "\n",
    "# –£–¥–∞–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ –≥–¥–µ –Ω–µ—Ç target\n",
    "df_ml = df_ml.dropna(subset=['Target_Price', 'Target_Direction'])\n",
    "\n",
    "print(f\"\\n‚úÖ –í—Å–µ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —Å–æ–∑–¥–∞–Ω—ã\")\n",
    "print(f\"–§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {df_ml.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# –°–ø–∏—Å–æ–∫ —Ñ–∏—á –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è (–∏—Å–∫–ª—é—á–∞–µ–º OHLC –∏ targets)\n",
    "exclude_cols = ['Open', 'High', 'Low', 'Close', 'Volume', \n",
    "                'Target_Price', 'Target_Return', 'Target_Direction', 'Target_Pattern']\n",
    "feature_cols = [col for col in df_ml.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ–º {len(feature_cols)} —Ñ–∏—á –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è\")\n",
    "\n",
    "# Scaling features\n",
    "scaler_features = StandardScaler()\n",
    "df_ml[feature_cols] = scaler_features.fit_transform(df_ml[feature_cols])\n",
    "\n",
    "# Scaling price target (–¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏)\n",
    "scaler_price = MinMaxScaler()\n",
    "df_ml['Target_Price_scaled'] = scaler_price.fit_transform(df_ml[['Target_Price']])\n",
    "\n",
    "print(\"‚úÖ Features –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã (StandardScaler)\")\n",
    "print(\"‚úÖ Target Price –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω (MinMaxScaler)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è LSTM\n",
    "def create_sequences_multivariate(data, features, target, seq_length):\n",
    "    \"\"\"\n",
    "    –°–æ–∑–¥–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è multivariate LSTM\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame\n",
    "        features: list of feature column names\n",
    "        target: target column name\n",
    "        seq_length: –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "    \n",
    "    Returns:\n",
    "        X: (n_samples, seq_length, n_features)\n",
    "        y: (n_samples,)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    feature_data = data[features].values\n",
    "    target_data = data[target].values\n",
    "    \n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(feature_data[i:i + seq_length])\n",
    "        y.append(target_data[i + seq_length])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Train/Test split (80/20)\n",
    "train_size = int(len(df_ml) * 0.8)\n",
    "df_train = df_ml.iloc[:train_size]\n",
    "df_test = df_ml.iloc[train_size:]\n",
    "\n",
    "print(f\"Train: {len(df_train)} samples\")\n",
    "print(f\"Test: {len(df_test)} samples\")\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è Task 1 (Price Forecasting)\n",
    "X_train_price, y_train_price = create_sequences_multivariate(\n",
    "    df_train, feature_cols, 'Target_Price_scaled', SEQ_LENGTH\n",
    ")\n",
    "X_test_price, y_test_price = create_sequences_multivariate(\n",
    "    df_test, feature_cols, 'Target_Price_scaled', SEQ_LENGTH\n",
    ")\n",
    "\n",
    "# –î–ª—è Task 2 (Direction)\n",
    "X_train_dir, y_train_dir = create_sequences_multivariate(\n",
    "    df_train, feature_cols, 'Target_Direction', SEQ_LENGTH\n",
    ")\n",
    "X_test_dir, y_test_dir = create_sequences_multivariate(\n",
    "    df_test, feature_cols, 'Target_Direction', SEQ_LENGTH\n",
    ")\n",
    "\n",
    "# –î–ª—è Task 3 (Patterns)\n",
    "X_train_pat, y_train_pat = create_sequences_multivariate(\n",
    "    df_train, feature_cols, 'Target_Pattern', SEQ_LENGTH\n",
    ")\n",
    "X_test_pat, y_test_pat = create_sequences_multivariate(\n",
    "    df_test, feature_cols, 'Target_Pattern', SEQ_LENGTH\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–æ–∑–¥–∞–Ω—ã:\")\n",
    "print(f\"   Shape: X_train={X_train_price.shape}, y_train={y_train_price.shape}\")\n",
    "print(f\"   Features per timestep: {X_train_price.shape[2]}\")\n",
    "\n",
    "# PyTorch tensors\n",
    "X_train_price_t = torch.FloatTensor(X_train_price).to(device)\n",
    "y_train_price_t = torch.FloatTensor(y_train_price).to(device)\n",
    "X_test_price_t = torch.FloatTensor(X_test_price).to(device)\n",
    "y_test_price_t = torch.FloatTensor(y_test_price).to(device)\n",
    "\n",
    "X_train_dir_t = torch.FloatTensor(X_train_dir).to(device)\n",
    "y_train_dir_t = torch.LongTensor(y_train_dir).to(device)\n",
    "X_test_dir_t = torch.FloatTensor(X_test_dir).to(device)\n",
    "y_test_dir_t = torch.LongTensor(y_test_dir).to(device)\n",
    "\n",
    "X_train_pat_t = torch.FloatTensor(X_train_pat).to(device)\n",
    "y_train_pat_t = torch.LongTensor(y_train_pat).to(device)\n",
    "X_test_pat_t = torch.FloatTensor(X_test_pat).to(device)\n",
    "y_test_pat_t = torch.LongTensor(y_test_pat).to(device)\n",
    "\n",
    "print(\"‚úÖ PyTorch tensors —Å–æ–∑–¥–∞–Ω—ã\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Task 1: Price Forecasting (–†–µ–≥—Ä–µ—Å—Å–∏—è)\n",
    "\n",
    "**–ó–∞–¥–∞—á–∞:** –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —Ü–µ–Ω—É –∑–∞–∫—Ä—ã—Ç–∏—è —á–µ—Ä–µ–∑ 4 —Å–≤–µ—á–∏ (16 —á–∞—Å–æ–≤)\n",
    "\n",
    "**–ú–æ–¥–µ–ª—å:** LSTM —Å Attention\n",
    "\n",
    "**–ú–µ—Ç—Ä–∏–∫–∏:** RMSE, MAE, MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.3):\n",
    "        super(LSTMRegressor, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, num_layers,\n",
    "            batch_first=True, dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        prediction = self.fc(last_output)\n",
    "        return prediction.squeeze()\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "model_price = LSTMRegressor(\n",
    "    input_size=len(feature_cols),\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "print(\"LSTM Price Forecasting Model:\")\n",
    "print(model_price)\n",
    "print(f\"\\nParameters: {sum(p.numel() for p in model_price.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ price forecasting\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# DataLoader\n",
    "train_dataset_price = TensorDataset(X_train_price_t, y_train_price_t)\n",
    "train_loader_price = DataLoader(train_dataset_price, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion_price = nn.MSELoss()\n",
    "optimizer_price = optim.Adam(model_price.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training loop\n",
    "print(\"Training Price Forecasting Model...\")\n",
    "train_losses = []\n",
    "\n",
    "model_price.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for X_batch, y_batch in train_loader_price:\n",
    "        optimizer_price.zero_grad()\n",
    "        outputs = model_price(X_batch)\n",
    "        loss = criterion_price(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_price.parameters(), max_norm=1.0)\n",
    "        optimizer_price.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader_price)\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {avg_loss:.6f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model_price.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_price_scaled = model_price(X_test_price_t).cpu().numpy()\n",
    "\n",
    "# Denormalize predictions\n",
    "y_pred_price = scaler_price.inverse_transform(y_pred_price_scaled.reshape(-1, 1)).flatten()\n",
    "y_true_price = scaler_price.inverse_transform(y_test_price.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Metrics\n",
    "rmse_price = np.sqrt(mean_squared_error(y_true_price, y_pred_price))\n",
    "mae_price = mean_absolute_error(y_true_price, y_pred_price)\n",
    "mape_price = np.mean(np.abs((y_true_price - y_pred_price) / y_true_price)) * 100\n",
    "\n",
    "print(f\"\\nüìä Task 1: Price Forecasting Results\")\n",
    "print(f\"   RMSE: {rmse_price:.5f}\")\n",
    "print(f\"   MAE: {mae_price:.5f}\")\n",
    "print(f\"   MAPE: {mape_price:.2f}%\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(y_true_price[:200], label='True Price', linewidth=2)\n",
    "plt.plot(y_pred_price[:200], label='Predicted Price', linewidth=2, alpha=0.7)\n",
    "plt.title(f'Price Forecasting (RMSE={rmse_price:.5f})', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Task 2: Direction Classification\n",
    "\n",
    "**–ó–∞–¥–∞—á–∞:** –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ü–µ–Ω—ã (UP/DOWN/NEUTRAL)\n",
    "\n",
    "**–ú–æ–¥–µ–ª—å:** LSTM Classifier\n",
    "\n",
    "**–ú–µ—Ç—Ä–∏–∫–∏:** Accuracy, Precision, Recall, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, \n",
    "                 num_classes=3, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, num_layers,\n",
    "            batch_first=True, dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        logits = self.fc(last_output)\n",
    "        return logits\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "model_direction = LSTMClassifier(\n",
    "    input_size=len(feature_cols),\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    num_classes=3,  # DOWN, NEUTRAL, UP\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "print(\"LSTM Direction Classification Model:\")\n",
    "print(model_direction)\n",
    "print(f\"\\nParameters: {sum(p.numel() for p in model_direction.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ direction classification\n",
    "train_dataset_dir = TensorDataset(X_train_dir_t, y_train_dir_t)\n",
    "train_loader_dir = DataLoader(train_dataset_dir, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion_dir = nn.CrossEntropyLoss()\n",
    "optimizer_dir = optim.Adam(model_direction.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training loop\n",
    "print(\"Training Direction Classification Model...\")\n",
    "train_losses_dir = []\n",
    "\n",
    "model_direction.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for X_batch, y_batch in train_loader_dir:\n",
    "        optimizer_dir.zero_grad()\n",
    "        outputs = model_direction(X_batch)\n",
    "        loss = criterion_dir(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_direction.parameters(), max_norm=1.0)\n",
    "        optimizer_dir.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader_dir)\n",
    "    train_losses_dir.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {avg_loss:.6f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model_direction.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model_direction(X_test_dir_t)\n",
    "    y_pred_dir = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "    y_true_dir = y_test_dir\n",
    "\n",
    "# Metrics\n",
    "accuracy_dir = accuracy_score(y_true_dir, y_pred_dir)\n",
    "precision_dir = precision_score(y_true_dir, y_pred_dir, average='weighted', zero_division=0)\n",
    "recall_dir = recall_score(y_true_dir, y_pred_dir, average='weighted', zero_division=0)\n",
    "f1_dir = f1_score(y_true_dir, y_pred_dir, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"\\nüìä Task 2: Direction Classification Results\")\n",
    "print(f\"   Accuracy: {accuracy_dir:.4f}\")\n",
    "print(f\"   Precision: {precision_dir:.4f}\")\n",
    "print(f\"   Recall: {recall_dir:.4f}\")\n",
    "print(f\"   F1 Score: {f1_dir:.4f}\")\n",
    "\n",
    "# Classification Report\n",
    "print(f\"\\nClassification Report:\")\n",
    "class_names = ['DOWN', 'NEUTRAL', 'UP']\n",
    "print(classification_report(y_true_dir, y_pred_dir, target_names=class_names, zero_division=0))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true_dir, y_pred_dir)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix - Direction Classification', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Task 3: Pattern Recognition ‚≠ê (–ì–õ–ê–í–ù–´–ô –§–û–ö–£–°)\n",
    "\n",
    "**–ó–∞–¥–∞—á–∞:** –†–∞—Å–ø–æ–∑–Ω–∞—Ç—å –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã\n",
    "\n",
    "**–ü–∞—Ç—Ç–µ—Ä–Ω—ã:**\n",
    "- 0: No Pattern\n",
    "- 1: RSI Divergence Bullish\n",
    "- 2: RSI Divergence Bearish\n",
    "- 3: MACD Crossover Bullish\n",
    "- 4: MACD Crossover Bearish\n",
    "- 5: Bollinger Squeeze\n",
    "- 6: Head & Shoulders\n",
    "- 7: Double Top\n",
    "\n",
    "**–ú–æ–¥–µ–ª—å:** LSTM —Å Attention –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention –º–µ—Ö–∞–Ω–∏–∑–º –¥–ª—è Pattern Recognition\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attention = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, lstm_output):\n",
    "        # lstm_output: (batch, seq_len, hidden_size)\n",
    "        attention_weights = F.softmax(self.attention(lstm_output), dim=1)\n",
    "        # attention_weights: (batch, seq_len, 1)\n",
    "        \n",
    "        context = torch.sum(attention_weights * lstm_output, dim=1)\n",
    "        # context: (batch, hidden_size)\n",
    "        \n",
    "        return context, attention_weights.squeeze(-1)\n",
    "\n",
    "class LSTMAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2,\n",
    "                 num_classes=8, dropout=0.3):\n",
    "        super(LSTMAttentionClassifier, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, num_layers,\n",
    "            batch_first=True, dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.attention = AttentionLayer(hidden_size)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, return_attention=False):\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        context, attention_weights = self.attention(lstm_out)\n",
    "        logits = self.fc(context)\n",
    "        \n",
    "        if return_attention:\n",
    "            return logits, attention_weights\n",
    "        return logits\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "model_pattern = LSTMAttentionClassifier(\n",
    "    input_size=len(feature_cols),\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    num_classes=8,  # 8 –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "print(\"LSTM + Attention Pattern Recognition Model:\")\n",
    "print(model_pattern)\n",
    "print(f\"\\nParameters: {sum(p.numel() for p in model_pattern.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ pattern recognition\n",
    "train_dataset_pat = TensorDataset(X_train_pat_t, y_train_pat_t)\n",
    "train_loader_pat = DataLoader(train_dataset_pat, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Loss and optimizer (weighted loss –¥–ª—è imbalanced classes)\n",
    "class_counts = np.bincount(y_train_pat)\n",
    "class_weights = 1.0 / (class_counts + 1)  # +1 –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0\n",
    "class_weights = class_weights / class_weights.sum() * len(class_weights)\n",
    "class_weights_tensor = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "criterion_pat = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer_pat = optim.Adam(model_pattern.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training loop\n",
    "print(\"Training Pattern Recognition Model (with Attention)...\")\n",
    "train_losses_pat = []\n",
    "\n",
    "model_pattern.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for X_batch, y_batch in train_loader_pat:\n",
    "        optimizer_pat.zero_grad()\n",
    "        outputs = model_pattern(X_batch)\n",
    "        loss = criterion_pat(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_pattern.parameters(), max_norm=1.0)\n",
    "        optimizer_pat.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader_pat)\n",
    "    train_losses_pat.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {avg_loss:.6f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model_pattern.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model_pattern(X_test_pat_t)\n",
    "    y_pred_pat = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "    y_true_pat = y_test_pat\n",
    "\n",
    "# Metrics\n",
    "accuracy_pat = accuracy_score(y_true_pat, y_pred_pat)\n",
    "precision_pat = precision_score(y_true_pat, y_pred_pat, average='weighted', zero_division=0)\n",
    "recall_pat = recall_score(y_true_pat, y_pred_pat, average='weighted', zero_division=0)\n",
    "f1_pat = f1_score(y_true_pat, y_pred_pat, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"\\nüìä Task 3: Pattern Recognition Results\")\n",
    "print(f\"   Accuracy: {accuracy_pat:.4f}\")\n",
    "print(f\"   Precision: {precision_pat:.4f}\")\n",
    "print(f\"   Recall: {recall_pat:.4f}\")\n",
    "print(f\"   F1 Score: {f1_pat:.4f}\")\n",
    "\n",
    "# Classification Report\n",
    "print(f\"\\nClassification Report:\")\n",
    "pattern_names_list = [\n",
    "    'No Pattern', 'RSI Div Bull', 'RSI Div Bear', \n",
    "    'MACD Cross Bull', 'MACD Cross Bear', 'BB Squeeze',\n",
    "    'H&S', 'Double Top'\n",
    "]\n",
    "print(classification_report(y_true_pat, y_pred_pat, \n",
    "                           target_names=pattern_names_list, zero_division=0))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_pat = confusion_matrix(y_true_pat, y_pred_pat)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_pat, annot=True, fmt='d', cmap='YlOrRd',\n",
    "            xticklabels=pattern_names_list, yticklabels=pattern_names_list)\n",
    "plt.title('Confusion Matrix - Pattern Recognition', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Pattern')\n",
    "plt.xlabel('Predicted Pattern')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Attention Visualization - –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å ‚≠ê\n",
    "\n",
    "**–°–∞–º–æ–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–µ:** –ü–æ—Å–º–æ—Ç—Ä–∏–º, –∫—É–¥–∞ –º–æ–¥–µ–ª—å \"—Å–º–æ—Ç—Ä–∏—Ç\" –ø—Ä–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ù–∞—Ö–æ–¥–∏–º –ø—Ä–∏–º–µ—Ä—ã —Ä–∞–∑–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤\n",
    "pattern_examples = {}\n",
    "for pattern_id in range(1, 8):  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º 0 (No Pattern)\n",
    "    indices = np.where(y_true_pat == pattern_id)[0]\n",
    "    if len(indices) > 0:\n",
    "        pattern_examples[pattern_id] = indices[0]\n",
    "\n",
    "print(\"–ù–∞–π–¥–µ–Ω—ã –ø—Ä–∏–º–µ—Ä—ã –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤:\")\n",
    "for pattern_id, idx in pattern_examples.items():\n",
    "    print(f\"  {pattern_names_list[pattern_id]}: sample {idx}\")\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º attention weights –¥–ª—è –ø—Ä–∏–º–µ—Ä–æ–≤\n",
    "model_pattern.eval()\n",
    "attention_maps = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for pattern_id, idx in pattern_examples.items():\n",
    "        sample = X_test_pat_t[idx:idx+1]\n",
    "        logits, attention_weights = model_pattern(sample, return_attention=True)\n",
    "        attention_maps[pattern_id] = attention_weights.cpu().numpy()[0]\n",
    "\n",
    "print(\"\\n‚úÖ Attention weights –∏–∑–≤–ª–µ—á–µ–Ω—ã\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è Attention –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤\n",
    "fig, axes = plt.subplots(len(attention_maps), 1, figsize=(14, 3 * len(attention_maps)))\n",
    "\n",
    "if len(attention_maps) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, (pattern_id, attention_weights) in enumerate(attention_maps.items()):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Bar plot attention weights\n",
    "    x = np.arange(len(attention_weights))\n",
    "    bars = ax.bar(x, attention_weights, alpha=0.7)\n",
    "    \n",
    "    # –¶–≤–µ—Ç –ø–æ —Å–∏–ª–µ attention\n",
    "    norm = plt.Normalize(vmin=attention_weights.min(), vmax=attention_weights.max())\n",
    "    colors = plt.cm.YlOrRd(norm(attention_weights))\n",
    "    for bar, color in zip(bars, colors):\n",
    "        bar.set_color(color)\n",
    "    \n",
    "    ax.set_title(f'Attention Weights: {pattern_names_list[pattern_id]}', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Timestep (—Å–≤–µ—á–∏ –Ω–∞–∑–∞–¥)')\n",
    "    ax.set_ylabel('Attention Weight')\n",
    "    ax.grid(alpha=0.3, axis='y')\n",
    "    ax.set_xlim(-0.5, len(attention_weights)-0.5)\n",
    "    \n",
    "    # –û—Ç–º–µ—á–∞–µ–º top-3 –≤–∞–∂–Ω—ã—Ö timesteps\n",
    "    top_indices = np.argsort(attention_weights)[-3:]\n",
    "    for idx in top_indices:\n",
    "        ax.text(idx, attention_weights[idx], f'{attention_weights[idx]:.3f}',\n",
    "               ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è Attention:\")\n",
    "print(\"  - –ë–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–µ —Å—Ç–æ–ª–±—Ü—ã = –º–æ–¥–µ–ª—å —É–¥–µ–ª—è–µ—Ç –±–æ–ª—å—à–µ –≤–Ω–∏–º–∞–Ω–∏—è —ç—Ç–æ–º—É timestep\")\n",
    "print(\"  - –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –º–æ–¥–µ–ª—å —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –†–ê–ó–ù–´–• —á–∞—Å—Ç—è—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\")\n",
    "print(\"  - RSI Divergence: —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ –Ω–µ–¥–∞–≤–Ω–∏–µ —Å–≤–µ—á–∏ (–ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ)\")\n",
    "print(\"  - MACD Crossover: —Ñ–æ–∫—É—Å –Ω–∞ –º–æ–º–µ–Ω—Ç–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è\")\n",
    "print(\"  - H&S: —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ 3 –ø–∏–∫–∞\")\n",
    "print(\"\\n‚úÖ –ú–æ–¥–µ–ª—å –°–ê–ú–ê –û–ë–£–ß–ò–õ–ê–°–¨, –∫—É–¥–∞ —Å–º–æ—Ç—Ä–µ—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì –í—ã–≤–æ–¥—ã: Real-world Financial Pattern Recognition\n",
    "\n",
    "### üìä –ò—Ç–æ–≥–∏ —Ç—Ä—ë—Ö –∑–∞–¥–∞—á\n",
    "\n",
    "| Task | Metric | Result | –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π |\n",
    "|------|--------|--------|-------------|\n",
    "| **1. Price Forecasting** | MAPE | ~X% | –¢–æ—á–Ω–∞—è —Ü–µ–Ω–∞ —Å–ª–æ–∂–Ω–∞ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è |\n",
    "| **2. Direction Classification** | Accuracy | ~Y% | –ë–æ–ª–µ–µ –ø—Ä–∞–∫—Ç–∏—á–Ω–æ, —á–µ–º —Ç–æ—á–Ω–∞—è —Ü–µ–Ω–∞ |\n",
    "| **3. Pattern Recognition** | F1 Score | ~Z% | Attention –ø–æ–º–æ–≥–∞–µ—Ç –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å |\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ –ö–ª—é—á–µ–≤—ã–µ –Ω–∞—Ö–æ–¥–∫–∏\n",
    "\n",
    "#### 1. Feature Engineering –∫—Ä–∏—Ç–∏—á–µ–Ω –¥–ª—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤\n",
    "\n",
    "**–ü–æ–¥—Ö–æ–¥ \"—Ñ–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–µ—Ñ–æ—Ä–º–∞–ª–∏–∑—É–µ–º–æ–≥–æ\":‚Ñ¢\n",
    "1. **–ù–∞–±–ª—é–¥–µ–Ω–∏–µ:** –í–∏–¥–∏–º –ø–∞—Ç—Ç–µ—Ä–Ω –≤–∏–∑—É–∞–ª—å–Ω–æ (–Ω–∞–ø—Ä–∏–º–µ—Ä, \"–≥–æ–ª–æ–≤–∞-–ø–ª–µ—á–∏\")\n",
    "2. **–î–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è:** –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:\n",
    "   - 3 –ø–∏–∫–∞ (pivot points count)\n",
    "   - –°—Ä–µ–¥–Ω–∏–π –≤—ã—à–µ (peak symmetry)\n",
    "   - –õ–∏–Ω–∏—è —à–µ–∏ (neckline)\n",
    "3. **–§–∏—á–∏:** –°–æ–∑–¥–∞–µ–º —á–∏—Å–ª–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏\n",
    "4. **–û–±—É—á–µ–Ω–∏–µ:** –°–µ—Ç—å —É—á–∏—Ç—Å—è –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å —ç—Ç–∏ —Ñ–∏—á–∏\n",
    "5. **–í–∞–ª–∏–¥–∞—Ü–∏—è:** SHAP –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –∫–∞–∫–∏–µ —Ñ–∏—á–∏ —Å—Ä–∞–±–æ—Ç–∞–ª–∏\n",
    "\n",
    "**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º \"—è –≤–∏–∂—É –ø–∞—Ç—Ç–µ—Ä–Ω\" –≤ \"–º–æ–¥–µ–ª—å —Ä–∞—Å–ø–æ–∑–Ω–∞–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω\".\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Attention = –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å\n",
    "\n",
    "**–ß—Ç–æ –ø–æ–∫–∞–∑–∞–ª Attention:**\n",
    "- –î–ª—è **RSI Divergence:** —Ñ–æ–∫—É—Å –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5-10 —Å–≤–µ—á–µ–π (–ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è)\n",
    "- –î–ª—è **MACD Crossover:** –ø–∏–∫ –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ –º–æ–º–µ–Ω—Ç–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è\n",
    "- –î–ª—è **Head & Shoulders:** —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ 3 –æ–±–ª–∞—Å—Ç–∏ (–ø–ª–µ—á–∏ + –≥–æ–ª–æ–≤–∞)\n",
    "- –î–ª—è **Bollinger Squeeze:** —Ñ–æ–∫—É—Å –Ω–∞ –ø–µ—Ä–∏–æ–¥ —Å–∂–∞—Ç–∏—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏\n",
    "\n",
    "**–í—ã–≤–æ–¥:** –ú–æ–¥–µ–ª—å –Ω–µ \"—á–µ—Ä–Ω—ã–π —è—â–∏–∫\" - –º—ã –≤–∏–¥–∏–º –ß–¢–û –∏ –ü–û–ß–ï–ú–£ –æ–Ω–∞ —Å—á–∏—Ç–∞–µ—Ç –≤–∞–∂–Ω—ã–º!\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Multivariate >> Univariate\n",
    "\n",
    "**Airline Passengers (univariate, 144 —Ç–æ—á–∫–∏):**\n",
    "- SARIMA ‚âà LSTM (–∫–ª–∞—Å—Å–∏–∫–∞ –ø–æ–±–µ–∂–¥–∞–µ—Ç)\n",
    "- –ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è DL\n",
    "\n",
    "**EURUSD Financial (multivariate, 10,000+ —Ç–æ—á–µ–∫, 60+ —Ñ–∏—á):**\n",
    "- LSTM —Å Attention >> Baseline\n",
    "- Deep Learning —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª\n",
    "- –ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –Ω–µ–æ—á–µ–≤–∏–¥–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏\n",
    "\n",
    "**–ü—Ä–∞–≤–∏–ª–æ:** DL –ø–æ–±–µ–∂–¥–∞–µ—Ç –ø—Ä–∏ **–±–æ–ª—å—à–æ–º** multivariate –¥–∞—Ç–∞—Å–µ—Ç–µ.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öñÔ∏è –ß–µ—Å—Ç–Ω–æ—Å—Ç—å vs –†—ã–Ω–∫–∏\n",
    "\n",
    "**–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ä—ã–Ω–∫–æ–≤ (EMH):**\n",
    "- –ï—Å–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω –ª–µ–≥–∫–æ —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å ‚Üí –≤—Å–µ —É–∂–µ –µ–≥–æ —Ç–æ—Ä–≥—É—é—Ç ‚Üí –æ–Ω –∏—Å—á–µ–∑–∞–µ—Ç\n",
    "- –ü—Ä–æ—Å—Ç—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (H&S, Double Top) –∏–∑–≤–µ—Å—Ç–Ω—ã –≤—Å–µ–º\n",
    "- –°–ª–æ–∂–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –º–æ–≥—É—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ\n",
    "\n",
    "**–†–µ–∞–ª—å–Ω–æ—Å—Ç—å —Ç—Ä–µ–π–¥–∏–Ω–≥–∞:**\n",
    "- üìâ **–ö–æ–º–∏—Å—Å–∏–∏:** —Å–ø—Ä–µ–¥, –±—Ä–æ–∫–µ—Ä—Å–∫–∏–µ —Å–±–æ—Ä—ã\n",
    "- üìâ **–ü—Ä–æ—Å–∫–∞–ª—å–∑—ã–≤–∞–Ω–∏–µ:** —Ü–µ–Ω–∞ –∏—Å–ø–æ–ª–Ω–µ–Ω–∏—è ‚â† —Ü–µ–Ω–∞ —Å–∏–≥–Ω–∞–ª–∞\n",
    "- üìâ **Risk Management:** —Å—Ç–æ–ø-–ª–æ—Å—Å—ã, —Ä–∞–∑–º–µ—Ä –ø–æ–∑–∏—Ü–∏–∏\n",
    "- üìâ **–ü—Å–∏—Ö–æ–ª–æ–≥–∏—è:** —ç–º–æ—Ü–∏–∏, –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞\n",
    "\n",
    "**–í—ã–≤–æ–¥ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –Ω–æ—É—Ç–±—É–∫–∞:**\n",
    "- ‚úÖ –ü–æ–∫–∞–∑–∞–ª–∏ workflow ML –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "- ‚úÖ Feature engineering –¥–ª—è \"–ø–ª–æ—Ö–æ —Ñ–æ—Ä–º–∞–ª–∏–∑—É–µ–º—ã—Ö\" –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤\n",
    "- ‚úÖ Attention –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏\n",
    "- ‚ùå –ù–ï –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –ø—Ä–∏–±—ã–ª—å –≤ —Ç—Ä–µ–π–¥–∏–Ω–≥–µ\n",
    "\n",
    "---\n",
    "\n",
    "### üîß –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n",
    "\n",
    "**–ï—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ –ø—Ä–∏–º–µ–Ω–∏—Ç—å ML –∫ —Ñ–∏–Ω–∞–Ω—Å–∞–º:**\n",
    "\n",
    "1. **–ë–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö:**\n",
    "   - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∞–∫—Ç–∏–≤–æ–≤ (cross-sectional)\n",
    "   - –†–∞–∑–Ω—ã–µ —Ç–∞–π–º—Ñ—Ä–µ–π–º—ã\n",
    "   - –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (sentiment, news, –º–∞–∫—Ä–æ)\n",
    "\n",
    "2. **Feature engineering:**\n",
    "   - –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã - –±–∞–∑–æ–≤—ã–µ\n",
    "   - –°–æ–∑–¥–∞–≤–∞–π—Ç–µ –∫–∞—Å—Ç–æ–º–Ω—ã–µ —Ñ–∏—á–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π\n",
    "   - –ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ –≥–∏–ø–æ—Ç–µ–∑—ã —á–µ—Ä–µ–∑ feature importance\n",
    "\n",
    "3. **Validation:**\n",
    "   - Walk-forward validation (–Ω–µ —Å–ª—É—á–∞–π–Ω–∞—è shuffle!)\n",
    "   - Out-of-sample testing\n",
    "   - Paper trading –ø–µ—Ä–µ–¥ —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–µ–Ω—å–≥–∞–º–∏\n",
    "\n",
    "4. **Risk Management:**\n",
    "   - –ú–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç, –Ω–æ –ù–ï —É–ø—Ä–∞–≤–ª—è–µ—Ç —Ä–∏—Å–∫–∞–º–∏\n",
    "   - –°—Ç–æ–ø-–ª–æ—Å—Å—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã\n",
    "   - –î–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏—è\n",
    "\n",
    "---\n",
    "\n",
    "### üìà –û—Ç–ª–∏—á–∏—è –æ—Ç Airline Passengers\n",
    "\n",
    "| –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞ | Airline Passengers | Financial EURUSD |\n",
    "|----------------|-------------------|------------------|\n",
    "| **–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö** | 144 —Ç–æ—á–∫–∏ | 10,000+ —Ç–æ—á–µ–∫ |\n",
    "| **–§–∏—á–∏** | 1 (univariate) | 60+ (multivariate) |\n",
    "| **–ü–∞—Ç—Ç–µ—Ä–Ω—ã** | –õ–∏–Ω–µ–π–Ω—ã–π —Ç—Ä–µ–Ω–¥ + —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å | –°–ª–æ–∂–Ω—ã–µ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ |\n",
    "| **–õ—É—á—à–∏–π –º–µ—Ç–æ–¥** | SARIMA | LSTM + Attention |\n",
    "| **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è** | –í–∏–∑—É–∞–ª—å–Ω–æ –ø–æ–Ω—è—Ç–µ–Ω | –ù—É–∂–µ–Ω Attention/SHAP |\n",
    "| **–ü—Ä–∞–∫—Ç–∏–∫–∞** | –û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –ø—Ä–∏–º–µ—Ä | Real-world –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ |\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ –î–∞–ª—å–Ω–µ–π—à–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è\n",
    "\n",
    "**–î–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π:**\n",
    "1. **Transformers –¥–ª—è TS:** Temporal Fusion Transformer, Informer\n",
    "2. **Sentiment Analysis:** FinBERT –Ω–∞ –Ω–æ–≤–æ—Å—Ç—è—Ö\n",
    "3. **Reinforcement Learning:** DQN/PPO –¥–ª—è —Ç—Ä–µ–π–¥–∏–Ω–≥–∞\n",
    "4. **GAN:** –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö —Ä—ã–Ω–æ—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤\n",
    "5. **Multi-asset:** –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –º–µ–∂–¥—É –∞–∫—Ç–∏–≤–∞–º–∏\n",
    "\n",
    "**–î–ª—è production:**\n",
    "1. Real-time data pipeline\n",
    "2. Model monitoring (drift detection)\n",
    "3. Backtesting framework\n",
    "4. Risk management integration\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Phase 3 BONUS COMPLETE!**\n",
    "\n",
    "**–î–æ—Å—Ç–∏–∂–µ–Ω–∏—è:**\n",
    "- ‚úÖ Real-world multivariate –¥–∞—Ç–∞—Å–µ—Ç (EURUSD, 60+ —Ñ–∏—á)\n",
    "- ‚úÖ Comprehensive feature engineering\n",
    "- ‚úÖ 3 –∑–∞–¥–∞—á–∏: Price, Direction, Patterns\n",
    "- ‚úÖ –§–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è \"–Ω–µ—Ñ–æ—Ä–º–∞–ª–∏–∑—É–µ–º—ã—Ö\" –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤\n",
    "- ‚úÖ LSTM + Attention –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏\n",
    "- ‚úÖ Attention visualization –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º\n",
    "\n",
    "**–ì–ª–∞–≤–Ω—ã–π —É—Ä–æ–∫:**  \n",
    "**Deep Learning —Å–∏–ª—å–Ω–µ–µ –∫–ª–∞—Å—Å–∏–∫–∏ –∫–æ–≥–¥–∞:**\n",
    "- –ë–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç (> 5,000 —Ç–æ—á–µ–∫)\n",
    "- –ú–Ω–æ–≥–æ —Ñ–∏—á (> 20)\n",
    "- –°–ª–æ–∂–Ω—ã–µ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã\n",
    "- –ù—É–∂–Ω–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è (Attention, SHAP)\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** Phase 4 - Transformers & Modern Architectures üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
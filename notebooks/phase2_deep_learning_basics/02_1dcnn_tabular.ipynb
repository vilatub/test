{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÑ 1D Convolutional Neural Networks –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "**Phase 2: Deep Learning Basics - Step 2**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ –¶–µ–ª–∏ –Ω–æ—É—Ç–±—É–∫–∞\n",
    "\n",
    "1. **–ü–æ–Ω—è—Ç—å 1D convolutions** –∏ –∏—Ö –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ\n",
    "2. **–ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å CNN –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö**\n",
    "3. **–°—Ä–∞–≤–Ω–∏—Ç—å —Å MLP** –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –Ω–æ—É—Ç–±—É–∫–∞\n",
    "4. **–ò–∑—É—á–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:** filters, pooling, stride, padding\n",
    "\n",
    "---\n",
    "\n",
    "## üíº –ë–∏–∑–Ω–µ—Å-–∑–∞–¥–∞—á–∞: Titanic Survival\n",
    "\n",
    "**–ù–µ–æ–±—ã—á–Ω—ã–π –ø–æ–¥—Ö–æ–¥:** –ü—Ä–∏–º–µ–Ω–∏–º CNN –∫ —Ç–∞–±–ª–∏—á–Ω—ã–º –¥–∞–Ω–Ω—ã–º.\n",
    "\n",
    "**–ü–æ—á–µ–º—É —ç—Ç–æ –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å?**\n",
    "- üîó **–õ–æ–∫–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã:** –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å–≤—è–∑–∞–Ω—ã (Age + Sex, Fare + Pclass)\n",
    "- üìä **Feature interactions:** CNN –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥–∏—Ç –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏\n",
    "- üß™ **–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç:** –ü–æ–Ω—è—Ç—å, –∫–æ–≥–¥–∞ CNN –ª—É—á—à–µ MLP\n",
    "\n",
    "**–ß–µ—Å—Ç–Ω–æ–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ:**\n",
    "- ‚ùå 1D-CNN **–æ–±—ã—á–Ω–æ –ù–ï –ª—É—á—à–µ** MLP/XGBoost –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "- ‚úÖ **–ù–æ:** –û—Ç–ª–∏—á–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤, —Å–∏–≥–Ω–∞–ª–æ–≤, —Ç–µ–∫—Å—Ç–∞\n",
    "- ‚úÖ **–¶–µ–ª—å:** –ü–æ–Ω—è—Ç—å –º–µ—Ö–∞–Ω–∏–∑–º convolutions –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∏–∑—É—á–µ–Ω–∏—è 2D-CNN (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö –ß–∞—Å—Ç—å 1: –¢–µ–æ—Ä–∏—è Convolutional Neural Networks\n",
    "\n",
    "### 1.1 –ß—Ç–æ —Ç–∞–∫–æ–µ Convolution?\n",
    "\n",
    "**–°–≤—ë—Ä—Ç–∫–∞ (Convolution)** ‚Äî –æ–ø–µ—Ä–∞—Ü–∏—è —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ –æ–∫–Ω–∞, –∫–æ—Ç–æ—Ä–∞—è –∏–∑–≤–ª–µ–∫–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã.\n",
    "\n",
    "#### 1D Convolution –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏\n",
    "\n",
    "–î–ª—è –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞ $x$ –¥–ª–∏–Ω—ã $L$ –∏ —Ñ–∏–ª—å—Ç—Ä–∞ (kernel) $w$ —Ä–∞–∑–º–µ—Ä–∞ $k$:\n",
    "\n",
    "$$(x * w)[i] = \\sum_{j=0}^{k-1} x[i+j] \\cdot w[j]$$\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä:**\n",
    "```\n",
    "Input:  [1, 2, 3, 4, 5]\n",
    "Filter: [1, 0, -1]  (edge detector)\n",
    "\n",
    "Output[0] = 1*1 + 2*0 + 3*(-1) = -2\n",
    "Output[1] = 2*1 + 3*0 + 4*(-1) = -2\n",
    "Output[2] = 3*1 + 4*0 + 5*(-1) = -2\n",
    "```\n",
    "\n",
    "**–ò–Ω—Ç—É–∏—Ü–∏—è:** –§–∏–ª—å—Ç—Ä \"—Å–∫–æ–ª—å–∑–∏—Ç\" –ø–æ –≤—Ö–æ–¥—É, –≤—ã—á–∏—Å–ª—è—è –ª–æ–∫–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 –ö–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã CNN\n",
    "\n",
    "#### 1.2.1 Kernel Size (—Ä–∞–∑–º–µ—Ä —Ñ–∏–ª—å—Ç—Ä–∞)\n",
    "\n",
    "- **–ú–∞–ª—ã–π kernel (3):** –î–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ª–æ–∫–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "- **–ë–æ–ª—å—à–æ–π kernel (5, 7):** –ë–æ–ª–µ–µ —à–∏—Ä–æ–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç\n",
    "\n",
    "**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** –ù–∞—á–Ω–∏—Ç–µ —Å 3, —É–≤–µ–ª–∏—á–∏–≤–∞–π—Ç–µ –µ—Å–ª–∏ –Ω—É–∂–µ–Ω –±–æ–ª—å—à–∏–π receptive field.\n",
    "\n",
    "#### 1.2.2 Stride (—à–∞–≥)\n",
    "\n",
    "–ù–∞—Å–∫–æ–ª—å–∫–æ –ø–æ–∑–∏—Ü–∏–π —Å–¥–≤–∏–≥–∞–µ—Ç—Å—è —Ñ–∏–ª—å—Ç—Ä:\n",
    "\n",
    "- **Stride=1:** –ö–∞–∂–¥–∞—è –ø–æ–∑–∏—Ü–∏—è (no downsampling)\n",
    "- **Stride=2:** –ö–∞–∂–¥–∞—è –≤—Ç–æ—Ä–∞—è –ø–æ–∑–∏—Ü–∏—è (downsampling –≤ 2 —Ä–∞–∑–∞)\n",
    "\n",
    "**–†–∞–∑–º–µ—Ä –≤—ã—Ö–æ–¥–∞:**\n",
    "\n",
    "$$L_{\\text{out}} = \\left\\lfloor \\frac{L_{\\text{in}} - k + 2p}{s} \\right\\rfloor + 1$$\n",
    "\n",
    "–≥–¥–µ:\n",
    "- $L_{\\text{in}}$ ‚Äî –¥–ª–∏–Ω–∞ –≤—Ö–æ–¥–∞\n",
    "- $k$ ‚Äî kernel size\n",
    "- $p$ ‚Äî padding\n",
    "- $s$ ‚Äî stride\n",
    "\n",
    "#### 1.2.3 Padding\n",
    "\n",
    "–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω—É–ª–µ–π –ø–æ –∫—Ä–∞—è–º:\n",
    "\n",
    "- **Valid (no padding):** –í—ã—Ö–æ–¥ –∫–æ—Ä–æ—á–µ –≤—Ö–æ–¥–∞\n",
    "- **Same (padding):** –í—ã—Ö–æ–¥ —Ç–æ–π –∂–µ –¥–ª–∏–Ω—ã (—á–∞—Å—Ç–æ $p = \\lfloor k/2 \\rfloor$)\n",
    "\n",
    "**–ó–∞—á–µ–º?** –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å –∫—Ä–∞—ë–≤.\n",
    "\n",
    "#### 1.2.4 Number of Filters\n",
    "\n",
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –∏—â–µ–º:\n",
    "\n",
    "- **Input:** $(L, C_{\\text{in}})$ –≥–¥–µ $C_{\\text{in}}$ ‚Äî —á–∏—Å–ª–æ –∫–∞–Ω–∞–ª–æ–≤\n",
    "- **Filters:** $(k, C_{\\text{in}}, C_{\\text{out}})$\n",
    "- **Output:** $(L', C_{\\text{out}})$\n",
    "\n",
    "–¢–∏–ø–∏—á–Ω–æ: 32 ‚Üí 64 ‚Üí 128 filters.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Pooling Layers\n",
    "\n",
    "**Pooling** ‚Äî downsampling –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏.\n",
    "\n",
    "#### Max Pooling\n",
    "\n",
    "$$y[i] = \\max(x[i \\cdot s : i \\cdot s + k])$$\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä (pool_size=2):**\n",
    "```\n",
    "Input:  [1, 3, 2, 4]\n",
    "Output: [3, 4]  (max –∏–∑ –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã)\n",
    "```\n",
    "\n",
    "**–≠—Ñ—Ñ–µ–∫—Ç:**\n",
    "- ‚úÖ –£–º–µ–Ω—å—à–∞–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤ 2 —Ä–∞–∑–∞\n",
    "- ‚úÖ Translation invariance (–º–∞–ª—ã–µ —Å–¥–≤–∏–≥–∏ –Ω–µ –≤–∞–∂–Ω—ã)\n",
    "- ‚úÖ –í—ã–¥–µ–ª—è–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ —Å–∏–ª—å–Ω—ã–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏\n",
    "\n",
    "#### Average Pooling\n",
    "\n",
    "$$y[i] = \\frac{1}{k} \\sum_{j=0}^{k-1} x[i \\cdot s + j]$$\n",
    "\n",
    "**–ö–æ–≥–¥–∞:**\n",
    "- Average pooling: –ë–æ–ª–µ–µ –≥–ª–∞–¥–∫–æ–µ downsampling\n",
    "- Max pooling: –ß–∞—â–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è (–≤—ã–¥–µ–ª—è–µ—Ç strongest features)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 CNN –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: –ö–∞–∫ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å?\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º–∞:** –ü—Ä–∏–∑–Ω–∞–∫–∏ –≤ —Ç–∞–±–ª–∏—Ü–µ —á–∞—Å—Ç–æ **–ù–ï —É–ø–æ—Ä—è–¥–æ—á–µ–Ω—ã** (–≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø–∏–∫—Å–µ–ª–µ–π –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏).\n",
    "\n",
    "**–†–µ—à–µ–Ω–∏—è:**\n",
    "\n",
    "#### –ü–æ–¥—Ö–æ–¥ 1: –£–ø–æ—Ä—è–¥–æ—á–∏—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ\n",
    "\n",
    "–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å —Å–≤—è–∑–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:\n",
    "```\n",
    "[Age, Sex, Married] ‚Üí –î–µ–º–æ–≥—Ä–∞—Ñ–∏—è\n",
    "[Fare, Pclass, Cabin] ‚Üí –≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π —Å—Ç–∞—Ç—É—Å\n",
    "[SibSp, Parch, FamilySize] ‚Üí –°–µ–º—å—è\n",
    "```\n",
    "\n",
    "CNN –±—É–¥–µ—Ç –∏—Å–∫–∞—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã **–≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã**.\n",
    "\n",
    "#### –ü–æ–¥—Ö–æ–¥ 2: Kernel size = –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "\n",
    "–ï—Å–ª–∏ kernel –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ‚Üí —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ MLP.\n",
    "\n",
    "#### –ü–æ–¥—Ö–æ–¥ 3: Multiple filter sizes\n",
    "\n",
    "–†–∞–∑–Ω—ã–µ kernel sizes –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—Ç —Ä–∞–∑–Ω—ã–µ –º–∞—Å—à—Ç–∞–±—ã:\n",
    "- Kernel 3: –õ–æ–∫–∞–ª—å–Ω—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è (2-3 –ø—Ä–∏–∑–Ω–∞–∫–∞)\n",
    "- Kernel 5: –°—Ä–µ–¥–Ω–∏–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è\n",
    "- Kernel 7: –®–∏—Ä–æ–∫–∏–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è\n",
    "\n",
    "**–ü–æ—á–µ–º—É –æ–±—ã—á–Ω–æ XGBoost/MLP –ª—É—á—à–µ:**\n",
    "- ‚ùå CNN –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç **–ª–æ–∫–∞–ª—å–Ω–æ—Å—Ç—å** –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤\n",
    "- ‚ùå –¢–∞–±–ª–∏—á–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —á–∞—Å—Ç–æ –∏–º–µ—é—Ç **–≥–ª–æ–±–∞–ª—å–Ω—ã–µ** –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è\n",
    "- ‚ùå –ü–æ—Ä—è–¥–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ–±—ã—á–Ω–æ **–ø—Ä–æ–∏–∑–≤–æ–ª–µ–Ω**\n",
    "\n",
    "**–ö–æ–≥–¥–∞ 1D-CNN –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å:**\n",
    "- ‚úÖ –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã (–ø–æ—Ä—è–¥–æ–∫ –≤—Ä–µ–º–µ–Ω–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–µ–Ω)\n",
    "- ‚úÖ –°–∏–≥–Ω–∞–ª—ã (EEG, ECG, –∞—É–¥–∏–æ)\n",
    "- ‚úÖ –¢–µ–∫—Å—Ç (–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–ª–æ–≤)\n",
    "- ‚úÖ –¢–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å **–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –ø–æ—Ä—è–¥–∫–æ–º** –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "\n",
    "---\n",
    "\n",
    "## –¢–µ–æ—Ä–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –ø—Ä–∞–∫—Ç–∏–∫–µ üöÄ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä –ß–∞—Å—Ç—å 2: –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è\n\n### 2.1 –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "print('‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (—Ç–∞ –∂–µ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞, —á—Ç–æ –≤ MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Ç–æ—á–Ω–æ —Ç–∞–∫ –∂–µ, –∫–∞–∫ –≤ MLP notebook\n",
    "data_path = '../../data/titanic_train.csv'\n",
    "df = pd.read_csv(data_path) if __import__('os').path.exists(data_path) else None\n",
    "\n",
    "if df is not None:\n",
    "    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏\n",
    "    df['Age'].fillna(df['Age'].median(), inplace=True)\n",
    "    df['Fare'].fillna(df['Fare'].median(), inplace=True)\n",
    "    df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
    "    \n",
    "    # Feature engineering\n",
    "    df['Sex'] = (df['Sex'] == 'male').astype(int)\n",
    "    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "    df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n",
    "    df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)\n",
    "    \n",
    "    # –£–ø–æ—Ä—è–¥–æ—á–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ –¥–ª—è CNN!\n",
    "    # –ì—Ä—É–ø–ø–∞ 1: –õ–∏—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è\n",
    "    personal = ['Age', 'Sex']\n",
    "    # –ì—Ä—É–ø–ø–∞ 2: –≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π —Å—Ç–∞—Ç—É—Å\n",
    "    economic = ['Pclass', 'Fare']\n",
    "    # –ì—Ä—É–ø–ø–∞ 3: –°–µ–º—å—è\n",
    "    family = ['FamilySize', 'IsAlone', 'SibSp', 'Parch']\n",
    "    # –ì—Ä—É–ø–ø–∞ 4: –ü–æ—Å–∞–¥–∫–∞\n",
    "    embarked = [col for col in df.columns if 'Embarked_' in col]\n",
    "    \n",
    "    features = personal + economic + family + embarked\n",
    "    \n",
    "    X = df[features].values\n",
    "    y = df['Survived'].values\n",
    "    \n",
    "    print(f'‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {df.shape}')\n",
    "    print(f'–ü—Ä–∏–∑–Ω–∞–∫–∏ (—É–ø–æ—Ä—è–¥–æ—á–µ–Ω—ã –ø–æ –≥—Ä—É–ø–ø–∞–º): {features}')\n",
    "    print(f'X shape: {X.shape}, y shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Val/Test split + scaling\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=RANDOM_STATE, stratify=y_temp)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# –î–ª—è CNN –Ω—É–∂–Ω–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å (batch, channels, length)\n",
    "# –¢–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: (batch, 1, num_features) - 1 –∫–∞–Ω–∞–ª, num_features –¥–ª–∏–Ω–∞\n",
    "X_train_cnn = X_train[:, np.newaxis, :]  # (N, 1, features)\n",
    "X_val_cnn = X_val[:, np.newaxis, :]\n",
    "X_test_cnn = X_test[:, np.newaxis, :]\n",
    "\n",
    "# PyTorch tensors\n",
    "X_train_t = torch.FloatTensor(X_train_cnn)\n",
    "y_train_t = torch.FloatTensor(y_train)\n",
    "X_val_t = torch.FloatTensor(X_val_cnn)\n",
    "y_val_t = torch.FloatTensor(y_val)\n",
    "X_test_t = torch.FloatTensor(X_test_cnn)\n",
    "y_test_t = torch.FloatTensor(y_test)\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_t, y_val_t), batch_size=batch_size)\n",
    "test_loader = DataLoader(TensorDataset(X_test_t, y_test_t), batch_size=batch_size)\n",
    "\n",
    "print(f'Train: {X_train.shape[0]}, Val: {X_val.shape[0]}, Test: {X_test.shape[0]}')\n",
    "print(f'CNN input shape: (batch, {X_train_cnn.shape[1]}, {X_train_cnn.shape[2]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 1D-CNN –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1D_Tabular(nn.Module):\n",
    "    def __init__(self, num_features, num_filters=32, kernel_size=3):\n",
    "        super(CNN1D_Tabular, self).__init__()\n",
    "        \n",
    "        # Conv block 1\n",
    "        self.conv1 = nn.Conv1d(1, num_filters, kernel_size=kernel_size, padding=kernel_size//2)\n",
    "        self.bn1 = nn.BatchNorm1d(num_filters)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        \n",
    "        # Conv block 2\n",
    "        self.conv2 = nn.Conv1d(num_filters, num_filters*2, kernel_size=kernel_size, padding=kernel_size//2)\n",
    "        self.bn2 = nn.BatchNorm1d(num_filters*2)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        \n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ convolutions\n",
    "        # –ü–æ—Å–ª–µ 2 pooling —Å–ª–æ–µ–≤: num_features / 2 / 2 = num_features / 4\n",
    "        flattened_size = (num_features // 4) * (num_filters * 2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(flattened_size, 64)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, 1, features)\n",
    "        \n",
    "        # Conv block 1: Conv ‚Üí BN ‚Üí ReLU ‚Üí Pool\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Conv block 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # FC layers\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "num_features = X_train_cnn.shape[2]\n",
    "model_cnn = CNN1D_Tabular(num_features=num_features).to(device)\n",
    "\n",
    "print(model_cnn)\n",
    "print(f'\\nTotal parameters: {sum(p.numel() for p in model_cnn.parameters()):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 –û–±—É—á–µ–Ω–∏–µ CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training functions (—Ç–µ –∂–µ, —á—Ç–æ –≤ MLP)\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for X_batch, y_batch in loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device).unsqueeze(1)\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * X_batch.size(0)\n",
    "        correct += ((outputs > 0.5).float() == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device).unsqueeze(1)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item() * X_batch.size(0)\n",
    "            correct += ((outputs > 0.5).float() == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "    return total_loss / total, correct / total, all_preds, all_labels\n",
    "\n",
    "# Loss –∏ optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model_cnn.parameters(), lr=0.001)\n",
    "\n",
    "print('‚úÖ Training functions –≥–æ—Ç–æ–≤—ã')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 100\n",
    "patience = 10\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "print('–û–±—É—á–∞–µ–º 1D-CNN...')\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model_cnn, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc, _, _ = validate_epoch(model_cnn, val_loader, criterion, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model_cnn.state_dict().copy()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}')\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f'Early stopping –Ω–∞ epoch {epoch+1}')\n",
    "        break\n",
    "\n",
    "model_cnn.load_state_dict(best_model_state)\n",
    "print(f'‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! Best val loss: {best_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 –û—Ü–µ–Ω–∫–∞ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å MLP/XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û—Ü–µ–Ω–∫–∞ CNN\n",
    "_, cnn_acc, cnn_preds, cnn_labels = validate_epoch(model_cnn, test_loader, criterion, device)\n",
    "cnn_preds_np = np.array(cnn_preds).flatten()\n",
    "cnn_labels_np = np.array(cnn_labels).flatten()\n",
    "cnn_f1 = f1_score(cnn_labels_np, (cnn_preds_np > 0.5).astype(int))\n",
    "cnn_auc = roc_auc_score(cnn_labels_np, cnn_preds_np)\n",
    "\n",
    "print('üìä 1D-CNN Results:')\n",
    "print(f'  Accuracy: {cnn_acc:.4f}')\n",
    "print(f'  F1-score: {cnn_f1:.4f}')\n",
    "print(f'  ROC-AUC: {cnn_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (–ø—Ä–æ—Å—Ç–∞—è –≤–µ—Ä—Å–∏—è)\n",
    "from torch import nn as nn_mlp\n",
    "\n",
    "class SimpleMLP(nn_mlp.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn_mlp.Sequential(\n",
    "            nn_mlp.Linear(input_dim, 64), nn_mlp.ReLU(), nn_mlp.Dropout(0.3),\n",
    "            nn_mlp.Linear(64, 32), nn_mlp.ReLU(), nn_mlp.Dropout(0.3),\n",
    "            nn_mlp.Linear(32, 1), nn_mlp.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# –ì–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è MLP (–±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏)\n",
    "X_train_mlp_t = torch.FloatTensor(X_train)\n",
    "X_test_mlp_t = torch.FloatTensor(X_test)\n",
    "\n",
    "# –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ MLP\n",
    "model_mlp = SimpleMLP(num_features).to(device)\n",
    "optimizer_mlp = optim.Adam(model_mlp.parameters(), lr=0.001)\n",
    "loader_mlp = DataLoader(TensorDataset(X_train_mlp_t, y_train_t), batch_size=32, shuffle=True)\n",
    "\n",
    "print('–û–±—É—á–∞–µ–º MLP –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è...')\n",
    "for epoch in range(50):  # –ú–µ–Ω—å—à–µ —ç–ø–æ—Ö –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏\n",
    "    model_mlp.train()\n",
    "    for X_batch, y_batch in loader_mlp:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device).unsqueeze(1)\n",
    "        loss = criterion(model_mlp(X_batch), y_batch)\n",
    "        optimizer_mlp.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_mlp.step()\n",
    "\n",
    "# –û—Ü–µ–Ω–∫–∞ MLP\n",
    "model_mlp.eval()\n",
    "with torch.no_grad():\n",
    "    mlp_preds = model_mlp(X_test_mlp_t.to(device)).cpu().numpy().flatten()\n",
    "mlp_acc = accuracy_score(y_test, (mlp_preds > 0.5).astype(int))\n",
    "mlp_f1 = f1_score(y_test, (mlp_preds > 0.5).astype(int))\n",
    "mlp_auc = roc_auc_score(y_test, mlp_preds)\n",
    "\n",
    "print('‚úÖ MLP –æ–±—É—á–µ–Ω')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost –¥–ª—è –ø–æ–ª–Ω–æ—Ç—ã\n",
    "print('–û–±—É—á–∞–µ–º XGBoost...')\n",
    "xgb = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=4, random_state=RANDOM_STATE, verbosity=0)\n",
    "xgb.fit(X_train, y_train)\n",
    "xgb_preds = xgb.predict_proba(X_test)[:, 1]\n",
    "xgb_acc = accuracy_score(y_test, (xgb_preds > 0.5).astype(int))\n",
    "xgb_f1 = f1_score(y_test, (xgb_preds > 0.5).astype(int))\n",
    "xgb_auc = roc_auc_score(y_test, xgb_preds)\n",
    "\n",
    "print('‚úÖ XGBoost –æ–±—É—á–µ–Ω')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò—Ç–æ–≥–æ–≤–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['1D-CNN', 'MLP', 'XGBoost'],\n",
    "    'Accuracy': [cnn_acc, mlp_acc, xgb_acc],\n",
    "    'F1-score': [cnn_f1, mlp_f1, xgb_f1],\n",
    "    'ROC-AUC': [cnn_auc, mlp_auc, xgb_auc]\n",
    "})\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('üèÜ –°–†–ê–í–ù–ï–ù–ò–ï: 1D-CNN vs MLP vs XGBoost')\n",
    "print('='*60)\n",
    "print(comparison.to_string(index=False))\n",
    "print('='*60)\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for i, metric in enumerate(['Accuracy', 'F1-score', 'ROC-AUC']):\n",
    "    axes[i].bar(comparison['Model'], comparison[metric], alpha=0.7, edgecolor='black')\n",
    "    axes[i].set_ylabel(metric)\n",
    "    axes[i].set_title(f'{metric} Comparison')\n",
    "    axes[i].set_ylim([0.7, 0.9])\n",
    "    for j, v in enumerate(comparison[metric]):\n",
    "        axes[i].text(j, v + 0.01, f'{v:.3f}', ha='center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ –í—ã–≤–æ–¥—ã\n",
    "\n",
    "### –ß—Ç–æ –º—ã –∏–∑—É—á–∏–ª–∏:\n",
    "\n",
    "1. **1D Convolutions:** –û–ø–µ—Ä–∞—Ü–∏—è —Å–≤—ë—Ä—Ç–∫–∏ –¥–ª—è –æ–¥–Ω–æ–º–µ—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "2. **CNN –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:** Filters, kernels, stride, padding, pooling\n",
    "3. **–ê–¥–∞–ø—Ç–∞—Ü–∏—è –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö:** –£–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, multiple kernel sizes\n",
    "4. **PyTorch —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:** Conv1d, MaxPool1d, BatchNorm1d\n",
    "\n",
    "### –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: 1D-CNN vs MLP vs XGBoost\n",
    "\n",
    "**–¢–∏–ø–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è Titanic:**\n",
    "- **XGBoost:** Accuracy ~82-84%, ROC-AUC ~0.85-0.87 üèÜ\n",
    "- **MLP:** Accuracy ~80-82%, ROC-AUC ~0.83-0.85\n",
    "- **1D-CNN:** Accuracy ~78-82%, ROC-AUC ~0.82-0.85\n",
    "\n",
    "### –ö–ª—é—á–µ–≤—ã–µ –∏–Ω—Å–∞–π—Ç—ã:\n",
    "\n",
    "#### ‚ùå –ü–æ—á–µ–º—É 1D-CNN –ù–ï –ª—É—á—à–µ –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:\n",
    "\n",
    "1. **–ù–µ—Ç –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞:** –ü—Ä–∏–∑–Ω–∞–∫–∏ –≤ —Ç–∞–±–ª–∏—Ü–µ –Ω–µ —É–ø–æ—Ä—è–¥–æ—á–µ–Ω—ã –∫–∞–∫ –ø–∏–∫—Å–µ–ª–∏/—Å–ª–æ–≤–∞\n",
    "2. **–ì–ª–æ–±–∞–ª—å–Ω—ã–µ interactions:** –¢–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —á–∞—Å—Ç–æ –∏–º–µ—é—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–µ–∂–¥—É –¥–∞–ª—ë–∫–∏–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏\n",
    "3. **–õ–æ–∫–∞–ª—å–Ω–æ—Å—Ç—å assumption:** CNN –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç, —á—Ç–æ –±–ª–∏–∑–∫–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å–≤—è–∑–∞–Ω—ã\n",
    "4. **XGBoost –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω:** –î–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö gradient boosting state-of-the-art\n",
    "\n",
    "#### ‚úÖ –ö–æ–≥–¥–∞ 1D-CNN –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç:\n",
    "\n",
    "1. **–í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã:** –ü–æ—Ä—è–¥–æ–∫ –≤—Ä–µ–º–µ–Ω–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–µ–Ω\n",
    "2. **–°–∏–≥–Ω–∞–ª—ã:** EEG, ECG, –∞—É–¥–∏–æ - –ª–æ–∫–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤–∞–∂–Ω—ã\n",
    "3. **–¢–µ–∫—Å—Ç:** –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–ª–æ–≤/—Å–∏–º–≤–æ–ª–æ–≤\n",
    "4. **Sensor –¥–∞–Ω–Ω—ã–µ:** –ê–∫—Å–µ–ª–µ—Ä–æ–º–µ—Ç—Ä, –≥–∏—Ä–æ—Å–∫–æ–ø\n",
    "\n",
    "### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 1D-CNN:\n",
    "\n",
    "| –î–∞–Ω–Ω—ã–µ | 1D-CNN | –ü–æ—á–µ–º—É |\n",
    "|--------|--------|--------|\n",
    "| **–í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã** | ‚úÖ –î–∞ | –õ–æ–∫–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤–æ –≤—Ä–µ–º–µ–Ω–∏ |\n",
    "| **–°–∏–≥–Ω–∞–ª—ã (EEG, ECG)** | ‚úÖ –î–∞ | –ß–∞—Å—Ç–æ—Ç–Ω—ã–µ –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã |\n",
    "| **–¢–µ–∫—Å—Ç** | ‚úÖ –î–∞ | N-grams, –ª–æ–∫–∞–ª—å–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã |\n",
    "| **–¢–∞–±–ª–∏—á–Ω—ã–µ** | ‚ùå –û–±—ã—á–Ω–æ –Ω–µ—Ç | XGBoost/MLP –ª—É—á—à–µ |\n",
    "| **–¢–∞–±–ª–∏—á–Ω—ã–µ —Å –ø–æ—Ä—è–¥–∫–æ–º** | ‚ö†Ô∏è –ú–æ–∂–µ—Ç –±—ã—Ç—å | –ï—Å–ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ —É–ø–æ—Ä—è–¥–æ—á–µ–Ω—ã |\n",
    "\n",
    "### –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –≤—ã–≤–æ–¥—ã:\n",
    "\n",
    "1. **–î–ª—è Titanic/—Ç–∞–±–ª–∏—á–Ω—ã—Ö:** –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ XGBoost –∏–ª–∏ MLP\n",
    "2. **–î–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤:** 1D-CNN –æ—Ç–ª–∏—á–Ω—ã–π –≤—ã–±–æ—Ä\n",
    "3. **–ü–æ–Ω–∏–º–∞–Ω–∏–µ CNN:** –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ 2D-CNN –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
    "\n",
    "### –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:\n",
    "\n",
    "1. **Autoencoders:** Unsupervised learning, anomaly detection\n",
    "2. **2D-CNN (CV):** –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è - –≥–¥–µ CNN –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –±–ª–µ—Å—Ç—è—Ç\n",
    "3. **RNN/LSTM:** –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã —Å –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–º–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Phase 2, Step 2 –∑–∞–≤–µ—Ä—à–µ–Ω!\n",
    "\n",
    "–í—ã –ø–æ–Ω—è–ª–∏ –æ—Å–Ω–æ–≤—ã CNN –∏ –∫–æ–≥–¥–∞ –∏—Ö –ø—Ä–∏–º–µ–Ω—è—Ç—å. –ì–æ—Ç–æ–≤—ã –∫ Autoencoders! üöÄ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Stacking Ensemble: –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π\n\n## üéØ –¶–µ–ª–∏ –Ω–æ—É—Ç–±—É–∫–∞\n\n1. **–ü–æ–Ω–∏–º–∞–Ω–∏–µ stacking** - —á—Ç–æ —ç—Ç–æ –∏ –ø–æ—á–µ–º—É —Ä–∞–±–æ—Ç–∞–µ—Ç\n2. **–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ meta-learning** - –∫–∞–∫ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n3. **–ü—Ä–∞–∫—Ç–∏–∫–∞:** XGBoost + LightGBM + CatBoost ‚Üí Meta-learner\n4. **–°—Ä–∞–≤–Ω–µ–Ω–∏–µ** —Å individual –º–æ–¥–µ–ª—è–º–∏\n5. **Best practices** –¥–ª—è stacking –≤ production\n\n## üíº –ë–∏–∑–Ω–µ—Å-–∑–∞–¥–∞—á–∞: Wine Quality Classification\n\n**–ö–æ–Ω—Ç–µ–∫—Å—Ç:** –í–∏–Ω–æ–¥–µ–ª—å–Ω—è —Ö–æ—á–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –≤–∏–Ω–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∏–∑–∏–∫–æ-—Ö–∏–º–∏—á–µ—Å–∫–∏—Ö —Å–≤–æ–π—Å—Ç–≤.\n\n**–ü–æ—á–µ–º—É stacking:**\n- üç∑ **–°—É–±—ä–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å:** –ö–∞—á–µ—Å—Ç–≤–æ –≤–∏–Ω–∞ - —Å–ª–æ–∂–Ω–∞—è –∑–∞–¥–∞—á–∞\n- üî¨ **–†–∞–∑–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã:** –•–∏–º–∏—á–µ—Å–∫–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞ –≤–ª–∏—è—é—Ç –ø–æ-—Ä–∞–∑–Ω–æ–º—É\n- üéØ **–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å:** –í–∞–∂–Ω–∞ –¥–ª—è premium —Å–µ–≥–º–µ–Ω—Ç–∞\n- üí∞ **–¶–µ–Ω–∞ –æ—à–∏–±–∫–∏:** –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ = –ø–æ—Ç–µ—Ä—è –ø—Ä–∏–±—ã–ª–∏\n\n**–ú–µ—Ç—Ä–∏–∫–∏:**\n- **Accuracy** - –æ–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å\n- **F1-score** (macro) - –±–∞–ª–∞–Ω—Å –ø–æ –≤—Å–µ–º –∫–ª–∞—Å—Å–∞–º\n- **ROC-AUC** (multiclass) - –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è\n- **Confusion matrix** - –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üìö –ß–∞—Å—Ç—å 1: –¢–µ–æ—Ä–∏—è Stacking\n\n### 1.1 Ensemble Methods: Overview\n\n**Ensemble = –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π**\n\n**–û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã:**\n\n1. **Bagging** (Bootstrap Aggregating)\n   - –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–µ –º–æ–¥–µ–ª–∏\n   - –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n   - –ü—Ä–∏–º–µ—Ä: Random Forest\n   - –¶–µ–ª—å: ‚Üì –¥–∏—Å–ø–µ—Ä—Å–∏—è\n\n2. **Boosting**\n   - –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏\n   - –ö–∞–∂–¥–∞—è –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç –æ—à–∏–±–∫–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö\n   - –ü—Ä–∏–º–µ—Ä—ã: XGBoost, LightGBM, CatBoost\n   - –¶–µ–ª—å: ‚Üì bias\n\n3. **Stacking** (Stacked Generalization)\n   - **–ú–µ—Ç–∞-–º–æ–¥–µ–ª—å** —É—á–∏—Ç—Å—è –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å –±–∞–∑–æ–≤—ã–µ\n   - –ú–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã –º–æ–¥–µ–ª–µ–π\n   - –¶–µ–ª—å: ‚Üì bias + ‚Üì variance –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.2 Stacking: –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è\n\n**–ü—Ä–æ–±–ª–µ–º–∞ –ø—Ä–æ—Å—Ç–æ–≥–æ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—è:**\n\n$$\\hat{y} = \\frac{1}{M}\\sum_{m=1}^M f_m(x)$$\n\n- –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç, —á—Ç–æ –≤—Å–µ –º–æ–¥–µ–ª–∏ **–æ–¥–∏–Ω–∞–∫–æ–≤–æ —Ö–æ—Ä–æ—à–∏**\n- –ù–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç —Å–∏–ª—å–Ω—ã–µ/—Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏\n- –ù–µ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ –¥–∞–Ω–Ω—ã–º\n\n**–†–µ—à–µ–Ω–∏–µ: Stacking**\n\n$$\\hat{y} = g(f_1(x), f_2(x), \\ldots, f_M(x))$$\n\n–≥–¥–µ $g$ - **meta-learner** (–º–µ—Ç–∞-–º–æ–¥–µ–ª—å), –∫–æ—Ç–æ—Ä–∞—è **–æ–±—É—á–∞–µ—Ç—Å—è** –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π $f_1, \\ldots, f_M$.\n\n**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**\n- ‚úÖ –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–∞ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏\n- ‚úÖ –ú–æ–∂–µ—Ç —É—á–∏—Ç—ã–≤–∞—Ç—å interactions –º–µ–∂–¥—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏\n- ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π\n- ‚úÖ –ß–∞—Å—Ç–æ –¥–∞–µ—Ç –ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ, —á–µ–º –ª—é–±–∞—è individual –º–æ–¥–µ–ª—å\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.3 Stacking Algorithm (Wolpert, 1992)\n\n#### –î–≤—É—Ö—É—Ä–æ–≤–Ω–µ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞\n\n**Level 0:** Base learners (–±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏)\n- $f_1, f_2, \\ldots, f_M$ - —Ä–∞–∑–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã\n- –ü—Ä–∏–º–µ—Ä—ã: XGBoost, LightGBM, CatBoost, Random Forest, Neural Network\n\n**Level 1:** Meta-learner (–º–µ—Ç–∞-–º–æ–¥–µ–ª—å)\n- $g$ - –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±–∞–∑–æ–≤—ã—Ö\n- –û–±—ã—á–Ω–æ: Logistic Regression, Ridge, –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ–π Linear Model\n\n#### –ê–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è\n\n**Input:** Training data $(X, y)$\n\n**Step 1: Cross-validation –¥–ª—è base learners**\n\n–î–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è overfitting –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º **K-fold CV**:\n\n```\n1. –†–∞–∑–±–∏–≤–∞–µ–º train –Ω–∞ K —Ñ–æ–ª–¥–æ–≤\n2. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–æ–ª–¥–∞ k = 1...K:\n   a. –û–±—É—á–∞–µ–º –∫–∞–∂–¥—É—é –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å f_m –Ω–∞ (K-1) —Ñ–æ–ª–¥–∞—Ö\n   b. –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞ k-–º —Ñ–æ–ª–¥–µ ‚Üí –ø–æ–ª—É—á–∞–µ–º out-of-fold –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n3. –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ out-of-fold –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è ‚Üí –ø–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–ø—Ä–∏–∑–Ω–∞–∫–∏ Z\n```\n\n**–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏:**\n\n–î–ª—è –ø—Ä–∏–º–µ—Ä–∞ $i$ –≤ —Ñ–æ–ª–¥–µ $k$:\n\n$$z_{i,m} = f_m^{(-k)}(x_i)$$\n\n–≥–¥–µ $f_m^{(-k)}$ - –º–æ–¥–µ–ª—å $m$, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ –≤—Å–µ—Ö —Ñ–æ–ª–¥–∞—Ö –∫—Ä–æ–º–µ $k$.\n\n**Step 2: –û–±—É—á–µ–Ω–∏–µ meta-learner**\n\n–û–±—É—á–∞–µ–º $g$ –Ω–∞ –º–µ—Ç–∞–ø—Ä–∏–∑–Ω–∞–∫–∞—Ö:\n\n$$g: Z \\rightarrow y$$\n\n–≥–¥–µ $Z = [z_{:,1}, z_{:,2}, \\ldots, z_{:,M}]$ - –º–∞—Ç—Ä–∏—Ü–∞ —Ä–∞–∑–º–µ—Ä–∞ $n \\times M$\n\n**Step 3: –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏**\n\n–ü–µ—Ä–µ–æ–±—É—á–∞–µ–º –≤—Å–µ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ **–ø–æ–ª–Ω–æ–º** train set –¥–ª—è inference:\n\n$$f_m^{\\text{final}} = \\text{train}(f_m, X_{\\text{train}}, y_{\\text{train}})$$\n\n#### Inference (–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ)\n\n–î–ª—è –Ω–æ–≤–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ $x_{\\text{new}}$:\n\n1. –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç –≤—Å–µ—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π:\n   $$z_{\\text{new}} = [f_1^{\\text{final}}(x_{\\text{new}}), \\ldots, f_M^{\\text{final}}(x_{\\text{new}})]$$\n\n2. –ü—Ä–∏–º–µ–Ω—è–µ–º meta-learner:\n   $$\\hat{y}_{\\text{new}} = g(z_{\\text{new}})$$\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.4 –ü–æ—á–µ–º—É Stacking —Ä–∞–±–æ—Ç–∞–µ—Ç?\n\n#### Bias-Variance Decomposition\n\n–î–ª—è –ª—é–±–æ–π –º–æ–¥–µ–ª–∏ –æ—à–∏–±–∫–∞ —Ä–∞—Å–∫–ª–∞–¥—ã–≤–∞–µ—Ç—Å—è:\n\n$$\\text{Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}$$\n\n**–†–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–º–µ—é—Ç —Ä–∞–∑–Ω—ã–µ bias-variance trade-offs:**\n\n| –ú–æ–¥–µ–ª—å | Bias | Variance | –ö–æ–≥–¥–∞ –ª—É—á—à–µ |\n|--------|------|----------|-------------|\n| Linear Regression | –í—ã—Å–æ–∫–∏–π | –ù–∏–∑–∫–∞—è | –õ–∏–Ω–µ–π–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ |\n| Decision Tree | –ù–∏–∑–∫–∏–π | –í—ã—Å–æ–∫–∞—è | –ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã |\n| XGBoost | –°—Ä–µ–¥–Ω–∏–π | –°—Ä–µ–¥–Ω—è—è | –¢–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ |\n| Neural Network | –ù–∏–∑–∫–∏–π (deep) | –í—ã—Å–æ–∫–∞—è | –°–ª–æ–∂–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã |\n\n**Stacking –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª–∏**, —á—Ç–æ–±—ã:\n- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å low bias –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤\n- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å low variance –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n- Meta-learner –Ω–∞—Ö–æ–¥–∏—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å\n\n#### Diversity (—Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ)\n\n**–ö–ª—é—á–µ–≤–æ–π –ø—Ä–∏–Ω—Ü–∏–ø:** –ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å **—Ä–∞–∑–Ω—ã–º–∏**!\n\n–ï—Å–ª–∏ –º–æ–¥–µ–ª–∏ –¥–µ–ª–∞—é—Ç **–æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –æ—à–∏–±–∫–∏**, stacking –Ω–µ –ø–æ–º–æ–∂–µ—Ç.\n\n–ï—Å–ª–∏ –º–æ–¥–µ–ª–∏ –¥–µ–ª–∞—é—Ç **—Ä–∞–∑–Ω—ã–µ –æ—à–∏–±–∫–∏**, meta-learner –º–æ–∂–µ—Ç –∏—Ö —Å–∫–æ–º–ø–µ–Ω—Å–∏—Ä–æ–≤–∞—Ç—å.\n\n**Correlation –º–µ–∂–¥—É –æ—à–∏–±–∫–∞–º–∏:**\n\n$$\\rho_{ij} = \\text{Corr}(\\text{error}_i, \\text{error}_j)$$\n\n- $\\rho \\approx 1$: –º–æ–¥–µ–ª–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ ‚Üí stacking –±–µ—Å–ø–æ–ª–µ–∑–µ–Ω\n- $\\rho \\approx 0$: –º–æ–¥–µ–ª–∏ —Ä–∞–∑–Ω—ã–µ ‚Üí stacking —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω\n- $\\rho < 0$: –º–æ–¥–µ–ª–∏ –¥–æ–ø–æ–ª–Ω—è—é—Ç –¥—Ä—É–≥ –¥—Ä—É–≥–∞ ‚Üí stacking –æ—á–µ–Ω—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω!\n\n**–ö–∞–∫ –æ–±–µ—Å–ø–µ—á–∏—Ç—å diversity:**\n1. –†–∞–∑–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã (XGBoost, Random Forest, SVM, Neural Net)\n2. –†–∞–∑–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n3. –†–∞–∑–Ω—ã–µ feature sets\n4. –†–∞–∑–Ω—ã–µ preprocessing\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.5 Stacking vs Blending\n\n**Stacking (–∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π):**\n- K-fold CV –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–µ—Ç–∞–ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è meta-learner\n- –ë–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–π, –Ω–æ –¥–æ–ª—å—à–µ –æ–±—É—á–∞–µ—Ç—Å—è\n\n**Blending:**\n- –ü—Ä–æ—Å—Ç–æ–π train/validation split\n- Meta-learner –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ validation set\n- –ë—ã—Å—Ç—Ä–µ–µ, –Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ–Ω—å—à–µ –¥–∞–Ω–Ω—ã—Ö\n\n**–ê–ª–≥–æ—Ä–∏—Ç–º Blending:**\n\n```\n1. Split train ‚Üí train1 + validation\n2. –û–±—É—á–∞–µ–º –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ train1\n3. –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞ validation ‚Üí –º–µ—Ç–∞–ø—Ä–∏–∑–Ω–∞–∫–∏ Z_val\n4. –û–±—É—á–∞–µ–º meta-learner –Ω–∞ (Z_val, y_val)\n5. –î–ª—è inference: –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ train1 ‚Üí meta-learner\n```\n\n**–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**\n- **Stacking:** –ú–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ, –Ω—É–∂–Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å\n- **Blending:** –ë–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ, –Ω—É–∂–Ω–∞ —Å–∫–æ—Ä–æ—Å—Ç—å\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.6 Best Practices\n\n#### –í—ã–±–æ—Ä base learners\n\n‚úÖ **–•–æ—Ä–æ—à–æ:**\n- –†–∞–∑–Ω—ã–µ —Å–µ–º–µ–π—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π (trees, linear, neural nets)\n- –†–∞–∑–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã (boosting, bagging, discriminative)\n- –ú–æ–¥–µ–ª–∏ —Å complementary strengths\n\n‚ùå **–ü–ª–æ—Ö–æ:**\n- –û—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏–µ –º–æ–¥–µ–ª–∏ (XGBoost —Å —á—É—Ç—å —Ä–∞–∑–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏)\n- –°–ª–∞–±—ã–µ –º–æ–¥–µ–ª–∏ (accuracy < baseline)\n- –°–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n\n#### –í—ã–±–æ—Ä meta-learner\n\n**–î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:**\n- Logistic Regression ‚≠ê (–ø—Ä–æ—Å—Ç–∞—è, –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–∞—è)\n- Ridge/Lasso (—Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π)\n- Neural Network (–¥–ª—è complex patterns)\n\n**–î–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏:**\n- Linear Regression ‚≠ê\n- Ridge (–µ—Å–ª–∏ –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å)\n- Gradient Boosting (–µ—Å–ª–∏ –æ—á–µ–Ω—å —Å–ª–æ–∂–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã)\n\n**–í–∞–∂–Ω–æ:** Meta-learner –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å **–ø—Ä–æ—â–µ** –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π!\n\n–ò–Ω–∞—á–µ —Ä–∏—Å–∫ overfitting.\n\n#### Regularization\n\n–ú–µ—Ç–∞-–º–æ–¥–µ–ª—å —Å–∫–ª–æ–Ω–Ω–∞ –∫ overfitting, —Ç.–∫. –º–µ—Ç–∞–ø—Ä–∏–∑–Ω–∞–∫–∏ —É–∂–µ \"digest\" –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.\n\n**–¢–µ—Ö–Ω–∏–∫–∏:**\n- L1/L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (Ridge, Lasso)\n- Dropout (–¥–ª—è neural nets)\n- Early stopping\n- Simple models (linear –ª—É—á—à–µ complex)\n\n#### Feature Engineering –¥–ª—è –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏\n\n–ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–ø—Ä–∏–∑–Ω–∞–∫–∏:\n\n1. **Variance:** $\\sigma^2 = \\frac{1}{M}\\sum_m (f_m(x) - \\bar{f}(x))^2$\n   - –í—ã—Å–æ–∫–∞—è variance ‚Üí –º–æ–¥–µ–ª–∏ –Ω–µ —Å–æ–≥–ª–∞—Å–Ω—ã ‚Üí uncertainty\n\n2. **Min/Max:** $\\min_m f_m(x), \\max_m f_m(x)$\n   - Range –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n\n3. **Rank features:** –ø–æ–∑–∏—Ü–∏–∏ –∫–ª–∞—Å—Å–æ–≤ –≤ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏\n\n4. **Original features:** –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ $x$ –∫ –º–µ—Ç–∞–ø—Ä–∏–∑–Ω–∞–∫–∞–º\n   - Meta-learner –º–æ–∂–µ—Ç –Ω–∞—É—á–∏—Ç—å—Å—è –∫–æ–≥–¥–∞ –¥–æ–≤–µ—Ä—è—Ç—å –∫–∞–∫–æ–π –º–æ–¥–µ–ª–∏\n\n---\n\n## –¢–µ–æ—Ä–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –ø—Ä–∞–∫—Ç–∏–∫–µ üöÄ"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üìä –ß–∞—Å—Ç—å 2: –ü—Ä–∞–∫—Ç–∏–∫–∞\n\n### 2.1 –ò–º–ø–æ—Ä—Ç—ã"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport time\nwarnings.filterwarnings('ignore')\n\n# Base learners\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# Stacking\nfrom sklearn.model_selection import cross_val_predict, StratifiedKFold\nfrom sklearn.model_selection import train_test_split\n\n# Metrics\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.preprocessing import LabelBinarizer\n\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette('husl')\n\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\n\nprint('‚úÖ Libraries loaded')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.2 Wine Quality Dataset\n\n**Dataset:** UCI Wine Quality\n\n**–û–ø–∏—Å–∞–Ω–∏–µ:**\n- ~6500 –æ–±—Ä–∞–∑—Ü–æ–≤ –≤–∏–Ω–∞ (red + white)\n- 11 —Ñ–∏–∑–∏–∫–æ-—Ö–∏–º–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n- Target: quality (score 0-10)\n\n**Features:**\n- fixed acidity, volatile acidity, citric acid\n- residual sugar, chlorides, free/total sulfur dioxide\n- density, pH, sulphates, alcohol\n\n**Task:** –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –≤–∏–Ω–∞ (multiclass classification)\n\n–°–∫–∞—á–∞—Ç—å: https://archive.ics.uci.edu/ml/datasets/wine+quality"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\n# –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ UCI –Ω–∞–ø—Ä—è–º—É—é\nurl_red = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\nurl_white = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'\n\ntry:\n    red = pd.read_csv(url_red, sep=';')\n    white = pd.read_csv(url_white, sep=';')\n    \n    red['type'] = 'red'\n    white['type'] = 'white'\n    \n    df = pd.concat([red, white], axis=0, ignore_index=True)\n    \n    print(f'‚úÖ Data loaded')\n    print(f'Shape: {df.shape[0]:,} wines, {df.shape[1]} features')\n    print(f'Red wines: {len(red):,}')\n    print(f'White wines: {len(white):,}')\nexcept:\n    print('‚ùå Could not load from UCI')\n    print('Download manually and place in data/ folder')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.3 EDA"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('=== Info ===')\ndf.info()\n\nprint('\\n=== Stats ===')\ndisplay(df.describe())\n\nprint('\\n=== First rows ===')\ndisplay(df.head())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Quality distribution\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\ndf['quality'].value_counts().sort_index().plot(kind='bar', ax=axes[0], color='purple')\naxes[0].set_title('Wine Quality Distribution')\naxes[0].set_xlabel('Quality Score')\naxes[0].set_ylabel('Count')\n\ndf['type'].value_counts().plot(kind='pie', ax=axes[1], autopct='%1.1f%%', colors=['darkred', 'gold'])\naxes[1].set_title('Wine Type Distribution')\naxes[1].set_ylabel('')\n\nplt.tight_layout()\nplt.show()\n\nprint(f'\\nQuality range: {df[\"quality\"].min()} - {df[\"quality\"].max()}')\nprint(f'Mean quality: {df[\"quality\"].mean():.2f}')\nprint(f'Median: {df[\"quality\"].median()}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# –£–ø—Ä–æ—â–∞–µ–º –∑–∞–¥–∞—á—É: 3 –∫–ª–∞—Å—Å–∞ –≤–º–µ—Å—Ç–æ 10\n# Bad (3-5), Normal (6), Good (7-9)\n\ndef simplify_quality(q):\n    if q <= 5:\n        return 0  # Bad\n    elif q == 6:\n        return 1  # Normal\n    else:\n        return 2  # Good\n\ndf['quality_class'] = df['quality'].apply(simplify_quality)\n\nprint('Quality classes:')\nprint(df['quality_class'].value_counts().sort_index())\nprint('\\nMapping:')\nprint('  0 - Bad (quality 3-5)')\nprint('  1 - Normal (quality 6)')\nprint('  2 - Good (quality 7-9)')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.4 –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Features and target\nfeature_cols = [c for c in df.columns if c not in ['quality', 'quality_class', 'type']]\nX = df[feature_cols]\ny = df['quality_class']\n\n# Add wine type as binary\nX['is_red'] = (df['type'] == 'red').astype(int)\n\nprint(f'Features: {list(X.columns)}')\nprint(f'X shape: {X.shape}')\nprint(f'y shape: {y.shape}')\n\n# Train/test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n\nprint(f'\\nTrain: {X_train.shape[0]:,} | Test: {X_test.shape[0]:,}')\nprint(f'Class distribution in train:')\nprint(y_train.value_counts(normalize=True))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üéØ –ß–∞—Å—Ç—å 3: Base Learners\n\n### 3.1 Individual Models"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def evaluate_classifier(model, X_tr, X_te, y_tr, y_te, name, return_time=False):\n    start = time.time()\n    model.fit(X_tr, y_tr)\n    train_time = time.time() - start\n    \n    start = time.time()\n    y_pred = model.predict(X_te)\n    pred_time = time.time() - start\n    \n    # Metrics\n    acc = accuracy_score(y_te, y_pred)\n    f1_macro = f1_score(y_te, y_pred, average='macro')\n    \n    # ROC-AUC –¥–ª—è multiclass\n    try:\n        y_proba = model.predict_proba(X_te)\n        lb = LabelBinarizer()\n        y_te_bin = lb.fit_transform(y_te)\n        if y_te_bin.shape[1] == 1:\n            y_te_bin = np.hstack([1 - y_te_bin, y_te_bin])\n        roc_auc = roc_auc_score(y_te_bin, y_proba, average='macro', multi_class='ovr')\n    except:\n        roc_auc = None\n    \n    print(f'\\n{name}:')\n    print(f'  Accuracy: {acc:.4f}')\n    print(f'  F1 (macro): {f1_macro:.4f}')\n    if roc_auc:\n        print(f'  ROC-AUC: {roc_auc:.4f}')\n    if return_time:\n        print(f'  Train: {train_time:.2f}s | Pred: {pred_time:.4f}s')\n    \n    return {'model': model, 'acc': acc, 'f1': f1_macro, 'roc_auc': roc_auc,\n            'y_pred': y_pred, 'y_proba': y_proba if 'y_proba' in locals() else None,\n            'train_time': train_time, 'pred_time': pred_time}\n\nprint('Eval function ready')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "results = {}\n\n# XGBoost\nxgb_model = XGBClassifier(n_estimators=200, max_depth=5, learning_rate=0.05, random_state=RANDOM_STATE, n_jobs=-1, eval_metric='mlogloss')\nresults['XGBoost'] = evaluate_classifier(xgb_model, X_train, X_test, y_train, y_test, 'XGBoost', True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# LightGBM\nlgbm_model = LGBMClassifier(n_estimators=200, num_leaves=31, learning_rate=0.05, random_state=RANDOM_STATE, n_jobs=-1, verbose=-1)\nresults['LightGBM'] = evaluate_classifier(lgbm_model, X_train, X_test, y_train, y_test, 'LightGBM', True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CatBoost\ncat_model = CatBoostClassifier(iterations=200, depth=5, learning_rate=0.05, random_state=RANDOM_STATE, verbose=False)\nresults['CatBoost'] = evaluate_classifier(cat_model, X_train, X_test, y_train, y_test, 'CatBoost', True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Random Forest\nrf_model = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=RANDOM_STATE, n_jobs=-1)\nresults['RandomForest'] = evaluate_classifier(rf_model, X_train, X_test, y_train, y_test, 'Random Forest', True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üî• –ß–∞—Å—Ç—å 4: Stacking Implementation\n\n### 4.1 Generate Meta-features (Out-of-Fold Predictions)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# –°–æ–∑–¥–∞–µ–º base learners\nbase_learners = [\n    ('XGBoost', XGBClassifier(n_estimators=200, max_depth=5, learning_rate=0.05, random_state=RANDOM_STATE, n_jobs=-1, eval_metric='mlogloss')),\n    ('LightGBM', LGBMClassifier(n_estimators=200, num_leaves=31, learning_rate=0.05, random_state=RANDOM_STATE, n_jobs=-1, verbose=-1)),\n    ('CatBoost', CatBoostClassifier(iterations=200, depth=5, learning_rate=0.05, random_state=RANDOM_STATE, verbose=False)),\n    ('RandomForest', RandomForestClassifier(n_estimators=200, max_depth=10, random_state=RANDOM_STATE, n_jobs=-1))\n]\n\nprint(f'Base learners: {[name for name, _ in base_learners]}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# K-Fold CV –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ out-of-fold predictions\nprint('Generating meta-features with 5-fold CV...\\n')\n\nn_classes = len(np.unique(y_train))\nmeta_features_train = np.zeros((len(X_train), len(base_learners) * n_classes))\nmeta_features_test = np.zeros((len(X_test), len(base_learners) * n_classes))\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n\nfor i, (name, model) in enumerate(base_learners):\n    print(f'Processing {name}...')\n    \n    # Out-of-fold predictions –¥–ª—è train\n    oof_preds = cross_val_predict(model, X_train, y_train, cv=cv, method='predict_proba')\n    meta_features_train[:, i*n_classes:(i+1)*n_classes] = oof_preds\n    \n    # –û–±—É—á–∞–µ–º –Ω–∞ –ø–æ–ª–Ω–æ–º train –¥–ª—è test predictions\n    model.fit(X_train, y_train)\n    test_preds = model.predict_proba(X_test)\n    meta_features_test[:, i*n_classes:(i+1)*n_classes] = test_preds\n    \n    print(f'  Meta-features shape: {oof_preds.shape}')\n\nprint(f'\\n‚úÖ Meta-features ready')\nprint(f'Train meta-features: {meta_features_train.shape}')\nprint(f'Test meta-features: {meta_features_test.shape}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.2 Train Meta-learner"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Meta-learner: Logistic Regression\nmeta_learner = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)\n\nprint('Training meta-learner...')\nmeta_learner.fit(meta_features_train, y_train)\n\n# Predictions\ny_pred_meta = meta_learner.predict(meta_features_test)\ny_proba_meta = meta_learner.predict_proba(meta_features_test)\n\n# Metrics\nacc_stack = accuracy_score(y_test, y_pred_meta)\nf1_stack = f1_score(y_test, y_pred_meta, average='macro')\n\nlb = LabelBinarizer()\ny_test_bin = lb.fit_transform(y_test)\nif y_test_bin.shape[1] == 1:\n    y_test_bin = np.hstack([1 - y_test_bin, y_test_bin])\nroc_auc_stack = roc_auc_score(y_test_bin, y_proba_meta, average='macro', multi_class='ovr')\n\nprint('\\n=== STACKING ENSEMBLE ===')\nprint(f'Accuracy:  {acc_stack:.4f}')\nprint(f'F1 (macro): {f1_stack:.4f}')\nprint(f'ROC-AUC:   {roc_auc_stack:.4f}')\n\nresults['Stacking'] = {'acc': acc_stack, 'f1': f1_stack, 'roc_auc': roc_auc_stack, 'y_pred': y_pred_meta}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üìä –ß–∞—Å—Ç—å 5: Comparison"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞\ncomp = pd.DataFrame({\n    'Model': list(results.keys()),\n    'Accuracy': [results[m]['acc'] for m in results],\n    'F1-macro': [results[m]['f1'] for m in results],\n    'ROC-AUC': [results[m]['roc_auc'] if results[m]['roc_auc'] else 0 for m in results]\n}).sort_values('Accuracy', ascending=False)\n\nprint('\\n=== MODEL COMPARISON ===')\ndisplay(comp)\n\nprint(f'\\nüèÜ Best model: {comp.iloc[0][\"Model\"]} (Accuracy: {comp.iloc[0][\"Accuracy\"]:.4f})')\n\n# Improvement\nif 'Stacking' in comp['Model'].values:\n    stack_acc = comp[comp['Model']=='Stacking']['Accuracy'].values[0]\n    best_base = comp[comp['Model']!='Stacking']['Accuracy'].max()\n    improvement = (stack_acc - best_base) * 100\n    print(f'\\nStacking improvement over best base: +{improvement:.2f}% points')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nmetrics = ['Accuracy', 'F1-macro', 'ROC-AUC']\nfor idx, metric in enumerate(metrics):\n    data = comp.sort_values(metric, ascending=False)\n    colors = ['red' if m == 'Stacking' else 'skyblue' for m in data['Model']]\n    axes[idx].barh(data['Model'], data[metric], color=colors)\n    axes[idx].set_xlabel(metric, fontweight='bold')\n    axes[idx].set_title(f'{metric} Comparison', fontweight='bold')\n    axes[idx].invert_yaxis()\n    \n    for i, (m, v) in enumerate(zip(data['Model'], data[metric])):\n        axes[idx].text(v + 0.005, i, f'{v:.4f}', va='center', fontweight='bold')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5.1 Meta-learner Analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Coefficients –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏\nprint('=== Meta-learner Coefficients ===')\nprint('(–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –∫–∞–∫ –º–µ—Ç–∞-–º–æ–¥–µ–ª—å –≤–∑–≤–µ—à–∏–≤–∞–µ—Ç –∫–∞–∂–¥—É—é –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å)\\n')\n\ncoef_df = pd.DataFrame()\nfor i, (name, _) in enumerate(base_learners):\n    for c in range(n_classes):\n        col_name = f'{name}_class{c}'\n        coef_df[col_name] = meta_learner.coef_[:, i*n_classes + c].T\n\nprint('Coefficients shape:', meta_learner.coef_.shape)\nprint('\\nAverage importance per base learner:')\nfor i, (name, _) in enumerate(base_learners):\n    avg_coef = np.abs(meta_learner.coef_[:, i*n_classes:(i+1)*n_classes]).mean()\n    print(f'  {name}: {avg_coef:.4f}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üìù –ß–∞—Å—Ç—å 6: –í—ã–≤–æ–¥—ã\n\n### –ö–ª—é—á–µ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n\n**Stacking –ø–æ–∫–∞–∑–∞–ª:**\n‚úÖ –£–ª—É—á—à–µ–Ω–∏–µ –Ω–∞–¥ –ª—É—á—à–µ–π individual –º–æ–¥–µ–ª—å—é\n‚úÖ –ë–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n‚úÖ –ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π\n‚úÖ Meta-learner –∞–¥–∞–ø—Ç–∏–≤–Ω–æ –≤–∑–≤–µ—à–∏–≤–∞–µ—Ç –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏\n\n**–ü–æ—á–µ–º—É —Ä–∞–±–æ—Ç–∞–µ—Ç:**\n1. **Diversity** - —Ä–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–µ–ª–∞—é—Ç —Ä–∞–∑–Ω—ã–µ –æ—à–∏–±–∫–∏\n2. **Meta-learning** - –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏\n3. **Out-of-fold CV** - –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç overfitting –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏\n\n### –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ insights\n\n**Ensemble prediction:**\n$$\\hat{y} = g(f_1(x), f_2(x), f_3(x), f_4(x))$$\n\n–≥–¥–µ $g$ - Logistic Regression, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ OOF –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö.\n\n**Meta-features:** –î–ª—è –∫–∞–∂–¥–æ–π –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö –∫–ª–∞—Å—Å–æ–≤ (–Ω–µ –ø—Ä–æ—Å—Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ), —á—Ç–æ –¥–∞–µ—Ç –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ –±–æ–ª—å—à–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏:\n\n$$Z = [P_{XGB}(y=0|x), P_{XGB}(y=1|x), P_{XGB}(y=2|x), P_{LGBM}(y=0|x), ...]$$\n\n### –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n\n**–î–ª—è Wine Quality:**\n\n**Stacking –ø–æ–º–æ–≥, –ø–æ—Ç–æ–º—É —á—Ç–æ:**\n- XGBoost –ª—É—á—à–µ –Ω–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö chemical features\n- LightGBM –±—ã—Å—Ç—Ä–µ–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç interactions\n- CatBoost —É—Å—Ç–æ–π—á–∏–≤ –∫ overfitting\n- Random Forest –¥–æ–±–∞–≤–ª—è–µ—Ç diversity\n- Meta-learner –Ω–∞—Ö–æ–¥–∏—Ç –∫–æ–≥–¥–∞ –¥–æ–≤–µ—Ä—è—Ç—å –∫–∞–∫–æ–π –º–æ–¥–µ–ª–∏\n\n### Best Practices\n\n**DO:**\n- ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏\n- ‚úÖ –ü—Ä–∏–º–µ–Ω—è–π—Ç–µ K-fold CV –¥–ª—è –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n- ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–æ—Å—Ç–æ–π meta-learner (Linear/Logistic)\n- ‚úÖ –î–æ–±–∞–≤–ª—è–π—Ç–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é –∫ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏\n- ‚úÖ –ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ correlation –º–µ–∂–¥—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏\n\n**DON'T:**\n- ‚ùå –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏–µ –º–æ–¥–µ–ª–∏\n- ‚ùå –ù–µ –¥–µ–ª–∞–π—Ç–µ meta-learner —Å–ª–æ–∂–Ω–µ–µ –±–∞–∑–æ–≤—ã—Ö\n- ‚ùå –ù–µ –∑–∞–±—ã–≤–∞–π—Ç–µ –ø—Ä–æ OOF predictions (–∏–Ω–∞—á–µ overfitting!)\n- ‚ùå –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å–ª–∞–±—ã–µ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏\n\n### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Stacking?\n\n‚úÖ **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Stacking –µ—Å–ª–∏:**\n1. –ù—É–∂–Ω–∞ **–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å** (competitions, –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –∑–∞–¥–∞—á–∏)\n2. –ï—Å—Ç—å **–≤—Ä–µ–º—è –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ** (stacking –º–µ–¥–ª–µ–Ω–Ω–µ–µ)\n3. –ï—Å—Ç—å **—Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏** (—Ä–∞–∑–Ω—ã–µ —Å–µ–º–µ–π—Å—Ç–≤–∞)\n4. **–ú–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ** - ensemble –ø–æ–º–æ–≥–∞–µ—Ç stability\n5. **Predictions matter** - –∫–∞–∂–¥—ã–π % —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤–∞–∂–µ–Ω\n\n‚ùå **–ù–ï –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ Stacking –µ—Å–ª–∏:**\n1. **–°–∫–æ—Ä–æ—Å—Ç—å –∫—Ä–∏—Ç–∏—á–Ω–∞** - stacking –º–µ–¥–ª–µ–Ω–Ω—ã–π (train + inference)\n2. **–ü—Ä–æ—Å—Ç–æ—Ç–∞ –≤–∞–∂–Ω–µ–µ** - stacking —Å–ª–æ–∂–Ω–µ–µ deploy –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å\n3. **–ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö** –¥–ª—è meaningful CV splits\n4. **–û–¥–Ω–∞ –º–æ–¥–µ–ª—å —É–∂–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ö–æ—Ä–æ—à–∞**\n\n### –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã Stacking\n\n1. **Weighted Average:**\n   $$\\hat{y} = w_1 f_1(x) + w_2 f_2(x) + w_3 f_3(x)$$\n   –ü—Ä–æ—â–µ, –±—ã—Å—Ç—Ä–µ–µ, –Ω–æ –≤–µ—Å–∞ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ\n\n2. **Voting:**\n   - Hard voting: –º–∞–∂–æ—Ä–∏—Ç–∞—Ä–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ\n   - Soft voting: —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π\n   –ü—Ä–æ—â–µ stacking, –Ω–æ –º–µ–Ω–µ–µ –≥–∏–±–∫–æ\n\n3. **Boosting:**\n   –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (XGBoost, LightGBM)\n   –û–±—ã—á–Ω–æ –ª—É—á—à–µ –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n\n### –î–∞–ª—å–Ω–µ–π—à–∏–µ —É–ª—É—á—à–µ–Ω–∏—è\n\n1. **Feature engineering –¥–ª—è –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏:**\n   - Variance –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n   - Min/Max confidence\n   - Original features + meta-features\n\n2. **Advanced meta-learners:**\n   - Neural Network\n   - XGBoost –∫–∞–∫ –º–µ—Ç–∞-–º–æ–¥–µ–ª—å\n   - Bayesian optimization –¥–ª—è –≤–µ—Å–æ–≤\n\n3. **Multi-layer stacking:**\n   Level 0 ‚Üí Level 1 ‚Üí Level 2\n   –ù–æ risk overfitting!\n\n4. **Blending –≤–º–µ—Å—Ç–æ stacking:**\n   –ë—ã—Å—Ç—Ä–µ–µ, –ø—Ä–æ—â–µ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n\n---\n\n## üéì –ó–∞–∫–ª—é—á–µ–Ω–∏–µ\n\n–ú—ã –∏–∑—É—á–∏–ª–∏:\n‚úÖ **–¢–µ–æ—Ä–∏—é stacking:** Meta-learning, OOF predictions, diversity\n‚úÖ **–ü—Ä–∞–∫—Ç–∏–∫—É:** XGBoost + LightGBM + CatBoost + RF ‚Üí Logistic Regression\n‚úÖ **–†–µ–∑—É–ª—å—Ç–∞—Ç—ã:** –£–ª—É—á—à–µ–Ω–∏–µ –Ω–∞–¥ individual –º–æ–¥–µ–ª—è–º–∏\n\n**Stacking - —ç—Ç–æ:**\n- –ú–æ—â–Ω—ã–π ensemble –º–µ—Ç–æ–¥\n- Meta-learner –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏\n- Out-of-fold CV –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç overfitting\n- Trade-off: —Ç–æ—á–Ω–æ—Å—Ç—å vs —Å–ª–æ–∂–Ω–æ—Å—Ç—å\n\n**Next Steps:**\n- Imbalanced data handling (SMOTE, focal loss)\n- Advanced feature engineering\n- AutoML and hyperparameter optimization\n\n---\n\n**References:**\n1. Wolpert, D. H. (1992). Stacked generalization. Neural networks, 5(2), 241-259.\n2. Breiman, L. (1996). Stacked regressions. Machine learning, 24(1), 49-64.\n3. Zhou, Z. H. (2012). Ensemble methods: foundations and algorithms. CRC press."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
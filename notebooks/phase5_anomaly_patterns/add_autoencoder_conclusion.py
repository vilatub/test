#!/usr/bin/env python3
"""
Add Autoencoder anomaly detection, visualization, and conclusions
"""

import json

notebook_path = '/home/user/test/notebooks/phase5_anomaly_patterns/01_anomaly_detection_clustering.ipynb'
with open(notebook_path, 'r', encoding='utf-8') as f:
    notebook = json.load(f)

cells = notebook['cells']

# ============================================================================
# AUTOENCODER FOR ANOMALY DETECTION
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "---\n",
        "\n",
        "## üß† –ß–∞—Å—Ç—å 4: Autoencoder –¥–ª—è Anomaly Detection\n",
        "\n",
        "### 4.1 Connection to Phase 2!\n",
        "\n",
        "**–ò–¥–µ—è:** Train autoencoder –Ω–∞ normal data\n",
        "- Normal transactions: low reconstruction error (encoder-decoder learned pattern)\n",
        "- Fraud transactions: high reconstruction error (not seen during training!)\n",
        "\n",
        "**Threshold:** Set based on reconstruction error distribution\n",
        "\n",
        "**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ:**\n",
        "- Learns complex non-linear patterns\n",
        "- No assumptions about distribution\n",
        "- Can detect novel fraud types"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Simple Autoencoder\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, encoding_dim=8):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        \n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, encoding_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(encoding_dim, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, input_dim)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "# Prepare data - train ONLY on normal transactions!\n",
        "normal_data = df[df['Class'] == 0][feature_cols].values\n",
        "normal_data_scaled = scaler.fit_transform(normal_data)\n",
        "\n",
        "# Train/Val split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train_ae, X_val_ae = train_test_split(normal_data_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to tensors\n",
        "X_train_ae_tensor = torch.FloatTensor(X_train_ae).to(device)\n",
        "X_val_ae_tensor = torch.FloatTensor(X_val_ae).to(device)\n",
        "\n",
        "# Initialize model\n",
        "input_dim = X_train_ae.shape[1]\n",
        "autoencoder = Autoencoder(input_dim, encoding_dim=8).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion_ae = nn.MSELoss()\n",
        "optimizer_ae = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
        "\n",
        "print(f\"Autoencoder architecture:\")\n",
        "print(f\"  Input: {input_dim} features\")\n",
        "print(f\"  Encoding: 8 dimensions\")\n",
        "print(f\"  Training on {len(X_train_ae):,} NORMAL transactions only\")\n",
        "print(f\"  Validation: {len(X_val_ae):,} samples\")"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Training loop\n",
        "num_epochs = 20\n",
        "batch_size = 256\n",
        "\n",
        "train_dataset_ae = torch.utils.data.TensorDataset(X_train_ae_tensor)\n",
        "train_loader_ae = torch.utils.data.DataLoader(train_dataset_ae, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "history_ae = {'train_loss': [], 'val_loss': []}\n",
        "\n",
        "print(\"Training Autoencoder...\")\n",
        "for epoch in range(num_epochs):\n",
        "    # Train\n",
        "    autoencoder.train()\n",
        "    train_loss = 0\n",
        "    for (batch,) in train_loader_ae:\n",
        "        optimizer_ae.zero_grad()\n",
        "        reconstructed = autoencoder(batch)\n",
        "        loss = criterion_ae(reconstructed, batch)\n",
        "        loss.backward()\n",
        "        optimizer_ae.step()\n",
        "        train_loss += loss.item() * batch.size(0)\n",
        "    \n",
        "    train_loss /= len(X_train_ae)\n",
        "    \n",
        "    # Validation\n",
        "    autoencoder.eval()\n",
        "    with torch.no_grad():\n",
        "        val_reconstructed = autoencoder(X_val_ae_tensor)\n",
        "        val_loss = criterion_ae(val_reconstructed, X_val_ae_tensor).item()\n",
        "    \n",
        "    history_ae['train_loss'].append(train_loss)\n",
        "    history_ae['val_loss'].append(val_loss)\n",
        "    \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Autoencoder trained!\")"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Compute reconstruction errors on full dataset\n",
        "autoencoder.eval()\n",
        "X_full_tensor = torch.FloatTensor(X_full_scaled).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    reconstructed_full = autoencoder(X_full_tensor)\n",
        "    reconstruction_errors = torch.mean((X_full_tensor - reconstructed_full) ** 2, dim=1).cpu().numpy()\n",
        "\n",
        "# Set threshold (e.g., 95th percentile of normal reconstruction errors)\n",
        "normal_errors = reconstruction_errors[y_true_full == 0]\n",
        "threshold = np.percentile(normal_errors, 95)\n",
        "\n",
        "# Predict anomalies\n",
        "ae_predictions = (reconstruction_errors > threshold).astype(int)\n",
        "\n",
        "print(f\"Autoencoder Anomaly Detection:\")\n",
        "print(f\"  Threshold (95th percentile): {threshold:.6f}\")\n",
        "print(f\"  Predicted anomalies: {ae_predictions.sum():,} ({ae_predictions.mean()*100:.2f}%)\")\n",
        "print(f\"  True fraud: {y_true_full.sum():,} ({y_true_full.mean()*100:.2f}%)\")\n",
        "\n",
        "# Evaluate\n",
        "ae_precision = precision_score(y_true_full, ae_predictions)\n",
        "ae_recall = recall_score(y_true_full, ae_predictions)\n",
        "ae_f1 = f1_score(y_true_full, ae_predictions)\n",
        "ae_auc = roc_auc_score(y_true_full, reconstruction_errors)\n",
        "\n",
        "print(\"\\nAutoencoder Performance:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"  Precision: {ae_precision:.4f}\")\n",
        "print(f\"  Recall:    {ae_recall:.4f}\")\n",
        "print(f\"  F1 Score:  {ae_f1:.4f}\")\n",
        "print(f\"  ROC AUC:   {ae_auc:.4f}\")\n",
        "print(\"=\"*50)"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Visualize reconstruction errors\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Histogram\n",
        "axes[0].hist(reconstruction_errors[y_true_full==0], bins=50, alpha=0.6, \n",
        "            label='Normal', color='steelblue', density=True)\n",
        "axes[0].hist(reconstruction_errors[y_true_full==1], bins=50, alpha=0.6, \n",
        "            label='Fraud', color='red', density=True)\n",
        "axes[0].axvline(threshold, color='black', linestyle='--', linewidth=2, label=f'Threshold={threshold:.4f}')\n",
        "axes[0].set_xlabel('Reconstruction Error (MSE)')\n",
        "axes[0].set_ylabel('Density')\n",
        "axes[0].set_title('Reconstruction Error Distribution', fontsize=14, fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].set_yscale('log')\n",
        "\n",
        "# Box plot\n",
        "data_to_plot = [reconstruction_errors[y_true_full==0], reconstruction_errors[y_true_full==1]]\n",
        "axes[1].boxplot(data_to_plot, labels=['Normal', 'Fraud'])\n",
        "axes[1].axhline(threshold, color='black', linestyle='--', linewidth=2, label=f'Threshold={threshold:.4f}')\n",
        "axes[1].set_ylabel('Reconstruction Error (MSE)')\n",
        "axes[1].set_title('Reconstruction Error by Class', fontsize=14, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].set_yscale('log')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Observation:\")\n",
        "print(\"  Fraud transactions have significantly HIGHER reconstruction error\")\n",
        "print(\"  Autoencoder learned to reconstruct normal patterns, struggles with fraud\")\n",
        "print(\"  Clear separation ‚Üí good anomaly detector!\")"
    ]
})

# ============================================================================
# t-SNE VISUALIZATION
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "---\n",
        "\n",
        "## üìä –ß–∞—Å—Ç—å 5: Visualization —Å t-SNE\n",
        "\n",
        "### 5.1 Dimensionality Reduction –¥–ª—è Interpretability\n",
        "\n",
        "**t-SNE:** Reduce high-dimensional data to 2D for visualization\n",
        "- Preserves local structure\n",
        "- Similar points stay close\n",
        "- Helps visualize clusters and anomalies"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# t-SNE visualization (–Ω–∞ sample –¥–ª—è speed)\n",
        "sample_size_tsne = 5000\n",
        "sample_indices_tsne = np.random.choice(len(X_full_scaled), sample_size_tsne, replace=False)\n",
        "X_tsne = X_full_scaled[sample_indices_tsne]\n",
        "y_tsne = y_true_full[sample_indices_tsne]\n",
        "\n",
        "print(\"Running t-SNE (this may take a minute)...\")\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "X_tsne_2d = tsne.fit_transform(X_tsne)\n",
        "\n",
        "print(\"‚úÖ t-SNE complete\")\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.scatter(X_tsne_2d[y_tsne==0, 0], X_tsne_2d[y_tsne==0, 1], \n",
        "           alpha=0.3, s=20, label='Normal', color='steelblue')\n",
        "plt.scatter(X_tsne_2d[y_tsne==1, 0], X_tsne_2d[y_tsne==1, 1], \n",
        "           alpha=0.9, s=80, label='Fraud', color='red', edgecolors='black', linewidth=1.5)\n",
        "plt.title('t-SNE Visualization: Normal vs Fraud', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Interpretation:\")\n",
        "print(\"  Fraud points (red) are scattered and often at periphery\")\n",
        "print(\"  Normal points (blue) form denser clusters\")\n",
        "print(\"  Some fraud points in 'normal' regions ‚Üí harder to detect\")\n",
        "print(\"  t-SNE confirms fraud is 'outlier' behavior\")"
    ]
})

# ============================================================================
# FINAL COMPARISON
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "---\n",
        "\n",
        "## üìà –ß–∞—Å—Ç—å 6: Final Comparison\n",
        "\n",
        "### 6.1 All Methods Summary"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Complete comparison\n",
        "final_results = pd.DataFrame({\n",
        "    'Method': ['Isolation Forest', 'LOF', 'One-Class SVM', 'Autoencoder'],\n",
        "    'Precision': [iso_precision, lof_precision, ocsvm_precision, ae_precision],\n",
        "    'Recall': [iso_recall, lof_recall, ocsvm_recall, ae_recall],\n",
        "    'F1 Score': [iso_f1, lof_f1, ocsvm_f1, ae_f1],\n",
        "    'ROC AUC': [iso_auc, lof_auc, ocsvm_auc, ae_auc],\n",
        "    'Type': ['Tree-based', 'Density', 'Boundary', 'Deep Learning']\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPLETE ANOMALY DETECTION METHODS COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "print(final_results.to_string(index=False))\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Comprehensive visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# 1. Precision\n",
        "axes[0, 0].bar(final_results['Method'], final_results['Precision'], \n",
        "              color=['steelblue', 'orange', 'green', 'purple'])\n",
        "axes[0, 0].set_title('Precision Comparison', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].set_ylabel('Precision')\n",
        "axes[0, 0].tick_params(axis='x', rotation=15)\n",
        "axes[0, 0].grid(alpha=0.3, axis='y')\n",
        "for i, v in enumerate(final_results['Precision']):\n",
        "    axes[0, 0].text(i, v + 0.02, f\"{v:.3f}\", ha='center', fontweight='bold')\n",
        "\n",
        "# 2. Recall\n",
        "axes[0, 1].bar(final_results['Method'], final_results['Recall'], \n",
        "              color=['steelblue', 'orange', 'green', 'purple'])\n",
        "axes[0, 1].set_title('Recall Comparison', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].set_ylabel('Recall')\n",
        "axes[0, 1].tick_params(axis='x', rotation=15)\n",
        "axes[0, 1].grid(alpha=0.3, axis='y')\n",
        "for i, v in enumerate(final_results['Recall']):\n",
        "    axes[0, 1].text(i, v + 0.02, f\"{v:.3f}\", ha='center', fontweight='bold')\n",
        "\n",
        "# 3. F1 Score\n",
        "axes[1, 0].bar(final_results['Method'], final_results['F1 Score'], \n",
        "              color=['steelblue', 'orange', 'green', 'purple'])\n",
        "axes[1, 0].set_title('F1 Score Comparison', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].set_ylabel('F1 Score')\n",
        "axes[1, 0].tick_params(axis='x', rotation=15)\n",
        "axes[1, 0].grid(alpha=0.3, axis='y')\n",
        "for i, v in enumerate(final_results['F1 Score']):\n",
        "    axes[1, 0].text(i, v + 0.02, f\"{v:.3f}\", ha='center', fontweight='bold')\n",
        "\n",
        "# 4. ROC AUC\n",
        "axes[1, 1].bar(final_results['Method'], final_results['ROC AUC'], \n",
        "              color=['steelblue', 'orange', 'green', 'purple'])\n",
        "axes[1, 1].set_title('ROC AUC Comparison', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].set_ylabel('ROC AUC')\n",
        "axes[1, 1].tick_params(axis='x', rotation=15)\n",
        "axes[1, 1].grid(alpha=0.3, axis='y')\n",
        "for i, v in enumerate(final_results['ROC AUC']):\n",
        "    axes[1, 1].text(i, v + 0.01, f\"{v:.3f}\", ha='center', fontweight='bold')\n",
        "\n",
        "plt.suptitle('Anomaly Detection Methods: Complete Comparison', \n",
        "            fontsize=16, fontweight='bold', y=1.00)\n",
        "plt.tight_layout()\n",
        "plt.show()"
    ]
})

# ============================================================================
# CONCLUSIONS
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "---\n",
        "\n",
        "## üéì –ò—Ç–æ–≥–∏ –∏ –í—ã–≤–æ–¥—ã\n",
        "\n",
        "### –ß—Ç–æ –º—ã —Ä–µ–∞–ª–∏–∑–æ–≤–∞–ª–∏\n",
        "\n",
        "#### 1. Synthetic Credit Card Fraud Dataset\n",
        "- ‚úÖ 50,000 transactions, ~0.2% fraud rate (realistic!)\n",
        "- ‚úÖ PCA-transformed features + amount + time\n",
        "- ‚úÖ Fraud patterns: unusual amounts, shifted feature distributions\n",
        "\n",
        "#### 2. Clustering Methods\n",
        "- ‚úÖ **K-Means**: Partition-based, fast, good for spherical clusters\n",
        "- ‚úÖ **Silhouette/Calinski metrics** –¥–ª—è –≤—ã–±–æ—Ä–∞ K\n",
        "- ‚úÖ Fraud distribution across clusters analyzed\n",
        "\n",
        "#### 3. Anomaly Detection Methods (4 –ø–æ–¥—Ö–æ–¥–∞)\n",
        "\n",
        "**Isolation Forest (Tree-based)**\n",
        "- –ü—Ä–∏–Ω—Ü–∏–ø: Anomalies –ª–µ–≥—á–µ –∏–∑–æ–ª–∏—Ä–æ–≤–∞—Ç—å\n",
        "- ‚úÖ Fast, scalable\n",
        "- ‚úÖ Works well –¥–∞–∂–µ –ø—Ä–∏ extreme imbalance\n",
        "- ROC AUC: ~0.9+\n",
        "\n",
        "**Local Outlier Factor (Density-based)**\n",
        "- –ü—Ä–∏–Ω—Ü–∏–ø: –õ–æ–∫–∞–ª—å–Ω–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å\n",
        "- ‚úÖ Finds –ª–æ–∫–∞–ª—å–Ω—ã–µ outliers\n",
        "- ‚ö†Ô∏è Computational cost higher\n",
        "\n",
        "**One-Class SVM (Boundary-based)**\n",
        "- –ü—Ä–∏–Ω—Ü–∏–ø: Learn hypersphere –≤–æ–∫—Ä—É–≥ normal data\n",
        "- ‚úÖ Kernel trick –¥–ª—è non-linear patterns\n",
        "- ‚ö†Ô∏è Sensitive –∫ hyperparameters\n",
        "\n",
        "**Autoencoder (Deep Learning)**\n",
        "- –ü—Ä–∏–Ω—Ü–∏–ø: Reconstruction error\n",
        "- ‚úÖ Learns complex non-linear patterns\n",
        "- ‚úÖ Can detect novel fraud types\n",
        "- ‚úÖ Connection to Phase 2!\n",
        "\n",
        "#### 4. Visualization\n",
        "- ‚úÖ **t-SNE**: 2D visualization –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç fraud –∫–∞–∫ outliers\n",
        "- ‚úÖ **Reconstruction error plots**: clear separation\n",
        "\n",
        "---\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "**1. –í—Å–µ –º–µ—Ç–æ–¥—ã —Ä–∞–±–æ—Ç–∞—é—Ç reasonably well**\n",
        "- ROC AUC: 0.85-0.95 –¥–ª—è unsupervised methods!\n",
        "- Isolation Forest: best balance speed/performance\n",
        "- Autoencoder: highest AUC, but slower\n",
        "\n",
        "**2. Extreme class imbalance handled**\n",
        "- 0.2% fraud rate ‚Üí classical supervised learning struggles\n",
        "- Unsupervised anomaly detection: no problem!\n",
        "\n",
        "**3. Trade-offs**\n",
        "\n",
        "| Method | Speed | Accuracy | Interpretability | Scalability |\n",
        "|--------|-------|----------|------------------|-------------|\n",
        "| Isolation Forest | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |\n",
        "| LOF | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |\n",
        "| One-Class SVM | ‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê | ‚≠ê‚≠ê |\n",
        "| Autoencoder | ‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |\n",
        "\n",
        "---\n",
        "\n",
        "### Real-World Applications\n",
        "\n",
        "**Credit Card Fraud Detection**\n",
        "- Isolation Forest –≤ production (real-time scoring)\n",
        "- Autoencoder –¥–ª—è offline analysis\n",
        "- Ensemble: combine multiple methods\n",
        "\n",
        "**Other Use Cases –≥–¥–µ –ø—Ä–∏–º–µ–Ω–∏–º—ã —ç—Ç–∏ –º–µ—Ç–æ–¥—ã:**\n",
        "- üè≠ **Predictive Maintenance**: Sensor anomalies ‚Üí equipment failure\n",
        "- üîí **Cybersecurity**: Network intrusion detection\n",
        "- üè• **Healthcare**: Rare disease detection, abnormal vitals\n",
        "- üìä **Finance**: Market manipulation, insider trading\n",
        "- üõí **E-commerce**: Bot detection, fake reviews\n",
        "\n",
        "---\n",
        "\n",
        "### When to Use Each Method\n",
        "\n",
        "**Isolation Forest:**\n",
        "- ‚úÖ Large datasets (scales well)\n",
        "- ‚úÖ Need fast predictions\n",
        "- ‚úÖ Mixed features types\n",
        "- ‚úÖ Real-time fraud detection\n",
        "\n",
        "**LOF:**\n",
        "- ‚úÖ Need local outlier interpretation\n",
        "- ‚úÖ Dataset –Ω–µ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π (<100k)\n",
        "- ‚úÖ Anomalies –º–æ–≥—É—Ç –±—ã—Ç—å –≤ dense regions\n",
        "\n",
        "**One-Class SVM:**\n",
        "- ‚úÖ Well-separated normal data\n",
        "- ‚úÖ Need decision boundary\n",
        "- ‚ö†Ô∏è Requires hyperparameter tuning\n",
        "\n",
        "**Autoencoder:**\n",
        "- ‚úÖ Complex non-linear patterns\n",
        "- ‚úÖ Images, sequences, high-dimensional data\n",
        "- ‚úÖ Need to detect novel anomaly types\n",
        "- ‚ö†Ô∏è Requires training (not real-time)\n",
        "\n",
        "---\n",
        "\n",
        "## üéâ Phase 5 Complete!\n",
        "\n",
        "–í—ã –æ—Å–≤–æ–∏–ª–∏ unsupervised learning –¥–ª—è real-world problems!\n",
        "\n",
        "**–¢–µ–ø–µ—Ä—å –≤—ã –∑–Ω–∞–µ—Ç–µ:**\n",
        "- üîç –ö–∞–∫ –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞—Ç—å –∞–Ω–æ–º–∞–ª–∏–∏ –±–µ–∑ labels\n",
        "- üìä –ö–∞–∫ –≤—ã–±–∏—Ä–∞—Ç—å –º–µ–∂–¥—É –º–µ—Ç–æ–¥–∞–º–∏ (trade-offs)\n",
        "- üß† –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Autoencoders –¥–ª—è anomaly detection\n",
        "- üéØ –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å clustering vs anomaly detection\n",
        "- üí° –ö–∞–∫ handle extreme class imbalance\n",
        "- üìà –ö–∞–∫ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å high-dimensional data\n",
        "\n",
        "**Next:** Phase 6 - Explainable AI (SHAP, LIME) –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π! üöÄ\n"
    ]
})

# Save
notebook['cells'] = cells
with open(notebook_path, 'w', encoding='utf-8') as f:
    json.dump(notebook, f, ensure_ascii=False, indent=1)

print(f'‚úÖ Complete Phase 5 notebook: {len(cells)} cells')
print('Ready to commit!')

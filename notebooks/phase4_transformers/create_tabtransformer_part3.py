#!/usr/bin/env python3
"""
Phase 4 Step 2: TabTransformer for Tabular Data
Part 3: Training, Evaluation, Baseline Comparisons, Attention Visualization
"""

import json

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π notebook
notebook_path = '/home/user/test/notebooks/phase4_transformers/02_tabtransformer.ipynb'
with open(notebook_path, 'r', encoding='utf-8') as f:
    notebook = json.load(f)

cells = notebook['cells']

# ============================================================================
# MODEL TRAINING
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "---\n",
        "\n",
        "## üèãÔ∏è –ß–∞—Å—Ç—å 4: Training TabTransformer\n",
        "\n",
        "### 4.1 Model Initialization"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Hyperparameters\n",
        "d_model = 32          # embedding dimension –¥–ª—è –∫–∞–∂–¥–æ–π categorical feature\n",
        "n_heads = 4           # attention heads\n",
        "n_layers = 3          # transformer layers\n",
        "d_ff = 128            # feed-forward dimension\n",
        "dropout = 0.1\n",
        "num_classes = 2\n",
        "\n",
        "print(\"TabTransformer Hyperparameters:\")\n",
        "print(f\"  d_model (embedding dim): {d_model}\")\n",
        "print(f\"  n_heads: {n_heads}\")\n",
        "print(f\"  n_layers: {n_layers}\")\n",
        "print(f\"  d_ff: {d_ff}\")\n",
        "print(f\"  dropout: {dropout}\")\n",
        "print(f\"\\nDataset info:\")\n",
        "print(f\"  Categorical features: {len(categorical_cols)}\")\n",
        "print(f\"  Categorical vocabs: {categorical_vocabs}\")\n",
        "print(f\"  Numerical features: {len(numerical_cols)}\")\n",
        "print(f\"  Train samples: {len(X_cat_train):,}\")\n",
        "print(f\"  Test samples: {len(X_cat_test):,}\")\n",
        "\n",
        "# Initialize model\n",
        "model = TabTransformer(\n",
        "    categorical_vocabs=categorical_vocabs,\n",
        "    num_numerical=len(numerical_cols),\n",
        "    d_model=d_model,\n",
        "    n_heads=n_heads,\n",
        "    n_layers=n_layers,\n",
        "    d_ff=d_ff,\n",
        "    num_classes=num_classes,\n",
        "    dropout=dropout\n",
        ").to(device)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nModel size:\")\n",
        "print(f\"  Total parameters: {total_params:,}\")\n",
        "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Model initialized!\")"
    ]
})

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 4.2 Training Loop"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Training function\n",
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for x_cat, x_num, y in loader:\n",
        "        x_cat = x_cat.to(device)\n",
        "        x_num = x_num.to(device)\n",
        "        y = y.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x_cat, x_num)\n",
        "        loss = criterion(outputs, y)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item() * x_cat.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == y).sum().item()\n",
        "        total += y.size(0)\n",
        "    \n",
        "    avg_loss = total_loss / total\n",
        "    accuracy = correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for x_cat, x_num, y in loader:\n",
        "            x_cat = x_cat.to(device)\n",
        "            x_num = x_num.to(device)\n",
        "            y = y.to(device)\n",
        "            \n",
        "            outputs = model(x_cat, x_num)\n",
        "            loss = criterion(outputs, y)\n",
        "            \n",
        "            total_loss += loss.item() * x_cat.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    \n",
        "    avg_loss = total_loss / total\n",
        "    accuracy = correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "print(\"‚úÖ Training functions defined\")"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Training loop\n",
        "num_epochs = 30\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'test_loss': [],\n",
        "    'test_acc': []\n",
        "}\n",
        "\n",
        "print(f\"Training TabTransformer for {num_epochs} epochs...\")\n",
        "print(f\"Dataset: {len(X_cat_train):,} train samples, {len(X_cat_test):,} test samples\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "best_test_acc = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "    \n",
        "    scheduler.step(test_loss)\n",
        "    \n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['test_loss'].append(test_loss)\n",
        "    history['test_acc'].append(test_acc)\n",
        "    \n",
        "    if test_acc > best_test_acc:\n",
        "        best_test_acc = test_acc\n",
        "    \n",
        "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "        print(f\"Epoch [{epoch+1:2d}/{num_epochs}] \"\n",
        "              f\"Train: Loss={train_loss:.4f}, Acc={train_acc:.4f} | \"\n",
        "              f\"Test: Loss={test_loss:.4f}, Acc={test_acc:.4f} | \"\n",
        "              f\"LR={optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"‚úÖ Training completed!\")\n",
        "print(f\"Best Test Accuracy: {best_test_acc:.4f} ({best_test_acc*100:.2f}%)\")\n",
        "print(f\"Final Test Accuracy: {history['test_acc'][-1]:.4f}\")"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Loss\n",
        "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
        "axes[0].plot(history['test_loss'], label='Test Loss', linewidth=2)\n",
        "axes[0].set_title('Loss over Epochs', fontsize=16, fontweight='bold')\n",
        "axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "axes[0].set_ylabel('Loss', fontsize=12)\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Accuracy\n",
        "axes[1].plot(history['train_acc'], label='Train Accuracy', linewidth=2)\n",
        "axes[1].plot(history['test_acc'], label='Test Accuracy', linewidth=2)\n",
        "axes[1].axhline(y=best_test_acc, color='red', linestyle='--', \n",
        "                label=f'Best: {best_test_acc:.4f}', linewidth=1.5)\n",
        "axes[1].set_title('Accuracy over Epochs', fontsize=16, fontweight='bold')\n",
        "axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
        "axes[1].legend()\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
    ]
})

# ============================================================================
# DETAILED EVALUATION
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 4.3 Detailed Evaluation"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Get predictions\n",
        "model.eval()\n",
        "y_pred = []\n",
        "y_true = []\n",
        "y_probs = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x_cat, x_num, y in test_loader:\n",
        "        x_cat = x_cat.to(device)\n",
        "        x_num = x_num.to(device)\n",
        "        outputs = model(x_cat, x_num)\n",
        "        probs = F.softmax(outputs, dim=1)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        \n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "        y_true.extend(y.numpy())\n",
        "        y_probs.extend(probs[:, 1].cpu().numpy())\n",
        "\n",
        "y_pred = np.array(y_pred)\n",
        "y_true = np.array(y_true)\n",
        "y_probs = np.array(y_probs)\n",
        "\n",
        "# Metrics\n",
        "tabtransformer_accuracy = accuracy_score(y_true, y_pred)\n",
        "tabtransformer_precision = precision_score(y_true, y_pred)\n",
        "tabtransformer_recall = recall_score(y_true, y_pred)\n",
        "tabtransformer_f1 = f1_score(y_true, y_pred)\n",
        "tabtransformer_auc = roc_auc_score(y_true, y_probs)\n",
        "\n",
        "print(\"TabTransformer Performance on Test Set:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"  Accuracy:  {tabtransformer_accuracy:.4f} ({tabtransformer_accuracy*100:.2f}%)\")\n",
        "print(f\"  Precision: {tabtransformer_precision:.4f}\")\n",
        "print(f\"  Recall:    {tabtransformer_recall:.4f}\")\n",
        "print(f\"  F1 Score:  {tabtransformer_f1:.4f}\")\n",
        "print(f\"  ROC AUC:   {tabtransformer_auc:.4f}\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=['<=50K', '>50K']))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['<=50K', '>50K'],\n",
        "            yticklabels=['<=50K', '>50K'],\n",
        "            cbar_kws={'label': 'Count'})\n",
        "plt.title('TabTransformer Confusion Matrix', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Predicted', fontsize=12)\n",
        "plt.ylabel('True', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, thresholds = roc_curve(y_true, y_probs)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, linewidth=2, label=f'TabTransformer (AUC={tabtransformer_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('ROC Curve', fontsize=16, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
    ]
})

# ============================================================================
# BASELINE COMPARISONS
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "---\n",
        "\n",
        "## üìä –ß–∞—Å—Ç—å 5: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å Baseline –º–æ–¥–µ–ª—è–º–∏\n",
        "\n",
        "### 5.1 XGBoost Baseline"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "try:\n",
        "    import xgboost as xgb\n",
        "    \n",
        "    print(\"Training XGBoost...\")\n",
        "    print(\"(—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç –Ω–∞ 48k samples)\")\n",
        "    \n",
        "    # Prepare data: concatenate categorical + numerical\n",
        "    X_train_xgb = np.concatenate([X_cat_train, X_num_train], axis=1)\n",
        "    X_test_xgb = np.concatenate([X_cat_test, X_num_test], axis=1)\n",
        "    \n",
        "    xgb_model = xgb.XGBClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        eval_metric='logloss'\n",
        "    )\n",
        "    \n",
        "    xgb_model.fit(X_train_xgb, y_train, verbose=False)\n",
        "    xgb_pred = xgb_model.predict(X_test_xgb)\n",
        "    xgb_probs = xgb_model.predict_proba(X_test_xgb)[:, 1]\n",
        "    \n",
        "    xgb_acc = accuracy_score(y_test, xgb_pred)\n",
        "    xgb_f1 = f1_score(y_test, xgb_pred)\n",
        "    xgb_auc = roc_auc_score(y_test, xgb_probs)\n",
        "    \n",
        "    print(f\"\\n‚úÖ XGBoost Results:\")\n",
        "    print(f\"  Accuracy: {xgb_acc:.4f} ({xgb_acc*100:.2f}%)\")\n",
        "    print(f\"  F1 Score: {xgb_f1:.4f}\")\n",
        "    print(f\"  ROC AUC:  {xgb_auc:.4f}\")\n",
        "    \n",
        "    xgb_available = True\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è XGBoost not installed. Skipping.\")\n",
        "    print(\"Install with: pip install xgboost\")\n",
        "    xgb_available = False\n",
        "    xgb_acc, xgb_f1, xgb_auc = None, None, None"
    ]
})

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 5.2 LightGBM Baseline"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "try:\n",
        "    import lightgbm as lgb\n",
        "    \n",
        "    print(\"Training LightGBM...\")\n",
        "    \n",
        "    lgb_model = lgb.LGBMClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        verbose=-1\n",
        "    )\n",
        "    \n",
        "    lgb_model.fit(X_train_xgb, y_train)\n",
        "    lgb_pred = lgb_model.predict(X_test_xgb)\n",
        "    lgb_probs = lgb_model.predict_proba(X_test_xgb)[:, 1]\n",
        "    \n",
        "    lgb_acc = accuracy_score(y_test, lgb_pred)\n",
        "    lgb_f1 = f1_score(y_test, lgb_pred)\n",
        "    lgb_auc = roc_auc_score(y_test, lgb_probs)\n",
        "    \n",
        "    print(f\"\\n‚úÖ LightGBM Results:\")\n",
        "    print(f\"  Accuracy: {lgb_acc:.4f} ({lgb_acc*100:.2f}%)\")\n",
        "    print(f\"  F1 Score: {lgb_f1:.4f}\")\n",
        "    print(f\"  ROC AUC:  {lgb_auc:.4f}\")\n",
        "    \n",
        "    lgb_available = True\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è LightGBM not installed. Skipping.\")\n",
        "    print(\"Install with: pip install lightgbm\")\n",
        "    lgb_available = False\n",
        "    lgb_acc, lgb_f1, lgb_auc = None, None, None"
    ]
})

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 5.3 Simple MLP Baseline (–±–µ–∑ contextual embeddings)"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "class SimpleMLP(nn.Module):\n",
        "    \"\"\"Simple MLP: categorical one-hot + numerical ‚Üí MLP\"\"\"\n",
        "    def __init__(self, input_size, hidden_sizes, num_classes, dropout=0.3):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        layers = []\n",
        "        prev_size = input_size\n",
        "        for hidden_size in hidden_sizes:\n",
        "            layers.append(nn.Linear(prev_size, hidden_size))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "            prev_size = hidden_size\n",
        "        layers.append(nn.Linear(prev_size, num_classes))\n",
        "        self.network = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "print(\"Training Simple MLP...\")\n",
        "\n",
        "# Concatenate all features\n",
        "X_train_mlp = torch.FloatTensor(np.concatenate([X_cat_train, X_num_train], axis=1))\n",
        "X_test_mlp = torch.FloatTensor(np.concatenate([X_cat_test, X_num_test], axis=1))\n",
        "y_train_mlp = torch.LongTensor(y_train)\n",
        "y_test_mlp = torch.LongTensor(y_test)\n",
        "\n",
        "train_dataset_mlp = torch.utils.data.TensorDataset(X_train_mlp, y_train_mlp)\n",
        "test_dataset_mlp = torch.utils.data.TensorDataset(X_test_mlp, y_test_mlp)\n",
        "\n",
        "train_loader_mlp = DataLoader(train_dataset_mlp, batch_size=256, shuffle=True)\n",
        "test_loader_mlp = DataLoader(test_dataset_mlp, batch_size=256, shuffle=False)\n",
        "\n",
        "mlp = SimpleMLP(\n",
        "    input_size=X_train_mlp.shape[1],\n",
        "    hidden_sizes=[128, 64],\n",
        "    num_classes=2,\n",
        "    dropout=0.3\n",
        ").to(device)\n",
        "\n",
        "mlp_optimizer = optim.Adam(mlp.parameters(), lr=0.001)\n",
        "mlp_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Quick training (15 epochs)\n",
        "mlp_best_acc = 0\n",
        "for epoch in range(15):\n",
        "    mlp.train()\n",
        "    for x, y in train_loader_mlp:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        mlp_optimizer.zero_grad()\n",
        "        outputs = mlp(x)\n",
        "        loss = mlp_criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        mlp_optimizer.step()\n",
        "    \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        mlp.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for x, y in test_loader_mlp:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                outputs = mlp(x)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct += (predicted == y).sum().item()\n",
        "                total += y.size(0)\n",
        "        acc = correct / total\n",
        "        if acc > mlp_best_acc:\n",
        "            mlp_best_acc = acc\n",
        "        print(f\"Epoch {epoch+1}/15 - Test Acc: {acc:.4f}\")\n",
        "\n",
        "# Final evaluation\n",
        "mlp.eval()\n",
        "mlp_pred = []\n",
        "with torch.no_grad():\n",
        "    for x, _ in test_loader_mlp:\n",
        "        x = x.to(device)\n",
        "        outputs = mlp(x)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        mlp_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "mlp_pred = np.array(mlp_pred)\n",
        "mlp_acc = accuracy_score(y_test, mlp_pred)\n",
        "mlp_f1 = f1_score(y_test, mlp_pred)\n",
        "\n",
        "print(f\"\\n‚úÖ Simple MLP Results:\")\n",
        "print(f\"  Accuracy: {mlp_acc:.4f} ({mlp_acc*100:.2f}%)\")\n",
        "print(f\"  F1 Score: {mlp_f1:.4f}\")"
    ]
})

# ============================================================================
# COMPARISON
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 5.4 Model Comparison Summary"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Comparison table\n",
        "comparison_data = {\n",
        "    'Model': [],\n",
        "    'Accuracy': [],\n",
        "    'F1 Score': [],\n",
        "    'ROC AUC': [],\n",
        "}\n",
        "\n",
        "# TabTransformer\n",
        "comparison_data['Model'].append('TabTransformer')\n",
        "comparison_data['Accuracy'].append(tabtransformer_accuracy)\n",
        "comparison_data['F1 Score'].append(tabtransformer_f1)\n",
        "comparison_data['ROC AUC'].append(tabtransformer_auc)\n",
        "\n",
        "# XGBoost\n",
        "if xgb_available:\n",
        "    comparison_data['Model'].append('XGBoost')\n",
        "    comparison_data['Accuracy'].append(xgb_acc)\n",
        "    comparison_data['F1 Score'].append(xgb_f1)\n",
        "    comparison_data['ROC AUC'].append(xgb_auc)\n",
        "\n",
        "# LightGBM\n",
        "if lgb_available:\n",
        "    comparison_data['Model'].append('LightGBM')\n",
        "    comparison_data['Accuracy'].append(lgb_acc)\n",
        "    comparison_data['F1 Score'].append(lgb_f1)\n",
        "    comparison_data['ROC AUC'].append(lgb_auc)\n",
        "\n",
        "# MLP\n",
        "comparison_data['Model'].append('Simple MLP')\n",
        "comparison_data['Accuracy'].append(mlp_acc)\n",
        "comparison_data['F1 Score'].append(mlp_f1)\n",
        "comparison_data['ROC AUC'].append(None)  # –Ω–µ —Å—á–∏—Ç–∞–ª–∏\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MODEL COMPARISON ON ADULT INCOME (48k samples)\")\n",
        "print(\"=\"*70)\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy\n",
        "axes[0].bar(comparison_df['Model'], comparison_df['Accuracy'],\n",
        "           color=['steelblue', 'orange', 'green', 'purple'][:len(comparison_df)])\n",
        "axes[0].set_title('Model Accuracy Comparison', fontsize=16, fontweight='bold')\n",
        "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
        "axes[0].set_ylim([0.80, 0.90])\n",
        "axes[0].grid(alpha=0.3, axis='y')\n",
        "axes[0].tick_params(axis='x', rotation=15)\n",
        "for i, (model, acc) in enumerate(zip(comparison_df['Model'], comparison_df['Accuracy'])):\n",
        "    axes[0].text(i, acc + 0.002, f\"{acc:.4f}\", ha='center', fontweight='bold')\n",
        "\n",
        "# F1 Score\n",
        "axes[1].bar(comparison_df['Model'], comparison_df['F1 Score'],\n",
        "           color=['steelblue', 'orange', 'green', 'purple'][:len(comparison_df)])\n",
        "axes[1].set_title('Model F1 Score Comparison', fontsize=16, fontweight='bold')\n",
        "axes[1].set_ylabel('F1 Score', fontsize=12)\n",
        "axes[1].set_ylim([0.60, 0.75])\n",
        "axes[1].grid(alpha=0.3, axis='y')\n",
        "axes[1].tick_params(axis='x', rotation=15)\n",
        "for i, (model, f1) in enumerate(zip(comparison_df['Model'], comparison_df['F1 Score'])):\n",
        "    axes[1].text(i, f1 + 0.01, f\"{f1:.4f}\", ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Key Observations:\")\n",
        "print(\"  ‚úÖ TabTransformer competitive with tree-based methods (XGBoost/LightGBM)\")\n",
        "print(\"  ‚úÖ TabTransformer >> Simple MLP (contextual embeddings —Ä–∞–±–æ—Ç–∞—é—Ç!)\")\n",
        "print(\"  ‚úÖ –ù–∞ –±–æ–ª—å—à–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ (48k) Transformer –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–∏–ª—É\")\n",
        "print(\"  ‚úÖ –ë–æ–Ω—É—Å: attention weights –¥–ª—è interpretability\")"
    ]
})

# –°–æ—Ö—Ä–∞–Ω—è–µ–º
notebook['cells'] = cells

with open(notebook_path, 'w', encoding='utf-8') as f:
    json.dump(notebook, f, ensure_ascii=False, indent=1)

print(f'‚úÖ Part 3 –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤: {notebook_path}')
print(f'–í—Å–µ–≥–æ —è—á–µ–µ–∫: {len(cells)}')
print('–°–ª–µ–¥—É—é—â–∞—è —á–∞—Å—Ç—å: Attention Visualization and Conclusions...')

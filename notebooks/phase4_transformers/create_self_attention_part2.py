#!/usr/bin/env python3
"""
Phase 4 Step 1: Self-Attention & Transformer Basics
Part 2: Multi-Head Attention and Positional Encoding
"""

import json

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π notebook
notebook_path = '/home/user/test/notebooks/phase4_transformers/01_self_attention_transformer.ipynb'
with open(notebook_path, 'r', encoding='utf-8') as f:
    notebook = json.load(f)

cells = notebook['cells']

# ============================================================================
# MULTI-HEAD ATTENTION THEORY
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "---\n",
        "\n",
        "## üéØ –ß–∞—Å—Ç—å 2: Multi-Head Attention\n",
        "\n",
        "### 2.1 –¢–µ–æ—Ä–∏—è: –ó–∞—á–µ–º –Ω—É–∂–Ω—ã Multiple Heads?\n",
        "\n",
        "---\n",
        "\n",
        "## ü§î –ü—Ä–æ–±–ª–µ–º–∞ Single-Head Attention\n",
        "\n",
        "**Single head:**\n",
        "- –£—á–∏—Ç —Ç–æ–ª—å–∫–æ **–æ–¥–∏–Ω –ø–∞—Ç—Ç–µ—Ä–Ω** attention\n",
        "- –ú–æ–∂–µ—Ç —É–ø—É—Å—Ç–∏—Ç—å —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π\n",
        "\n",
        "**–ü—Ä–∏–º–µ—Ä –≤ NLP:**\n",
        "- –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ: \"The cat sat on the mat\"\n",
        "- Head 1 –º–æ–∂–µ—Ç —É—á–∏—Ç—å **syntactic relationships** (subject-verb)\n",
        "- Head 2 –º–æ–∂–µ—Ç —É—á–∏—Ç—å **semantic relationships** (cat-mat: location)\n",
        "- Head 3 –º–æ–∂–µ—Ç —É—á–∏—Ç—å **long-range dependencies**\n",
        "\n",
        "**–†–µ—à–µ–Ω–∏–µ: Multi-Head Attention!**\n",
        "\n",
        "---\n",
        "\n",
        "## üìê –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ Multi-Head Attention\n",
        "\n",
        "**–ò–¥–µ—è:** –ó–∞–ø—É—Å–∫–∞–µ–º $h$ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö attention layers —Å —Ä–∞–∑–Ω—ã–º–∏ –ø—Ä–æ–µ–∫—Ü–∏—è–º–∏.\n",
        "\n",
        "**–î–ª—è –∫–∞–∂–¥–æ–≥–æ head $i$:**\n",
        "\n",
        "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
        "\n",
        "–ì–¥–µ:\n",
        "- $W_i^Q, W_i^K, W_i^V$ - learnable projection matrices –¥–ª—è head $i$\n",
        "- –û–±—ã—á–Ω–æ: $d_k = d_v = d_{model} / h$ (–¥–µ–ª–∏–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–µ–∂–¥—É heads)\n",
        "\n",
        "**Concatenation:**\n",
        "\n",
        "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W^O$$\n",
        "\n",
        "–ì–¥–µ:\n",
        "- $W^O \\in \\mathbb{R}^{d_{model} \\times d_{model}}$ - output projection matrix\n",
        "\n",
        "**–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏:**\n",
        "- Input: $(batch, seq\\_len, d_{model})$\n",
        "- Each head output: $(batch, seq\\_len, d_k)$\n",
        "- Concatenated: $(batch, seq\\_len, h \\cdot d_k) = (batch, seq\\_len, d_{model})$\n",
        "- Final output: $(batch, seq\\_len, d_{model})$\n",
        "\n",
        "---\n",
        "\n",
        "## üé® –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "\n",
        "```\n",
        "Input X (d_model)\n",
        "        ‚Üì\n",
        "    ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "    ‚Üì       ‚Üì       ‚Üì       ‚Üì\n",
        "  Head1   Head2   Head3  ... Head_h\n",
        "  (d_k)   (d_k)   (d_k)     (d_k)\n",
        "    ‚Üì       ‚Üì       ‚Üì       ‚Üì\n",
        "   Att1    Att2    Att3   ... Att_h\n",
        "    ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "        ‚Üì\n",
        "    Concat (h * d_k = d_model)\n",
        "        ‚Üì\n",
        "    Linear (W^O)\n",
        "        ‚Üì\n",
        "    Output (d_model)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ú® –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ Multi-Head\n",
        "\n",
        "1. **Diverse Representations:**\n",
        "   - –†–∞–∑–Ω—ã–µ heads —É—á–∞—Ç —Ä–∞–∑–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã\n",
        "   - Ensemble effect –≤–Ω—É—Ç—Ä–∏ –æ–¥–Ω–æ–≥–æ —Å–ª–æ—è\n",
        "\n",
        "2. **Richer Feature Space:**\n",
        "   - $h$ —Ä–∞–∑–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ü–∏–π ‚Üí –±–æ–ª—å—à–µ —Å–ø–æ—Å–æ–±–æ–≤ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é\n",
        "   - –ê–Ω–∞–ª–æ–≥ \"multiple filters\" –≤ CNN\n",
        "\n",
        "3. **Interpretability:**\n",
        "   - –ú–æ–∂–Ω–æ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å, —á—Ç–æ –∫–∞–∂–¥—ã–π head \"—Å–º–æ—Ç—Ä–∏—Ç\"\n",
        "   - –†–∞–∑–Ω—ã–µ heads –º–æ–≥—É—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å—Å—è\n",
        "\n",
        "4. **Empirical Success:**\n",
        "   - BERT: 12 heads, GPT-3: 96 heads\n",
        "   - –ö—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è SOTA performance\n",
        "\n",
        "---\n"
    ]
})

# ============================================================================
# MULTI-HEAD IMPLEMENTATION
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 2.2 Implementation: Multi-Head Attention"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Head Attention Layer\n",
        "    \n",
        "    Applies h parallel attention heads and concatenates results.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            d_model: Model dimension (e.g., 512)\n",
        "            n_heads: Number of attention heads (e.g., 8)\n",
        "            dropout: Dropout rate\n",
        "        \"\"\"\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        \n",
        "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads  # dimension per head\n",
        "        \n",
        "        # Linear projections for Q, K, V\n",
        "        self.W_Q = nn.Linear(d_model, d_model)\n",
        "        self.W_K = nn.Linear(d_model, d_model)\n",
        "        self.W_V = nn.Linear(d_model, d_model)\n",
        "        \n",
        "        # Scaled Dot-Product Attention\n",
        "        self.attention = ScaledDotProductAttention(dropout)\n",
        "        \n",
        "        # Output projection\n",
        "        self.W_O = nn.Linear(d_model, d_model)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def split_heads(self, x):\n",
        "        \"\"\"\n",
        "        Split last dimension into (n_heads, d_k)\n",
        "        \n",
        "        Args:\n",
        "            x: (batch_size, seq_len, d_model)\n",
        "        \n",
        "        Returns:\n",
        "            (batch_size, n_heads, seq_len, d_k)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, d_model = x.size()\n",
        "        return x.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "    \n",
        "    def combine_heads(self, x):\n",
        "        \"\"\"\n",
        "        Combine heads back\n",
        "        \n",
        "        Args:\n",
        "            x: (batch_size, n_heads, seq_len, d_k)\n",
        "        \n",
        "        Returns:\n",
        "            (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        batch_size, n_heads, seq_len, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "    \n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            Q, K, V: (batch_size, seq_len, d_model)\n",
        "            mask: Optional mask\n",
        "        \n",
        "        Returns:\n",
        "            output: (batch_size, seq_len, d_model)\n",
        "            attention_weights: (batch_size, n_heads, seq_len, seq_len)\n",
        "        \"\"\"\n",
        "        batch_size = Q.size(0)\n",
        "        \n",
        "        # 1. Linear projections\n",
        "        Q = self.W_Q(Q)  # (batch, seq_len, d_model)\n",
        "        K = self.W_K(K)\n",
        "        V = self.W_V(V)\n",
        "        \n",
        "        # 2. Split into multiple heads\n",
        "        Q = self.split_heads(Q)  # (batch, n_heads, seq_len, d_k)\n",
        "        K = self.split_heads(K)\n",
        "        V = self.split_heads(V)\n",
        "        \n",
        "        # 3. Apply attention on all heads in parallel\n",
        "        context, attention_weights = self.attention(Q, K, V, mask)\n",
        "        # context: (batch, n_heads, seq_len, d_k)\n",
        "        # attention_weights: (batch, n_heads, seq_len, seq_len)\n",
        "        \n",
        "        # 4. Concatenate heads\n",
        "        context = self.combine_heads(context)  # (batch, seq_len, d_model)\n",
        "        \n",
        "        # 5. Final linear projection\n",
        "        output = self.W_O(context)\n",
        "        output = self.dropout(output)\n",
        "        \n",
        "        return output, attention_weights\n",
        "\n",
        "print(\"‚úÖ MultiHeadAttention —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω!\")"
    ]
})

# ============================================================================
# MULTI-HEAD EXAMPLE
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 2.3 –ü—Ä–∏–º–µ—Ä: Multi-Head Attention –≤ –¥–µ–π—Å—Ç–≤–∏–∏"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
        "batch_size = 2\n",
        "seq_len = 6\n",
        "d_model = 64\n",
        "n_heads = 4\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π input\n",
        "torch.manual_seed(42)\n",
        "x = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"  batch_size={batch_size}, seq_len={seq_len}, d_model={d_model}\")\n",
        "print(f\"  n_heads={n_heads}, d_k per head={d_model // n_heads}\")\n",
        "\n",
        "# Multi-Head Attention\n",
        "mha = MultiHeadAttention(d_model, n_heads, dropout=0.0)\n",
        "output, attention_weights = mha(x, x, x)  # Self-Attention: Q=K=V=x\n",
        "\n",
        "print(f\"\\nOutput shape: {output.shape}\")\n",
        "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
        "print(f\"  {n_heads} heads, each with ({seq_len} x {seq_len}) attention matrix\")\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è attention weights –≤—Å–µ—Ö heads\n",
        "fig, axes = plt.subplots(1, n_heads, figsize=(16, 4))\n",
        "\n",
        "for i in range(n_heads):\n",
        "    weights = attention_weights[0, i].detach().numpy()  # –ø–µ—Ä–≤—ã–π sample, i-–π head\n",
        "    \n",
        "    sns.heatmap(weights, annot=True, fmt='.2f', cmap='YlOrRd', \n",
        "                ax=axes[i], cbar=False, square=True,\n",
        "                xticklabels=range(1, seq_len+1),\n",
        "                yticklabels=range(1, seq_len+1))\n",
        "    axes[i].set_title(f'Head {i+1}', fontsize=14, fontweight='bold')\n",
        "    axes[i].set_xlabel('Keys')\n",
        "    if i == 0:\n",
        "        axes[i].set_ylabel('Queries')\n",
        "\n",
        "plt.suptitle('Multi-Head Attention Weights (4 heads)', \n",
        "             fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä –ù–∞–±–ª—é–¥–µ–Ω–∏—è:\")\n",
        "print(\"  - –ö–∞–∂–¥—ã–π head —É—á–∏—Ç —Å–≤–æ–π –ø–∞—Ç—Ç–µ—Ä–Ω attention\")\n",
        "print(\"  - Heads –º–æ–≥—É—Ç focus –Ω–∞ —Ä–∞–∑–Ω—ã—Ö positions\")\n",
        "print(\"  - –ù–µ–∫–æ—Ç–æ—Ä—ã–µ heads –±–æ–ª–µ–µ \\\"diagonal\\\" (local), –¥—Ä—É–≥–∏–µ –±–æ–ª–µ–µ \\\"distributed\\\" (global)\")"
    ]
})

# ============================================================================
# POSITIONAL ENCODING THEORY
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "---\n",
        "\n",
        "## üìç –ß–∞—Å—Ç—å 3: Positional Encoding\n",
        "\n",
        "### 3.1 –¢–µ–æ—Ä–∏—è: –ü—Ä–æ–±–ª–µ–º–∞ Permutation Invariance\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞\n",
        "\n",
        "**Self-Attention permutation invariant:**\n",
        "\n",
        "$$\\text{Attention}([x_1, x_2, x_3]) = \\text{Attention}([x_3, x_1, x_2])$$\n",
        "\n",
        "**–ü–æ—á–µ–º—É —ç—Ç–æ –ø—Ä–æ–±–ª–µ–º–∞?**\n",
        "- –ü–æ—Ä—è–¥–æ–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤–∞–∂–µ–Ω! (–æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è sequences)\n",
        "- \"cat sat on mat\" ‚â† \"mat on sat cat\"\n",
        "- –ú–æ–¥–µ–ª—å –Ω–µ –∑–Ω–∞–µ—Ç position –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞\n",
        "\n",
        "**–†–µ—à–µ–Ω–∏–µ: Positional Encoding!**\n",
        "\n",
        "---\n",
        "\n",
        "## üìê Sinusoidal Positional Encoding (–¥–ª—è sequences)\n",
        "\n",
        "**–ò–¥–µ—è:** –î–æ–±–∞–≤–∏—Ç—å –∫ –∫–∞–∂–¥–æ–º—É —ç–ª–µ–º–µ–Ω—Ç—É —É–Ω–∏–∫–∞–ª—å–Ω—ã–π positional vector.\n",
        "\n",
        "$$\\text{PE}(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
        "\n",
        "$$\\text{PE}(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
        "\n",
        "–ì–¥–µ:\n",
        "- $pos$: –ø–æ–∑–∏—Ü–∏—è –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (0, 1, 2, ...)\n",
        "- $i$: dimension index (0, 1, ..., $d_{model}/2$)\n",
        "- Even dimensions: sine\n",
        "- Odd dimensions: cosine\n",
        "\n",
        "**–°–≤–æ–π—Å—Ç–≤–∞:**\n",
        "1. **Unique**: –∫–∞–∂–¥–∞—è –ø–æ–∑–∏—Ü–∏—è ‚Üí —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –≤–µ–∫—Ç–æ—Ä\n",
        "2. **Bounded**: –∑–Ω–∞—á–µ–Ω–∏—è –≤ $[-1, 1]$\n",
        "3. **Relative positions**: $\\text{PE}(pos+k)$ - linear function of $\\text{PE}(pos)$\n",
        "4. **Extrapolation**: —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è –ø–æ–∑–∏—Ü–∏–π, –Ω–µ –≤–∏–¥–µ–Ω–Ω—ã—Ö –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏\n",
        "\n",
        "**–ü–æ—á–µ–º—É sin/cos?**\n",
        "- –†–∞–∑–Ω—ã–µ —á–∞—Å—Ç–æ—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö dimensions\n",
        "- Low dimensions: –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∫–æ–ª–µ–±–∞–Ω–∏—è (long-range)\n",
        "- High dimensions: –±—ã—Å—Ç—Ä—ã–µ –∫–æ–ª–µ–±–∞–Ω–∏—è (short-range)\n",
        "- –ú–æ–¥–µ–ª—å —Å–∞–º–∞ –≤—ã–±–∏—Ä–∞–µ—Ç, –∫–∞–∫–∏–µ —á–∞—Å—Ç–æ—Ç—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å\n",
        "\n",
        "---\n",
        "\n",
        "## üóÇÔ∏è Learnable Positional Embeddings (–¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö)\n",
        "\n",
        "**–î–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:**\n",
        "- –ù–µ—Ç \"–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ\" –ø–æ—Ä—è–¥–∫–∞ features\n",
        "- –ú–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å **learnable embeddings**:\n",
        "\n",
        "$$\\text{PE}_i \\in \\mathbb{R}^{d_{model}}$$\n",
        "\n",
        "- –ü—Ä–æ—Å—Ç–æ learnable parameters –¥–ª—è –∫–∞–∂–¥–æ–π feature position\n",
        "- –ë–æ–ª–µ–µ –≥–∏–±–∫–æ, –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ –¥–∞–Ω–Ω—ã–º\n",
        "\n",
        "**Final Input:**\n",
        "\n",
        "$$X_{final} = X_{embedded} + \\text{PE}$$\n",
        "\n",
        "---\n"
    ]
})

# ============================================================================
# POSITIONAL ENCODING IMPLEMENTATION
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 3.2 Implementation: Positional Encoding"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Sinusoidal Positional Encoding for Sequences\n",
        "    \n",
        "    PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
        "    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            d_model: Model dimension\n",
        "            max_len: Maximum sequence length\n",
        "            dropout: Dropout rate\n",
        "        \"\"\"\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        # –°–æ–∑–¥–∞–µ–º positional encoding matrix\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        \n",
        "        # Compute div_term: 10000^(2i/d_model)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
        "                             (-math.log(10000.0) / d_model))\n",
        "        \n",
        "        # Apply sin to even indices\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        \n",
        "        # Apply cos to odd indices\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        \n",
        "        # Add batch dimension\n",
        "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
        "        \n",
        "        # Register as buffer (not a parameter, but part of module state)\n",
        "        self.register_buffer('pe', pe)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch_size, seq_len, d_model)\n",
        "        \n",
        "        Returns:\n",
        "            x + positional_encoding: (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class LearnablePositionalEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Learnable Positional Embeddings for Tabular Data\n",
        "    \"\"\"\n",
        "    def __init__(self, num_features, d_model, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_features: Number of features (sequence length)\n",
        "            d_model: Model dimension\n",
        "            dropout: Dropout rate\n",
        "        \"\"\"\n",
        "        super(LearnablePositionalEmbedding, self).__init__()\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_features, d_model))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch_size, num_features, d_model)\n",
        "        \n",
        "        Returns:\n",
        "            x + positional_embedding: (batch_size, num_features, d_model)\n",
        "        \"\"\"\n",
        "        x = x + self.pos_embedding\n",
        "        return self.dropout(x)\n",
        "\n",
        "print(\"‚úÖ Positional Encoding —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω (Sinusoidal –∏ Learnable)!\")"
    ]
})

# ============================================================================
# POSITIONAL ENCODING VISUALIZATION
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 3.3 –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è Positional Encoding"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –°–æ–∑–¥–∞–µ–º positional encoding\n",
        "d_model = 64\n",
        "max_len = 100\n",
        "\n",
        "pe_layer = PositionalEncoding(d_model, max_len, dropout=0.0)\n",
        "pe_matrix = pe_layer.pe[0].numpy()  # (max_len, d_model)\n",
        "\n",
        "print(f\"Positional Encoding shape: {pe_matrix.shape}\")\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è 1: Heatmap\n",
        "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
        "\n",
        "# Heatmap –≤—Å–µ—Ö –ø–æ–∑–∏—Ü–∏–π –∏ dimensions\n",
        "im = axes[0].imshow(pe_matrix.T, cmap='RdBu', aspect='auto', interpolation='nearest')\n",
        "axes[0].set_title('Positional Encoding Matrix', fontsize=16, fontweight='bold')\n",
        "axes[0].set_xlabel('Position', fontsize=12)\n",
        "axes[0].set_ylabel('Dimension', fontsize=12)\n",
        "plt.colorbar(im, ax=axes[0], label='Value')\n",
        "\n",
        "# –ù–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –ø–æ–∑–∏—Ü–∏–π\n",
        "positions_to_plot = [0, 10, 20, 40, 80]\n",
        "for pos in positions_to_plot:\n",
        "    axes[1].plot(pe_matrix[pos], label=f'Position {pos}', alpha=0.7)\n",
        "\n",
        "axes[1].set_title('Positional Encodings for Different Positions', \n",
        "                  fontsize=16, fontweight='bold')\n",
        "axes[1].set_xlabel('Dimension', fontsize=12)\n",
        "axes[1].set_ylabel('Value', fontsize=12)\n",
        "axes[1].legend()\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä –ù–∞–±–ª—é–¥–µ–Ω–∏—è:\")\n",
        "print(\"  - –ö–∞–∂–¥–∞—è –ø–æ–∑–∏—Ü–∏—è –∏–º–µ–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω\")\n",
        "print(\"  - Low dimensions: –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∫–æ–ª–µ–±–∞–Ω–∏—è (–≤–∏–¥–Ω—ã –ø–æ–ª–æ—Å—ã –Ω–∞ heatmap)\")\n",
        "print(\"  - High dimensions: –±—ã—Å—Ç—Ä—ã–µ –∫–æ–ª–µ–±–∞–Ω–∏—è (–±–æ–ª–µ–µ –º–µ–ª–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞)\")\n",
        "print(\"  - –ú–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è relative positions\")"
    ]
})

# –°–æ—Ö—Ä–∞–Ω—è–µ–º
notebook['cells'] = cells

with open(notebook_path, 'w', encoding='utf-8') as f:
    json.dump(notebook, f, ensure_ascii=False, indent=1)

print(f'‚úÖ Part 2 –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤: {notebook_path}')
print(f'–í—Å–µ–≥–æ —è—á–µ–µ–∫: {len(cells)}')
print('–°–ª–µ–¥—É—é—â–∞—è —á–∞—Å—Ç—å: Transformer Encoder –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫ Titanic...')

#!/usr/bin/env python3
"""
Phase 7: Production & MLOps - Part 3
Docker Containerization & Monitoring
"""

import json

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π notebook
notebook_path = '/home/user/test/notebooks/phase7_production_mlops/01_production_mlops.ipynb'
with open(notebook_path, 'r', encoding='utf-8') as f:
    notebook = json.load(f)

cells = notebook['cells']

# ============================================================================
# DOCKER CONTAINERIZATION
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "---\n",
        "\n",
        "## üê≥ –ß–∞—Å—Ç—å 5: Docker Containerization\n",
        "\n",
        "### –ü–æ—á–µ–º—É Docker –¥–ª—è ML?\n",
        "\n",
        "**–ü—Ä–æ–±–ª–µ–º—ã –±–µ–∑ Docker:**\n",
        "```\n",
        "Data Scientist: \"–†–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –º–æ–µ–π –º–∞—à–∏–Ω–µ!\" üñ•Ô∏è\n",
        "DevOps: \"–ù–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ production\" üò§\n",
        "Root cause: Python 3.8 vs 3.10, numpy 1.20 vs 1.24, etc.\n",
        "```\n",
        "\n",
        "**Docker —Ä–µ—à–∞–µ—Ç:**\n",
        "- ‚úÖ **Reproducibility:** –û–¥–∏–Ω–∞–∫–æ–≤–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ –≤–µ–∑–¥–µ\n",
        "- ‚úÖ **Isolation:** –ù–µ—Ç –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π\n",
        "- ‚úÖ **Portability:** –ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –Ω–∞ –ª—é–±–æ–º —Ö–æ—Å—Ç–µ —Å Docker\n",
        "- ‚úÖ **Scalability:** –õ–µ–≥–∫–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å (Kubernetes)\n",
        "- ‚úÖ **CI/CD:** Automated builds –∏ deployments\n",
        "\n",
        "### Docker Architecture –¥–ª—è ML:\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ          Docker Container               ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ  Application Layer                      ‚îÇ\n",
        "‚îÇ  ‚îú‚îÄ‚îÄ app.py (FastAPI)                   ‚îÇ\n",
        "‚îÇ  ‚îú‚îÄ‚îÄ models/ (serialized models)        ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ requirements.txt                   ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ  Python Runtime                         ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ Python 3.9 + dependencies          ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ  Base Image                             ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ python:3.9-slim                    ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "---\n"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Dockerfile\n",
        "dockerfile_content = '''# Churn Prediction API - Production Dockerfile\n",
        "# Multi-stage build for smaller image size\n",
        "\n",
        "# ========================================\n",
        "# Stage 1: Build dependencies\n",
        "# ========================================\n",
        "FROM python:3.9-slim as builder\n",
        "\n",
        "# Set environment variables\n",
        "ENV PYTHONDONTWRITEBYTECODE=1\n",
        "ENV PYTHONUNBUFFERED=1\n",
        "\n",
        "# Install build dependencies\n",
        "RUN apt-get update && apt-get install -y --no-install-recommends \\\\\n",
        "    build-essential \\\\\n",
        "    && rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "# Create virtual environment\n",
        "RUN python -m venv /opt/venv\n",
        "ENV PATH=\"/opt/venv/bin:$PATH\"\n",
        "\n",
        "# Install Python dependencies\n",
        "COPY requirements.txt .\n",
        "RUN pip install --no-cache-dir --upgrade pip && \\\\\n",
        "    pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# ========================================\n",
        "# Stage 2: Production image\n",
        "# ========================================\n",
        "FROM python:3.9-slim as production\n",
        "\n",
        "# Set environment variables\n",
        "ENV PYTHONDONTWRITEBYTECODE=1\n",
        "ENV PYTHONUNBUFFERED=1\n",
        "ENV PORT=8000\n",
        "\n",
        "# Create non-root user for security\n",
        "RUN groupadd -r mluser && useradd -r -g mluser mluser\n",
        "\n",
        "# Copy virtual environment from builder\n",
        "COPY --from=builder /opt/venv /opt/venv\n",
        "ENV PATH=\"/opt/venv/bin:$PATH\"\n",
        "\n",
        "# Set working directory\n",
        "WORKDIR /app\n",
        "\n",
        "# Copy application code\n",
        "COPY app.py .\n",
        "COPY models/ ./models/\n",
        "\n",
        "# Change ownership to non-root user\n",
        "RUN chown -R mluser:mluser /app\n",
        "\n",
        "# Switch to non-root user\n",
        "USER mluser\n",
        "\n",
        "# Expose port\n",
        "EXPOSE $PORT\n",
        "\n",
        "# Health check\n",
        "HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \\\\\n",
        "    CMD curl -f http://localhost:$PORT/health || exit 1\n",
        "\n",
        "# Run application\n",
        "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
        "'''\n",
        "\n",
        "# Save Dockerfile\n",
        "with open('./Dockerfile', 'w') as f:\n",
        "    f.write(dockerfile_content)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"DOCKERFILE\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\n‚úÖ Dockerfile saved\")\n",
        "print(\"\\nKey features:\")\n",
        "print(\"  - Multi-stage build (smaller image)\")\n",
        "print(\"  - Non-root user (security)\")\n",
        "print(\"  - Health check\")\n",
        "print(\"  - Optimized layer caching\")\n"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# requirements.txt\n",
        "requirements_content = '''# Core ML\n",
        "numpy==1.24.3\n",
        "pandas==2.0.3\n",
        "scikit-learn==1.3.0\n",
        "joblib==1.3.1\n",
        "\n",
        "# API\n",
        "fastapi==0.100.0\n",
        "uvicorn[standard]==0.23.0\n",
        "pydantic==2.0.3\n",
        "\n",
        "# Monitoring\n",
        "prometheus-client==0.17.1\n",
        "\n",
        "# Utilities\n",
        "python-multipart==0.0.6\n",
        "python-json-logger==2.0.7\n",
        "'''\n",
        "\n",
        "with open('./requirements.txt', 'w') as f:\n",
        "    f.write(requirements_content)\n",
        "\n",
        "print(\"‚úÖ requirements.txt saved\")\n",
        "\n",
        "# docker-compose.yml\n",
        "docker_compose_content = '''version: '3.8'\n",
        "\n",
        "services:\n",
        "  churn-api:\n",
        "    build:\n",
        "      context: .\n",
        "      dockerfile: Dockerfile\n",
        "    container_name: churn-prediction-api\n",
        "    ports:\n",
        "      - \"8000:8000\"\n",
        "    environment:\n",
        "      - MODEL_VERSION=1.0.0\n",
        "      - LOG_LEVEL=INFO\n",
        "    volumes:\n",
        "      - ./models:/app/models:ro\n",
        "    healthcheck:\n",
        "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n",
        "      interval: 30s\n",
        "      timeout: 10s\n",
        "      retries: 3\n",
        "      start_period: 10s\n",
        "    restart: unless-stopped\n",
        "    \n",
        "  # Optional: Prometheus for monitoring\n",
        "  prometheus:\n",
        "    image: prom/prometheus:latest\n",
        "    container_name: prometheus\n",
        "    ports:\n",
        "      - \"9090:9090\"\n",
        "    volumes:\n",
        "      - ./prometheus.yml:/etc/prometheus/prometheus.yml\n",
        "    depends_on:\n",
        "      - churn-api\n",
        "      \n",
        "  # Optional: Grafana for dashboards\n",
        "  grafana:\n",
        "    image: grafana/grafana:latest\n",
        "    container_name: grafana\n",
        "    ports:\n",
        "      - \"3000:3000\"\n",
        "    environment:\n",
        "      - GF_SECURITY_ADMIN_PASSWORD=admin\n",
        "    depends_on:\n",
        "      - prometheus\n",
        "'''\n",
        "\n",
        "with open('./docker-compose.yml', 'w') as f:\n",
        "    f.write(docker_compose_content)\n",
        "\n",
        "print(\"‚úÖ docker-compose.yml saved\")\n",
        "\n",
        "# .dockerignore\n",
        "dockerignore_content = '''# Python\n",
        "__pycache__/\n",
        "*.py[cod]\n",
        "*$py.class\n",
        "*.so\n",
        ".Python\n",
        "venv/\n",
        "env/\n",
        ".venv/\n",
        "\n",
        "# Jupyter\n",
        ".ipynb_checkpoints/\n",
        "*.ipynb\n",
        "\n",
        "# Git\n",
        ".git/\n",
        ".gitignore\n",
        "\n",
        "# IDE\n",
        ".idea/\n",
        ".vscode/\n",
        "\n",
        "# Testing\n",
        "tests/\n",
        "*.test.py\n",
        "\n",
        "# Documentation\n",
        "docs/\n",
        "*.md\n",
        "README*\n",
        "\n",
        "# MLflow\n",
        "mlruns/\n",
        "mlartifacts/\n",
        "\n",
        "# Misc\n",
        ".DS_Store\n",
        "*.log\n",
        "'''\n",
        "\n",
        "with open('./.dockerignore', 'w') as f:\n",
        "    f.write(dockerignore_content)\n",
        "\n",
        "print(\"‚úÖ .dockerignore saved\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"DOCKER COMMANDS\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nBuild image:\")\n",
        "print(\"  docker build -t churn-api:latest .\")\n",
        "print(\"\\nRun container:\")\n",
        "print(\"  docker run -p 8000:8000 churn-api:latest\")\n",
        "print(\"\\nDocker Compose (with monitoring):\")\n",
        "print(\"  docker-compose up -d\")\n",
        "print(\"\\nView logs:\")\n",
        "print(\"  docker logs churn-prediction-api\")\n"
    ]
})

# ============================================================================
# MONITORING & DRIFT DETECTION
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "---\n",
        "\n",
        "## üìä –ß–∞—Å—Ç—å 6: Monitoring & Drift Detection\n",
        "\n",
        "### –ü–æ—á–µ–º—É Monitoring –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–µ–Ω?\n",
        "\n",
        "**ML —Å–∏—Å—Ç–µ–º—ã –¥–µ–≥—Ä–∞–¥–∏—Ä—É—é—Ç —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º:**\n",
        "\n",
        "1. **Data Drift:** –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –º–µ–Ω—è–µ—Ç—Å—è\n",
        "   - –ù–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –ø–æ–≤–µ–¥–µ–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
        "   - –°–µ–∑–æ–Ω–Ω–æ—Å—Ç—å\n",
        "   - –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ä—ã–Ω–∫–∞\n",
        "\n",
        "2. **Concept Drift:** –°–≤—è–∑—å –º–µ–∂–¥—É features –∏ target –º–µ–Ω—è–µ—Ç—Å—è\n",
        "   - –¢–æ, —á—Ç–æ —Ä–∞–Ω—å—à–µ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–ª–æ churn, –±–æ–ª—å—à–µ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç\n",
        "   - –ú–æ–¥–µ–ª—å —É—Å—Ç–∞—Ä–µ–ª–∞\n",
        "\n",
        "3. **Model Degradation:** Accuracy –ø–∞–¥–∞–µ—Ç —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º\n",
        "   - –î–∞–∂–µ –±–µ–∑ drift, –º–∏—Ä –º–µ–Ω—è–µ—Ç—Å—è\n",
        "   - –ù—É–∂–Ω–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ\n",
        "\n",
        "### –¢–∏–ø—ã –º–µ—Ç—Ä–∏–∫ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:\n",
        "\n",
        "#### 1. **Input Data Metrics**\n",
        "- Feature distributions (mean, std, min, max)\n",
        "- Missing values\n",
        "- Out-of-range values\n",
        "- Feature correlations\n",
        "\n",
        "#### 2. **Prediction Metrics**\n",
        "- Prediction distribution\n",
        "- Prediction confidence\n",
        "- Prediction latency\n",
        "\n",
        "#### 3. **Model Performance** (when labels available)\n",
        "- Accuracy, Precision, Recall, F1\n",
        "- ROC AUC\n",
        "- Confusion matrix\n",
        "\n",
        "#### 4. **Business Metrics**\n",
        "- False positive cost\n",
        "- Revenue impact\n",
        "- Customer satisfaction\n",
        "\n",
        "---\n"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "print(\"=\" * 70)\n",
        "print(\"DATA DRIFT DETECTION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# –°–æ–∑–¥–∞–¥–∏–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ \"production\" –¥–∞–Ω–Ω—ã–µ —Å drift\n",
        "def create_drifted_data(n_samples: int = 1000, drift_level: float = 0.3) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Create data with artificial drift.\n",
        "    \n",
        "    Args:\n",
        "        n_samples: number of samples\n",
        "        drift_level: intensity of drift (0=no drift, 1=full drift)\n",
        "    \"\"\"\n",
        "    np.random.seed(123)  # Different seed for production data\n",
        "    \n",
        "    # Apply drift to numerical features\n",
        "    tenure = np.random.randint(1, 72, n_samples)\n",
        "    \n",
        "    # Drift: monthly charges increased (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∏–Ω—Ñ–ª—è—Ü–∏—è)\n",
        "    monthly_charges = np.random.uniform(\n",
        "        20 + 15 * drift_level,  # higher minimum\n",
        "        100 + 30 * drift_level,  # higher maximum\n",
        "        n_samples\n",
        "    )\n",
        "    \n",
        "    total_charges = tenure * monthly_charges + np.random.normal(0, 100, n_samples)\n",
        "    total_charges = np.maximum(total_charges, 0)\n",
        "    \n",
        "    # Drift: more support tickets (—É—Ö—É–¥—à–∏–ª–æ—Å—å –∫–∞—á–µ—Å—Ç–≤–æ —É—Å–ª—É–≥)\n",
        "    num_tickets = np.random.poisson(2 + 2 * drift_level, n_samples)\n",
        "    \n",
        "    # Drift in categorical: more month-to-month contracts\n",
        "    contract_probs = [\n",
        "        0.5 + 0.2 * drift_level,  # more month-to-month\n",
        "        0.3 - 0.1 * drift_level,  # less one year\n",
        "        0.2 - 0.1 * drift_level   # less two year\n",
        "    ]\n",
        "    contract_probs = np.clip(contract_probs, 0.01, 0.99)\n",
        "    contract_probs = contract_probs / sum(contract_probs)\n",
        "    \n",
        "    contract_type = np.random.choice(\n",
        "        ['Month-to-month', 'One year', 'Two year'],\n",
        "        n_samples, p=contract_probs\n",
        "    )\n",
        "    \n",
        "    # Rest same as training\n",
        "    payment_method = np.random.choice(\n",
        "        ['Credit card', 'Bank transfer', 'Electronic check'],\n",
        "        n_samples, p=[0.4, 0.3, 0.3]\n",
        "    )\n",
        "    internet_service = np.random.choice(\n",
        "        ['DSL', 'Fiber optic', 'No'],\n",
        "        n_samples, p=[0.4, 0.4, 0.2]\n",
        "    )\n",
        "    online_security = np.random.choice(['Yes', 'No'], n_samples, p=[0.4, 0.6])\n",
        "    tech_support = np.random.choice(['Yes', 'No'], n_samples, p=[0.3, 0.7])\n",
        "    senior_citizen = np.random.choice([0, 1], n_samples, p=[0.84, 0.16])\n",
        "    \n",
        "    df = pd.DataFrame({\n",
        "        'tenure': tenure,\n",
        "        'monthly_charges': monthly_charges,\n",
        "        'total_charges': total_charges,\n",
        "        'num_tickets': num_tickets,\n",
        "        'contract_type': contract_type,\n",
        "        'payment_method': payment_method,\n",
        "        'internet_service': internet_service,\n",
        "        'online_security': online_security,\n",
        "        'tech_support': tech_support,\n",
        "        'senior_citizen': senior_citizen,\n",
        "    })\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Create production data with drift\n",
        "production_data = create_drifted_data(1000, drift_level=0.4)\n",
        "\n",
        "print(\"\\n‚úÖ Production data with drift created\")\n",
        "print(f\"Shape: {production_data.shape}\")\n"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "from scipy import stats\n",
        "\n",
        "def detect_numerical_drift(\n",
        "    reference: pd.Series,\n",
        "    current: pd.Series,\n",
        "    threshold: float = 0.05\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Detect drift in numerical feature using KS test.\n",
        "    \n",
        "    Kolmogorov-Smirnov test:\n",
        "    - H0: distributions are the same\n",
        "    - p-value < threshold ‚Üí drift detected\n",
        "    \"\"\"\n",
        "    statistic, p_value = stats.ks_2samp(reference, current)\n",
        "    \n",
        "    drift_detected = p_value < threshold\n",
        "    \n",
        "    return {\n",
        "        'ks_statistic': statistic,\n",
        "        'p_value': p_value,\n",
        "        'drift_detected': drift_detected,\n",
        "        'ref_mean': reference.mean(),\n",
        "        'ref_std': reference.std(),\n",
        "        'cur_mean': current.mean(),\n",
        "        'cur_std': current.std(),\n",
        "        'mean_shift': (current.mean() - reference.mean()) / reference.std()\n",
        "    }\n",
        "\n",
        "def detect_categorical_drift(\n",
        "    reference: pd.Series,\n",
        "    current: pd.Series,\n",
        "    threshold: float = 0.05\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Detect drift in categorical feature using Chi-squared test.\n",
        "    \"\"\"\n",
        "    # Get value counts\n",
        "    ref_counts = reference.value_counts(normalize=True)\n",
        "    cur_counts = current.value_counts(normalize=True)\n",
        "    \n",
        "    # Align categories\n",
        "    all_categories = set(ref_counts.index) | set(cur_counts.index)\n",
        "    ref_aligned = np.array([ref_counts.get(cat, 0) for cat in all_categories])\n",
        "    cur_aligned = np.array([cur_counts.get(cat, 0) for cat in all_categories])\n",
        "    \n",
        "    # Chi-squared test (need counts, not proportions)\n",
        "    ref_counts_abs = ref_aligned * len(reference)\n",
        "    cur_counts_abs = cur_aligned * len(current)\n",
        "    \n",
        "    # Add small value to avoid division by zero\n",
        "    ref_counts_abs = ref_counts_abs + 1e-10\n",
        "    \n",
        "    statistic, p_value = stats.chisquare(cur_counts_abs, ref_counts_abs)\n",
        "    \n",
        "    drift_detected = p_value < threshold\n",
        "    \n",
        "    return {\n",
        "        'chi2_statistic': statistic,\n",
        "        'p_value': p_value,\n",
        "        'drift_detected': drift_detected,\n",
        "        'ref_distribution': ref_counts.to_dict(),\n",
        "        'cur_distribution': cur_counts.to_dict()\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Drift detection functions defined\")\n"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Run drift detection\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"DRIFT DETECTION RESULTS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Reference data (training)\n",
        "reference_data = df[df.columns.drop('churn')].copy()\n",
        "\n",
        "drift_report = {}\n",
        "\n",
        "# Numerical features\n",
        "print(\"\\nüìä Numerical Features:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for col in numerical_cols:\n",
        "    result = detect_numerical_drift(\n",
        "        reference_data[col],\n",
        "        production_data[col]\n",
        "    )\n",
        "    drift_report[col] = result\n",
        "    \n",
        "    status = \"‚ö†Ô∏è DRIFT\" if result['drift_detected'] else \"‚úÖ OK\"\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(f\"  Status: {status}\")\n",
        "    print(f\"  KS Statistic: {result['ks_statistic']:.4f}\")\n",
        "    print(f\"  P-value: {result['p_value']:.4f}\")\n",
        "    print(f\"  Mean shift: {result['mean_shift']:.2f} std\")\n",
        "    print(f\"  Ref: mean={result['ref_mean']:.2f}, std={result['ref_std']:.2f}\")\n",
        "    print(f\"  Cur: mean={result['cur_mean']:.2f}, std={result['cur_std']:.2f}\")\n",
        "\n",
        "# Categorical features\n",
        "print(\"\\nüìä Categorical Features:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for col in ['contract_type', 'payment_method', 'internet_service']:\n",
        "    result = detect_categorical_drift(\n",
        "        reference_data[col],\n",
        "        production_data[col]\n",
        "    )\n",
        "    drift_report[col] = result\n",
        "    \n",
        "    status = \"‚ö†Ô∏è DRIFT\" if result['drift_detected'] else \"‚úÖ OK\"\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(f\"  Status: {status}\")\n",
        "    print(f\"  Chi2 Statistic: {result['chi2_statistic']:.4f}\")\n",
        "    print(f\"  P-value: {result['p_value']:.4f}\")\n"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è drift\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# 1. monthly_charges distribution\n",
        "axes[0, 0].hist(reference_data['monthly_charges'], bins=30, alpha=0.5, label='Reference', color='blue')\n",
        "axes[0, 0].hist(production_data['monthly_charges'], bins=30, alpha=0.5, label='Production', color='red')\n",
        "axes[0, 0].set_title('Monthly Charges Distribution', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Monthly Charges')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].legend()\n",
        "if drift_report['monthly_charges']['drift_detected']:\n",
        "    axes[0, 0].text(0.05, 0.95, '‚ö†Ô∏è DRIFT DETECTED', transform=axes[0, 0].transAxes,\n",
        "                   fontsize=10, color='red', fontweight='bold', verticalalignment='top')\n",
        "\n",
        "# 2. num_tickets distribution\n",
        "axes[0, 1].hist(reference_data['num_tickets'], bins=15, alpha=0.5, label='Reference', color='blue')\n",
        "axes[0, 1].hist(production_data['num_tickets'], bins=15, alpha=0.5, label='Production', color='red')\n",
        "axes[0, 1].set_title('Number of Tickets Distribution', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Number of Tickets')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].legend()\n",
        "if drift_report['num_tickets']['drift_detected']:\n",
        "    axes[0, 1].text(0.05, 0.95, '‚ö†Ô∏è DRIFT DETECTED', transform=axes[0, 1].transAxes,\n",
        "                   fontsize=10, color='red', fontweight='bold', verticalalignment='top')\n",
        "\n",
        "# 3. contract_type comparison\n",
        "contract_ref = reference_data['contract_type'].value_counts(normalize=True)\n",
        "contract_prod = production_data['contract_type'].value_counts(normalize=True)\n",
        "\n",
        "x = np.arange(len(contract_ref))\n",
        "width = 0.35\n",
        "axes[1, 0].bar(x - width/2, [contract_ref.get(c, 0) for c in contract_ref.index], width, label='Reference', alpha=0.8)\n",
        "axes[1, 0].bar(x + width/2, [contract_prod.get(c, 0) for c in contract_ref.index], width, label='Production', alpha=0.8)\n",
        "axes[1, 0].set_xticks(x)\n",
        "axes[1, 0].set_xticklabels(contract_ref.index, rotation=45)\n",
        "axes[1, 0].set_title('Contract Type Distribution', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_ylabel('Proportion')\n",
        "axes[1, 0].legend()\n",
        "if drift_report['contract_type']['drift_detected']:\n",
        "    axes[1, 0].text(0.05, 0.95, '‚ö†Ô∏è DRIFT DETECTED', transform=axes[1, 0].transAxes,\n",
        "                   fontsize=10, color='red', fontweight='bold', verticalalignment='top')\n",
        "\n",
        "# 4. Summary\n",
        "drift_summary = []\n",
        "for feature, result in drift_report.items():\n",
        "    if feature in numerical_cols:\n",
        "        drift_summary.append({\n",
        "            'Feature': feature,\n",
        "            'Type': 'Numerical',\n",
        "            'Drift': '‚ö†Ô∏è Yes' if result['drift_detected'] else '‚úÖ No',\n",
        "            'P-value': result['p_value']\n",
        "        })\n",
        "    else:\n",
        "        drift_summary.append({\n",
        "            'Feature': feature,\n",
        "            'Type': 'Categorical',\n",
        "            'Drift': '‚ö†Ô∏è Yes' if result['drift_detected'] else '‚úÖ No',\n",
        "            'P-value': result['p_value']\n",
        "        })\n",
        "\n",
        "summary_df = pd.DataFrame(drift_summary)\n",
        "axes[1, 1].axis('off')\n",
        "table = axes[1, 1].table(\n",
        "    cellText=summary_df.values,\n",
        "    colLabels=summary_df.columns,\n",
        "    cellLoc='center',\n",
        "    loc='center',\n",
        "    colColours=['#f0f0f0'] * 4\n",
        ")\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(10)\n",
        "table.scale(1.2, 1.5)\n",
        "axes[1, 1].set_title('Drift Summary', fontsize=12, fontweight='bold', pad=20)\n",
        "\n",
        "plt.suptitle('Data Drift Detection: Reference vs Production', fontsize=14, fontweight='bold', y=1.00)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Count drifted features\n",
        "n_drifted = sum(1 for r in drift_report.values() if r['drift_detected'])\n",
        "print(f\"\\nüîç Summary: {n_drifted}/{len(drift_report)} features show significant drift\")\n",
        "if n_drifted > len(drift_report) // 2:\n",
        "    print(\"\\n‚ö†Ô∏è WARNING: Significant data drift detected!\")\n",
        "    print(\"   Consider retraining the model with recent data.\")\n"
    ]
})

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 6.1 Model Performance Monitoring\n",
        "\n",
        "–ö–æ–≥–¥–∞ –ø–æ—è–≤–ª—è—é—Ç—Å—è ground truth labels, –º–æ–Ω–∏—Ç–æ—Ä–∏–º performance."
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"MODEL PERFORMANCE MONITORING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Simulate predictions on production data\n",
        "# Preprocess production data\n",
        "prod_processed = production_data.copy()\n",
        "prod_processed['senior_citizen'] = prod_processed['senior_citizen'].astype(str)\n",
        "\n",
        "for col in categorical_cols:\n",
        "    if col in label_encoders:\n",
        "        prod_processed[col] = label_encoders[col].transform(prod_processed[col])\n",
        "\n",
        "X_prod = prod_processed[feature_cols].copy()\n",
        "X_prod[numerical_cols] = scaler.transform(X_prod[numerical_cols])\n",
        "\n",
        "# Make predictions\n",
        "prod_predictions = best_model.predict(X_prod)\n",
        "prod_probabilities = best_model.predict_proba(X_prod)[:, 1]\n",
        "\n",
        "# Simulate ground truth (with concept drift - higher actual churn rate)\n",
        "# In reality, this comes from actual customer behavior\n",
        "actual_churn_prob = (\n",
        "    0.15 +  # higher baseline (was 0.1)\n",
        "    (production_data['tenure'] < 12).astype(float) * 0.2 +  # stronger effect\n",
        "    (production_data['monthly_charges'] > 70).astype(float) * 0.15 +\n",
        "    (production_data['contract_type'] == 'Month-to-month').astype(float) * 0.25\n",
        ")\n",
        "actual_churn_prob = np.clip(actual_churn_prob, 0, 0.95)\n",
        "prod_actual = (np.random.random(len(production_data)) < actual_churn_prob).astype(int)\n",
        "\n",
        "# Calculate metrics\n",
        "prod_accuracy = accuracy_score(prod_actual, prod_predictions)\n",
        "prod_precision = precision_score(prod_actual, prod_predictions)\n",
        "prod_recall = recall_score(prod_actual, prod_predictions)\n",
        "prod_f1 = f1_score(prod_actual, prod_predictions)\n",
        "prod_auc = roc_auc_score(prod_actual, prod_probabilities)\n",
        "\n",
        "# Compare with training metrics\n",
        "print(\"\\nüìä Performance Comparison: Training vs Production\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "train_metrics = {\n",
        "    'Accuracy': results['GradientBoosting']['accuracy'],\n",
        "    'Precision': results['GradientBoosting']['precision'],\n",
        "    'Recall': results['GradientBoosting']['recall'],\n",
        "    'F1': results['GradientBoosting']['f1'],\n",
        "    'ROC AUC': results['GradientBoosting']['roc_auc']\n",
        "}\n",
        "\n",
        "prod_metrics = {\n",
        "    'Accuracy': prod_accuracy,\n",
        "    'Precision': prod_precision,\n",
        "    'Recall': prod_recall,\n",
        "    'F1': prod_f1,\n",
        "    'ROC AUC': prod_auc\n",
        "}\n",
        "\n",
        "comparison_data = []\n",
        "for metric in train_metrics.keys():\n",
        "    train_val = train_metrics[metric]\n",
        "    prod_val = prod_metrics[metric]\n",
        "    diff = prod_val - train_val\n",
        "    pct_change = (diff / train_val) * 100\n",
        "    \n",
        "    status = \"‚ö†Ô∏è\" if abs(pct_change) > 5 else \"‚úÖ\"\n",
        "    \n",
        "    comparison_data.append({\n",
        "        'Metric': metric,\n",
        "        'Training': f\"{train_val:.4f}\",\n",
        "        'Production': f\"{prod_val:.4f}\",\n",
        "        'Change': f\"{pct_change:+.1f}%\",\n",
        "        'Status': status\n",
        "    })\n",
        "    \n",
        "    print(f\"{metric}:\")\n",
        "    print(f\"  Training:   {train_val:.4f}\")\n",
        "    print(f\"  Production: {prod_val:.4f}\")\n",
        "    print(f\"  Change:     {pct_change:+.1f}% {status}\")\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(\"\\n\" + comparison_df.to_string(index=False))\n",
        "\n",
        "# Alert if significant degradation\n",
        "if any(abs(float(row['Change'].replace('%', ''))) > 10 for row in comparison_data):\n",
        "    print(\"\\n\" + \"‚ö†Ô∏è\" * 20)\n",
        "    print(\"\\nüö® ALERT: Significant model performance degradation detected!\")\n",
        "    print(\"   Actions:\")\n",
        "    print(\"   1. Investigate data drift\")\n",
        "    print(\"   2. Check for data quality issues\")\n",
        "    print(\"   3. Consider model retraining\")\n",
        "    print(\"   4. Review feature engineering\")\n",
        "    print(\"\\n\" + \"‚ö†Ô∏è\" * 20)\n"
    ]
})

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π notebook
notebook['cells'] = cells

with open(notebook_path, 'w', encoding='utf-8') as f:
    json.dump(notebook, f, ensure_ascii=False, indent=1)

print(f'\\n‚úÖ Updated notebook: {notebook_path}')
print(f'Total cells: {len(cells)}')
print('Docker and Monitoring added!')

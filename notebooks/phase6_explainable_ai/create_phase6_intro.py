#!/usr/bin/env python3
"""
Phase 6: Explainable AI (XAI) - Interpretability & Fairness
Part 1: Introduction, Setup, Dataset, Model Training
"""

import json

# –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –Ω–æ—É—Ç–±—É–∫–∞
notebook = {
    "cells": [],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

cells = []

# ============================================================================
# TITLE AND INTRODUCTION
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "# üî¨ Explainable AI (XAI): Interpretability & Fairness\n",
        "\n",
        "**Phase 6: Understanding HOW and WHY ML Models Make Decisions**\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ The Black Box Problem\n",
        "\n",
        "### –î–æ —Å–∏—Ö –ø–æ—Ä –º—ã —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞–ª–∏—Å—å –Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏:\n",
        "\n",
        "- ‚úÖ **Phase 1-2:** Accuracy, AUC, F1-Score\n",
        "- ‚úÖ **Phase 3:** Ensemble methods –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫\n",
        "- ‚úÖ **Phase 4:** Transformers –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤\n",
        "- ‚úÖ **Phase 5:** Unsupervised learning –¥–ª—è anomaly detection\n",
        "\n",
        "**–ù–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –º–∏—Ä–µ —ç—Ç–æ–≥–æ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ:**\n",
        "\n",
        "### üè• –ú–µ–¥–∏—Ü–∏–Ω–∞\n",
        "```\n",
        "–ú–æ–¥–µ–ª—å: \"–£ –ø–∞—Ü–∏–µ–Ω—Ç–∞ —Ä–∞–∫ –ª–µ–≥–∫–∏—Ö —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é 85%\"\n",
        "–í—Ä–∞—á: \"–ü–æ—á–µ–º—É? –ù–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –∫–∞–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤?\"\n",
        "–ú–æ–¥–µ–ª—å: \"ü§∑ (black box)\"\n",
        "```\n",
        "‚ùå **–ù–ï–ü–†–ò–ï–ú–õ–ï–ú–û** - FDA –∏ GDPR —Ç—Ä–µ–±—É—é—Ç –æ–±—ä—è—Å–Ω–∏–º–æ—Å—Ç–∏\n",
        "\n",
        "### üí≥ –§–∏–Ω–∞–Ω—Å—ã\n",
        "```\n",
        "–ú–æ–¥–µ–ª—å: \"–ö—Ä–µ–¥–∏—Ç –æ—Ç–∫–ª–æ–Ω–µ–Ω\"\n",
        "–ö–ª–∏–µ–Ω—Ç: \"–ü–æ—á–µ–º—É? –ß—Ç–æ –Ω—É–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å?\"\n",
        "–ë–∞–Ω–∫: \"ü§∑ (black box)\"\n",
        "```\n",
        "‚ùå **ILLEGAL** - –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ —Ç—Ä–µ–±—É–µ—Ç –æ–±—ä—è—Å–Ω–µ–Ω–∏–π (Equal Credit Opportunity Act)\n",
        "\n",
        "### üè¢ –†–µ–∫—Ä—É—Ç–∏–Ω–≥\n",
        "```\n",
        "–ú–æ–¥–µ–ª—å: 95% –º—É–∂—á–∏–Ω –ø–æ–ª—É—á–∞—é—Ç job offers –¥–ª—è tech –ø–æ–∑–∏—Ü–∏–π\n",
        "HR: \"–ú–æ–¥–µ–ª—å –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∏—Ä—É–µ—Ç –ø–æ –ø–æ–ª—É?\"\n",
        "Data Scientist: \"ü§∑ Accuracy 92%, —á—Ç–æ –Ω–µ —Ç–∞–∫?\"\n",
        "```\n",
        "‚ùå **BIAS PROBLEM** - –≤—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å != fair predictions\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Enter Explainable AI (XAI)\n",
        "\n",
        "### –ö–ª—é—á–µ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã:\n",
        "\n",
        "1. **Global Interpretability:** –ö–∞–∫ –º–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ —Ü–µ–ª–æ–º?\n",
        "   - –ö–∞–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ?\n",
        "   - –ö–∞–∫ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤–ª–∏—è—é—Ç –Ω–∞ predictions?\n",
        "   - –ï—Å—Ç—å –ª–∏ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è?\n",
        "\n",
        "2. **Local Interpretability:** –ü–æ—á–µ–º—É –∏–º–µ–Ω–Ω–æ —ç—Ç–æ prediction?\n",
        "   - –ü–æ—á–µ–º—É —ç—Ç–æ–º—É –∫–ª–∏–µ–Ω—Ç—É –æ—Ç–∫–∞–∑–∞–ª–∏ –≤ –∫—Ä–µ–¥–∏—Ç–µ?\n",
        "   - –ö–∞–∫–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã –ø–æ–≤–ª–∏—è–ª–∏ –Ω–∞ —ç—Ç–æ—Ç –¥–∏–∞–≥–Ω–æ–∑?\n",
        "   - –ß—Ç–æ –Ω—É–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –¥–ª—è –¥—Ä—É–≥–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞?\n",
        "\n",
        "3. **Fairness:** –ú–æ–¥–µ–ª—å —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–∞?\n",
        "   - –ï—Å—Ç—å –ª–∏ bias –ø–æ –ø–æ–ª—É, —Ä–∞—Å–µ, –≤–æ–∑—Ä–∞—Å—Ç—É?\n",
        "   - –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω—ã –¥–ª—è –≤—Å–µ—Ö –≥—Ä—É–ø–ø?\n",
        "   - Demographic parity vs Equal opportunity?\n",
        "\n",
        "---\n",
        "\n",
        "## üìä XAI Methods Overview\n",
        "\n",
        "### 1. Model-Agnostic Methods (—Ä–∞–±–æ—Ç–∞—é—Ç —Å –ª—é–±–æ–π –º–æ–¥–µ–ª—å—é)\n",
        "\n",
        "#### **SHAP (SHapley Additive exPlanations)**\n",
        "- ‚úÖ **–¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω** (game theory, Shapley values)\n",
        "- ‚úÖ **Consistent:** –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –±–æ–ª—å—à–µ –ø–æ–ª–∞–≥–∞–µ—Ç—Å—è –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫, SHAP value –≤—ã—à–µ\n",
        "- ‚úÖ **Local + Global interpretability**\n",
        "- ‚ö†Ô∏è **Computational cost:** TreeSHAP –±—ã—Å—Ç—Ä, KernelSHAP –º–µ–¥–ª–µ–Ω–Ω–µ–µ\n",
        "\n",
        "**Variants:**\n",
        "- `TreeSHAP`: –¥–ª—è tree-based –º–æ–¥–µ–ª–µ–π (XGBoost, RandomForest) - –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ\n",
        "- `KernelSHAP`: –¥–ª—è –ª—é–±—ã—Ö –º–æ–¥–µ–ª–µ–π - –º–µ–¥–ª–µ–Ω–Ω–µ–µ\n",
        "- `DeepSHAP`: –¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π\n",
        "\n",
        "#### **LIME (Local Interpretable Model-agnostic Explanations)**\n",
        "- ‚úÖ **Fast:** –±—ã—Å—Ç—Ä–µ–µ SHAP –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –æ–±—ä—è—Å–Ω–µ–Ω–∏–π\n",
        "- ‚úÖ **Intuitive:** –∞–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –ª–æ–∫–∞–ª—å–Ω–æ –ø—Ä–æ—Å—Ç–æ–π –º–æ–¥–µ–ª—å—é\n",
        "- ‚ö†Ô∏è **Unstable:** —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–≥—É—Ç –≤–∞—Ä—å–∏—Ä–æ–≤–∞—Ç—å—Å—è\n",
        "- ‚ö†Ô∏è **Only local:** –Ω–µ –¥–∞—ë—Ç –≥–ª–æ–±–∞–ª—å–Ω–æ–π –∫–∞—Ä—Ç–∏–Ω—ã\n",
        "\n",
        "#### **Partial Dependence Plots (PDP)**\n",
        "- ‚úÖ **Global view:** –≤–ª–∏—è–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∞ –Ω–∞ predictions –≤ —Å—Ä–µ–¥–Ω–µ–º\n",
        "- ‚úÖ **Easy to interpret:** –≤–∏–∑—É–∞–ª—å–Ω–æ –ø–æ–Ω—è—Ç–Ω–æ\n",
        "- ‚ö†Ô∏è **Assumes independence:** –º–æ–∂–µ—Ç –≤–≤–æ–¥–∏—Ç—å –≤ –∑–∞–±–ª—É–∂–¥–µ–Ω–∏–µ –ø—Ä–∏ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è—Ö\n",
        "\n",
        "#### **Permutation Importance**\n",
        "- ‚úÖ **Simple:** shuffle –ø—Ä–∏–∑–Ω–∞–∫ ‚Üí measure drop in accuracy\n",
        "- ‚úÖ **True importance:** —É—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ (–≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç Gini importance)\n",
        "- ‚ö†Ô∏è **Computational cost:** —Ç—Ä–µ–±—É–µ—Ç –º–Ω–æ–≥–∏—Ö –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–æ–∫\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Model-Specific Methods\n",
        "\n",
        "#### **Feature Importance (Tree-based models)**\n",
        "- ‚úÖ **Built-in:** –±—ã—Å—Ç—Ä–æ, –¥–æ—Å—Ç—É–ø–Ω–æ –∏–∑ –∫–æ—Ä–æ–±–∫–∏\n",
        "- ‚ö†Ô∏è **Gini bias:** –ø–µ—Ä–µ–æ—Ü–µ–Ω–∏–≤–∞–µ—Ç high-cardinality features\n",
        "\n",
        "#### **Attention Weights (Transformers)**\n",
        "- ‚úÖ **Direct:** –º–æ–¥–µ–ª—å \"–≥–æ–≤–æ—Ä–∏—Ç\", –Ω–∞ —á—Ç–æ —Å–º–æ—Ç—Ä–∏—Ç\n",
        "- ‚ö†Ô∏è **Interpretation caveats:** attention ‚â† importance (—Å–ø–æ—Ä–Ω—ã–π –º–æ–º–µ–Ω—Ç)\n",
        "\n",
        "#### **Linear Model Coefficients**\n",
        "- ‚úÖ **Direct interpretation:** –≤–µ—Å = –≤–ª–∏—è–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∞\n",
        "- ‚ö†Ô∏è **Only linear models:** –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è deep learning\n",
        "\n",
        "---\n",
        "\n",
        "## üìä –ß—Ç–æ –º—ã —Ä–µ–∞–ª–∏–∑—É–µ–º\n",
        "\n",
        "### Dataset: Income Prediction (Adult Census)\n",
        "\n",
        "**–ü–æ—á–µ–º—É —ç—Ç–æ—Ç –¥–∞—Ç–∞—Å–µ—Ç?**\n",
        "- ‚úÖ **Fairness concerns:** –ø–æ–ª, —Ä–∞—Å–∞, –≤–æ–∑—Ä–∞—Å—Ç –º–æ–≥—É—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å bias\n",
        "- ‚úÖ **Real-world problem:** income prediction –≤–∞–∂–µ–Ω –¥–ª—è –∫—Ä–µ–¥–∏—Ç–Ω–æ–≥–æ —Å–∫–æ—Ä–∏–Ω–≥–∞\n",
        "- ‚úÖ **Interpretability –≤–∞–∂–Ω–∞:** –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ, –ø–æ—á–µ–º—É –∫—Ç–æ-—Ç–æ –≤ high/low income –≥—Ä—É–ø–ø–µ\n",
        "- ‚úÖ **Multiple feature types:** numerical, categorical\n",
        "\n",
        "### –ó–∞–¥–∞—á–∏:\n",
        "\n",
        "**Part 1: Setup & Model Training**\n",
        "1. –ó–∞–≥—Ä—É–∑–∫–∞ Adult Census dataset\n",
        "2. Preprocessing\n",
        "3. –û–±—É—á–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π (Logistic Regression, RandomForest, XGBoost)\n",
        "\n",
        "**Part 2: SHAP Analysis**\n",
        "1. TreeSHAP –¥–ª—è RandomForest –∏ XGBoost\n",
        "2. Global feature importance (summary plots)\n",
        "3. Local explanations (waterfall plots, force plots)\n",
        "4. Dependence plots (feature interactions)\n",
        "\n",
        "**Part 3: LIME Analysis**\n",
        "1. Local explanations –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö predictions\n",
        "2. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ SHAP vs LIME\n",
        "\n",
        "**Part 4: Global Interpretability**\n",
        "1. Partial Dependence Plots (PDP)\n",
        "2. Individual Conditional Expectation (ICE) curves\n",
        "3. Permutation Importance\n",
        "\n",
        "**Part 5: Fairness Analysis**\n",
        "1. Demographic Parity –ø–æ –ø–æ–ª—É\n",
        "2. Equal Opportunity analysis\n",
        "3. Calibration –ø–æ –≥—Ä—É–ø–ø–∞–º\n",
        "4. Bias mitigation strategies\n",
        "\n",
        "**Part 6: Decision Tree Visualization**\n",
        "1. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∞–≤–∏–ª Decision Tree\n",
        "2. Rule extraction\n",
        "\n",
        "---\n"
    ]
})

# ============================================================================
# IMPORTS
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "## üíª –ß–∞—Å—Ç—å 1: Setup –∏ Dataset\n",
        "\n",
        "### 1.1 –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ë–∞–∑–æ–≤—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Sklearn - Models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import xgboost as xgb\n",
        "\n",
        "# Sklearn - Metrics\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, confusion_matrix, classification_report,\n",
        "    roc_curve, precision_recall_curve\n",
        ")\n",
        "\n",
        "# Sklearn - Interpretability\n",
        "from sklearn.inspection import (\n",
        "    permutation_importance,\n",
        "    PartialDependenceDisplay,\n",
        "    partial_dependence\n",
        ")\n",
        "from sklearn.tree import plot_tree, export_text\n",
        "\n",
        "# SHAP\n",
        "import shap\n",
        "shap.initjs()  # –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ Jupyter\n",
        "\n",
        "# LIME\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n",
        "\n",
        "# Reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"\\n‚úÖ –í—Å–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
        "print(f\"SHAP version: {shap.__version__}\")\n",
        "print(f\"LIME version: {lime.__version__}\")\n"
    ]
})

# ============================================================================
# DATASET LOADING
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 1.2 –ó–∞–≥—Ä—É–∑–∫–∞ Adult Census Dataset\n",
        "\n",
        "**Adult Income Dataset** (—Ç–∞–∫–∂–µ –∏–∑–≤–µ—Å—Ç–µ–Ω –∫–∞–∫ Census Income):\n",
        "- **–ó–∞–¥–∞—á–∞:** –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å, –∑–∞—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ª–∏ —á–µ–ª–æ–≤–µ–∫ >50K –≤ –≥–æ–¥\n",
        "- **–†–∞–∑–º–µ—Ä:** ~48,000 –∑–∞–ø–∏—Å–µ–π\n",
        "- **–ü—Ä–∏–∑–Ω–∞–∫–∏:** age, workclass, education, marital-status, occupation, relationship, race, sex, capital-gain, capital-loss, hours-per-week, native-country\n",
        "- **Target:** income (<=50K or >50K)\n",
        "\n",
        "**–í–∞–∂–Ω–æ –¥–ª—è XAI:**\n",
        "- Sensitive attributes: sex, race ‚Üí fairness analysis\n",
        "- Categorical features ‚Üí –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n",
        "- Real-world implications ‚Üí ethical AI"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º Adult Census dataset\n",
        "# –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π dataset –∏–ª–∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ UCI repository\n",
        "\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
        "\n",
        "column_names = [\n",
        "    'age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
        "    'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
        "    'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'\n",
        "]\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
        "try:\n",
        "    df = pd.read_csv(url, names=column_names, sep=',\\s*', engine='python', na_values='?')\n",
        "    print(\"‚úÖ Dataset –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ UCI repository\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ UCI, —Å–æ–∑–¥–∞—é —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π dataset\")\n",
        "    # –°–æ–∑–¥–∞—ë–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π dataset, –µ—Å–ª–∏ –Ω–µ—Ç –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞\n",
        "    np.random.seed(42)\n",
        "    n_samples = 30000\n",
        "    \n",
        "    df = pd.DataFrame({\n",
        "        'age': np.random.randint(17, 90, n_samples),\n",
        "        'workclass': np.random.choice(['Private', 'Self-emp', 'Govt', 'Without-pay'], n_samples, p=[0.7, 0.15, 0.1, 0.05]),\n",
        "        'education': np.random.choice(['HS-grad', 'Some-college', 'Bachelors', 'Masters', 'Doctorate'], n_samples, p=[0.3, 0.25, 0.3, 0.1, 0.05]),\n",
        "        'education-num': np.random.randint(1, 16, n_samples),\n",
        "        'marital-status': np.random.choice(['Married', 'Never-married', 'Divorced'], n_samples, p=[0.5, 0.35, 0.15]),\n",
        "        'occupation': np.random.choice(['Tech', 'Sales', 'Service', 'Craft', 'Prof'], n_samples, p=[0.15, 0.25, 0.25, 0.2, 0.15]),\n",
        "        'relationship': np.random.choice(['Husband', 'Wife', 'Own-child', 'Not-in-family'], n_samples, p=[0.3, 0.25, 0.2, 0.25]),\n",
        "        'race': np.random.choice(['White', 'Black', 'Asian', 'Other'], n_samples, p=[0.8, 0.1, 0.05, 0.05]),\n",
        "        'sex': np.random.choice(['Male', 'Female'], n_samples, p=[0.67, 0.33]),\n",
        "        'capital-gain': np.random.choice([0] * 90 + list(np.random.randint(1000, 100000, 10)), n_samples),\n",
        "        'capital-loss': np.random.choice([0] * 95 + list(np.random.randint(100, 5000, 5)), n_samples),\n",
        "        'hours-per-week': np.random.randint(1, 100, n_samples),\n",
        "        'native-country': np.random.choice(['United-States', 'Other'], n_samples, p=[0.9, 0.1]),\n",
        "    })\n",
        "    \n",
        "    # –°–æ–∑–¥–∞—ë–º target —Å –Ω–µ–∫–æ—Ç–æ—Ä–æ–π –ª–æ–≥–∏–∫–æ–π\n",
        "    income_prob = (\n",
        "        (df['age'] > 30).astype(int) * 0.2 +\n",
        "        (df['education-num'] > 12).astype(int) * 0.3 +\n",
        "        (df['hours-per-week'] > 40).astype(int) * 0.2 +\n",
        "        (df['capital-gain'] > 0).astype(int) * 0.25\n",
        "    ) / 1.0\n",
        "    \n",
        "    df['income'] = (np.random.random(n_samples) < income_prob).astype(int)\n",
        "    df['income'] = df['income'].map({0: '<=50K', 1: '>50K'})\n",
        "\n",
        "# –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
        "\n",
        "# –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏\n",
        "df = df.dropna()\n",
        "\n",
        "print(f\"\\n–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {df.shape}\")\n",
        "print(f\"–ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {df.shape[1] - 1}\")\n",
        "print(f\"\\n–ü–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏:\")\n",
        "df.head()\n"
    ]
})

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
notebook['cells'] = cells

output_path = '/home/user/test/notebooks/phase6_explainable_ai/01_explainable_ai_xai.ipynb'
with open(output_path, 'w', encoding='utf-8') as f:
    json.dump(notebook, f, ensure_ascii=False, indent=1)

print(f'‚úÖ Started notebook: {output_path}')
print(f'–Ø—á–µ–µ–∫: {len(cells)}')
print('–ü—Ä–æ–¥–æ–ª–∂–∞—é —Å–æ–∑–¥–∞–Ω–∏–µ...')

#!/usr/bin/env python3
"""
Phase 4 Step 2: TabTransformer for Tabular Data
Part 4: Attention Visualization, Embedding Analysis, Conclusions
"""

import json

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π notebook
notebook_path = '/home/user/test/notebooks/phase4_transformers/02_tabtransformer.ipynb'
with open(notebook_path, 'r', encoding='utf-8') as f:
    notebook = json.load(f)

cells = notebook['cells']

# ============================================================================
# ATTENTION VISUALIZATION
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "---\n",
        "\n",
        "## üîç –ß–∞—Å—Ç—å 6: Interpretability —á–µ—Ä–µ–∑ Attention\n",
        "\n",
        "### 6.1 Attention Weights Visualization\n",
        "\n",
        "**–ß—Ç–æ –º—ã –∏—â–µ–º:**\n",
        "- –ö–∞–∫–∏–µ categorical features –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—é—Ç –¥—Ä—É–≥ —Å –¥—Ä—É–≥–æ–º?\n",
        "- –ï—Å—Ç—å –ª–∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã? (education ‚Üî occupation, marital-status ‚Üî relationship)\n",
        "- –ö–∞–∫ —Ä–∞–∑–Ω—ã–µ layers —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è?"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Visualize attention –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö samples\n",
        "sample_indices = [10, 100, 500]  # –≤—ã–±–∏—Ä–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã\n",
        "\n",
        "model.eval()\n",
        "\n",
        "for idx in sample_indices:\n",
        "    x_cat_sample = X_cat_test_tensor[idx:idx+1].to(device)\n",
        "    x_num_sample = X_num_test_tensor[idx:idx+1].to(device)\n",
        "    y_sample = y_test[idx]\n",
        "    \n",
        "    # Forward pass with attention\n",
        "    logits, attention_list = model(x_cat_sample, x_num_sample, return_attention=True)\n",
        "    pred_prob = F.softmax(logits, dim=1)[0, 1].item()\n",
        "    pred_class = torch.argmax(logits, dim=1).item()\n",
        "    \n",
        "    # Decode categorical values –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏\n",
        "    cat_values = x_cat_sample[0].cpu().numpy()\n",
        "    cat_labels = []\n",
        "    for i, (col, val) in enumerate(zip(categorical_cols, cat_values)):\n",
        "        decoded = label_encoders[col].inverse_transform([val])[0]\n",
        "        cat_labels.append(f\"{col[:10]}={decoded[:10]}\")\n",
        "    \n",
        "    print(f\"\\n\" + \"=\"*70)\n",
        "    print(f\"Sample {idx}\")\n",
        "    print(f\"True: {'>50K' if y_sample == 1 else '<=50K'} | \"\n",
        "          f\"Pred: {'>50K' if pred_class == 1 else '<=50K'} | \"\n",
        "          f\"Prob(>50K): {pred_prob:.3f}\")\n",
        "    print(f\"Categorical values: {cat_labels}\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Visualize attention –¥–ª—è –≤—Å–µ—Ö layers –∏ heads\n",
        "    n_layers_viz = len(attention_list)\n",
        "    n_heads_viz = attention_list[0].size(1)\n",
        "    \n",
        "    fig, axes = plt.subplots(n_layers_viz, n_heads_viz,\n",
        "                            figsize=(n_heads_viz * 4, n_layers_viz * 4))\n",
        "    \n",
        "    if n_layers_viz == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "    \n",
        "    for layer_idx in range(n_layers_viz):\n",
        "        attn_weights = attention_list[layer_idx][0].cpu().numpy()  # (n_heads, seq, seq)\n",
        "        \n",
        "        for head_idx in range(n_heads_viz):\n",
        "            ax = axes[layer_idx, head_idx]\n",
        "            attn_matrix = attn_weights[head_idx]  # (num_categorical, num_categorical)\n",
        "            \n",
        "            sns.heatmap(attn_matrix, annot=False, cmap='YlOrRd',\n",
        "                       xticklabels=cat_labels, yticklabels=cat_labels,\n",
        "                       ax=ax, cbar=True, square=True, vmin=0, vmax=0.5)\n",
        "            ax.set_title(f'Layer {layer_idx+1}, Head {head_idx+1}',\n",
        "                       fontsize=11, fontweight='bold')\n",
        "            ax.set_xlabel('Keys (attending to)', fontsize=9)\n",
        "            if head_idx == 0:\n",
        "                ax.set_ylabel('Queries (attending from)', fontsize=9)\n",
        "            ax.tick_params(axis='both', labelsize=7)\n",
        "    \n",
        "    plt.suptitle(f'Sample {idx}: Attention Weights Across Layers/Heads\\n'\n",
        "                f'True: {y_sample}, Pred: {pred_class}, Prob: {pred_prob:.3f}',\n",
        "                fontsize=14, fontweight='bold', y=0.995)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Aggregate attention weights –ø–æ –≤—Å–µ–º—É test set\n",
        "print(\"Aggregating attention weights across all test samples...\")\n",
        "\n",
        "all_attention_weights = [[] for _ in range(n_layers)]  # list per layer\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for x_cat, x_num, _ in test_loader:\n",
        "        x_cat = x_cat.to(device)\n",
        "        x_num = x_num.to(device)\n",
        "        _, attention_list = model(x_cat, x_num, return_attention=True)\n",
        "        \n",
        "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º attention –¥–ª—è –∫–∞–∂–¥–æ–≥–æ layer\n",
        "        for layer_idx, attn in enumerate(attention_list):\n",
        "            # Average –ø–æ heads: (batch, num_cat, num_cat)\n",
        "            attn_avg = attn.mean(dim=1)\n",
        "            all_attention_weights[layer_idx].append(attn_avg.cpu())\n",
        "\n",
        "# Concatenate –∏ average\n",
        "mean_attention_per_layer = []\n",
        "for layer_idx in range(n_layers):\n",
        "    all_attn = torch.cat(all_attention_weights[layer_idx], dim=0)\n",
        "    mean_attn = all_attn.mean(dim=0).numpy()  # (num_categorical, num_categorical)\n",
        "    mean_attention_per_layer.append(mean_attn)\n",
        "\n",
        "print(f\"‚úÖ Aggregated attention for {len(X_cat_test):,} test samples\")\n",
        "\n",
        "# Visualize mean attention –¥–ª—è –∫–∞–∂–¥–æ–≥–æ layer\n",
        "fig, axes = plt.subplots(1, n_layers, figsize=(n_layers * 7, 6))\n",
        "\n",
        "if n_layers == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for layer_idx in range(n_layers):\n",
        "    mean_attn = mean_attention_per_layer[layer_idx]\n",
        "    \n",
        "    sns.heatmap(mean_attn, annot=True, fmt='.2f', cmap='YlOrRd',\n",
        "               xticklabels=categorical_cols, yticklabels=categorical_cols,\n",
        "               ax=axes[layer_idx], cbar_kws={'label': 'Attention Weight'})\n",
        "    axes[layer_idx].set_title(f'Layer {layer_idx+1}: Mean Attention\\n(averaged over test set)',\n",
        "                             fontsize=13, fontweight='bold')\n",
        "    axes[layer_idx].set_xlabel('Keys (attending to)', fontsize=11)\n",
        "    if layer_idx == 0:\n",
        "        axes[layer_idx].set_ylabel('Queries (attending from)', fontsize=11)\n",
        "\n",
        "plt.suptitle('Mean Attention Weights Across All Test Samples',\n",
        "             fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä –ù–∞–±–ª—é–¥–µ–Ω–∏—è:\")\n",
        "print(\"  - Diagonal elements: self-attention (–∫–∞–∂–¥–∞—è feature –Ω–∞ —Å–µ–±—è)\")\n",
        "print(\"  - Off-diagonal: cross-feature interactions\")\n",
        "print(\"  - –°–º–æ—Ç—Ä–∏—Ç–µ –Ω–∞ –≤—ã—Å–æ–∫–∏–µ –≤–µ—Å–∞: –∫–∞–∫–∏–µ features –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—é—Ç —Å–∏–ª—å–Ω–µ–µ\")"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Feature interaction analysis\n",
        "# –î–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ layer: –∫–∞–∫–∏–µ pairs features –∏–º–µ—é—Ç —Å–∏–ª—å–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ?\n",
        "\n",
        "last_layer_attn = mean_attention_per_layer[-1]  # (num_categorical, num_categorical)\n",
        "\n",
        "# –ü–æ–ª—É—á–∞–µ–º top interactions (excluding diagonal)\n",
        "interactions = []\n",
        "for i in range(len(categorical_cols)):\n",
        "    for j in range(len(categorical_cols)):\n",
        "        if i != j:  # exclude self-attention\n",
        "            interactions.append((\n",
        "                categorical_cols[i],\n",
        "                categorical_cols[j],\n",
        "                last_layer_attn[i, j]\n",
        "            ))\n",
        "\n",
        "# Sort by attention weight\n",
        "interactions.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "print(\"\\nTop 15 Feature Interactions (based on attention weights):\")\n",
        "print(\"=\"*60)\n",
        "for i, (feat1, feat2, weight) in enumerate(interactions[:15], 1):\n",
        "    print(f\"{i:2d}. {feat1:20s} ‚Üí {feat2:20s} : {weight:.4f}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nüìä –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:\")\n",
        "print(\"  - –í—ã—Å–æ–∫–∏–µ –≤–µ—Å–∞ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å —Å—á–∏—Ç–∞–µ—Ç —ç—Ç–∏ interactions –≤–∞–∂–Ω—ã–º–∏\")\n",
        "print(\"  - –û—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ pairs (–Ω–∞–ø—Ä–∏–º–µ—Ä, education ‚Üî occupation) –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å —É—á–∏—Ç —Å–µ–º–∞–Ω—Ç–∏–∫—É\")\n",
        "print(\"  - –†–∞–∑–Ω—ã–µ layers –º–æ–≥—É—Ç focus –Ω–∞ —Ä–∞–∑–Ω—ã—Ö interactions\")"
    ]
})

# ============================================================================
# EMBEDDING ANALYSIS
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 6.2 Categorical Embeddings Analysis\n",
        "\n",
        "**–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º learned embeddings:**\n",
        "- –ü–æ—Ö–æ–∂–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–º–µ—é—Ç –±–ª–∏–∑–∫–∏–µ embeddings?\n",
        "- t-SNE visualization"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ò–∑–≤–ª–µ–∫–∞–µ–º embeddings –¥–ª—è –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è (education)\n",
        "# education - –æ–¥–Ω–∞ –∏–∑ —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö features\n",
        "\n",
        "education_idx = categorical_cols.index('education')\n",
        "education_embedding = model.cat_embeddings[education_idx].weight.data.cpu().numpy()\n",
        "\n",
        "print(f\"Education Embedding shape: {education_embedding.shape}\")\n",
        "print(f\"  {len(label_encoders['education'].classes_)} education categories √ó {d_model} dimensions\")\n",
        "\n",
        "# Education categories\n",
        "education_categories = label_encoders['education'].classes_\n",
        "print(f\"\\nEducation categories: {list(education_categories)}\")\n",
        "\n",
        "# t-SNE visualization\n",
        "try:\n",
        "    from sklearn.manifold import TSNE\n",
        "    \n",
        "    print(\"\\nRunning t-SNE...\")\n",
        "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(education_categories)-1))\n",
        "    education_tsne = tsne.fit_transform(education_embedding)\n",
        "    \n",
        "    plt.figure(figsize=(12, 8))\n",
        "    scatter = plt.scatter(education_tsne[:, 0], education_tsne[:, 1], \n",
        "                         s=100, alpha=0.7, c=range(len(education_categories)),\n",
        "                         cmap='tab20')\n",
        "    \n",
        "    # Annotate\n",
        "    for i, cat in enumerate(education_categories):\n",
        "        plt.annotate(cat, (education_tsne[i, 0], education_tsne[i, 1]),\n",
        "                    fontsize=9, alpha=0.8, fontweight='bold')\n",
        "    \n",
        "    plt.title('t-SNE Visualization of Education Embeddings', \n",
        "             fontsize=16, fontweight='bold')\n",
        "    plt.xlabel('t-SNE Component 1', fontsize=12)\n",
        "    plt.ylabel('t-SNE Component 2', fontsize=12)\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nüìä –û–∂–∏–¥–∞–µ–º—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã:\")\n",
        "    print(\"  - –ë–ª–∏–∑–∫–∏–µ —É—Ä–æ–≤–Ω–∏ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –±–ª–∏–∑–∫–æ (HS-grad ‚âà Some-college)\")\n",
        "    print(\"  - Doctorate, Masters, Bachelors –º–æ–≥—É—Ç —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –∫–ª–∞—Å—Ç–µ—Ä\")\n",
        "    print(\"  - Preschool, 1st-4th –º–æ–≥—É—Ç –±—ã—Ç—å –≤–º–µ—Å—Ç–µ\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è sklearn not available for t-SNE. Skipping visualization.\")"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Similarity matrix –º–µ–∂–¥—É education categories\n",
        "# Cosine similarity\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "education_sim = cosine_similarity(education_embedding)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(education_sim, annot=True, fmt='.2f', cmap='RdYlGn',\n",
        "           xticklabels=education_categories, yticklabels=education_categories,\n",
        "           cbar_kws={'label': 'Cosine Similarity'})\n",
        "plt.title('Education Embeddings Similarity Matrix', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Education Level', fontsize=12)\n",
        "plt.ylabel('Education Level', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:\")\n",
        "print(\"  - Diagonal: –∏–¥–µ–∞–ª—å–Ω–∞—è similarity (1.0)\")\n",
        "print(\"  - –í—ã—Å–æ–∫–∏–µ off-diagonal –∑–Ω–∞—á–µ–Ω–∏—è: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\")\n",
        "print(\"  - –ù–∏–∑–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è: dissimilar (–Ω–∞–ø—Ä–∏–º–µ—Ä, Doctorate vs Preschool)\")"
    ]
})

# ============================================================================
# CONCLUSIONS
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "---\n",
        "\n",
        "## üéì –ò—Ç–æ–≥–∏ –∏ –í—ã–≤–æ–¥—ã\n",
        "\n",
        "### –ß—Ç–æ –º—ã –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∏\n",
        "\n",
        "#### 1. TabTransformer –Ω–∞ –†–µ–∞–ª—å–Ω–æ–º –î–∞—Ç–∞—Å–µ—Ç–µ\n",
        "\n",
        "**Adult Income: 48,842 samples (vs Titanic 891)**\n",
        "- ‚úÖ –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–æ–π –¥–ª—è Deep Learning\n",
        "- ‚úÖ 8 categorical features —Å –≤—ã—Å–æ–∫–æ–π cardinality\n",
        "- ‚úÖ –°–ª–æ–∂–Ω—ã–µ feature interactions (education √ó occupation √ó marital-status)\n",
        "\n",
        "**–†–µ–∑—É–ª—å—Ç–∞—Ç—ã:**\n",
        "- TabTransformer: ~86% accuracy\n",
        "- XGBoost/LightGBM: ~87% accuracy\n",
        "- Simple MLP: ~84% accuracy\n",
        "\n",
        "**–í—ã–≤–æ–¥:** TabTransformer **competitive** —Å tree-based –º–µ—Ç–æ–¥–∞–º–∏!\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. Contextual Embeddings –†–∞–±–æ—Ç–∞—é—Ç\n",
        "\n",
        "**TabTransformer >> Simple MLP:**\n",
        "- Contextual embeddings –¥–∞—é—Ç ~2% accuracy boost\n",
        "- Categorical features —É—á–∞—Ç—Å—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥—Ä—É–≥ –¥—Ä—É–≥–∞\n",
        "- Transformer –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥–∏—Ç interactions\n",
        "\n",
        "**–ü—Ä–∏–º–µ—Ä:**\n",
        "```\n",
        "\"Occupation=Teacher\" –∏–º–µ–µ—Ç —Ä–∞–∑–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:\n",
        "- —Å \"Education=HS-grad\" ‚Üí –Ω–∏–∑–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å >50K\n",
        "- —Å \"Education=Masters\" ‚Üí –≤—ã—Å–æ–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å >50K\n",
        "\n",
        "TabTransformer —É—á–∏—Ç —ç—Ç–æ —á–µ—Ä–µ–∑ attention!\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. Interpretability —á–µ—Ä–µ–∑ Attention\n",
        "\n",
        "**Attention weights –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç:**\n",
        "- –ö–∞–∫–∏–µ categorical features –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—é—Ç (education ‚Üî occupation)\n",
        "- –ö–∞–∫ —Ä–∞–∑–Ω—ã–µ layers —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è\n",
        "- –ö–∞–∫–∏–µ interactions –º–æ–¥–µ–ª—å —Å—á–∏—Ç–∞–µ—Ç –≤–∞–∂–Ω—ã–º–∏\n",
        "\n",
        "**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –Ω–∞–¥ XGBoost:**\n",
        "- XGBoost: feature importance (—Å—Ç–∞—Ç–∏—á–µ—Å–∫–∞—è)\n",
        "- TabTransformer: attention weights (–¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è, per-sample)\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. Learned Embeddings Semantic\n",
        "\n",
        "**Embeddings –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è:**\n",
        "- –ü–æ—Ö–æ–∂–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –±–ª–∏–∑–∫–∏ –≤ embedding space\n",
        "- t-SNE –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç meaningful –∫–ª–∞—Å—Ç–µ—Ä—ã\n",
        "- Cosine similarity –º–µ–∂–¥—É –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω–∞\n",
        "\n",
        "**–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª:**\n",
        "- Transfer learning: pre-train –Ω–∞ –±–æ–ª—å—à–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
        "- Fine-tune –Ω–∞ —Ü–µ–ª–µ–≤–æ–π –∑–∞–¥–∞—á–µ\n",
        "- Embeddings –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –¥—Ä—É–≥–∏—Ö –∑–∞–¥–∞—á\n",
        "\n",
        "---\n",
        "\n",
        "### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ: TabTransformer vs XGBoost/LightGBM\n",
        "\n",
        "| Aspect | XGBoost/LightGBM | TabTransformer |\n",
        "|--------|------------------|----------------|\n",
        "| **Performance** | Excellent (87%) | Very Good (86%) |\n",
        "| **Training Speed** | Fast | Slower (GPU helps) |\n",
        "| **Categorical Handling** | One-hot/Label encode | Contextual Embeddings |\n",
        "| **Feature Interactions** | Implicit (trees) | Explicit (attention) |\n",
        "| **Interpretability** | Feature importance | Attention weights (per-sample) |\n",
        "| **Transfer Learning** | No | Yes (embeddings) |\n",
        "| **Scalability** | Limited | Excellent (GPU) |\n",
        "| **Tuning** | Many hyperparameters | Fewer hyperparameters |\n",
        "\n",
        "---\n",
        "\n",
        "### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å TabTransformer?\n",
        "\n",
        "**‚úÖ –•–æ—Ä–æ—à–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –∫–æ–≥–¥–∞:**\n",
        "1. **–ë–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç** (>10k, –ª—É—á—à–µ >50k samples)\n",
        "2. **–ú–Ω–æ–≥–æ categorical features** —Å –≤—ã—Å–æ–∫–æ–π cardinality\n",
        "3. **–°–ª–æ–∂–Ω—ã–µ interactions** –º–µ–∂–¥—É categorical features\n",
        "4. **–ù—É–∂–Ω–∞ interpretability** (attention weights)\n",
        "5. **Transfer learning** - pre-training –Ω–∞ –ø–æ—Ö–æ–∂–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "6. **GPU –¥–æ—Å—Ç—É–ø–µ–Ω** - —É—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ\n",
        "\n",
        "**‚ùå –ú–æ–∂–µ—Ç —É—Å—Ç—É–ø–∞—Ç—å tree-based –∫–æ–≥–¥–∞:**\n",
        "1. **–ú–∞–ª–µ–Ω—å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç** (<5k samples)\n",
        "2. **–¢–æ–ª—å–∫–æ numerical features** (–Ω–µ—Ç categorical)\n",
        "3. **–ü—Ä–æ—Å—Ç—ã–µ interactions** (linear/additive)\n",
        "4. **–ù—É–∂–Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å** inference\n",
        "5. **–û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã** (CPU only, –º–∞–ª–æ –ø–∞–º—è—Ç–∏)\n",
        "\n",
        "---\n",
        "\n",
        "### –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏: Phase 4 Step 3\n",
        "\n",
        "**Advanced Transformer Architectures:**\n",
        "\n",
        "1. **FT-Transformer (Feature Tokenizer Transformer)**\n",
        "   - Tokenization –¥–ª—è –≤—Å–µ—Ö features (categorical + numerical)\n",
        "   - Uniform treatment\n",
        "   - SOTA –Ω–∞ –º–Ω–æ–≥–∏—Ö —Ç–∞–±–ª–∏—á–Ω—ã—Ö benchmarks\n",
        "\n",
        "2. **SAINT (Self-Attention and Intersample Attention)**\n",
        "   - Attention –Ω–µ —Ç–æ–ª—å–∫–æ –º–µ–∂–¥—É features, –Ω–æ –∏ –º–µ–∂–¥—É samples\n",
        "   - –£—á–∏—Ç –æ—Ç –ø–æ—Ö–æ–∂–∏—Ö samples\n",
        "\n",
        "3. **Temporal Fusion Transformer**\n",
        "   - –î–ª—è time series —Å categorical/numerical features\n",
        "   - Multi-horizon forecasting\n",
        "\n",
        "---\n",
        "\n",
        "## üéâ –ü–æ–∑–¥—Ä–∞–≤–ª—è–µ–º!\n",
        "\n",
        "–í—ã –æ—Å–≤–æ–∏–ª–∏ TabTransformer - state-of-the-art –ø–æ–¥—Ö–æ–¥ –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö!\n",
        "\n",
        "**–¢–µ–ø–µ—Ä—å –≤—ã –∑–Ω–∞–µ—Ç–µ:**\n",
        "- üìä –ö–∞–∫ –ø—Ä–∏–º–µ–Ω—è—Ç—å Transformers –∫ —Ç–∞–±–ª–∏—á–Ω—ã–º –¥–∞–Ω–Ω—ã–º\n",
        "- üß† –ß—Ç–æ —Ç–∞–∫–æ–µ contextual embeddings –¥–ª—è categorical features\n",
        "- üîç –ö–∞–∫ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ attention weights\n",
        "- üìà –ö–æ–≥–¥–∞ TabTransformer –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç tree-based –º–µ—Ç–æ–¥—ã\n",
        "- üöÄ –ö–∞–∫ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –±–æ–ª—å—à–∏–µ –¥–∞—Ç–∞—Å–µ—Ç—ã\n",
        "\n",
        "**–ö–ª—é—á–µ–≤–æ–π takeaway:**\n",
        "> \"Contextual embeddings —á–µ—Ä–µ–∑ Transformer –ø–æ–∑–≤–æ–ª—è—é—Ç categorical features —É—á–∏—Ç—å—Å—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥—Ä—É–≥ –¥—Ä—É–≥–∞, –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥—è —Å–ª–æ–∂–Ω—ã–µ interactions –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è simple MLP, –ø—Ä–∏ —ç—Ç–æ–º –æ—Å—Ç–∞–≤–∞—è—Å—å competitive —Å XGBoost/LightGBM!\"\n",
        "\n",
        "**–°–ª–µ–¥—É—é—â–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞:** Advanced Transformers (FT-Transformer, SAINT) üöÄ\n"
    ]
})

# –°–æ—Ö—Ä–∞–Ω—è–µ–º
notebook['cells'] = cells

with open(notebook_path, 'w', encoding='utf-8') as f:
    json.dump(notebook, f, ensure_ascii=False, indent=1)

print(f'‚úÖ Part 4 –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤: {notebook_path}')
print(f'–í—Å–µ–≥–æ —è—á–µ–µ–∫: {len(cells)}')
print('Notebook –ó–ê–í–ï–†–®–ï–ù!')
print('\n–°—Ç—Ä—É–∫—Ç—É—Ä–∞:')
print('  Part 1: Introduction, Adult Income Dataset (48k samples), EDA')
print('  Part 2: Preprocessing, TabTransformer Theory, Implementation')
print('  Part 3: Training (30 epochs), Evaluation, Baseline Comparisons')
print('  Part 4: Attention Visualization, Embedding Analysis, Conclusions')

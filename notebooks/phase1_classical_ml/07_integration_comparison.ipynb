{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Integration & Comparison: Phase 1 Classical ML\n",
    "\n",
    "**–¶–µ–ª—å:** –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –∏–∑—É—á–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –Ω–∞ –µ–¥–∏–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ –¶–µ–ª–∏ –Ω–æ—É—Ç–±—É–∫–∞\n",
    "\n",
    "1. **–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π** –∏–∑ –≤—Å–µ—Ö –Ω–æ—É—Ç–±—É–∫–æ–≤ Phase 1\n",
    "2. **–ß–µ—Å—Ç–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ** –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤ –Ω–∞ –æ–¥–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
    "3. **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏** –ø–æ –≤—ã–±–æ—Ä—É –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤\n",
    "4. **Best practices** –¥–ª—è production ML —Å–∏—Å—Ç–µ–º\n",
    "\n",
    "---\n",
    "\n",
    "## üìö –ß—Ç–æ –º—ã –∏–∑—É—á–∏–ª–∏ –≤ Phase 1:\n",
    "\n",
    "| –ù–æ—É—Ç–±—É–∫ | –¢–µ–º–∞ | –ö–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ |\n",
    "|---------|------|-------------------|\n",
    "| **01** | XGBoost Deep Dive | Gradient boosting, second-order optimization, regularization |\n",
    "| **02** | LightGBM Deep Dive | Histogram-based, GOSS, EFB, leaf-wise growth |\n",
    "| **03** | CatBoost Deep Dive | Ordered boosting, categorical features, symmetric trees |\n",
    "| **04** | Stacking Ensemble | Meta-learning, out-of-fold predictions, diversity |\n",
    "| **05** | Imbalanced Data | SMOTE, cost-sensitive learning, threshold optimization |\n",
    "| **06** | Advanced Feature Engineering | Polynomial, target encoding, feature selection |\n",
    "\n",
    "---\n",
    "\n",
    "## üíº –ë–∏–∑–Ω–µ—Å-–∑–∞–¥–∞—á–∞: Telco Customer Churn\n",
    "\n",
    "**–ö–æ–Ω—Ç–µ–∫—Å—Ç:** –¢–µ–ª–µ–∫–æ–º –∫–æ–º–ø–∞–Ω–∏—è —Ç–µ—Ä—è–µ—Ç 27% –∫–ª–∏–µ–Ω—Ç–æ–≤ –µ–∂–µ–≥–æ–¥–Ω–æ.\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º—ã:**\n",
    "- üìä **Imbalanced data:** ~27% churn (–Ω–µ extreme, –Ω–æ moderate)\n",
    "- üî¢ **–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:** –£—Å–ª—É–≥–∏, —Ç–∏–ø –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞ (high cardinality)\n",
    "- üí∞ **–†–∞–∑–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –æ—à–∏–±–æ–∫:** FN (—É–ø—É—â–µ–Ω–Ω—ã–π churn) –¥–æ—Ä–æ–∂–µ FP\n",
    "- üéØ **–ë–∏–∑–Ω–µ—Å-—Ü–µ–ª—å:** –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å churn —Å retention campaigns\n",
    "\n",
    "**–ú–µ—Ç—Ä–∏–∫–∏:**\n",
    "- **ROC-AUC:** –û–±—â–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Ä–∞–∑–ª–∏—á–∞—Ç—å –∫–ª–∞—Å—Å—ã\n",
    "- **Precision/Recall/F1:** –ë–∞–ª–∞–Ω—Å FP/FN\n",
    "- **Business Cost:** Weighted cost (FN = $500, FP = $50)\n",
    "\n",
    "**–ü–æ—á–µ–º—É —ç—Ç–æ—Ç –¥–∞—Ç–∞—Å–µ—Ç?**\n",
    "- ‚úÖ –ü–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–∏–º–µ–Ω–∏—Ç—å **–≤—Å–µ** —Ç–µ—Ö–Ω–∏–∫–∏ Phase 1\n",
    "- ‚úÖ –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ‚Üí —Ç–µ—Å—Ç–∏—Ä—É–µ–º CatBoost, Target Encoding\n",
    "- ‚úÖ Moderate imbalance ‚Üí SMOTE, class weights\n",
    "- ‚úÖ –ú–∞–ª—ã–π —Ä–∞–∑–º–µ—Ä (~7k) ‚Üí –º–æ–∂–Ω–æ –±—ã—Å—Ç—Ä–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö –ß–∞—Å—Ç—å 1: –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤\n",
    "\n",
    "### 1.1 Gradient Boosting —Å–µ–º–µ–π—Å—Ç–≤–æ\n",
    "\n",
    "#### –û–±—â–∞—è –∏–¥–µ—è Gradient Boosting\n",
    "\n",
    "**–§–æ—Ä–º—É–ª–∞ (Friedman, 2001):**\n",
    "\n",
    "$$F_m(x) = F_{m-1}(x) + \\nu \\cdot h_m(x)$$\n",
    "\n",
    "–≥–¥–µ:\n",
    "- $F_m(x)$ ‚Äî –∞–Ω—Å–∞–º–±–ª—å –∏–∑ $m$ –¥–µ—Ä–µ–≤—å–µ–≤\n",
    "- $h_m(x)$ ‚Äî –Ω–æ–≤–æ–µ –¥–µ—Ä–µ–≤–æ, –æ–±—É—á–∞–µ–º–æ–µ –Ω–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞—Ö\n",
    "- $\\nu$ ‚Äî learning rate (shrinkage)\n",
    "\n",
    "**–ì—Ä–∞–¥–∏–µ–Ω—Ç:**\n",
    "\n",
    "$$g_i = \\frac{\\partial L(y_i, F_{m-1}(x_i))}{\\partial F_{m-1}(x_i)}$$\n",
    "\n",
    "–ö–∞–∂–¥–æ–µ –¥–µ—Ä–µ–≤–æ –ø—Ä–∏–±–ª–∏–∂–∞–µ—Ç **anti-gradient** loss —Ñ—É–Ω–∫—Ü–∏–∏.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 XGBoost vs LightGBM vs CatBoost\n",
    "\n",
    "#### –î–µ—Ç–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ\n",
    "\n",
    "**1. XGBoost (Chen & Guestrin, 2016)**\n",
    "\n",
    "**–ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è:** Second-order optimization\n",
    "\n",
    "$$L = \\sum_{i=1}^n \\left[ g_i f_t(x_i) + \\frac{1}{2} h_i f_t^2(x_i) \\right] + \\Omega(f_t)$$\n",
    "\n",
    "–≥–¥–µ:\n",
    "- $g_i$ ‚Äî first-order gradient\n",
    "- $h_i$ ‚Äî second-order gradient (Hessian)\n",
    "- $\\Omega(f_t) = \\gamma T + \\frac{1}{2}\\lambda \\sum_{j=1}^T w_j^2$ ‚Äî regularization\n",
    "\n",
    "**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**\n",
    "- ‚úÖ –¢–æ—á–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è (Newton's method)\n",
    "- ‚úÖ –°–∏–ª—å–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (L1/L2, gamma, alpha)\n",
    "- ‚úÖ –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –∏ –∑—Ä–µ–ª–æ—Å—Ç—å\n",
    "- ‚úÖ –û—Ç–ª–∏—á–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –∏ community\n",
    "\n",
    "**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:**\n",
    "- ‚ùå –ú–µ–¥–ª–µ–Ω–Ω–µ–µ LightGBM –Ω–∞ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "- ‚ùå –ë–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏ (pre-sorted –∏–ª–∏ histogram)\n",
    "- ‚ùå –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –Ω—É–∂–Ω–æ –∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –≤—Ä—É—á–Ω—É—é\n",
    "\n",
    "---\n",
    "\n",
    "**2. LightGBM (Ke et al., 2017)**\n",
    "\n",
    "**–ö–ª—é—á–µ–≤—ã–µ –∏–¥–µ–∏:**\n",
    "\n",
    "**a) Histogram-based splits**\n",
    "\n",
    "–í–º–µ—Å—Ç–æ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –≤—Å–µ—Ö –∑–Ω–∞—á–µ–Ω–∏–π ‚Üí bins (–æ–±—ã—á–Ω–æ 255):\n",
    "\n",
    "$$\\text{bin}(x) = \\left\\lfloor \\frac{x - \\min(x)}{\\text{bin\\_width}} \\right\\rfloor$$\n",
    "\n",
    "**b) GOSS (Gradient-based One-Side Sampling)**\n",
    "\n",
    "```\n",
    "1. –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã –ø–æ |gradient|\n",
    "2. –í–∑—è—Ç—å top-a% (–±–æ–ª—å—à–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã)\n",
    "3. –°–ª—É—á–∞–π–Ω–æ –≤–∑—è—Ç—å b% –∏–∑ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è\n",
    "4. –í–∑–≤–µ—Å–∏—Ç—å –º–∞–ª—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω–∞ (1-a)/b\n",
    "```\n",
    "\n",
    "**c) Leaf-wise (best-first) growth**\n",
    "\n",
    "–í–º–µ—Å—Ç–æ level-wise ‚Üí –≤—ã–±–∏—Ä–∞–µ–º leaf —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º gain:\n",
    "\n",
    "$$\\text{Gain} = \\frac{1}{2} \\left[ \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{(G_L + G_R)^2}{H_L + H_R + \\lambda} \\right] - \\gamma$$\n",
    "\n",
    "**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**\n",
    "- ‚úÖ **5-20x –±—ã—Å—Ç—Ä–µ–µ** XGBoost –Ω–∞ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "- ‚úÖ –ú–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏ (~50%)\n",
    "- ‚úÖ Native categorical support\n",
    "- ‚úÖ –õ—É—á—à–µ –Ω–∞ sparse –¥–∞–Ω–Ω—ã—Ö (EFB)\n",
    "\n",
    "**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:**\n",
    "- ‚ùå –°–∫–ª–æ–Ω–µ–Ω –∫ overfitting (leaf-wise aggressive)\n",
    "- ‚ùå –¢—Ä–µ–±—É–µ—Ç —Ç—â–∞—Ç–µ–ª—å–Ω–æ–≥–æ tuning\n",
    "- ‚ùå –•—É–∂–µ –Ω–∞ –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö (<10k)\n",
    "\n",
    "---\n",
    "\n",
    "**3. CatBoost (Prokhorenkova et al., 2018)**\n",
    "\n",
    "**–ö–ª—é—á–µ–≤—ã–µ –∏–¥–µ–∏:**\n",
    "\n",
    "**a) Ordered Boosting**\n",
    "\n",
    "–†–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã **prediction shift** ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏:\n",
    "\n",
    "$$F_{<i}(x_i) = \\sum_{t=1}^{T} h_t^{<i}(x_i)$$\n",
    "\n",
    "–ú–æ–¥–µ–ª—å $F_{<i}$ –æ–±—É—á–µ–Ω–∞ **–±–µ–∑** –ø—Ä–∏–º–µ—Ä–∞ $i$.\n",
    "\n",
    "**b) Ordered Target Statistics (–¥–ª—è categorical)**\n",
    "\n",
    "$$\\hat{x}_k^i = \\frac{\\sum_{j=1}^{p-1} [x_j = x_k] \\cdot y_{\\sigma_j} + a \\cdot p}{\\sum_{j=1}^{p-1} [x_j = x_k] + a}$$\n",
    "\n",
    "–≥–¥–µ $\\sigma$ ‚Äî —Å–ª—É—á–∞–π–Ω–∞—è –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∞, $p$ ‚Äî –ø–æ–∑–∏—Ü–∏—è –ø—Ä–∏–º–µ—Ä–∞ $i$, $a$ ‚Äî prior weight.\n",
    "\n",
    "**c) Symmetric (Oblivious) Trees**\n",
    "\n",
    "–í—Å–µ –ª–∏—Å—Ç—å—è –Ω–∞ –æ–¥–Ω–æ–º —É—Ä–æ–≤–Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π split ‚Üí –±—ã—Å—Ç—Ä—ã–π inference.\n",
    "\n",
    "**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**\n",
    "- ‚úÖ **–õ—É—á—à–∏–µ categorical features** –∏–∑ –∫–æ—Ä–æ–±–∫–∏\n",
    "- ‚úÖ –ú–µ–Ω—å—à–µ overfitting (ordered boosting)\n",
    "- ‚úÖ –û—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ **–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é** (–º–µ–Ω—å—à–µ tuning)\n",
    "- ‚úÖ GPU support\n",
    "\n",
    "**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:**\n",
    "- ‚ùå –ú–µ–¥–ª–µ–Ω–Ω–µ–µ –æ–±—É—á–µ–Ω–∏–µ (ordered boosting —Å–ª–æ–∂–Ω–µ–µ)\n",
    "- ‚ùå –ë–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏\n",
    "- ‚ùå Symmetric trees –º–æ–≥—É—Ç –±—ã—Ç—å –º–µ–Ω–µ–µ –≥–∏–±–∫–∏–º–∏\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫–æ–π –∞–ª–≥–æ—Ä–∏—Ç–º?\n",
    "\n",
    "#### –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ç–∞–±–ª–∏—Ü–∞ –≤—ã–±–æ—Ä–∞\n",
    "\n",
    "| –°—Ü–µ–Ω–∞—Ä–∏–π | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è | –ü–æ—á–µ–º—É |\n",
    "|----------|-------------|--------|\n",
    "| **–ú–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ (<10k)** | XGBoost | –°—Ç–∞–±–∏–ª—å–Ω–µ–µ, level-wise –±–µ–∑–æ–ø–∞—Å–Ω–µ–µ |\n",
    "| **–°—Ä–µ–¥–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ (10k-1M)** | XGBoost –∏–ª–∏ CatBoost | –ó–∞–≤–∏—Å–∏—Ç –æ—Ç categorical features |\n",
    "| **–ë–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ (>1M)** | LightGBM | –í 5-20x –±—ã—Å—Ç—Ä–µ–µ |\n",
    "| **–ú–Ω–æ–≥–æ categorical** | CatBoost | Native support, ordered target stats |\n",
    "| **–†–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ** | LightGBM | EFB –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è |\n",
    "| **–ü–µ—Ä–≤–∞—è –º–æ–¥–µ–ª—å (baseline)** | XGBoost | –ú–µ–Ω—å—à–µ tuning, —Ö–æ—Ä–æ—à–∏–µ defaults |\n",
    "| **Production-critical** | XGBoost | –ó—Ä–µ–ª–æ—Å—Ç—å, —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å |\n",
    "| **–°–∫–æ—Ä–æ—Å—Ç—å inference** | CatBoost | Symmetric trees –±—ã—Å—Ç—Ä–µ–µ |\n",
    "| **–û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è –ø–∞–º—è—Ç—å** | LightGBM | Histogram-based |\n",
    "| **Kaggle competition** | Ensemble –≤—Å–µ—Ö —Ç—Ä–µ—Ö! | Diversity ‚Üí –ª—É—á—à–µ ensemble |\n",
    "\n",
    "#### –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "\n",
    "**–ò–∑ Kaggle competitions –∏ research:**\n",
    "\n",
    "1. **–ö–∞—á–µ—Å—Ç–≤–æ (ROC-AUC):**\n",
    "   - –ë–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ: LightGBM ‚â• CatBoost > XGBoost (—Ä–∞–∑–Ω–∏—Ü–∞ ~0.5-2%)\n",
    "   - –ú–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ: XGBoost ‚â• CatBoost ‚â• LightGBM\n",
    "   - –° –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º–∏: CatBoost —á–∞—Å—Ç–æ –ª—É—á—à–∏–π\n",
    "\n",
    "2. **–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è:**\n",
    "   - LightGBM: **5-20x** –±—ã—Å—Ç—Ä–µ–µ XGBoost –Ω–∞ >100k –ø—Ä–∏–º–µ—Ä–æ–≤\n",
    "   - CatBoost: **2-3x** –º–µ–¥–ª–µ–Ω–Ω–µ–µ XGBoost (ordered boosting)\n",
    "   - XGBoost: baseline —Å–∫–æ—Ä–æ—Å—Ç—å\n",
    "\n",
    "3. **–ü–∞–º—è—Ç—å:**\n",
    "   - LightGBM: **~50%** –æ—Ç XGBoost\n",
    "   - CatBoost: **~150%** –æ—Ç XGBoost –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏\n",
    "   - XGBoost: baseline\n",
    "\n",
    "4. **Tuning —Å–ª–æ–∂–Ω–æ—Å—Ç—å:**\n",
    "   - XGBoost: –°—Ä–µ–¥–Ω—è—è (—Ö–æ—Ä–æ—à–∏–µ defaults)\n",
    "   - LightGBM: –í—ã—Å–æ–∫–∞—è (leaf-wise –Ω—É–∂–µ–Ω –∫–æ–Ω—Ç—Ä–æ–ª—å)\n",
    "   - CatBoost: **–ù–∏–∑–∫–∞—è** (–æ—Ç–ª–∏—á–Ω—ã–µ defaults!)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Stacking: –ö–æ–≥–¥–∞ –∏ –∫–∞–∫?\n",
    "\n",
    "#### –¢–µ–æ—Ä–∏—è Stacking\n",
    "\n",
    "**Wolpert's Stacked Generalization (1992):**\n",
    "\n",
    "**Level 0 (Base learners):**\n",
    "\n",
    "$$\\hat{y}_i^{(k)} = f_k(x_i), \\quad k = 1, \\ldots, K$$\n",
    "\n",
    "**Level 1 (Meta-learner):**\n",
    "\n",
    "$$\\hat{y}_i = g\\left( \\hat{y}_i^{(1)}, \\hat{y}_i^{(2)}, \\ldots, \\hat{y}_i^{(K)} \\right)$$\n",
    "\n",
    "**Out-of-fold predictions (–∏–∑–±–µ–≥–∞–µ–º overfitting):**\n",
    "\n",
    "```\n",
    "For fold k in K-Fold:\n",
    "  1. Train base learners –Ω–∞ train_k\n",
    "  2. Predict –Ω–∞ val_k ‚Üí oof_predictions_k\n",
    "  3. Predict –Ω–∞ test ‚Üí test_predictions_k\n",
    "  \n",
    "Meta features = concat(oof_predictions_1, ..., oof_predictions_K)\n",
    "Meta learner.fit(Meta features, y)\n",
    "```\n",
    "\n",
    "#### –ö–æ–≥–¥–∞ Stacking —Ä–∞–±–æ—Ç–∞–µ—Ç?\n",
    "\n",
    "**–ö–ª—é—á–µ–≤–æ–µ —É—Å–ª–æ–≤–∏–µ:** Diversity –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π!\n",
    "\n",
    "**Ambiguity Decomposition (Krogh & Vedelsby, 1995):**\n",
    "\n",
    "$$E_{\\text{ensemble}} = \\bar{E} - \\bar{A}$$\n",
    "\n",
    "–≥–¥–µ:\n",
    "- $\\bar{E}$ ‚Äî —Å—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π\n",
    "- $\\bar{A}$ ‚Äî —Å—Ä–µ–¥–Ω—è—è ambiguity (diversity)\n",
    "\n",
    "**–ß–µ–º –±–æ–ª—å—à–µ diversity, —Ç–µ–º –º–µ–Ω—å—à–µ –æ—à–∏–±–∫–∞ ensemble!**\n",
    "\n",
    "#### –ö–∞–∫ —Å–æ–∑–¥–∞—Ç—å diversity?\n",
    "\n",
    "1. **–†–∞–∑–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã:** XGBoost + LightGBM + CatBoost + Logistic Regression\n",
    "2. **–†–∞–∑–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:** –û–¥–∏–Ω —Å polynomial, –¥—Ä—É–≥–æ–π —Å target encoding\n",
    "3. **–†–∞–∑–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã:** Shallow vs deep trees\n",
    "4. **–†–∞–∑–Ω—ã–µ –ø–æ–¥–≤—ã–±–æ—Ä–∫–∏:** Bagging –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —á–∞—Å—Ç—è—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "#### –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:\n",
    "\n",
    "‚úÖ **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Stacking –µ—Å–ª–∏:**\n",
    "- –ï—Å—Ç—å –≤—Ä–µ–º—è –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π\n",
    "- –ù—É–∂–Ω—ã –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø—Ä–æ—Ü–µ–Ω—Ç—ã –∫–∞—á–µ—Å—Ç–≤–∞ (Kaggle, research)\n",
    "- –ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ä–∞–∑–Ω—ã–µ\n",
    "\n",
    "‚ùå **–ù–ï –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ Stacking –µ—Å–ª–∏:**\n",
    "- –ù—É–∂–Ω–∞ —Å–∫–æ—Ä–æ—Å—Ç—å inference (–≤ 3-5x –º–µ–¥–ª–µ–Ω–Ω–µ–µ)\n",
    "- –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã (–æ–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –¥–æ–ª–≥–æ)\n",
    "- –ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏ (–º–∞–ª–æ diversity)\n",
    "- Production —Å–∏—Å—Ç–µ–º–∞ —Å –ø—Ä–æ—Å—Ç–æ—Ç–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏\n",
    "\n",
    "**–¢–∏–ø–∏—á–Ω—ã–π –ø—Ä–∏—Ä–æ—Å—Ç:** 0.5-3% ROC-AUC –ø–æ–≤–µ—Ä—Ö –ª—É—á—à–µ–π –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Feature Engineering: –í–æ–∑–≤—Ä–∞—Ç –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–π\n",
    "\n",
    "#### –í–ª–∏—è–Ω–∏–µ –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏\n",
    "\n",
    "**–≠–º–ø–∏—Ä–∏—á–µ—Å–∫–æ–µ –ø—Ä–∞–≤–∏–ª–æ (Andrew Ng):**\n",
    "\n",
    "| –ö–æ–º–ø–æ–Ω–µ–Ω—Ç | –í–ª–∏—è–Ω–∏–µ –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ | –í—Ä–µ–º—è –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–π |\n",
    "|-----------|---------------------|------------------|\n",
    "| **Feature Engineering** | 60-70% | –í—ã—Å–æ–∫–æ–µ |\n",
    "| Algorithm selection | 15-20% | –°—Ä–µ–¥–Ω–µ–µ |\n",
    "| Hyperparameter tuning | 10-15% | –°—Ä–µ–¥–Ω–µ–µ |\n",
    "| Ensemble | 5-10% | –ù–∏–∑–∫–æ–µ |\n",
    "\n",
    "#### –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ç–µ—Ö–Ω–∏–∫\n",
    "\n",
    "**–ò–∑ –Ω–∞—à–µ–≥–æ –æ–ø—ã—Ç–∞ (Notebook 06):**\n",
    "\n",
    "| –¢–µ—Ö–Ω–∏–∫–∞ | –ü—Ä–∏—Ä–æ—Å—Ç –∫–∞—á–µ—Å—Ç–≤–∞ | –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å |\n",
    "|---------|-----------------|-------------------|\n",
    "| **Log transform** | 5-10% | Skewed –¥–∞–Ω–Ω—ã–µ (—Ü–µ–Ω—ã, –ø–ª–æ—â–∞–¥–∏) |\n",
    "| **Target encoding** | 3-7% | High cardinality categorical |\n",
    "| **Polynomial features** | 5-15% | –õ–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏, –º–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ |\n",
    "| **Feature selection** | 2-5% | –ú–Ω–æ–≥–æ —à—É–º–∞, overfitting |\n",
    "| **Domain features** | 10-30%! | –ö–æ–≥–¥–∞ –µ—Å—Ç—å domain knowledge |\n",
    "\n",
    "#### –í–∞–∂–Ω–æ—Å—Ç—å –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤\n",
    "\n",
    "**Linear models (Logistic Regression, SVM):**\n",
    "- ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê –ö—Ä–∏—Ç–∏—á–Ω–æ! (–Ω—É–∂–Ω—ã interactions, scaling, transformations)\n",
    "\n",
    "**Tree-based models (XGBoost, RF):**\n",
    "- ‚≠ê‚≠ê‚≠ê –ü–æ–ª–µ–∑–Ω–æ (domain features, target encoding)\n",
    "- Polynomial interactions –ù–ï –Ω—É–∂–Ω—ã (–¥–µ—Ä–µ–≤—å—è —Å–∞–º–∏ –∏—Ö –Ω–∞—Ö–æ–¥—è—Ç)\n",
    "\n",
    "**Neural Networks:**\n",
    "- ‚≠ê‚≠ê –ú–æ–∂–µ—Ç –ø–æ–º–æ—á—å, –Ω–æ —Å–µ—Ç–∏ —á–∞—Å—Ç–æ —É—á–∞—Ç —Å–∞–º–∏ (feature learning)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Imbalanced Data: –°—Ç—Ä–∞—Ç–µ–≥–∏—è –≤—ã–±–æ—Ä–∞ —Ç–µ—Ö–Ω–∏–∫–∏\n",
    "\n",
    "#### –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ —Å—Ç–µ–ø–µ–Ω–∏ imbalance\n",
    "\n",
    "| Imbalance ratio | –ö–∞—Ç–µ–≥–æ—Ä–∏—è | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |\n",
    "|----------------|-----------|-------------|\n",
    "| 1:1 - 1:4 | Balanced/Mild | –ù–∏—á–µ–≥–æ –Ω–µ –Ω—É–∂–Ω–æ |\n",
    "| 1:4 - 1:20 | Moderate | Class weights |\n",
    "| 1:20 - 1:100 | High | SMOTE + class weights |\n",
    "| 1:100+ | Extreme | Advanced (Focal loss, sampling) |\n",
    "\n",
    "#### –¢–µ—Ö–Ω–∏–∫–∏ –∏ –∏—Ö –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å\n",
    "\n",
    "**1. Class Weights (scale_pos_weight –≤ XGBoost)**\n",
    "\n",
    "$$w_{\\text{pos}} = \\frac{N_{\\text{neg}}}{N_{\\text{pos}}}$$\n",
    "\n",
    "- ‚úÖ –ü—Ä–æ—Å—Ç–æ—Ç–∞\n",
    "- ‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è tree-based models\n",
    "- ‚úÖ –ù–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "- ‚ùå –ù–µ —Å–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã\n",
    "\n",
    "**2. SMOTE (Synthetic Minority Over-sampling)**\n",
    "\n",
    "$$x_{\\text{new}} = x_i + \\lambda \\cdot (x_{\\text{nn}} - x_i), \\quad \\lambda \\in [0, 1]$$\n",
    "\n",
    "- ‚úÖ –°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã minority –∫–ª–∞—Å—Å–∞\n",
    "- ‚úÖ –ü–æ–º–æ–≥–∞–µ—Ç —Å diversity\n",
    "- ‚ùå –ú–æ–∂–µ—Ç —Å–æ–∑–¥–∞—Ç—å noise\n",
    "- ‚ùå –£–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "**3. Hybrid: SMOTE + Class Weights**\n",
    "\n",
    "- ‚úÖ –õ—É—á—à–µ–µ –∏–∑ –æ–±–æ–∏—Ö –º–∏—Ä–æ–≤\n",
    "- Moderate SMOTE (–¥–æ 1:10) + weights\n",
    "\n",
    "**4. Focal Loss (Lin et al., 2017)**\n",
    "\n",
    "$$FL(p_t) = -\\alpha_t (1 - p_t)^\\gamma \\log(p_t)$$\n",
    "\n",
    "- ‚úÖ –§–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ hard examples\n",
    "- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ down-weights easy examples\n",
    "- ‚ùå –ù—É–∂–Ω–∞ custom —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "\n",
    "#### –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è imbalanced data\n",
    "\n",
    "**–ù–ï –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ Accuracy!**\n",
    "\n",
    "**–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ:**\n",
    "1. **Precision/Recall/F1**\n",
    "2. **PR-AUC** (–ª—É—á—à–µ ROC-AUC –¥–ª—è imbalanced)\n",
    "3. **Business cost** (weighted FP –∏ FN)\n",
    "\n",
    "---\n",
    "\n",
    "## –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è —á–∞—Å—Ç—å –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –ø—Ä–∞–∫—Ç–∏–∫–µ üöÄ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä –ß–∞—Å—Ç—å 2: –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ\n",
    "\n",
    "### 2.1 –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û—Å–Ω–æ–≤–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Gradient Boosting\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "import catboost as cb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Baseline models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, cross_val_predict\n",
    "\n",
    "# Feature engineering\n",
    "from sklearn.preprocessing import PolynomialFeatures, PowerTransformer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Imbalanced data\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    HAS_IMBLEARN = True\n",
    "except ImportError:\n",
    "    print('‚ö†Ô∏è imbalanced-learn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω')\n",
    "    HAS_IMBLEARN = False\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score,\n",
    "    confusion_matrix, classification_report,\n",
    "    roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "# Seed\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print('‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã')\n",
    "print(f'XGBoost: {xgb.__version__}')\n",
    "print(f'LightGBM: {lgb.__version__}')\n",
    "print(f'CatBoost: {cb.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ Telco Churn –¥–∞–Ω–Ω—ã—Ö\n",
    "import os\n",
    "\n",
    "data_path = '../../data/telco_churn.csv'\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    print('‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω!')\n",
    "else:\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f'‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {df.shape[0]:,} —Å—Ç—Ä–æ–∫, {df.shape[1]} —Å—Ç–æ–ª–±—Ü–æ–≤')\n",
    "    print(f'Target: Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–µ—Ä–≤—ã–π –≤–∑–≥–ª—è–¥\n",
    "print('–ü–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏:')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA: Class balance\n",
    "churn_counts = df['Churn'].value_counts()\n",
    "churn_pct = df['Churn'].value_counts(normalize=True) * 100\n",
    "\n",
    "print('Class distribution:')\n",
    "print(f'No: {churn_counts[\"No\"]} ({churn_pct[\"No\"]:.1f}%)')\n",
    "print(f'Yes: {churn_counts[\"Yes\"]} ({churn_pct[\"Yes\"]:.1f}%)')\n",
    "print(f'\\nImbalance ratio: 1:{churn_counts[\"No\"] / churn_counts[\"Yes\"]:.1f}')\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "plt.figure(figsize=(8, 5))\n",
    "churn_counts.plot(kind='bar', color=['green', 'red'], alpha=0.7, edgecolor='black')\n",
    "plt.title('Class Distribution: Churn')\n",
    "plt.xlabel('Churn')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "for i, (idx, val) in enumerate(churn_counts.items()):\n",
    "    plt.text(i, val + 100, f'{val} ({churn_pct[idx]:.1f}%)', ha='center')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nüîç Moderate imbalance (~27% churn) ‚Üí Class weights –∏–ª–∏ moderate SMOTE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "# –£–¥–∞–ª—è–µ–º customerID (–Ω–µ –Ω—É–∂–µ–Ω)\n",
    "if 'customerID' in df.columns:\n",
    "    df = df.drop('customerID', axis=1)\n",
    "\n",
    "# TotalCharges –∏–Ω–æ–≥–¥–∞ –∏–º–µ–µ—Ç ' ' –≤–º–µ—Å—Ç–æ —á–∏—Å–µ–ª - –∏—Å–ø—Ä–∞–≤–ª—è–µ–º\n",
    "if 'TotalCharges' in df.columns:\n",
    "    df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "    df['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)\n",
    "\n",
    "# –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ numeric –∏ categorical\n",
    "numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "categorical_features.remove('Churn')  # Target\n",
    "\n",
    "print(f'Numeric features ({len(numeric_features)}): {numeric_features}')\n",
    "print(f'Categorical features ({len(categorical_features)}): {categorical_features[:5]}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target encoding\n",
    "y = (df['Churn'] == 'Yes').astype(int)\n",
    "\n",
    "# Features\n",
    "X = df.drop('Churn', axis=1)\n",
    "\n",
    "print(f'X shape: {X.shape}')\n",
    "print(f'y shape: {y.shape}')\n",
    "print(f'y distribution: {y.value_counts().to_dict()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split (stratified!)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f'Train: {X_train.shape[0]:,} samples')\n",
    "print(f'Test: {X_test.shape[0]:,} samples')\n",
    "print(f'Train churn rate: {y_train.mean():.1%}')\n",
    "print(f'Test churn rate: {y_test.mean():.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ä–∞–∑–Ω—ã—Ö –≤–µ—Ä—Å–∏–π –¥–∞–Ω–Ω—ã—Ö\\n\",\n",
    "\n",
    "–°–æ–∑–¥–∞–¥–∏–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–µ—Ä—Å–∏–π –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π:\n",
    "1. **One-hot encoded** ‚Äî –¥–ª—è Logistic Regression, XGBoost, LightGBM\n",
    "2. **Native categorical** ‚Äî –¥–ª—è CatBoost\n",
    "3. **Target encoded** ‚Äî –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1: One-hot encoding\n",
    "X_train_ohe = pd.get_dummies(X_train, columns=categorical_features, drop_first=True)\n",
    "X_test_ohe = pd.get_dummies(X_test, columns=categorical_features, drop_first=True)\n",
    "\n",
    "# –í—ã—Ä–æ–≤–Ω—è—Ç—å –∫–æ–ª–æ–Ω–∫–∏\n",
    "X_train_ohe, X_test_ohe = X_train_ohe.align(X_test_ohe, join='left', axis=1, fill_value=0)\n",
    "\n",
    "print(f'One-hot encoded: {X_train_ohe.shape[1]} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2: Label encoding –¥–ª—è CatBoost (–æ–Ω –ø–æ–π–º–µ—Ç categorical —Å–∞–º)\n",
    "X_train_cat = X_train.copy()\n",
    "X_test_cat = X_test.copy()\n",
    "\n",
    "# CatBoost –ø—Ä–∏–Ω–∏–º–∞–µ—Ç categorical –∫–∞–∫ –∏–Ω–¥–µ–∫—Å—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n",
    "cat_indices = []\n",
    "for i, col in enumerate(X_train_cat.columns):\n",
    "    if col in categorical_features:\n",
    "        le = LabelEncoder()\n",
    "        X_train_cat[col] = le.fit_transform(X_train_cat[col].astype(str))\n",
    "        X_test_cat[col] = le.transform(X_test_cat[col].astype(str))\n",
    "        cat_indices.append(i)\n",
    "\n",
    "print(f'CatBoost version: {len(cat_indices)} categorical features at indices {cat_indices[:5]}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è Logistic Regression\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_ohe)\n",
    "X_test_scaled = scaler.transform(X_test_ohe)\n",
    "\n",
    "print('‚úÖ –í—Å–µ –≤–µ—Ä—Å–∏–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 –§—É–Ω–∫—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_tr, X_te, y_tr, y_te, name, train_time=None):\n",
    "    \"\"\"\n",
    "    –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏\n",
    "    \"\"\"\n",
    "    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "    y_pred = model.predict(X_te)\n",
    "    y_proba = model.predict_proba(X_te)[:, 1]\n",
    "    \n",
    "    # –ú–µ—Ç—Ä–∏–∫–∏\n",
    "    acc = accuracy_score(y_te, y_pred)\n",
    "    prec = precision_score(y_te, y_pred)\n",
    "    rec = recall_score(y_te, y_pred)\n",
    "    f1 = f1_score(y_te, y_pred)\n",
    "    roc_auc = roc_auc_score(y_te, y_proba)\n",
    "    pr_auc = average_precision_score(y_te, y_proba)\n",
    "    \n",
    "    # Business cost (FN = $500, FP = $50)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_te, y_pred).ravel()\n",
    "    cost = fn * 500 + fp * 50\n",
    "    \n",
    "    # –í—ã–≤–æ–¥\n",
    "    print(f'\\nüìä {name}:')\n",
    "    print(f'  Accuracy: {acc:.4f}')\n",
    "    print(f'  Precision: {prec:.4f}')\n",
    "    print(f'  Recall: {rec:.4f}')\n",
    "    print(f'  F1-score: {f1:.4f}')\n",
    "    print(f'  ROC-AUC: {roc_auc:.4f}')\n",
    "    print(f'  PR-AUC: {pr_auc:.4f}')\n",
    "    print(f'  Business Cost: ${cost:,}')\n",
    "    if train_time:\n",
    "        print(f'  Train Time: {train_time:.2f}s')\n",
    "    print(f'  Confusion Matrix: TN={tn}, FP={fp}, FN={fn}, TP={tp}')\n",
    "    \n",
    "    return {\n",
    "        'Model': name,\n",
    "        'Accuracy': acc,\n",
    "        'Precision': prec,\n",
    "        'Recall': rec,\n",
    "        'F1': f1,\n",
    "        'ROC-AUC': roc_auc,\n",
    "        'PR-AUC': pr_auc,\n",
    "        'Cost': cost,\n",
    "        'Time': train_time if train_time else 0,\n",
    "        'TP': tp,\n",
    "        'FP': fp,\n",
    "        'TN': tn,\n",
    "        'FN': fn\n",
    "    }\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "results = []\n",
    "\n",
    "print('‚úÖ –§—É–Ω–∫—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ —Å–æ–∑–¥–∞–Ω–∞')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Baseline: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression baseline\n",
    "start = time.time()\n",
    "lr = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "lr_time = time.time() - start\n",
    "\n",
    "lr_results = evaluate_model(lr, X_train_scaled, X_test_scaled, y_train, y_test, \n",
    "                            'Logistic Regression', lr_time)\n",
    "results.append(lr_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 XGBoost: Baseline –∏ Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost baseline\n",
    "start = time.time()\n",
    "xgb_base = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbosity=0\n",
    ")\n",
    "xgb_base.fit(X_train_ohe, y_train)\n",
    "xgb_base_time = time.time() - start\n",
    "\n",
    "xgb_base_results = evaluate_model(xgb_base, X_train_ohe, X_test_ohe, y_train, y_test,\n",
    "                                  'XGBoost Baseline', xgb_base_time)\n",
    "results.append(xgb_base_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost tuned —Å class weights\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "start = time.time()\n",
    "xgb_tuned = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    min_child_weight=3,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    scale_pos_weight=scale_pos_weight,  # Class weights!\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbosity=0\n",
    ")\n",
    "xgb_tuned.fit(X_train_ohe, y_train)\n",
    "xgb_tuned_time = time.time() - start\n",
    "\n",
    "xgb_tuned_results = evaluate_model(xgb_tuned, X_train_ohe, X_test_ohe, y_train, y_test,\n",
    "                                   'XGBoost Tuned + Weights', xgb_tuned_time)\n",
    "results.append(xgb_tuned_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 LightGBM: Baseline –∏ Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM baseline\n",
    "start = time.time()\n",
    "lgbm_base = LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=31,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=-1\n",
    ")\n",
    "lgbm_base.fit(X_train_ohe, y_train)\n",
    "lgbm_base_time = time.time() - start\n",
    "\n",
    "lgbm_base_results = evaluate_model(lgbm_base, X_train_ohe, X_test_ohe, y_train, y_test,\n",
    "                                   'LightGBM Baseline', lgbm_base_time)\n",
    "results.append(lgbm_base_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM tuned —Å class weights\n",
    "start = time.time()\n",
    "lgbm_tuned = LGBMClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    max_depth=6,\n",
    "    min_child_samples=20,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=-1\n",
    ")\n",
    "lgbm_tuned.fit(X_train_ohe, y_train)\n",
    "lgbm_tuned_time = time.time() - start\n",
    "\n",
    "lgbm_tuned_results = evaluate_model(lgbm_tuned, X_train_ohe, X_test_ohe, y_train, y_test,\n",
    "                                    'LightGBM Tuned + Weights', lgbm_tuned_time)\n",
    "results.append(lgbm_tuned_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 CatBoost: Native Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost baseline (—Å native categorical)\n",
    "start = time.time()\n",
    "cat_base = CatBoostClassifier(\n",
    "    iterations=100,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    cat_features=cat_indices,  # NATIVE CATEGORICAL!\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=0\n",
    ")\n",
    "cat_base.fit(X_train_cat, y_train)\n",
    "cat_base_time = time.time() - start\n",
    "\n",
    "cat_base_results = evaluate_model(cat_base, X_train_cat, X_test_cat, y_train, y_test,\n",
    "                                  'CatBoost Baseline (Native Cat)', cat_base_time)\n",
    "results.append(cat_base_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost tuned —Å class weights\n",
    "start = time.time()\n",
    "cat_tuned = CatBoostClassifier(\n",
    "    iterations=200,\n",
    "    learning_rate=0.05,\n",
    "    depth=6,\n",
    "    l2_leaf_reg=3,\n",
    "    cat_features=cat_indices,\n",
    "    class_weights=[1, scale_pos_weight],  # Class weights!\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=0\n",
    ")\n",
    "cat_tuned.fit(X_train_cat, y_train)\n",
    "cat_tuned_time = time.time() - start\n",
    "\n",
    "cat_tuned_results = evaluate_model(cat_tuned, X_train_cat, X_test_cat, y_train, y_test,\n",
    "                                   'CatBoost Tuned + Weights', cat_tuned_time)\n",
    "results.append(cat_tuned_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Imbalanced Data: SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE + XGBoost\n",
    "if HAS_IMBLEARN:\n",
    "    smote = SMOTE(sampling_strategy=0.7, random_state=RANDOM_STATE)  # –î–æ 70% minority\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train_ohe, y_train)\n",
    "    \n",
    "    print(f'–ü–æ—Å–ª–µ SMOTE: {X_train_smote.shape[0]} samples')\n",
    "    print(f'Churn rate: {y_train_smote.mean():.1%}')\n",
    "    \n",
    "    start = time.time()\n",
    "    xgb_smote = XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=4,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbosity=0\n",
    "    )\n",
    "    xgb_smote.fit(X_train_smote, y_train_smote)\n",
    "    xgb_smote_time = time.time() - start\n",
    "    \n",
    "    xgb_smote_results = evaluate_model(xgb_smote, X_train_smote, X_test_ohe, \n",
    "                                       y_train_smote, y_test,\n",
    "                                       'XGBoost + SMOTE', xgb_smote_time)\n",
    "    results.append(xgb_smote_results)\n",
    "else:\n",
    "    print('‚ö†Ô∏è SMOTE –ø—Ä–æ–ø—É—â–µ–Ω (imbalanced-learn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 Stacking Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking: XGBoost + LightGBM + CatBoost ‚Üí Logistic Regression\n",
    "print('üöÄ –°–æ–∑–¥–∞–µ–º Stacking Ensemble...')\n",
    "\n",
    "# Out-of-fold predictions –¥–ª—è meta features\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# –ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (—É–∂–µ –æ–±—É—á–µ–Ω—ã)\n",
    "base_models = [\n",
    "    ('xgb', xgb_tuned, X_train_ohe, X_test_ohe),\n",
    "    ('lgbm', lgbm_tuned, X_train_ohe, X_test_ohe),\n",
    "    ('cat', cat_tuned, X_train_cat, X_test_cat)\n",
    "]\n",
    "\n",
    "# OOF predictions\n",
    "oof_train = np.zeros((X_train.shape[0], len(base_models)))\n",
    "oof_test = np.zeros((X_test.shape[0], len(base_models)))\n",
    "\n",
    "for i, (name, model, X_tr, X_te) in enumerate(base_models):\n",
    "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è test\n",
    "    oof_test[:, i] = model.predict_proba(X_te)[:, 1]\n",
    "    \n",
    "    # OOF –¥–ª—è train —á–µ—Ä–µ–∑ cross-validation\n",
    "    oof_preds = cross_val_predict(model, X_tr, y_train, cv=cv, method='predict_proba')[:, 1]\n",
    "    oof_train[:, i] = oof_preds\n",
    "    \n",
    "    print(f'‚úÖ {name}: OOF –≥–æ—Ç–æ–≤')\n",
    "\n",
    "print(f'Meta features shape: train {oof_train.shape}, test {oof_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta-learner: Logistic Regression\n",
    "start = time.time()\n",
    "meta_learner = LogisticRegression(random_state=RANDOM_STATE)\n",
    "meta_learner.fit(oof_train, y_train)\n",
    "stacking_time = time.time() - start\n",
    "\n",
    "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "stacking_pred_proba = meta_learner.predict_proba(oof_test)[:, 1]\n",
    "stacking_pred = (stacking_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# –û—Ü–µ–Ω–∫–∞\n",
    "acc = accuracy_score(y_test, stacking_pred)\n",
    "prec = precision_score(y_test, stacking_pred)\n",
    "rec = recall_score(y_test, stacking_pred)\n",
    "f1 = f1_score(y_test, stacking_pred)\n",
    "roc_auc = roc_auc_score(y_test, stacking_pred_proba)\n",
    "pr_auc = average_precision_score(y_test, stacking_pred_proba)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, stacking_pred).ravel()\n",
    "cost = fn * 500 + fp * 50\n",
    "\n",
    "print(f'\\nüìä Stacking Ensemble:')\n",
    "print(f'  Accuracy: {acc:.4f}')\n",
    "print(f'  Precision: {prec:.4f}')\n",
    "print(f'  Recall: {rec:.4f}')\n",
    "print(f'  F1-score: {f1:.4f}')\n",
    "print(f'  ROC-AUC: {roc_auc:.4f}')\n",
    "print(f'  PR-AUC: {pr_auc:.4f}')\n",
    "print(f'  Business Cost: ${cost:,}')\n",
    "print(f'  Meta-learner weights: {meta_learner.coef_[0]}')\n",
    "\n",
    "stacking_results = {\n",
    "    'Model': 'Stacking Ensemble',\n",
    "    'Accuracy': acc,\n",
    "    'Precision': prec,\n",
    "    'Recall': rec,\n",
    "    'F1': f1,\n",
    "    'ROC-AUC': roc_auc,\n",
    "    'PR-AUC': pr_auc,\n",
    "    'Cost': cost,\n",
    "    'Time': stacking_time,\n",
    "    'TP': tp,\n",
    "    'FP': fp,\n",
    "    'TN': tn,\n",
    "    'FN': fn\n",
    "}\n",
    "results.append(stacking_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11 –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞–µ–º —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É\n",
    "comparison_df = pd.DataFrame(results)\n",
    "comparison_df = comparison_df.sort_values('ROC-AUC', ascending=False)\n",
    "\n",
    "print('\\n' + '='*100)\n",
    "print('üèÜ –§–ò–ù–ê–õ–¨–ù–û–ï –°–†–ê–í–ù–ï–ù–ò–ï –í–°–ï–• –ú–ï–¢–û–î–û–í PHASE 1')\n",
    "print('='*100)\n",
    "print(comparison_df[['Model', 'ROC-AUC', 'F1', 'Precision', 'Recall', 'Cost', 'Time']].to_string(index=False))\n",
    "print('='*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# ROC-AUC\n",
    "ax = axes[0, 0]\n",
    "bars = ax.barh(comparison_df['Model'], comparison_df['ROC-AUC'], color='skyblue', edgecolor='black')\n",
    "ax.set_xlabel('ROC-AUC')\n",
    "ax.set_title('ROC-AUC Score')\n",
    "ax.axvline(comparison_df['ROC-AUC'].max(), color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# F1-score\n",
    "ax = axes[0, 1]\n",
    "ax.barh(comparison_df['Model'], comparison_df['F1'], color='lightgreen', edgecolor='black')\n",
    "ax.set_xlabel('F1-score')\n",
    "ax.set_title('F1 Score')\n",
    "\n",
    "# Precision vs Recall\n",
    "ax = axes[0, 2]\n",
    "ax.scatter(comparison_df['Recall'], comparison_df['Precision'], s=200, alpha=0.6, edgecolor='black')\n",
    "for i, model in enumerate(comparison_df['Model']):\n",
    "    ax.annotate(model.split()[0], \n",
    "                (comparison_df['Recall'].iloc[i], comparison_df['Precision'].iloc[i]),\n",
    "                fontsize=8)\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_title('Precision vs Recall')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Business Cost\n",
    "ax = axes[1, 0]\n",
    "comparison_sorted_cost = comparison_df.sort_values('Cost')\n",
    "ax.barh(comparison_sorted_cost['Model'], comparison_sorted_cost['Cost'], \n",
    "        color='lightcoral', edgecolor='black')\n",
    "ax.set_xlabel('Business Cost ($)')\n",
    "ax.set_title('Business Cost (Lower is Better)')\n",
    "\n",
    "# Training Time\n",
    "ax = axes[1, 1]\n",
    "comparison_sorted_time = comparison_df.sort_values('Time')\n",
    "ax.barh(comparison_sorted_time['Model'], comparison_sorted_time['Time'], \n",
    "        color='wheat', edgecolor='black')\n",
    "ax.set_xlabel('Time (seconds)')\n",
    "ax.set_title('Training Time')\n",
    "\n",
    "# Confusion Matrix for best model\n",
    "ax = axes[1, 2]\n",
    "best_model = comparison_df.iloc[0]\n",
    "cm = np.array([[best_model['TN'], best_model['FP']], \n",
    "               [best_model['FN'], best_model['TP']]])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, cbar=False)\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "ax.set_title(f'Best Model: {best_model[\"Model\"]}')\n",
    "ax.set_xticklabels(['No Churn', 'Churn'])\n",
    "ax.set_yticklabels(['No Churn', 'Churn'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ –í—ã–≤–æ–¥—ã –∏ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n",
    "\n",
    "### –ß—Ç–æ –º—ã –≤—ã—è—Å–Ω–∏–ª–∏:\n",
    "\n",
    "#### 1. –ö–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π (ROC-AUC)\n",
    "\n",
    "–¢–∏–ø–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç –¥–∞–Ω–Ω—ã—Ö):\n",
    "- **Stacking Ensemble:** –û–±—ã—á–Ω–æ –ª—É—á—à–∏–π (~0.85-0.87)\n",
    "- **CatBoost Tuned:** –ë–ª–∏–∑–∫–æ –∫ stacking (~0.84-0.86)\n",
    "- **XGBoost/LightGBM Tuned:** –°–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ (~0.83-0.85)\n",
    "- **–ë–∞–∑–æ–≤—ã–µ boosting:** –•–æ—Ä–æ—à–æ (~0.81-0.83)\n",
    "- **Logistic Regression:** Baseline (~0.75-0.78)\n",
    "\n",
    "**–í—ã–≤–æ–¥:** –†–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É tuned –º–æ–¥–µ–ª—è–º–∏ —á–∞—Å—Ç–æ <2% ROC-AUC\n",
    "\n",
    "#### 2. –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "- **LightGBM:** –°–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π (–æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö)\n",
    "- **XGBoost:** –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å\n",
    "- **CatBoost:** –ú–µ–¥–ª–µ–Ω–Ω–µ–µ (ordered boosting —Å–ª–æ–∂–Ω–µ–µ)\n",
    "- **Stacking:** –°–∞–º—ã–π –º–µ–¥–ª–µ–Ω–Ω—ã–π (–æ–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π)\n",
    "\n",
    "#### 3. Categorical features\n",
    "\n",
    "**CatBoost —Å native categorical –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ—Ç–ª–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ë–ï–ó:**\n",
    "- One-hot encoding\n",
    "- Target encoding\n",
    "- Feature engineering\n",
    "\n",
    "**–í—ã–≤–æ–¥:** –î–ª—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º–∏ - CatBoost –ª—É—á—à–∏–π –≤—ã–±–æ—Ä –∏–∑ –∫–æ—Ä–æ–±–∫–∏!\n",
    "\n",
    "#### 4. Imbalanced data\n",
    "\n",
    "**Class weights vs SMOTE:**\n",
    "- Class weights: –ü—Ä–æ—â–µ, –±—ã—Å—Ç—Ä–µ–µ, —Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ –¥–ª—è moderate imbalance\n",
    "- SMOTE: –ú–æ–∂–µ—Ç –ø–æ–º–æ—á—å –¥–ª—è extreme imbalance, –Ω–æ —Ä–∏—Å–∫ noise\n",
    "- **Hybrid** (moderate SMOTE + weights): –ß–∞—Å—Ç–æ –ª—É—á—à–∏–π –≤–∞—Ä–∏–∞–Ω—Ç\n",
    "\n",
    "#### 5. Stacking\n",
    "\n",
    "**–ü—Ä–∏—Ä–æ—Å—Ç:** –û–±—ã—á–Ω–æ 0.5-2% –ø–æ–≤–µ—Ä—Ö –ª—É—á—à–µ–π –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏\n",
    "\n",
    "**–°—Ç–æ–∏—Ç –ª–∏?**\n",
    "- ‚úÖ Kaggle, research: –î–∞!\n",
    "- ‚ùå Production —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏: –í–µ—Ä–æ—è—Ç–Ω–æ –Ω–µ—Ç\n",
    "\n",
    "---\n",
    "\n",
    "### –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è Production\n",
    "\n",
    "#### –°—Ü–µ–Ω–∞—Ä–∏–π 1: –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç\n",
    "\n",
    "**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** XGBoost —Å –¥–µ—Ñ–æ–ª—Ç–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n",
    "\n",
    "```python\n",
    "XGBClassifier(n_estimators=100, learning_rate=0.1)\n",
    "```\n",
    "\n",
    "- ‚úÖ –•–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑ –∫–æ—Ä–æ–±–∫–∏\n",
    "- ‚úÖ –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å\n",
    "- ‚úÖ –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π tuning\n",
    "\n",
    "#### –°—Ü–µ–Ω–∞—Ä–∏–π 2: –ú–Ω–æ–≥–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "\n",
    "**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** CatBoost —Å native categorical\n",
    "\n",
    "```python\n",
    "CatBoostClassifier(iterations=100, cat_features=cat_indices)\n",
    "```\n",
    "\n",
    "- ‚úÖ –ù–µ –Ω—É–∂–µ–Ω one-hot encoding\n",
    "- ‚úÖ –û—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é\n",
    "- ‚úÖ Ordered target statistics –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏\n",
    "\n",
    "#### –°—Ü–µ–Ω–∞—Ä–∏–π 3: –ë–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ (>1M —Å—Ç—Ä–æ–∫)\n",
    "\n",
    "**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** LightGBM\n",
    "\n",
    "```python\n",
    "LGBMClassifier(n_estimators=100, num_leaves=31)\n",
    "```\n",
    "\n",
    "- ‚úÖ –í 5-20x –±—ã—Å—Ç—Ä–µ–µ XGBoost\n",
    "- ‚úÖ –ú–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏\n",
    "- ‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç tuning –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è overfitting\n",
    "\n",
    "#### –°—Ü–µ–Ω–∞—Ä–∏–π 4: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ (Kaggle)\n",
    "\n",
    "**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** Stacking Ensemble\n",
    "\n",
    "```python\n",
    "Base: XGBoost + LightGBM + CatBoost + RF\n",
    "Meta: LogisticRegression / XGBoost\n",
    "```\n",
    "\n",
    "- ‚úÖ –í—ã–∂–∏–º–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø—Ä–æ—Ü–µ–Ω—Ç—ã\n",
    "- ‚úÖ Diversity –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π\n",
    "- ‚ùå –°–ª–æ–∂–Ω–æ—Å—Ç—å –∏ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "#### –°—Ü–µ–Ω–∞—Ä–∏–π 5: Imbalanced data\n",
    "\n",
    "**Moderate imbalance (1:10):**\n",
    "```python\n",
    "scale_pos_weight = n_negative / n_positive\n",
    "XGBClassifier(scale_pos_weight=scale_pos_weight)\n",
    "```\n",
    "\n",
    "**Extreme imbalance (1:100+):**\n",
    "```python\n",
    "smote = SMOTE(sampling_strategy=0.1)  # Moderate oversample\n",
    "+ class_weights\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### –û–±—â–∏–µ Best Practices\n",
    "\n",
    "1. **–í—Å–µ–≥–¥–∞ –Ω–∞—á–∏–Ω–∞–π—Ç–µ —Å baseline** (Logistic Regression)\n",
    "2. **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ cross-validation** –¥–ª—è —á–µ—Å—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–∏\n",
    "3. **Class imbalance:** –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ PR-AUC, –Ω–µ accuracy!\n",
    "4. **Feature engineering:** 60-70% –∫–∞—á–µ—Å—Ç–≤–∞, –∏–Ω–≤–µ—Å—Ç–∏—Ä—É–π—Ç–µ –≤—Ä–µ–º—è\n",
    "5. **Hyperparameter tuning:** –¢–æ–ª—å–∫–æ –ø–æ—Å–ª–µ feature engineering\n",
    "6. **Ensemble:** –ü–æ—Å–ª–µ–¥–Ω–∏–π —à–∞–≥, –µ—Å–ª–∏ –Ω—É–∂–Ω—ã –ø—Ä–æ—Ü–µ–Ω—Ç—ã\n",
    "7. **Production:** –ü—Ä–æ—Å—Ç–æ—Ç–∞ > —Å–ª–æ–∂–Ω–æ—Å—Ç—å (XGBoost –∏–ª–∏ CatBoost)\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Phase 1 Classical ML –∑–∞–≤–µ—Ä—à–µ–Ω!\n",
    "\n",
    "**–ú—ã –∏–∑—É—á–∏–ª–∏:**\n",
    "- ‚úÖ XGBoost, LightGBM, CatBoost –≤ –¥–µ—Ç–∞–ª—è—Ö\n",
    "- ‚úÖ Stacking Ensemble\n",
    "- ‚úÖ Imbalanced data —Ç–µ—Ö–Ω–∏–∫–∏\n",
    "- ‚úÖ Advanced Feature Engineering\n",
    "- ‚úÖ –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "**–°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:**\n",
    "- Phase 2: Deep Learning (MLP, CNN, RNN, Transformers)\n",
    "- Phase 3: Time Series\n",
    "- Phase 4: NLP\n",
    "- Phase 5: Computer Vision\n",
    "\n",
    "**–ü–æ–∑–¥—Ä–∞–≤–ª—è—é! üöÄ**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
#!/usr/bin/env python3
"""
–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —á–∞—Å—Ç–∏ –≤ MLP notebook - PyTorch —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
"""

import json

# –ß–∏—Ç–∞–µ–º
notebook_path = '01_mlp_basics.ipynb'
with open(notebook_path, 'r', encoding='utf-8') as f:
    notebook = json.load(f)

practical_cells = []

# ============================================================================
# PRACTICAL: IMPORTS
# ============================================================================

practical_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "## üìä –ß–∞—Å—Ç—å 2: –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "\n",
        "### 2.1 –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –û—Å–Ω–æ–≤–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "# Baseline –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette('husl')\n",
        "%matplotlib inline\n",
        "\n",
        "# Seed\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "torch.manual_seed(RANDOM_STATE)\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "print(f'PyTorch version: {torch.__version__}')\n",
        "\n",
        "print('‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã')"
    ]
})

# ============================================================================
# DATA LOADING
# ============================================================================

practical_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 2.2 –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¢–∏—Ç–∞–Ω–∏–∫–∞\n",
        "import os\n",
        "\n",
        "data_path = '../../data/titanic_train.csv'\n",
        "\n",
        "if not os.path.exists(data_path):\n",
        "    print('‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω! –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø—É—Ç—å...')\n",
        "    # –ü–æ–ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–æ–π –ø—É—Ç—å\n",
        "    data_path = '../titanic/titanic_train.csv'\n",
        "    if not os.path.exists(data_path):\n",
        "        print('‚ùå –î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Å Kaggle.')\n",
        "else:\n",
        "    df = pd.read_csv(data_path)\n",
        "    print(f'‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {df.shape[0]:,} —Å—Ç—Ä–æ–∫, {df.shape[1]} —Å—Ç–æ–ª–±—Ü–æ–≤')\n",
        "    print(f'Target: Survived')"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ü–µ—Ä–≤—ã–π –≤–∑–≥–ª—è–¥\n",
        "df.head()"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ø—Ä–æ—Å—Ç–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏)\n",
        "# –í—ã–±–µ—Ä–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "\n",
        "# –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏\n",
        "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
        "df['Fare'].fillna(df['Fare'].median(), inplace=True)\n",
        "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "df['Sex'] = (df['Sex'] == 'male').astype(int)\n",
        "df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
        "df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n",
        "\n",
        "# One-hot –¥–ª—è Embarked\n",
        "df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)\n",
        "\n",
        "# –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "features = ['Pclass', 'Sex', 'Age', 'Fare', 'FamilySize', 'IsAlone'] + \\\n",
        "           [col for col in df.columns if 'Embarked_' in col]\n",
        "\n",
        "X = df[features].values\n",
        "y = df['Survived'].values\n",
        "\n",
        "print(f'Features: {features}')\n",
        "print(f'X shape: {X.shape}')\n",
        "print(f'y shape: {y.shape}')\n",
        "print(f'Survival rate: {y.mean():.1%}')"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Train/Val/Test split\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.2, random_state=RANDOM_STATE, stratify=y_temp\n",
        ")\n",
        "\n",
        "print(f'Train: {X_train.shape[0]:,} samples')\n",
        "print(f'Val: {X_val.shape[0]:,} samples')\n",
        "print(f'Test: {X_test.shape[0]:,} samples')"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ (–∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π!)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f'Scaled X_train mean: {X_train_scaled.mean():.4f}')\n",
        "print(f'Scaled X_train std: {X_train_scaled.std():.4f}')\n",
        "print('‚úÖ –î–∞–Ω–Ω—ã–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω—ã (mean=0, std=1)')"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
        "y_train_tensor = torch.FloatTensor(y_train)\n",
        "\n",
        "X_val_tensor = torch.FloatTensor(X_val_scaled)\n",
        "y_val_tensor = torch.FloatTensor(y_val)\n",
        "\n",
        "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
        "y_test_tensor = torch.FloatTensor(y_test)\n",
        "\n",
        "print(f'X_train_tensor shape: {X_train_tensor.shape}')\n",
        "print(f'y_train_tensor shape: {y_train_tensor.shape}')"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –°–æ–∑–¥–∞–µ–º DataLoaders –¥–ª—è mini-batch training\n",
        "batch_size = 32\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f'Batch size: {batch_size}')\n",
        "print(f'Number of batches (train): {len(train_loader)}')\n",
        "print(f'Number of batches (val): {len(val_loader)}')"
    ]
})

# ============================================================================
# MLP ARCHITECTURE
# ============================================================================

practical_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 2.3 –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ MLP –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ü—Ä–æ—Å—Ç–∞—è MLP –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1=64, hidden_dim2=32, dropout_rate=0.3):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_dim1)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        \n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_dim2)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "        \n",
        "        self.fc3 = nn.Linear(hidden_dim2, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Layer 1: Linear ‚Üí BatchNorm ‚Üí ReLU ‚Üí Dropout\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.dropout1(x)\n",
        "        \n",
        "        # Layer 2: Linear ‚Üí BatchNorm ‚Üí ReLU ‚Üí Dropout\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        \n",
        "        # Output: Linear ‚Üí Sigmoid\n",
        "        x = self.fc3(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏\n",
        "input_dim = X_train_scaled.shape[1]\n",
        "model = SimpleMLP(input_dim=input_dim)\n",
        "model = model.to(device)\n",
        "\n",
        "print(model)\n",
        "print(f'\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
        "print(f'Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}')"
    ]
})

# ============================================================================
# TRAINING
# ============================================================================

practical_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 2.4 –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Loss –∏ optimizer\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(f'Loss function: {criterion}')\n",
        "print(f'Optimizer: {optimizer}')"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Training —Ñ—É–Ω–∫—Ü–∏—è\n",
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for X_batch, y_batch in loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device).unsqueeze(1)  # (batch_size, 1)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        \n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # –ú–µ—Ç—Ä–∏–∫–∏\n",
        "        total_loss += loss.item() * X_batch.size(0)\n",
        "        predicted = (outputs > 0.5).float()\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "        total += y_batch.size(0)\n",
        "    \n",
        "    avg_loss = total_loss / total\n",
        "    accuracy = correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Validation —Ñ—É–Ω–∫—Ü–∏—è\n",
        "def validate_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device).unsqueeze(1)\n",
        "            \n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            \n",
        "            total_loss += loss.item() * X_batch.size(0)\n",
        "            predicted = (outputs > 0.5).float()\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "            total += y_batch.size(0)\n",
        "            \n",
        "            all_preds.extend(outputs.cpu().numpy())\n",
        "            all_labels.extend(y_batch.cpu().numpy())\n",
        "    \n",
        "    avg_loss = total_loss / total\n",
        "    accuracy = correct / total\n",
        "    return avg_loss, accuracy, all_preds, all_labels\n",
        "\n",
        "print('‚úÖ Training functions –≥–æ—Ç–æ–≤—ã')"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Training loop\n",
        "num_epochs = 100\n",
        "patience = 10\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accs = []\n",
        "val_accs = []\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "best_model_state = None\n",
        "\n",
        "print('–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...')\n",
        "print('='*60)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    \n",
        "    # Validate\n",
        "    val_loss, val_acc, _, _ = validate_epoch(model, val_loader, criterion, device)\n",
        "    \n",
        "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_accs.append(val_acc)\n",
        "    \n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model_state = model.state_dict().copy()\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "    \n",
        "    # Print –∫–∞–∂–¥—ã–µ 10 —ç–ø–æ—Ö\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
        "        print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "        print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "        print(f'  Best Val Loss: {best_val_loss:.4f}, Patience: {patience_counter}/{patience}')\n",
        "    \n",
        "    # Early stopping\n",
        "    if patience_counter >= patience:\n",
        "        print(f'\\nEarly stopping –Ω–∞ epoch {epoch+1}')\n",
        "        break\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å\n",
        "model.load_state_dict(best_model_state)\n",
        "\n",
        "print('='*60)\n",
        "print(f'‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!')\n",
        "print(f'Best validation loss: {best_val_loss:.4f}')\n",
        "print(f'Total epochs: {epoch+1}')"
    ]
})

# ============================================================================
# VISUALIZATION
# ============================================================================

practical_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 2.5 –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Loss\n",
        "ax1.plot(train_losses, label='Train Loss', alpha=0.8)\n",
        "ax1.plot(val_losses, label='Val Loss', alpha=0.8)\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss (BCE)')\n",
        "ax1.set_title('Training and Validation Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy\n",
        "ax2.plot(train_accs, label='Train Accuracy', alpha=0.8)\n",
        "ax2.plot(val_accs, label='Val Accuracy', alpha=0.8)\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_title('Training and Validation Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f'Final train accuracy: {train_accs[-1]:.4f}')\n",
        "print(f'Final val accuracy: {val_accs[-1]:.4f}')\n",
        "print(f'Overfitting: {(train_accs[-1] - val_accs[-1]):.4f}')"
    ]
})

# ============================================================================
# EVALUATION
# ============================================================================

practical_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 2.6 –û—Ü–µ–Ω–∫–∞ –Ω–∞ test set"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –û—Ü–µ–Ω–∫–∞ –Ω–∞ test\n",
        "test_loss, test_acc, test_preds, test_labels = validate_epoch(\n",
        "    model, test_loader, criterion, device\n",
        ")\n",
        "\n",
        "# –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ numpy\n",
        "test_preds_np = np.array(test_preds).flatten()\n",
        "test_labels_np = np.array(test_labels).flatten()\n",
        "test_preds_binary = (test_preds_np > 0.5).astype(int)\n",
        "\n",
        "# –ú–µ—Ç—Ä–∏–∫–∏\n",
        "test_precision = precision_score(test_labels_np, test_preds_binary)\n",
        "test_recall = recall_score(test_labels_np, test_preds_binary)\n",
        "test_f1 = f1_score(test_labels_np, test_preds_binary)\n",
        "test_roc_auc = roc_auc_score(test_labels_np, test_preds_np)\n",
        "\n",
        "print('üìä MLP Test Results:')\n",
        "print('='*50)\n",
        "print(f'  Test Loss: {test_loss:.4f}')\n",
        "print(f'  Test Accuracy: {test_acc:.4f}')\n",
        "print(f'  Precision: {test_precision:.4f}')\n",
        "print(f'  Recall: {test_recall:.4f}')\n",
        "print(f'  F1-score: {test_f1:.4f}')\n",
        "print(f'  ROC-AUC: {test_roc_auc:.4f}')\n",
        "print('='*50)"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(test_labels_np, test_preds_binary)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('MLP Confusion Matrix (Test Set)')\n",
        "plt.show()\n",
        "\n",
        "print(f'True Negatives: {cm[0,0]}')\n",
        "print(f'False Positives: {cm[0,1]}')\n",
        "print(f'False Negatives: {cm[1,0]}')\n",
        "print(f'True Positives: {cm[1,1]}')"
    ]
})

# ============================================================================
# COMPARISON WITH XGBOOST
# ============================================================================

practical_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 2.7 –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å XGBoost (Phase 1)"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# XGBoost –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
        "print('–û–±—É—á–∞–µ–º XGBoost –¥–ª—è —á–µ—Å—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è...')\n",
        "\n",
        "xgb_model = XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=4,\n",
        "    random_state=RANDOM_STATE,\n",
        "    verbosity=0\n",
        ")\n",
        "\n",
        "xgb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è XGBoost\n",
        "xgb_preds = xgb_model.predict(X_test_scaled)\n",
        "xgb_proba = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# –ú–µ—Ç—Ä–∏–∫–∏ XGBoost\n",
        "xgb_acc = accuracy_score(y_test, xgb_preds)\n",
        "xgb_precision = precision_score(y_test, xgb_preds)\n",
        "xgb_recall = recall_score(y_test, xgb_preds)\n",
        "xgb_f1 = f1_score(y_test, xgb_preds)\n",
        "xgb_roc_auc = roc_auc_score(y_test, xgb_proba)\n",
        "\n",
        "print('‚úÖ XGBoost –æ–±—É—á–µ–Ω')"
    ]
})

practical_cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞\n",
        "comparison = pd.DataFrame({\n",
        "    'Model': ['MLP (PyTorch)', 'XGBoost'],\n",
        "    'Accuracy': [test_acc, xgb_acc],\n",
        "    'Precision': [test_precision, xgb_precision],\n",
        "    'Recall': [test_recall, xgb_recall],\n",
        "    'F1-score': [test_f1, xgb_f1],\n",
        "    'ROC-AUC': [test_roc_auc, xgb_roc_auc]\n",
        "})\n",
        "\n",
        "print('\\n' + '='*70)\n",
        "print('üèÜ –°–†–ê–í–ù–ï–ù–ò–ï: MLP vs XGBoost')\n",
        "print('='*70)\n",
        "print(comparison.to_string(index=False))\n",
        "print('='*70)\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "\n",
        "metrics = ['Accuracy', 'F1-score', 'ROC-AUC']\n",
        "for i, metric in enumerate(metrics):\n",
        "    axes[i].bar(['MLP', 'XGBoost'], comparison[metric], alpha=0.7, edgecolor='black')\n",
        "    axes[i].set_ylabel(metric)\n",
        "    axes[i].set_title(f'{metric} Comparison')\n",
        "    axes[i].set_ylim([0.7, 0.9])\n",
        "    axes[i].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è\n",
        "    for j, v in enumerate(comparison[metric]):\n",
        "        axes[i].text(j, v + 0.01, f'{v:.4f}', ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
    ]
})

# ============================================================================
# CONCLUSIONS
# ============================================================================

practical_cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "## üéØ –í—ã–≤–æ–¥—ã\n",
        "\n",
        "### –ß—Ç–æ –º—ã –∏–∑—É—á–∏–ª–∏:\n",
        "\n",
        "1. **MLP –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:**\n",
        "   - –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏ (Linear)\n",
        "   - –§—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (ReLU, Sigmoid)\n",
        "   - Batch Normalization\n",
        "   - Dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏\n",
        "\n",
        "2. **–û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π:**\n",
        "   - Backpropagation –∏ gradient descent\n",
        "   - Adam optimizer\n",
        "   - Binary Cross-Entropy loss\n",
        "   - Early stopping\n",
        "   - Mini-batch training\n",
        "\n",
        "3. **PyTorch –æ—Å–Ω–æ–≤—ã:**\n",
        "   - `nn.Module` –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–∏\n",
        "   - `DataLoader` –¥–ª—è batch processing\n",
        "   - Training loop (forward ‚Üí loss ‚Üí backward ‚Üí update)\n",
        "   - GPU support\n",
        "\n",
        "### MLP vs XGBoost –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:\n",
        "\n",
        "**–¢–∏–ø–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:**\n",
        "- MLP: Accuracy ~78-82%, ROC-AUC ~0.82-0.85\n",
        "- XGBoost: Accuracy ~80-84%, ROC-AUC ~0.84-0.87\n",
        "\n",
        "**–í—ã–≤–æ–¥:** XGBoost –æ–±—ã—á–Ω–æ **–ª—É—á—à–µ** –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö!\n",
        "\n",
        "### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å MLP?\n",
        "\n",
        "‚úÖ **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ MLP –∫–æ–≥–¥–∞:**\n",
        "- –ú–Ω–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö (>100k –ø—Ä–∏–º–µ—Ä–æ–≤)\n",
        "- –°–ª–æ–∂–Ω—ã–µ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏\n",
        "- –ù—É–∂–µ–Ω ensemble —Å Gradient Boosting\n",
        "- Transfer learning (–ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–º–µ–∂–Ω–æ–π –∑–∞–¥–∞—á–µ)\n",
        "- –ò–∑—É—á–µ–Ω–∏–µ Deep Learning –æ—Å–Ω–æ–≤\n",
        "\n",
        "‚ùå **–ù–ï –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ MLP –∫–æ–≥–¥–∞:**\n",
        "- –ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö (<10k)\n",
        "- –ù—É–∂–Ω–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å\n",
        "- –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã (XGBoost –±—ã—Å—Ç—Ä–µ–µ)\n",
        "- Production —Å–∏—Å—Ç–µ–º–∞ (XGBoost –ø—Ä–æ—â–µ deploy)\n",
        "\n",
        "### –ö–ª—é—á–µ–≤—ã–µ —É—Ä–æ–∫–∏:\n",
        "\n",
        "1. **Scaling –∫—Ä–∏—Ç–∏—á–µ–Ω!** –ù–µ–π—Ä–æ—Å–µ—Ç–∏ —Ç—Ä–µ–±—É—é—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ (mean=0, std=1)\n",
        "2. **Batch Normalization** —É—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ\n",
        "3. **Dropout** –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç overfitting\n",
        "4. **Early stopping** —ç–∫–æ–Ω–æ–º–∏—Ç –≤—Ä–µ–º—è –∏ —É–ª—É—á—à–∞–µ—Ç –æ–±–æ–±—â–µ–Ω–∏–µ\n",
        "5. **Learning rate** - –∫—Ä–∏—Ç–∏—á–Ω—ã–π –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä\n",
        "6. **Adam optimizer** —Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ out-of-the-box\n",
        "\n",
        "### –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:\n",
        "\n",
        "1. **–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã:**\n",
        "   - –†–∞–∑–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã (–≥–ª—É–±–∂–µ, —à–∏—Ä–µ)\n",
        "   - –î—Ä—É–≥–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (ELU, LeakyReLU)\n",
        "   - Learning rate scheduling\n",
        "   - –†–∞–∑–Ω—ã–µ optimizers (SGD, AdamW)\n",
        "\n",
        "2. **1D-CNN:** Convolutional layers –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "3. **Autoencoders:** Unsupervised learning, anomaly detection\n",
        "4. **Transfer Learning:** –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥—Ä—É–≥–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "\n",
        "---\n",
        "\n",
        "## üéâ Phase 2, Step 1 –∑–∞–≤–µ—Ä—à–µ–Ω!\n",
        "\n",
        "–í—ã –æ—Å–≤–æ–∏–ª–∏ –æ—Å–Ω–æ–≤—ã Deep Learning —Å MLP. –¢–µ–ø–µ—Ä—å –≥–æ—Ç–æ–≤—ã –∫ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º! üöÄ\n"
    ]
})

# –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —è—á–µ–π–∫–∏
for cell in practical_cells:
    notebook['cells'].append(cell)

# –°–æ—Ö—Ä–∞–Ω—è–µ–º
with open(notebook_path, 'w', encoding='utf-8') as f:
    json.dump(notebook, f, ensure_ascii=False, indent=1)

print(f'‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–∞: {len(practical_cells)} —è—á–µ–µ–∫')
print(f'–í—Å–µ–≥–æ —è—á–µ–µ–∫ –≤ –Ω–æ—É—Ç–±—É–∫–µ: {len(notebook["cells"])}')

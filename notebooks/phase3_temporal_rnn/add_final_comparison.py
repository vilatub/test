#!/usr/bin/env python3
"""
–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è, –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ Attention –∏ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –≤—ã–≤–æ–¥–æ–≤
"""

import json

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –Ω–æ—É—Ç–±—É–∫
notebook_path = '/home/user/test/notebooks/phase3_temporal_rnn/03_attention_seq2seq.ipynb'
with open(notebook_path, 'r', encoding='utf-8') as f:
    notebook = json.load(f)

cells = notebook['cells']

# ============================================================================
# TRAINING FUNCTIONS
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "## üèãÔ∏è –ß–∞—Å—Ç—å 3: –û–±—É—á–µ–Ω–∏–µ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ\n",
        "\n",
        "### 3.1 –§—É–Ω–∫—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "def train_seq2seq(model, train_loader, criterion, optimizer, num_epochs=100, patience=15):\n",
        "    \"\"\"\n",
        "    –û–±—É—á–µ–Ω–∏–µ Seq2Seq –º–æ–¥–µ–ª–∏\n",
        "    \"\"\"\n",
        "    history = {'train_loss': []}\n",
        "    best_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        \n",
        "        for X_batch, y_batch in train_loader:\n",
        "            # Forward pass\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            \n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            \n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            \n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        history['train_loss'].append(avg_loss)\n",
        "        \n",
        "        # Early stopping\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "        \n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.6f}\")\n",
        "        \n",
        "        if patience_counter >= patience:\n",
        "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "    \n",
        "    return history\n",
        "\n",
        "def evaluate_seq2seq(model, X, y, scaler):\n",
        "    \"\"\"\n",
        "    –û—Ü–µ–Ω–∫–∞ Seq2Seq –º–æ–¥–µ–ª–∏\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        predictions = model(X).cpu().numpy()\n",
        "    \n",
        "    # –î–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "    y_true = scaler.inverse_transform(y.cpu().numpy().reshape(-1, 1))\n",
        "    y_pred = scaler.inverse_transform(predictions.reshape(-1, 1))\n",
        "    \n",
        "    # –ú–µ—Ç—Ä–∏–∫–∏\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    \n",
        "    # Reshape –æ–±—Ä–∞—Ç–Ω–æ –≤ (n_samples, output_len)\n",
        "    y_true_seq = y_true.reshape(-1, OUTPUT_SEQ_LEN)\n",
        "    y_pred_seq = y_pred.reshape(-1, OUTPUT_SEQ_LEN)\n",
        "    \n",
        "    return y_pred_seq, y_true_seq, rmse, mae\n",
        "\n",
        "print(\"‚úÖ –§—É–Ω–∫—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è –≥–æ—Ç–æ–≤—ã\")"
    ]
})

# ============================================================================
# TRAIN MODELS
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 3.2 –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
        "NUM_EPOCHS = 200\n",
        "LEARNING_RATE = 0.001\n",
        "PATIENCE = 20\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Training Configuration\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Input sequence length: {INPUT_SEQ_LEN}\")\n",
        "print(f\"Output sequence length: {OUTPUT_SEQ_LEN}\")\n",
        "print(f\"Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"Learning Rate: {LEARNING_RATE}\")\n",
        "print(f\"Batch Size: {BATCH_SIZE}\")\n",
        "print(\"=\"*60)"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Training Simple Seq2Seq\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training Simple Seq2Seq (–±–µ–∑ Attention)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "simple_optimizer = optim.Adam(simple_seq2seq.parameters(), lr=LEARNING_RATE)\n",
        "simple_history = train_seq2seq(simple_seq2seq, train_loader, criterion, \n",
        "                               simple_optimizer, NUM_EPOCHS, PATIENCE)\n",
        "\n",
        "simple_pred, simple_true, simple_rmse, simple_mae = evaluate_seq2seq(\n",
        "    simple_seq2seq, X_test_tensor, y_test_tensor, scaler\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Simple Seq2Seq:\")\n",
        "print(f\"   RMSE: {simple_rmse:.2f}\")\n",
        "print(f\"   MAE: {simple_mae:.2f}\")"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Training Seq2Seq with Attention\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training Seq2Seq —Å Attention\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "attention_optimizer = optim.Adam(seq2seq_attention.parameters(), lr=LEARNING_RATE)\n",
        "attention_history = train_seq2seq(seq2seq_attention, train_loader, criterion,\n",
        "                                  attention_optimizer, NUM_EPOCHS, PATIENCE)\n",
        "\n",
        "attention_pred, attention_true, attention_rmse, attention_mae = evaluate_seq2seq(\n",
        "    seq2seq_attention, X_test_tensor, y_test_tensor, scaler\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Seq2Seq + Attention:\")\n",
        "print(f\"   RMSE: {attention_rmse:.2f}\")\n",
        "print(f\"   MAE: {attention_mae:.2f}\")"
    ]
})

# ============================================================================
# ATTENTION VISUALIZATION
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 3.3 –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è Attention Weights\n",
        "\n",
        "**–°–∞–º–æ–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–µ:** –ü–æ—Å–º–æ—Ç—Ä–∏–º, –∫—É–¥–∞ –º–æ–¥–µ–ª—å \"—Å–º–æ—Ç—Ä–∏—Ç\" –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏!"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ü–æ–ª—É—á–∞–µ–º attention weights –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞\n",
        "seq2seq_attention.eval()\n",
        "with torch.no_grad():\n",
        "    sample_idx = 0\n",
        "    sample_input = X_test_tensor[sample_idx:sample_idx+1]\n",
        "    sample_output, sample_attention = seq2seq_attention(\n",
        "        sample_input, return_attention=True\n",
        "    )\n",
        "\n",
        "# Attention weights: (1, output_len, input_len)\n",
        "attention_weights = sample_attention.cpu().numpy()[0]\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è heatmap\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(\n",
        "    attention_weights,\n",
        "    cmap='YlOrRd',\n",
        "    xticklabels=[f't-{INPUT_SEQ_LEN-i}' for i in range(INPUT_SEQ_LEN)],\n",
        "    yticklabels=[f't+{i+1}' for i in range(OUTPUT_SEQ_LEN)],\n",
        "    cbar_kws={'label': 'Attention Weight'},\n",
        "    annot=True,\n",
        "    fmt='.2f',\n",
        "    linewidths=0.5\n",
        ")\n",
        "\n",
        "plt.title('Attention Weights Heatmap\\n' + \n",
        "          '–°—Ç—Ä–æ–∫–∏ = –≤—ã—Ö–æ–¥–Ω—ã–µ —à–∞–≥–∏, –°—Ç–æ–ª–±—Ü—ã = –≤—Ö–æ–¥–Ω—ã–µ —à–∞–≥–∏',\n",
        "          fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Input Time Steps', fontsize=12)\n",
        "plt.ylabel('Output Time Steps', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è Attention:\")\n",
        "print(\"  - –ë–æ–ª–µ–µ —Å–≤–µ—Ç–ª—ã–µ —è—á–µ–π–∫–∏ = –º–æ–¥–µ–ª—å —É–¥–µ–ª—è–µ—Ç –±–æ–ª—å—à–µ –≤–Ω–∏–º–∞–Ω–∏—è\")\n",
        "print(\"  - –î–ª—è t+1: –º–æ–¥–µ–ª—å —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —à–∞–≥–∏ (–Ω–µ–¥–∞–≤–Ω–µ–µ –ø—Ä–æ—à–ª–æ–µ)\")\n",
        "print(\"  - –î–ª—è t+2, t+3: –º–æ–∂–µ—Ç —Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ —Å–µ–∑–æ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (12 –º–µ—Å—è—Ü–µ–≤ –Ω–∞–∑–∞–¥)\")\n",
        "print(\"  - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—É—á–∞–µ—Ç—Å—è —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞—Å—Ç—è—Ö!\")"
    ]
})

# ============================================================================
# COMPARISON
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 3.4 –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –ø–æ–¥—Ö–æ–¥–æ–≤"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Training curves\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(simple_history['train_loss'], label='Simple Seq2Seq', linewidth=2)\n",
        "plt.plot(attention_history['train_loss'], label='Seq2Seq + Attention', linewidth=2)\n",
        "plt.title('Training Loss Comparison', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Loss (MSE)', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Attention –º–æ–¥–µ–ª—å —Å—Ö–æ–¥–∏—Ç—Å—è –±—ã—Å—Ç—Ä–µ–µ –∏ –∫ –º–µ–Ω—å—à–µ–º—É loss\")"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Predictions visualization\n",
        "# –í—ã–±–∏—Ä–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
        "num_examples = 3\n",
        "\n",
        "fig, axes = plt.subplots(num_examples, 1, figsize=(14, 4*num_examples))\n",
        "\n",
        "for i in range(num_examples):\n",
        "    ax = axes[i] if num_examples > 1 else axes\n",
        "    \n",
        "    # –ò—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
        "    true_vals = simple_true[i]\n",
        "    \n",
        "    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "    simple_vals = simple_pred[i]\n",
        "    attention_vals = attention_pred[i]\n",
        "    \n",
        "    # X-axis\n",
        "    x = np.arange(OUTPUT_SEQ_LEN)\n",
        "    \n",
        "    ax.plot(x, true_vals, 'o-', label='True', linewidth=3, markersize=8, color='black')\n",
        "    ax.plot(x, simple_vals, 's--', label='Simple Seq2Seq', linewidth=2, markersize=6)\n",
        "    ax.plot(x, attention_vals, '^--', label='Seq2Seq + Attention', linewidth=2, markersize=6)\n",
        "    \n",
        "    ax.set_title(f'Multi-step Forecast Example {i+1}', fontsize=14, fontweight='bold')\n",
        "    ax.set_xlabel('Future Time Steps', fontsize=12)\n",
        "    ax.set_ylabel('Passengers', fontsize=12)\n",
        "    ax.legend()\n",
        "    ax.grid(alpha=0.3)\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels([f't+{j+1}' for j in x])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üìä Attention –º–æ–¥–µ–ª—å –æ–±—ã—á–Ω–æ —Ç–æ—á–Ω–µ–µ, –æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ –¥–∞–ª—å–Ω–∏—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞—Ö (t+2, t+3)\")"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Metrics comparison\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': ['Simple Seq2Seq', 'Seq2Seq + Attention'],\n",
        "    'RMSE': [simple_rmse, attention_rmse],\n",
        "    'MAE': [simple_mae, attention_mae],\n",
        "    'Parameters': [\n",
        "        sum(p.numel() for p in simple_seq2seq.parameters()),\n",
        "        sum(p.numel() for p in seq2seq_attention.parameters())\n",
        "    ],\n",
        "    'Attention': ['‚ùå', '‚úÖ']\n",
        "})\n",
        "\n",
        "comparison_df = comparison_df.sort_values('RMSE')\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MULTI-STEP FORECASTING COMPARISON\")\n",
        "print(\"=\"*70)\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Bar plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "comparison_df.plot(x='Model', y='RMSE', kind='bar', ax=axes[0], \n",
        "                   legend=False, color='steelblue')\n",
        "axes[0].set_title('RMSE Comparison', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('RMSE', fontsize=12)\n",
        "axes[0].set_xlabel('')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "comparison_df.plot(x='Model', y='MAE', kind='bar', ax=axes[1], \n",
        "                   legend=False, color='coral')\n",
        "axes[1].set_title('MAE Comparison', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('MAE', fontsize=12)\n",
        "axes[1].set_xlabel('')\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
    ]
})

# ============================================================================
# CONCLUSIONS - PHASE 3 FINALE
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "## üéì –í—ã–≤–æ–¥—ã: Phase 3 Complete\n",
        "\n",
        "### üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã Advanced RNN\n",
        "\n",
        "**Attention –º–µ—Ö–∞–Ω–∏–∑–º:**\n",
        "- ‚úÖ **–£–ª—É—á—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å** –Ω–∞ multi-step forecasting\n",
        "- ‚úÖ **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å:** –≤–∏–¥–∏–º, –∫—É–¥–∞ –º–æ–¥–µ–ª—å —Å–º–æ—Ç—Ä–∏—Ç\n",
        "- ‚úÖ **–ì–∏–±–∫–æ—Å—Ç—å:** –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Ñ–æ–∫—É—Å –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —á–∞—Å—Ç—è—Ö –≤—Ö–æ–¥–∞\n",
        "- ‚ö†Ô∏è **–ë–æ–ª—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤** ‚Üí —Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –Ω–∞ –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "\n",
        "**Seq2Seq –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:**\n",
        "- ‚úÖ **–ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–∞—è** –¥–ª—è multi-step forecasting\n",
        "- ‚úÖ **Encoder-Decoder** —Ä–∞–∑–¥–µ–ª—è–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é\n",
        "- ‚úÖ **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è** –Ω–∞ –¥–ª–∏–Ω–Ω—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "- ‚ùå **–°–ª–æ–∂–Ω–µ–µ** –æ–±—É—á–∞—Ç—å, —á–µ–º –ø—Ä–æ—Å—Ç–æ–π LSTM\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ –ò—Ç–æ–≥–∏ Phase 3: Temporal Data & RNN\n",
        "\n",
        "–ú—ã –ø—Ä–æ—à–ª–∏ –ø–æ–ª–Ω—ã–π –ø—É—Ç—å –æ—Ç –∫–ª–∞—Å—Å–∏–∫–∏ –∫ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º –º–µ—Ç–æ–¥–∞–º:\n",
        "\n",
        "#### Step 1: Classical Time Series\n",
        "- **ARIMA/SARIMA:** —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏, –æ—Ç–ª–∏—á–Ω–æ –Ω–∞ –º–∞–ª—ã—Ö univariate –¥–∞–Ω–Ω—ã—Ö\n",
        "- **Prophet:** –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è, –±–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫–∏, –ø—Ä–∞–∑–¥–Ω–∏–∫–∏\n",
        "- **–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:** < 1000 —Ç–æ—á–µ–∫, —á–µ—Ç–∫–∞—è —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å, –Ω—É–∂–Ω–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è\n",
        "\n",
        "#### Step 2: RNN/LSTM/GRU\n",
        "- **Vanilla RNN:** –ø—Ä–æ–±–ª–µ–º–∞ vanishing gradient\n",
        "- **LSTM:** —Ä–µ—à–∞–µ—Ç long-term dependencies —á–µ—Ä–µ–∑ gates\n",
        "- **GRU:** –±–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏\n",
        "- **–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:** > 1000 —Ç–æ—á–µ–∫, multivariate, –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã\n",
        "\n",
        "#### Step 3: Attention & Seq2Seq (—Å–µ–≥–æ–¥–Ω—è)\n",
        "- **Attention:** –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π —Ñ–æ–∫—É—Å –Ω–∞ –≤–∞–∂–Ω—ã—Ö —á–∞—Å—Ç—è—Ö\n",
        "- **Seq2Seq:** multi-step forecasting, encoder-decoder\n",
        "- **–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:** –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, multi-step ahead, –Ω—É–∂–Ω–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è\n",
        "\n",
        "---\n",
        "\n",
        "### üìà Practical Decision Tree\n",
        "\n",
        "**–í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤:**\n",
        "\n",
        "```\n",
        "START\n",
        "  ‚Üì\n",
        "–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö?\n",
        "  ‚îú‚îÄ < 500 —Ç–æ—á–µ–∫ ‚Üí ARIMA/SARIMA/Prophet\n",
        "  ‚îî‚îÄ > 500 —Ç–æ—á–µ–∫ ‚Üí ‚Üì\n",
        "       ‚Üì\n",
        "Univariate –∏–ª–∏ Multivariate?\n",
        "  ‚îú‚îÄ Univariate ‚Üí SARIMA vs LSTM (–ø–æ–ø—Ä–æ–±—É–π—Ç–µ –æ–±–∞)\n",
        "  ‚îî‚îÄ Multivariate ‚Üí LSTM/GRU\n",
        "       ‚Üì\n",
        "–ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è?\n",
        "  ‚îú‚îÄ 1 —à–∞–≥ (t+1) ‚Üí Simple LSTM\n",
        "  ‚îî‚îÄ –ú–Ω–æ–≥–æ —à–∞–≥–æ–≤ (t+1..t+N) ‚Üí Seq2Seq\n",
        "       ‚Üì\n",
        "–ù—É–∂–Ω–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è?\n",
        "  ‚îú‚îÄ –î–∞ ‚Üí Seq2Seq + Attention\n",
        "  ‚îî‚îÄ –ù–µ—Ç ‚Üí –ü—Ä–æ—Å—Ç–æ–π Seq2Seq –±—ã—Å—Ç—Ä–µ–µ\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ –ß—Ç–æ –¥–∞–ª—å—à–µ? Phase 4: Transformers\n",
        "\n",
        "**–ü—Ä–æ–±–ª–µ–º—ã RNN (–¥–∞–∂–µ —Å Attention):**\n",
        "- ‚ùå –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–º–µ–¥–ª–µ–Ω–Ω–æ)\n",
        "- ‚ùå –°–ª–æ–∂–Ω–æ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–æ–≤–∞—Ç—å\n",
        "- ‚ùå –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n",
        "\n",
        "**–†–µ—à–µ–Ω–∏–µ: Transformers**\n",
        "- ‚úÖ **Self-Attention:** –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
        "- ‚úÖ **Positional Encoding:** –ø–æ—Ä—è–¥–æ–∫ –±–µ–∑ —Ä–µ–∫—É—Ä—Å–∏–∏\n",
        "- ‚úÖ **Scalable:** –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –æ–≥—Ä–æ–º–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "\n",
        "**Transformers –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤:**\n",
        "- Temporal Fusion Transformer (Google)\n",
        "- Informer (–¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–µ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ)\n",
        "- TabTransformer (—Ç–∞–±–ª–∏—á–Ω—ã–µ + –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)\n",
        "\n",
        "**Transformers –¥–ª—è NLP/Vision:**\n",
        "- BERT, GPT (—Ç–µ–∫—Å—Ç)\n",
        "- Vision Transformer (ViT)\n",
        "- CLIP (–º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—å)\n",
        "\n",
        "---\n",
        "\n",
        "### üí° –ö–ª—é—á–µ–≤—ã–µ —É—Ä–æ–∫–∏ Phase 3\n",
        "\n",
        "**1. –ù–µ –≤—Å–µ–≥–¥–∞ —Å–ª–æ–∂–Ω–æ–µ = –ª—É—á—à–µ–µ**\n",
        "- SARIMA —á–∞—Å—Ç–æ –ø–æ–±–µ–∂–¥–∞–µ—Ç LSTM –Ω–∞ –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "- –ù–∞—á–∏–Ω–∞–π—Ç–µ —Å –ø—Ä–æ—Å—Ç–æ–≥–æ, —É—Å–ª–æ–∂–Ω—è–π—Ç–µ –ø–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏\n",
        "\n",
        "**2. Attention = –º–æ—â—å + –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è**\n",
        "- –£–ª—É—á—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å\n",
        "- –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –ö–ê–ö –º–æ–¥–µ–ª—å –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Ä–µ—à–µ–Ω–∏—è\n",
        "- –ö—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è production –≤ —Ä–µ–≥—É–ª–∏—Ä—É–µ–º—ã—Ö –∏–Ω–¥—É—Å—Ç—Ä–∏—è—Ö\n",
        "\n",
        "**3. Multi-step forecasting ‚â† –º–Ω–æ–≥–æ 1-step –º–æ–¥–µ–ª–µ–π**\n",
        "- Seq2Seq —É—á–∏—Ç—ã–≤–∞–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É –≤—ã—Ö–æ–¥–∞–º–∏\n",
        "- –ú–µ–Ω—å—à–µ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫\n",
        "\n",
        "**4. –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –≤–∞–∂–µ–Ω**\n",
        "- Deep Learning —Ç—Ä–µ–±—É–µ—Ç –¥–∞–Ω–Ω—ã—Ö\n",
        "- < 1k —Ç–æ—á–µ–∫: –∫–ª–∞—Å—Å–∏–∫–∞\n",
        "- > 10k —Ç–æ—á–µ–∫: DL –Ω–∞—á–∏–Ω–∞–µ—Ç –ø–æ–±–µ–∂–¥–∞—Ç—å\n",
        "\n",
        "---\n",
        "\n",
        "### üìö –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã\n",
        "\n",
        "**Attention –º–µ—Ö–∞–Ω–∏–∑–º:**\n",
        "- [\"Attention Is All You Need\" (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)\n",
        "- [\"Neural Machine Translation by Jointly Learning to Align and Translate\" (Bahdanau et al., 2014)](https://arxiv.org/abs/1409.0473)\n",
        "- [Visualizing Attention (distill.pub)](https://distill.pub/2016/augmented-rnns/)\n",
        "\n",
        "**Seq2Seq:**\n",
        "- [\"Sequence to Sequence Learning\" (Sutskever et al., 2014)](https://arxiv.org/abs/1409.3215)\n",
        "- [PyTorch Seq2Seq Tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n",
        "\n",
        "**Time Series Forecasting:**\n",
        "- [\"Temporal Fusion Transformers\" (Google, 2021)](https://arxiv.org/abs/1912.09363)\n",
        "- [\"Deep Learning for Time Series Forecasting\" (Januschowski et al., 2020)](https://arxiv.org/abs/2004.10240)\n",
        "\n",
        "---\n",
        "\n",
        "**üéâ Phase 3: Temporal Data & RNN - COMPLETE!**\n",
        "\n",
        "**–î–æ—Å—Ç–∏–∂–µ–Ω–∏—è:**\n",
        "- ‚úÖ –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã: ARIMA, SARIMA, Prophet\n",
        "- ‚úÖ –†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ —Å–µ—Ç–∏: RNN, LSTM, GRU\n",
        "- ‚úÖ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏: Attention, Seq2Seq\n",
        "- ‚úÖ Multi-step forecasting\n",
        "- ‚úÖ –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å —á–µ—Ä–µ–∑ Attention\n",
        "\n",
        "**Next Phase:** Transformers –∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã üöÄ"
    ]
})

# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –Ω–æ—É—Ç–±—É–∫
notebook['cells'] = cells

with open(notebook_path, 'w', encoding='utf-8') as f:
    json.dump(notebook, f, ensure_ascii=False, indent=1)

print(f'‚úÖ –§–∏–Ω–∞–ª—å–Ω–∞—è —á–∞—Å—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω–∞: {notebook_path}')
print(f'–í—Å–µ–≥–æ —è—á–µ–µ–∫: {len(cells)}')
print('üéâ Phase 3 FINALE - Attention & Seq2Seq –Ω–æ—É—Ç–±—É–∫ –≥–æ—Ç–æ–≤!')

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîß Advanced Feature Engineering\n",
    "\n",
    "**–¶–µ–ª—å:** –£–≥–ª—É–±–ª–µ–Ω–Ω–æ–µ –∏–∑—É—á–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏–∫ —Å–æ–∑–¥–∞–Ω–∏—è –∏ –æ—Ç–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ ML –º–æ–¥–µ–ª–µ–π\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ –¶–µ–ª–∏ –Ω–æ—É—Ç–±—É–∫–∞\n",
    "\n",
    "1. **Polynomial features** –∏ feature interactions\n",
    "2. **Feature transformations:** log, sqrt, Box-Cox, Yeo-Johnson\n",
    "3. **Binning** –∏ discretization\n",
    "4. **Target encoding** –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "5. **Feature selection:** Filter, Wrapper, Embedded –º–µ—Ç–æ–¥—ã\n",
    "6. **–ü—Ä–∞–∫—Ç–∏–∫–∞:** House Prices —Å feature engineering –æ—Ç baseline –¥–æ advanced\n",
    "\n",
    "---\n",
    "\n",
    "## üíº –ë–∏–∑–Ω–µ—Å-–∑–∞–¥–∞—á–∞: House Price Prediction\n",
    "\n",
    "**–ö–æ–Ω—Ç–µ–∫—Å—Ç:** Real estate –∫–æ–º–ø–∞–Ω–∏—è —Ö–æ—á–µ—Ç —Ç–æ—á–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Ü–µ–Ω—ã –¥–æ–º–æ–≤.\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º—ã:**\n",
    "- üè† **–ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:** –ü–ª–æ—â–∞–¥—å √ó –ö–∞—á–µ—Å—Ç–≤–æ –≤–ª–∏—è–µ—Ç –Ω–∞ —Ü–µ–Ω—É –Ω–µ–ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ\n",
    "- üìä **Skewed —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è:** –¶–µ–Ω—ã –∏ –ø–ª–æ—â–∞–¥–∏ –∏–º–µ—é—Ç long tail\n",
    "- üî¢ **–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:** –†–∞–π–æ–Ω, —Ç–∏–ø –¥–æ–º–∞ (high cardinality)\n",
    "- üßÆ **–ú–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:** 80+ features, –º–Ω–æ–≥–∏–µ redundant\n",
    "\n",
    "**–¶–µ–ª—å:** –°–Ω–∏–∑–∏—Ç—å RMSE –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ü–µ–Ω —á–µ—Ä–µ–∑ advanced feature engineering\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö –ß–∞—Å—Ç—å 1: –¢–µ–æ—Ä–∏—è Advanced Feature Engineering\n",
    "\n",
    "### 1.1 –ó–∞—á–µ–º –Ω—É–∂–µ–Ω Feature Engineering?\n",
    "\n",
    "**–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:** Feature engineering ‚Äî –ø—Ä–æ—Ü–µ—Å—Å —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "#### –ü–æ—á–µ–º—É —ç—Ç–æ –≤–∞–∂–Ω–æ?\n",
    "\n",
    "**–≠–º–ø–∏—Ä–∏—á–µ—Å–∫–æ–µ –ø—Ä–∞–≤–∏–ª–æ (Andrew Ng):**\n",
    "\n",
    "> \"Coming up with features is difficult, time-consuming, requires expert knowledge.  \n",
    "> Applied machine learning is basically feature engineering.\" ‚Äî Andrew Ng\n",
    "\n",
    "**–í–ª–∏—è–Ω–∏–µ –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ:**\n",
    "\n",
    "| –ö–æ–º–ø–æ–Ω–µ–Ω—Ç | –í–ª–∏—è–Ω–∏–µ –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ |\n",
    "|-----------|--------------------|\n",
    "| **Feature engineering** | 60-70% |\n",
    "| Algorithm selection | 15-20% |\n",
    "| Hyperparameter tuning | 10-15% |\n",
    "| Ensemble | 5-10% |\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä—ã –∏–∑ –ø—Ä–∞–∫—Ç–∏–∫–∏:**\n",
    "\n",
    "- **Kaggle:** –¢–æ–ø —Ä–µ—à–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Å–æ—Ç–Ω–∏ engineered features\n",
    "- **Industry:** Feature engineering —á–∞—Å—Ç–æ –¥–∞–µ—Ç –±–æ–ª—å—à–∏–π –ø—Ä–∏—Ä–æ—Å—Ç, —á–µ–º —Å–º–µ–Ω–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–∞\n",
    "\n",
    "#### –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è:\n",
    "\n",
    "1. **Feature creation** ‚Äî —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "2. **Feature transformation** ‚Äî –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö\n",
    "3. **Feature selection** ‚Äî –æ—Ç–±–æ—Ä –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã—Ö\n",
    "4. **Feature extraction** ‚Äî –ø–æ–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (PCA, –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Polynomial Features –∏ Interactions\n",
    "\n",
    "#### Polynomial Features\n",
    "\n",
    "**–ò–¥–µ—è:** –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç–µ–ø–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π.\n",
    "\n",
    "**–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏:**\n",
    "\n",
    "–î–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ $x_1, x_2, \\ldots, x_n$ –∏ —Å—Ç–µ–ø–µ–Ω–∏ $d$:\n",
    "\n",
    "$$\\phi(x_1, x_2) = [1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]$$\n",
    "\n",
    "–î–ª—è —Å—Ç–µ–ø–µ–Ω–∏ 2:\n",
    "\n",
    "$$\\phi(\\mathbf{x}) = [1, x_1, x_2, \\ldots, x_n, x_1^2, x_1 x_2, \\ldots, x_n^2]$$\n",
    "\n",
    "**–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:**\n",
    "\n",
    "$$N_{\\text{poly}} = \\binom{n + d}{d} = \\frac{(n+d)!}{d! \\cdot n!}$$\n",
    "\n",
    "–î–ª—è $n=10$ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\n",
    "- –°—Ç–µ–ø–µ–Ω—å 2: $\\binom{10+2}{2} = 66$ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "- –°—Ç–µ–ø–µ–Ω—å 3: $\\binom{10+3}{3} = 286$ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä (House Prices):**\n",
    "\n",
    "```python\n",
    "# –ò—Å—Ö–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "x1 = LotArea     # –ü–ª–æ—â–∞–¥—å —É—á–∞—Å—Ç–∫–∞\n",
    "x2 = OverallQual # –ö–∞—á–µ—Å—Ç–≤–æ (1-10)\n",
    "\n",
    "# Polynomial degree 2\n",
    "features = [\n",
    "    1,                    # bias\n",
    "    x1,                   # LotArea\n",
    "    x2,                   # OverallQual\n",
    "    x1^2,                 # LotArea^2 (–±–æ–ª—å—à–∏–µ —É—á–∞—Å—Ç–∫–∏ —Ü–µ–Ω–Ω–µ–µ –Ω–µ–ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "    x1 * x2,              # LotArea √ó OverallQual (INTERACTION!)\n",
    "    x2^2                  # OverallQual^2\n",
    "]\n",
    "```\n",
    "\n",
    "#### Feature Interactions\n",
    "\n",
    "**–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:** –ü—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –¥–≤—É—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –æ—Ç—Ä–∞–∂–∞—é—â–µ–µ –∏—Ö —Å–æ–≤–º–µ—Å—Ç–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ.\n",
    "\n",
    "**–ö–æ–≥–¥–∞ –≤–∞–∂–Ω–æ:**\n",
    "\n",
    "–ï—Å–ª–∏ –≤–ª–∏—è–Ω–∏–µ $x_1$ –Ω–∞ $y$ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∑–Ω–∞—á–µ–Ω–∏—è $x_2$:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\varepsilon$$\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä—ã:**\n",
    "\n",
    "| –ó–∞–¥–∞—á–∞ | –ü—Ä–∏–∑–Ω–∞–∫–∏ | Interaction | –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è |\n",
    "|--------|----------|-------------|---------------|\n",
    "| House prices | `GrLivArea` √ó `OverallQual` | –ü–ª–æ—â–∞–¥—å √ó –ö–∞—á–µ—Å—Ç–≤–æ | –ë–æ–ª—å—à–æ–π –¥–æ–º –Ω–∏–∑–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ —Å—Ç–æ–∏—Ç –º–µ–Ω—å—à–µ |\n",
    "| Ecommerce | `Price` √ó `Discount` | –¶–µ–Ω–∞ √ó –°–∫–∏–¥–∫–∞ | –°–∫–∏–¥–∫–∞ –Ω–∞ –¥–æ—Ä–æ–≥–æ–π —Ç–æ–≤–∞—Ä –≤–∞–∂–Ω–µ–µ |\n",
    "| Credit scoring | `Income` √ó `Debt` | –î–æ—Ö–æ–¥ √ó –î–æ–ª–≥ | Debt-to-income ratio |\n",
    "\n",
    "**Sklearn —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:**\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(\n",
    "    degree=2,              # –°—Ç–µ–ø–µ–Ω—å\n",
    "    interaction_only=False, # False = –≤–∫–ª—é—á–∏—Ç—å x^2, True = —Ç–æ–ª—å–∫–æ x1*x2\n",
    "    include_bias=False      # –ù–µ –¥–æ–±–∞–≤–ª—è—Ç—å —Å—Ç–æ–ª–±–µ—Ü –µ–¥–∏–Ω–∏—Ü\n",
    ")\n",
    "X_poly = poly.fit_transform(X)\n",
    "```\n",
    "\n",
    "**‚ö†Ô∏è –û—Å—Ç–æ—Ä–æ–∂–Ω–æ:**\n",
    "\n",
    "- **Curse of dimensionality:** –ß–∏—Å–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Ä–∞—Å—Ç–µ—Ç –∫–∞–∫ $O(n^d)$\n",
    "- **Overfitting:** –ù—É–∂–Ω–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (Ridge, Lasso)\n",
    "- **Multicollinearity:** –ü—Ä–∏–∑–Ω–∞–∫–∏ —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—Ç\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Feature Transformations\n",
    "\n",
    "#### –ó–∞—á–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤—ã–≤–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏?\n",
    "\n",
    "1. **–£–º–µ–Ω—å—à–∏—Ç—å skewness** (–∞—Å–∏–º–º–µ—Ç—Ä–∏—é)\n",
    "2. **–°—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å variance**\n",
    "3. **–°–¥–µ–ª–∞—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –±–ª–∏–∂–µ –∫ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–º—É** (–≤–∞–∂–Ω–æ –¥–ª—è –ª–∏–Ω–µ–π–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π)\n",
    "4. **–£–ª—É—á—à–∏—Ç—å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å**\n",
    "\n",
    "#### 1.3.1 Log Transform\n",
    "\n",
    "**–§–æ—Ä–º—É–ª–∞:**\n",
    "\n",
    "$$x_{\\text{log}} = \\log(x + c)$$\n",
    "\n",
    "–≥–¥–µ $c$ ‚Äî –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞ (–æ–±—ã—á–Ω–æ 1), —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å $\\log(0)$.\n",
    "\n",
    "**–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**\n",
    "\n",
    "- Right-skewed –¥–∞–Ω–Ω—ã–µ (long tail –≤–ø—Ä–∞–≤–æ)\n",
    "- –î–∞–Ω–Ω—ã–µ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø–æ—Ä—è–¥–∫–∞–º–∏ –≤–µ–ª–∏—á–∏–Ω—ã (—Ü–µ–Ω—ã, –¥–æ—Ö–æ–¥—ã, –ø–ª–æ—â–∞–¥–∏)\n",
    "\n",
    "**–≠—Ñ—Ñ–µ–∫—Ç:**\n",
    "\n",
    "- –°–∂–∏–º–∞–µ—Ç –±–æ–ª—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "- –†–∞—Å—Ç—è–≥–∏–≤–∞–µ—Ç –º–∞–ª—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä:**\n",
    "\n",
    "```python\n",
    "# –î–æ: [1, 10, 100, 1000, 10000]\n",
    "# –ü–æ—Å–ª–µ log: [0, 2.3, 4.6, 6.9, 9.2]\n",
    "```\n",
    "\n",
    "**–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:**\n",
    "\n",
    "–í —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ $\\log(y) = \\beta_0 + \\beta_1 x$:\n",
    "\n",
    "- –£–≤–µ–ª–∏—á–µ–Ω–∏–µ $x$ –Ω–∞ 1 ‚Üí $y$ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è –Ω–∞ $e^{\\beta_1}$ —Ä–∞–∑ (–Ω–µ –Ω–∞ $\\beta_1$!)\n",
    "\n",
    "#### 1.3.2 Square Root Transform\n",
    "\n",
    "**–§–æ—Ä–º—É–ª–∞:**\n",
    "\n",
    "$$x_{\\text{sqrt}} = \\sqrt{x}$$\n",
    "\n",
    "**–ö–æ–≥–¥–∞:**\n",
    "\n",
    "- Count data (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–±—ã—Ç–∏–π)\n",
    "- Moderate skewness (log —Å–ª–∏—à–∫–æ–º –∞–≥—Ä–µ—Å—Å–∏–≤–µ–Ω)\n",
    "\n",
    "#### 1.3.3 Box-Cox Transform\n",
    "\n",
    "**–§–æ—Ä–º—É–ª–∞:**\n",
    "\n",
    "$$\n",
    "x_{\\text{boxcox}}(\\lambda) = \n",
    "\\begin{cases}\n",
    "\\frac{x^\\lambda - 1}{\\lambda}, & \\text{if } \\lambda \\neq 0 \\\\\n",
    "\\log(x), & \\text{if } \\lambda = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "–≥–¥–µ $\\lambda$ –ø–æ–¥–±–∏—Ä–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥–æ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è.\n",
    "\n",
    "**–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏:**\n",
    "\n",
    "- $\\lambda = 1$: –ù–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ ($x - 1$)\n",
    "- $\\lambda = 0.5$: Square root\n",
    "- $\\lambda = 0$: Log transform\n",
    "- $\\lambda = -1$: Reciprocal ($1/x$)\n",
    "\n",
    "**‚ö†Ô∏è –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ:** –†–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –¥–ª—è $x > 0$!\n",
    "\n",
    "#### 1.3.4 Yeo-Johnson Transform\n",
    "\n",
    "**–§–æ—Ä–º—É–ª–∞:**\n",
    "\n",
    "$$\n",
    "x_{\\text{yj}}(\\lambda) = \n",
    "\\begin{cases}\n",
    "\\frac{(x+1)^\\lambda - 1}{\\lambda}, & \\text{if } \\lambda \\neq 0, x \\geq 0 \\\\\n",
    "\\log(x+1), & \\text{if } \\lambda = 0, x \\geq 0 \\\\\n",
    "\\frac{1 - (1-x)^{2-\\lambda}}{2-\\lambda}, & \\text{if } \\lambda \\neq 2, x < 0 \\\\\n",
    "-\\log(1-x), & \\text{if } \\lambda = 2, x < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ:** –†–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è **–ª—é–±—ã—Ö** –∑–Ω–∞—á–µ–Ω–∏–π (–≤–∫–ª—é—á–∞—è –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∏ –Ω—É–ª–∏)!\n",
    "\n",
    "**Sklearn —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:**\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Box-Cox (—Ç–æ–ª—å–∫–æ x > 0)\n",
    "pt_boxcox = PowerTransformer(method='box-cox')\n",
    "\n",
    "# Yeo-Johnson (–ª—é–±—ã–µ x)\n",
    "pt_yj = PowerTransformer(method='yeo-johnson')\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Binning –∏ Discretization\n",
    "\n",
    "**–ò–¥–µ—è:** –ü—Ä–µ–≤—Ä–∞—Ç–∏—Ç—å continuous –ø—Ä–∏–∑–Ω–∞–∫ –≤ categorical (–¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—è).\n",
    "\n",
    "#### –ó–∞—á–µ–º?\n",
    "\n",
    "1. **–ù–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å:** –ó–∞—Ö–≤–∞—Ç piecewise-linear –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π\n",
    "2. **Robustness:** –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ outliers\n",
    "3. **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å:** –ü—Ä–æ—â–µ –æ–±—ä—è—Å–Ω–∏—Ç—å –±–∏–∑–Ω–µ—Å—É (\"–º–æ–ª–æ–¥—ã–µ\", \"—Å—Ä–µ–¥–Ω–∏–µ\", \"–ø–æ–∂–∏–ª—ã–µ\")\n",
    "4. **Tree models:** –ò–Ω–æ–≥–¥–∞ –ø–æ–º–æ–≥–∞–µ—Ç –¥–µ—Ä–µ–≤—å—è–º –Ω–∞–π—Ç–∏ split points\n",
    "\n",
    "#### –¢–∏–ø—ã binning:\n",
    "\n",
    "**1. Equal-width binning (—Ä–∞–≤–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã)**\n",
    "\n",
    "$$\\text{width} = \\frac{\\max(x) - \\min(x)}{k}$$\n",
    "\n",
    "Bins: $[\\min, \\min + w), [\\min + w, \\min + 2w), \\ldots$\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º–∞:** Bins –º–æ–≥—É—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º–∏ –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ø–æ—á—Ç–∏ –≤—Å–µ –¥–∞–Ω–Ω—ã–µ (–µ—Å–ª–∏ skewed).\n",
    "\n",
    "**2. Equal-frequency binning (–∫–≤–∞–Ω—Ç–∏–ª–∏)**\n",
    "\n",
    "–ö–∞–∂–¥—ã–π bin —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ $n/k$ –ø—Ä–∏–º–µ—Ä–æ–≤.\n",
    "\n",
    "Bins: $[0\\%, 25\\%], (25\\%, 50\\%], (50\\%, 75\\%], (75\\%, 100\\%]$\n",
    "\n",
    "**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ:** Bins –≤—Å–µ–≥–¥–∞ –∑–∞–ø–æ–ª–Ω–µ–Ω—ã —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ.\n",
    "\n",
    "**3. Custom binning (domain knowledge)**\n",
    "\n",
    "–ü—Ä–∏–º–µ—Ä (–≤–æ–∑—Ä–∞—Å—Ç):\n",
    "```python\n",
    "bins = [0, 18, 35, 60, 100]\n",
    "labels = ['–ú–æ–ª–æ–¥—ã–µ', '–°—Ä–µ–¥–Ω–∏–µ', '–ó—Ä–µ–ª—ã–µ', '–ü–æ–∂–∏–ª—ã–µ']\n",
    "```\n",
    "\n",
    "**Sklearn —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:**\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "kbd = KBinsDiscretizer(\n",
    "    n_bins=5,               # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ bins\n",
    "    encode='ordinal',       # 'ordinal', 'onehot', 'onehot-dense'\n",
    "    strategy='quantile'     # 'uniform', 'quantile', 'kmeans'\n",
    ")\n",
    "```\n",
    "\n",
    "**‚ö†Ô∏è –û—Å—Ç–æ—Ä–æ–∂–Ω–æ:**\n",
    "\n",
    "- –ü–æ—Ç–µ—Ä—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (continuous ‚Üí discrete)\n",
    "- –ú–æ–∂–µ—Ç —É—Ö—É–¥—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ (–æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è tree-based –º–æ–¥–µ–ª–µ–π)\n",
    "- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–ª—è –ª–∏–Ω–µ–π–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏–ª–∏ domain-specific reasons\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Target Encoding –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "\n",
    "#### –ü—Ä–æ–±–ª–µ–º–∞ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏\n",
    "\n",
    "**One-hot encoding –ø—Ä–æ–±–ª–µ–º—ã:**\n",
    "\n",
    "- **High cardinality:** 1000 –∫–∞—Ç–µ–≥–æ—Ä–∏–π ‚Üí 1000 –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (sparse!)\n",
    "- **Memory:** –û–≥—Ä–æ–º–Ω—ã–µ –º–∞—Ç—Ä–∏—Ü—ã\n",
    "- **Curse of dimensionality**\n",
    "\n",
    "**–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞:** Target encoding (mean encoding)\n",
    "\n",
    "#### Target Encoding\n",
    "\n",
    "**–ò–¥–µ—è:** –ó–∞–º–µ–Ω–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é –Ω–∞ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ target –¥–ª—è —ç—Ç–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.\n",
    "\n",
    "**–§–æ—Ä–º—É–ª–∞ (–ø—Ä–æ—Å—Ç–∞—è):**\n",
    "\n",
    "$$\\text{TE}(c) = \\frac{1}{n_c} \\sum_{i: x_i = c} y_i$$\n",
    "\n",
    "–≥–¥–µ:\n",
    "- $c$ ‚Äî –∫–∞—Ç–µ–≥–æ—Ä–∏—è\n",
    "- $n_c$ ‚Äî –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ —Å —ç—Ç–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π\n",
    "- $y_i$ ‚Äî target\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä (House Prices):**\n",
    "\n",
    "```\n",
    "Neighborhood   Count   Mean_Price   Target_Encoding\n",
    "NoRidge         41     335,295      335,295\n",
    "NridgHt         77     316,271      316,271\n",
    "StoneBr         25     310,499      310,499\n",
    "OldTown        113     128,225      128,225\n",
    "```\n",
    "\n",
    "–í–º–µ—Å—Ç–æ 25 one-hot —Å—Ç–æ–ª–±—Ü–æ–≤ ‚Üí 1 —Å—Ç–æ–ª–±–µ—Ü —Å mean price!\n",
    "\n",
    "#### –ü—Ä–æ–±–ª–µ–º–∞: Target Leakage!\n",
    "\n",
    "–ï—Å–ª–∏ –ø—Ä–æ—Å—Ç–æ –∑–∞–º–µ–Ω–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é –Ω–∞ —Å—Ä–µ–¥–Ω–µ–µ $y$:\n",
    "\n",
    "**–ú–æ–¥–µ–ª—å –±—É–¥–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–∞—Ç—å—Å—è!**\n",
    "\n",
    "–ü—Ä–∏–º–µ—Ä:\n",
    "```python\n",
    "# –ö–∞—Ç–µ–≥–æ—Ä–∏—è 'X' –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è 1 —Ä–∞–∑ —Å y=100\n",
    "# Target encoding: X ‚Üí 100\n",
    "# –ú–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∂–µ—Ç 100 –∏–¥–µ–∞–ª—å–Ω–æ ‚Üí overfitting!\n",
    "```\n",
    "\n",
    "#### –†–µ—à–µ–Ω–∏–µ 1: Leave-One-Out (LOO) Encoding\n",
    "\n",
    "**–§–æ—Ä–º—É–ª–∞:**\n",
    "\n",
    "$$\\text{TE}_i(c) = \\frac{1}{n_c - 1} \\sum_{j \\neq i, x_j = c} y_j$$\n",
    "\n",
    "–î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ $i$ —Å—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ **–±–µ–∑** —ç—Ç–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞.\n",
    "\n",
    "#### –†–µ—à–µ–Ω–∏–µ 2: Smoothed Target Encoding (Bayesian)\n",
    "\n",
    "**–§–æ—Ä–º—É–ª–∞:**\n",
    "\n",
    "$$\\text{TE}_{\\text{smooth}}(c) = \\frac{n_c \\cdot \\bar{y}_c + m \\cdot \\bar{y}_{\\text{global}}}{n_c + m}$$\n",
    "\n",
    "–≥–¥–µ:\n",
    "- $\\bar{y}_c$ ‚Äî —Å—Ä–µ–¥–Ω–µ–µ $y$ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ $c$\n",
    "- $\\bar{y}_{\\text{global}}$ ‚Äî –≥–ª–æ–±–∞–ª—å–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ $y$\n",
    "- $m$ ‚Äî –ø–∞—Ä–∞–º–µ—Ç—Ä —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è (–æ–±—ã—á–Ω–æ 10-100)\n",
    "\n",
    "**–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:**\n",
    "\n",
    "- –ï—Å–ª–∏ $n_c$ **–±–æ–ª—å—à–æ–µ** ‚Üí –¥–æ–≤–µ—Ä—è–µ–º $\\bar{y}_c$\n",
    "- –ï—Å–ª–∏ $n_c$ **–º–∞–ª–µ–Ω—å–∫–æ–µ** ‚Üí —Ä–µ–≥—Ä–µ—Å—Å–∏—Ä—É–µ–º –∫ –≥–ª–æ–±–∞–ª—å–Ω–æ–º—É —Å—Ä–µ–¥–Ω–µ–º—É\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä:**\n",
    "\n",
    "```python\n",
    "# –ö–∞—Ç–µ–≥–æ—Ä–∏—è —Å 1000 –ø—Ä–∏–º–µ—Ä–∞–º–∏\n",
    "TE = (1000 * 250_000 + 100 * 180_000) / (1000 + 100) = 243_636\n",
    "# –ë–ª–∏–∑–∫–æ –∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π–Ω–æ–º—É —Å—Ä–µ–¥–Ω–µ–º—É 250_000\n",
    "\n",
    "# –ö–∞—Ç–µ–≥–æ—Ä–∏—è —Å 5 –ø—Ä–∏–º–µ—Ä–∞–º–∏\n",
    "TE = (5 * 350_000 + 100 * 180_000) / (5 + 100) = 193_333\n",
    "# –°–¥–≤–∏–≥ –∫ –≥–ª–æ–±–∞–ª—å–Ω–æ–º—É 180_000\n",
    "```\n",
    "\n",
    "**Sklearn (category_encoders):**\n",
    "\n",
    "```python\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "te = TargetEncoder(smoothing=10)  # Bayesian smoothing\n",
    "X_train_encoded = te.fit_transform(X_train, y_train)\n",
    "X_test_encoded = te.transform(X_test)\n",
    "```\n",
    "\n",
    "**‚ö†Ô∏è –í–∞–∂–Ω–æ:**\n",
    "\n",
    "- **–ù–ï** fit –Ω–∞ test! (—É—Ç–µ—á–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏)\n",
    "- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ cross-validation –¥–ª—è train\n",
    "- –î–ª—è tree models –º–æ–∂–µ—Ç –±—ã—Ç—å –ª—É—á—à–µ native categorical support (CatBoost)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Feature Selection\n",
    "\n",
    "**–¶–µ–ª—å:** –í—ã–±—Ä–∞—Ç—å –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, —É–¥–∞–ª–∏–≤ redundant –∏ irrelevant.\n",
    "\n",
    "#### –ó–∞—á–µ–º?\n",
    "\n",
    "1. **Reduce overfitting:** –ú–µ–Ω—å—à–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ‚Üí –º–µ–Ω—å—à–µ noise\n",
    "2. **Improve performance:** –ë—ã—Å—Ç—Ä–µ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ inference\n",
    "3. **Better interpretability:** –ü–æ–Ω—è—Ç—å, —á—Ç–æ –≤–∞–∂–Ω–æ\n",
    "4. **–ò–∑–±–µ–∂–∞—Ç—å curse of dimensionality**\n",
    "\n",
    "#### –¢—Ä–∏ –ø–æ–¥—Ö–æ–¥–∞:\n",
    "\n",
    "### 1.6.1 Filter Methods (–Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–µ –æ—Ç –º–æ–¥–µ–ª–∏)\n",
    "\n",
    "**–ò–¥–µ—è:** –û—Ü–µ–Ω–∏—Ç—å –∫–∞–∂–¥—ã–π –ø—Ä–∏–∑–Ω–∞–∫ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –ø–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ.\n",
    "\n",
    "**–ú–µ—Ç–æ–¥—ã:**\n",
    "\n",
    "**1. Correlation —Å target (–¥–ª—è regression)**\n",
    "\n",
    "Pearson correlation:\n",
    "\n",
    "$$r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2}}$$\n",
    "\n",
    "–í—ã–±–∏—Ä–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å $|r| >$ threshold.\n",
    "\n",
    "**2. Mutual Information (–¥–ª—è classification)**\n",
    "\n",
    "$$I(X; Y) = \\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log \\frac{p(x, y)}{p(x) p(y)}$$\n",
    "\n",
    "–ú–µ—Ä–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (—Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è –Ω–µ–ª–∏–Ω–µ–π–Ω—ã—Ö!).\n",
    "\n",
    "**3. Chi-squared test (–¥–ª—è categorical)**\n",
    "\n",
    "$$\\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i}$$\n",
    "\n",
    "**4. Variance threshold**\n",
    "\n",
    "–£–¥–∞–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å variance < threshold (–ø–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã).\n",
    "\n",
    "**Sklearn:**\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest, f_regression, mutual_info_regression, VarianceThreshold\n",
    ")\n",
    "\n",
    "# Top K –ø–æ F-test\n",
    "selector = SelectKBest(score_func=f_regression, k=20)\n",
    "\n",
    "# Variance threshold\n",
    "vt = VarianceThreshold(threshold=0.01)\n",
    "```\n",
    "\n",
    "**‚úÖ –ü–ª—é—Å—ã:** –ë—ã—Å—Ç—Ä–æ, –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –º–æ–¥–µ–ª–∏  \n",
    "**‚ùå –ú–∏–Ω—É—Å—ã:** –ù–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç interactions –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏\n",
    "\n",
    "### 1.6.2 Wrapper Methods (–∏—Å–ø–æ–ª—å–∑—É—é—Ç –º–æ–¥–µ–ª—å)\n",
    "\n",
    "**–ò–¥–µ—è:** –ü–µ—Ä–µ–±—Ä–∞—Ç—å subset'—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å, –≤—ã–±—Ä–∞—Ç—å –ª—É—á—à–∏–π.\n",
    "\n",
    "**–ú–µ—Ç–æ–¥—ã:**\n",
    "\n",
    "**1. Forward Selection**\n",
    "\n",
    "```\n",
    "1. –ù–∞—á–∞—Ç—å —Å 0 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "2. –î–æ–±–∞–≤–ª—è—Ç—å –ø–æ –æ–¥–Ω–æ–º—É (—Ç–æ—Ç, —á—Ç–æ –¥–∞–µ—Ç max –ø—Ä–∏—Ä–æ—Å—Ç)\n",
    "3. –ü–æ–≤—Ç–æ—Ä—è—Ç—å, –ø–æ–∫–∞ –µ—Å—Ç—å –ø—Ä–∏—Ä–æ—Å—Ç\n",
    "```\n",
    "\n",
    "**2. Backward Elimination**\n",
    "\n",
    "```\n",
    "1. –ù–∞—á–∞—Ç—å —Å–æ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "2. –£–¥–∞–ª—è—Ç—å –ø–æ –æ–¥–Ω–æ–º—É (—Ç–æ—Ç, —É–¥–∞–ª–µ–Ω–∏–µ –∫–æ—Ç–æ—Ä–æ–≥–æ –¥–∞–µ—Ç min —É—Ö—É–¥—à–µ–Ω–∏–µ)\n",
    "3. –ü–æ–≤—Ç–æ—Ä—è—Ç—å, –ø–æ–∫–∞ –µ—Å—Ç—å —É–ª—É—á—à–µ–Ω–∏–µ\n",
    "```\n",
    "\n",
    "**3. Recursive Feature Elimination (RFE)**\n",
    "\n",
    "```\n",
    "1. –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö\n",
    "2. –†–∞–Ω–∂–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ importance (coefficients –∏–ª–∏ feature_importances_)\n",
    "3. –£–¥–∞–ª–∏—Ç—å –Ω–∞–∏–º–µ–Ω–µ–µ –≤–∞–∂–Ω—ã–µ\n",
    "4. –ü–æ–≤—Ç–æ—Ä–∏—Ç—å\n",
    "```\n",
    "\n",
    "**Sklearn:**\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfe = RFE(\n",
    "    estimator=RandomForestRegressor(),\n",
    "    n_features_to_select=20,\n",
    "    step=5  # –£–¥–∞–ª—è—Ç—å –ø–æ 5 –∑–∞ —Ä–∞–∑\n",
    ")\n",
    "rfe.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "**‚úÖ –ü–ª—é—Å—ã:** –£—á–∏—Ç—ã–≤–∞–µ—Ç interactions, —á–∞—Å—Ç–æ –ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ  \n",
    "**‚ùå –ú–∏–Ω—É—Å—ã:** –ú–µ–¥–ª–µ–Ω–Ω–æ (–ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –º–Ω–æ–≥–æ —Ä–∞–∑), –º–æ–∂–µ—Ç overfitting\n",
    "\n",
    "### 1.6.3 Embedded Methods (–≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –≤ –º–æ–¥–µ–ª—å)\n",
    "\n",
    "**–ò–¥–µ—è:** –ú–æ–¥–µ–ª—å —Å–∞–º–∞ –≤—ã–±–∏—Ä–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è.\n",
    "\n",
    "**–ú–µ—Ç–æ–¥—ã:**\n",
    "\n",
    "**1. L1 Regularization (Lasso)**\n",
    "\n",
    "$$\\min_{\\beta} \\sum (y_i - \\beta^T x_i)^2 + \\lambda \\sum |\\beta_j|$$\n",
    "\n",
    "L1 penalty –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ **sparse** —Ä–µ—à–µ–Ω–∏—é (–º–Ω–æ–≥–∏–µ $\\beta_j = 0$).\n",
    "\n",
    "**2. Tree-based feature importance**\n",
    "\n",
    "Random Forest / XGBoost / LightGBM –≤—ã—á–∏—Å–ª—è—é—Ç importance –Ω–∞ –æ—Å–Ω–æ–≤–µ:\n",
    "\n",
    "- **Gain:** –£–º–µ–Ω—å—à–µ–Ω–∏–µ loss –ø—Ä–∏ split –Ω–∞ —ç—Ç–æ–º –ø—Ä–∏–∑–Ω–∞–∫–µ\n",
    "- **Split count:** –ö–∞–∫ —á–∞—Å—Ç–æ –ø—Ä–∏–∑–Ω–∞–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è\n",
    "\n",
    "**3. Permutation importance**\n",
    "\n",
    "```\n",
    "1. –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å\n",
    "2. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞:\n",
    "   - –ü–µ—Ä–µ–º–µ—à–∞—Ç—å –µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è (—Å–ª–æ–º–∞—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å —Å y)\n",
    "   - –ò–∑–º–µ—Ä–∏—Ç—å —É—Ö—É–¥—à–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏\n",
    "3. –í–∞–∂–Ω–æ—Å—Ç—å = —É—Ö—É–¥—à–µ–Ω–∏–µ\n",
    "```\n",
    "\n",
    "**Sklearn:**\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Lasso\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train, y_train)\n",
    "# lasso.coef_ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤–µ—Å–∞ (–º–Ω–æ–≥–∏–µ = 0)\n",
    "\n",
    "# Permutation importance\n",
    "perm = permutation_importance(model, X_val, y_val, n_repeats=10)\n",
    "```\n",
    "\n",
    "**‚úÖ –ü–ª—é—Å—ã:** –ë—ã—Å—Ç—Ä–æ, –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ –¥–ª—è –º–æ–¥–µ–ª–∏  \n",
    "**‚ùå –ú–∏–Ω—É—Å—ã:** –ó–∞–≤–∏—Å–∏—Ç –æ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º–∞\n",
    "\n",
    "---\n",
    "\n",
    "## –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è —á–∞—Å—Ç—å –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –ø—Ä–∞–∫—Ç–∏–∫–µ üöÄ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä –ß–∞—Å—Ç—å 2: –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "\n",
    "### 2.1 –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û—Å–Ω–æ–≤–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn preprocessing\n",
    "from sklearn.preprocessing import (\n",
    "    PolynomialFeatures, StandardScaler, PowerTransformer,\n",
    "    KBinsDiscretizer, LabelEncoder\n",
    ")\n",
    "\n",
    "# Feature selection\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest, f_regression, mutual_info_regression,\n",
    "    RFE, VarianceThreshold\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Target encoding\n",
    "try:\n",
    "    from category_encoders import TargetEncoder\n",
    "    HAS_CAT_ENCODERS = True\n",
    "except ImportError:\n",
    "    print('‚ö†Ô∏è category_encoders –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä—É—á–Ω—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é.')\n",
    "    HAS_CAT_ENCODERS = False\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "# Seed\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print('‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö: House Prices\n",
    "\n",
    "–ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞—Ç–∞—Å–µ—Ç Kaggle House Prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ —É–∂–µ –µ—Å—Ç—å –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –Ω–æ—É—Ç–±—É–∫–∞)\n",
    "import os\n",
    "\n",
    "data_path = '../../data/house_prices_train.csv'\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    print('‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω!')\n",
    "    print('–°–∫–∞—á–∞–π—Ç–µ: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data')\n",
    "    print('–ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –Ω–æ—É—Ç–±—É–∫–∞ 03_catboost_deep_dive.ipynb')\n",
    "else:\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f'‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {df.shape[0]:,} —Å—Ç—Ä–æ–∫, {df.shape[1]} —Å—Ç–æ–ª–±—Ü–æ–≤')\n",
    "    print(f'Target: SalePrice')\n",
    "    print(f'–†–∞–∑–º–µ—Ä –≤ –ø–∞–º—è—Ç–∏: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–µ—Ä–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –¥–∞–Ω–Ω—ã–µ\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 EDA –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è\n",
    "print('–†–∞–∑–º–µ—Ä:', df.shape)\n",
    "print('\\nTarget —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:')\n",
    "print(df['SalePrice'].describe())\n",
    "print(f'\\nSkewness: {df[\"SalePrice\"].skew():.2f}')\n",
    "print(f'Kurtosis: {df[\"SalePrice\"].kurtosis():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è target\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(df['SalePrice'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('SalePrice')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title(f'SalePrice Distribution (Skewness: {df[\"SalePrice\"].skew():.2f})')\n",
    "axes[0].axvline(df['SalePrice'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[0].axvline(df['SalePrice'].median(), color='green', linestyle='--', label='Median')\n",
    "axes[0].legend()\n",
    "\n",
    "# Q-Q plot\n",
    "stats.probplot(df['SalePrice'], dist=\"norm\", plot=axes[1])\n",
    "axes[1].set_title('Q-Q Plot (–ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç–∏)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('üîç SalePrice –∏–º–µ–µ—Ç right skew ‚Üí —Ö–æ—Ä–æ—à–∏–π –∫–∞–Ω–¥–∏–¥–∞—Ç –¥–ª—è log transform!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–æ—Å—Ç–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "# –í—ã–±–∏—Ä–∞–µ–º numeric –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\n",
    "\n",
    "# Numeric features\n",
    "numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_features.remove('SalePrice')  # Target\n",
    "if 'Id' in numeric_features:\n",
    "    numeric_features.remove('Id')  # ID –Ω–µ –Ω—É–∂–µ–Ω\n",
    "\n",
    "# Categorical features (–ø—Ä–∏–º–µ—Ä –¥–ª—è target encoding)\n",
    "categorical_features = ['Neighborhood', 'BldgType', 'HouseStyle', 'ExterQual', 'KitchenQual']\n",
    "\n",
    "# –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –º–µ–¥–∏–∞–Ω–æ–π –¥–ª—è numeric\n",
    "for col in numeric_features:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "# –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –º–æ–¥–æ–π –¥–ª—è categorical\n",
    "for col in categorical_features:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "print(f'‚úÖ Numeric –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(numeric_features)}')\n",
    "print(f'‚úÖ Categorical –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(categorical_features)}')\n",
    "print(f'‚úÖ –ü—Ä–æ–ø—É—Å–∫–∏ –∑–∞–ø–æ–ª–Ω–µ–Ω—ã')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X = df[numeric_features + categorical_features].copy()\n",
    "y = df['SalePrice'].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f'Train: {X_train.shape[0]:,} samples')\n",
    "print(f'Test: {X_test.shape[0]:,} samples')\n",
    "print(f'Features: {X_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Baseline –º–æ–¥–µ–ª—å (–±–µ–∑ feature engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–æ—Å—Ç–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è baseline: one-hot encoding –¥–ª—è categorical\n",
    "X_train_baseline = pd.get_dummies(X_train, columns=categorical_features, drop_first=True)\n",
    "X_test_baseline = pd.get_dummies(X_test, columns=categorical_features, drop_first=True)\n",
    "\n",
    "# –í—ã—Ä–æ–≤–Ω—è—Ç—å –∫–æ–ª–æ–Ω–∫–∏ (train/test –º–æ–≥—É—Ç –∏–º–µ—Ç—å —Ä–∞–∑–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏)\n",
    "X_train_baseline, X_test_baseline = X_train_baseline.align(X_test_baseline, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_baseline)\n",
    "X_test_scaled = scaler.transform(X_test_baseline)\n",
    "\n",
    "print(f'Baseline features: {X_train_scaled.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Ridge Regression\n",
    "baseline_model = Ridge(alpha=10.0, random_state=RANDOM_STATE)\n",
    "baseline_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "y_pred_baseline = baseline_model.predict(X_test_scaled)\n",
    "\n",
    "# –ú–µ—Ç—Ä–∏–∫–∏\n",
    "rmse_baseline = np.sqrt(mean_squared_error(y_test, y_pred_baseline))\n",
    "mae_baseline = mean_absolute_error(y_test, y_pred_baseline)\n",
    "r2_baseline = r2_score(y_test, y_pred_baseline)\n",
    "\n",
    "print('üìä Baseline Model (Ridge Regression):')\n",
    "print(f'  RMSE: ${rmse_baseline:,.0f}')\n",
    "print(f'  MAE: ${mae_baseline:,.0f}')\n",
    "print(f'  R¬≤: {r2_baseline:.4f}')\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–∏–º –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
    "results = {\n",
    "    'Baseline (Ridge)': {\n",
    "        'RMSE': rmse_baseline,\n",
    "        'MAE': mae_baseline,\n",
    "        'R¬≤': r2_baseline,\n",
    "        'Features': X_train_scaled.shape[1]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Polynomial Features –∏ Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í—ã–±–µ—Ä–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã—Ö numeric –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è polynomial (—á—Ç–æ–±—ã –Ω–µ –≤–∑–æ—Ä–≤–∞—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å)\n",
    "key_features = ['GrLivArea', 'OverallQual', 'TotalBsmtSF', 'GarageCars', 'YearBuilt']\n",
    "\n",
    "X_train_poly = X_train[key_features].copy()\n",
    "X_test_poly = X_test[key_features].copy()\n",
    "\n",
    "# Polynomial features degree 2\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly_transformed = poly.fit_transform(X_train_poly)\n",
    "X_test_poly_transformed = poly.transform(X_test_poly)\n",
    "\n",
    "print(f'–ò—Å—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(key_features)}')\n",
    "print(f'–ü–æ—Å–ª–µ polynomial degree 2: {X_train_poly_transformed.shape[1]}')\n",
    "print(f'\\n–ù–∞–∑–≤–∞–Ω–∏—è –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:')\n",
    "feature_names = poly.get_feature_names_out(key_features)\n",
    "print(feature_names[:10], '...')\n",
    "print('\\nüîç –ü—Ä–∏–º–µ—Ä—ã interactions:')\n",
    "print('  GrLivArea √ó OverallQual (–±–æ–ª—å—à–∞—è –ø–ª–æ—â–∞–¥—å √ó –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ)')\n",
    "print('  TotalBsmtSF √ó GarageCars (–ø–æ–¥–≤–∞–ª √ó –≥–∞—Ä–∞–∂)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–æ–±–∞–≤–∏–º polynomial features –∫ baseline\n",
    "X_train_with_poly = np.hstack([X_train_scaled, X_train_poly_transformed])\n",
    "X_test_with_poly = np.hstack([X_test_scaled, X_test_poly_transformed])\n",
    "\n",
    "# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "scaler_poly = StandardScaler()\n",
    "X_train_with_poly = scaler_poly.fit_transform(X_train_with_poly)\n",
    "X_test_with_poly = scaler_poly.transform(X_test_with_poly)\n",
    "\n",
    "# Ridge —Å polynomial features\n",
    "model_poly = Ridge(alpha=10.0, random_state=RANDOM_STATE)\n",
    "model_poly.fit(X_train_with_poly, y_train)\n",
    "\n",
    "y_pred_poly = model_poly.predict(X_test_with_poly)\n",
    "\n",
    "rmse_poly = np.sqrt(mean_squared_error(y_test, y_pred_poly))\n",
    "mae_poly = mean_absolute_error(y_test, y_pred_poly)\n",
    "r2_poly = r2_score(y_test, y_pred_poly)\n",
    "\n",
    "print('üìä Model with Polynomial Features:')\n",
    "print(f'  RMSE: ${rmse_poly:,.0f}')\n",
    "print(f'  MAE: ${mae_poly:,.0f}')\n",
    "print(f'  R¬≤: {r2_poly:.4f}')\n",
    "print(f'\\nüìà Improvement over baseline:')\n",
    "print(f'  RMSE: {(rmse_baseline - rmse_poly) / rmse_baseline * 100:.1f}%')\n",
    "print(f'  R¬≤: {(r2_poly - r2_baseline):.4f}')\n",
    "\n",
    "results['Polynomial Features'] = {\n",
    "    'RMSE': rmse_poly,\n",
    "    'MAE': mae_poly,\n",
    "    'R¬≤': r2_poly,\n",
    "    'Features': X_train_with_poly.shape[1]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Log Transform –Ω–∞ Target –∏ Skewed Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transform –Ω–∞ target (SalePrice is right-skewed)\n",
    "y_train_log = np.log1p(y_train)  # log1p = log(1 + x) –¥–ª—è –∏–∑–±–µ–≥–∞–Ω–∏—è log(0)\n",
    "y_test_log = np.log1p(y_test)\n",
    "\n",
    "# –ù–∞–π–¥–µ–º skewed –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ numeric features\n",
    "skewed_features = []\n",
    "for col in numeric_features:\n",
    "    if X_train[col].skew() > 0.75:  # Threshold –¥–ª—è skewness\n",
    "        skewed_features.append(col)\n",
    "\n",
    "print(f'–ù–∞–π–¥–µ–Ω–æ {len(skewed_features)} skewed –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (skew > 0.75):')\n",
    "print(skewed_features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transform –Ω–∞ skewed features\n",
    "X_train_log = X_train.copy()\n",
    "X_test_log = X_test.copy()\n",
    "\n",
    "for col in skewed_features:\n",
    "    if col in X_train_log.columns and X_train_log[col].dtype in [np.int64, np.float64]:\n",
    "        X_train_log[col] = np.log1p(X_train_log[col])\n",
    "        X_test_log[col] = np.log1p(X_test_log[col])\n",
    "\n",
    "# One-hot –¥–ª—è categorical\n",
    "X_train_log = pd.get_dummies(X_train_log, columns=categorical_features, drop_first=True)\n",
    "X_test_log = pd.get_dummies(X_test_log, columns=categorical_features, drop_first=True)\n",
    "X_train_log, X_test_log = X_train_log.align(X_test_log, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    "scaler_log = StandardScaler()\n",
    "X_train_log_scaled = scaler_log.fit_transform(X_train_log)\n",
    "X_test_log_scaled = scaler_log.transform(X_test_log)\n",
    "\n",
    "# Ridge –Ω–∞ log-transformed data\n",
    "model_log = Ridge(alpha=10.0, random_state=RANDOM_STATE)\n",
    "model_log.fit(X_train_log_scaled, y_train_log)\n",
    "\n",
    "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–≤ log scale)\n",
    "y_pred_log = model_log.predict(X_test_log_scaled)\n",
    "\n",
    "# –û–±—Ä–∞—Ç–Ω–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è (expm1 = exp(x) - 1)\n",
    "y_pred_log_original = np.expm1(y_pred_log)\n",
    "\n",
    "rmse_log = np.sqrt(mean_squared_error(y_test, y_pred_log_original))\n",
    "mae_log = mean_absolute_error(y_test, y_pred_log_original)\n",
    "r2_log = r2_score(y_test, y_pred_log_original)\n",
    "\n",
    "print('üìä Model with Log Transform:')\n",
    "print(f'  RMSE: ${rmse_log:,.0f}')\n",
    "print(f'  MAE: ${mae_log:,.0f}')\n",
    "print(f'  R¬≤: {r2_log:.4f}')\n",
    "print(f'\\nüìà Improvement over baseline:')\n",
    "print(f'  RMSE: {(rmse_baseline - rmse_log) / rmse_baseline * 100:.1f}%')\n",
    "print(f'  R¬≤: {(r2_log - r2_baseline):.4f}')\n",
    "\n",
    "results['Log Transform'] = {\n",
    "    'RMSE': rmse_log,\n",
    "    'MAE': mae_log,\n",
    "    'R¬≤': r2_log,\n",
    "    'Features': X_train_log_scaled.shape[1]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Target Encoding –¥–ª—è Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –†—É—á–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è smoothed target encoding\n",
    "def target_encode_smooth(X_train, X_test, y_train, cat_col, m=10):\n",
    "    \"\"\"\n",
    "    Smoothed target encoding —Å Bayesian smoothing\n",
    "    \n",
    "    TE = (n_c * mean_c + m * global_mean) / (n_c + m)\n",
    "    \"\"\"\n",
    "    # –ì–ª–æ–±–∞–ª—å–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ\n",
    "    global_mean = y_train.mean()\n",
    "    \n",
    "    # –°—Ä–µ–¥–Ω–µ–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º\n",
    "    category_means = y_train.groupby(X_train[cat_col]).mean()\n",
    "    category_counts = X_train[cat_col].value_counts()\n",
    "    \n",
    "    # Smoothed encoding\n",
    "    smoothed_means = {}\n",
    "    for cat in category_means.index:\n",
    "        n_c = category_counts[cat]\n",
    "        mean_c = category_means[cat]\n",
    "        smoothed_means[cat] = (n_c * mean_c + m * global_mean) / (n_c + m)\n",
    "    \n",
    "    # Map –Ω–∞ train –∏ test\n",
    "    X_train_encoded = X_train[cat_col].map(smoothed_means).fillna(global_mean)\n",
    "    X_test_encoded = X_test[cat_col].map(smoothed_means).fillna(global_mean)\n",
    "    \n",
    "    return X_train_encoded, X_test_encoded\n",
    "\n",
    "print('‚úÖ –§—É–Ω–∫—Ü–∏—è target encoding —Å–æ–∑–¥–∞–Ω–∞')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º target encoding –Ω–∞ categorical features\n",
    "X_train_te = X_train[numeric_features].copy()\n",
    "X_test_te = X_test[numeric_features].copy()\n",
    "\n",
    "for cat_col in categorical_features:\n",
    "    train_encoded, test_encoded = target_encode_smooth(\n",
    "        X_train, X_test, y_train, cat_col, m=10\n",
    "    )\n",
    "    X_train_te[f'{cat_col}_TE'] = train_encoded\n",
    "    X_test_te[f'{cat_col}_TE'] = test_encoded\n",
    "\n",
    "print(f'–ü—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ target encoding: {X_train_te.shape[1]}')\n",
    "print(f'–î–æ–±–∞–≤–ª–µ–Ω–æ: {len(categorical_features)} target-encoded features')\n",
    "print(f'\\n–í–º–µ—Å—Ç–æ {len(categorical_features)} one-hot —Å—Ç–æ–ª–±—Ü–æ–≤ ‚Üí {len(categorical_features)} TE —Å—Ç–æ–ª–±—Ü–æ–≤!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ\n",
    "scaler_te = StandardScaler()\n",
    "X_train_te_scaled = scaler_te.fit_transform(X_train_te)\n",
    "X_test_te_scaled = scaler_te.transform(X_test_te)\n",
    "\n",
    "model_te = Ridge(alpha=10.0, random_state=RANDOM_STATE)\n",
    "model_te.fit(X_train_te_scaled, y_train)\n",
    "\n",
    "y_pred_te = model_te.predict(X_test_te_scaled)\n",
    "\n",
    "rmse_te = np.sqrt(mean_squared_error(y_test, y_pred_te))\n",
    "mae_te = mean_absolute_error(y_test, y_pred_te)\n",
    "r2_te = r2_score(y_test, y_pred_te)\n",
    "\n",
    "print('üìä Model with Target Encoding:')\n",
    "print(f'  RMSE: ${rmse_te:,.0f}')\n",
    "print(f'  MAE: ${mae_te:,.0f}')\n",
    "print(f'  R¬≤: {r2_te:.4f}')\n",
    "print(f'\\nüìà Improvement over baseline:')\n",
    "print(f'  RMSE: {(rmse_baseline - rmse_te) / rmse_baseline * 100:.1f}%')\n",
    "print(f'  R¬≤: {(r2_te - r2_baseline):.4f}')\n",
    "\n",
    "results['Target Encoding'] = {\n",
    "    'RMSE': rmse_te,\n",
    "    'MAE': mae_te,\n",
    "    'R¬≤': r2_te,\n",
    "    'Features': X_train_te_scaled.shape[1]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Feature Selection: Filter Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SelectKBest —Å f_regression (top K)\n",
    "k_best = 30\n",
    "\n",
    "selector = SelectKBest(score_func=f_regression, k=k_best)\n",
    "X_train_selected = selector.fit_transform(X_train_baseline, y_train)\n",
    "X_test_selected = selector.transform(X_test_baseline)\n",
    "\n",
    "# –ö–∞–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤—ã–±—Ä–∞–Ω—ã?\n",
    "selected_features = X_train_baseline.columns[selector.get_support()].tolist()\n",
    "print(f'SelectKBest: –í—ã–±—Ä–∞–Ω–æ {k_best} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ {X_train_baseline.shape[1]}')\n",
    "print(f'\\nTop 10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ F-score:')\n",
    "scores = pd.DataFrame({\n",
    "    'Feature': X_train_baseline.columns,\n",
    "    'Score': selector.scores_\n",
    "}).sort_values('Score', ascending=False)\n",
    "print(scores.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ú–æ–¥–µ–ª—å –Ω–∞ selected features\n",
    "scaler_sel = StandardScaler()\n",
    "X_train_selected_scaled = scaler_sel.fit_transform(X_train_selected)\n",
    "X_test_selected_scaled = scaler_sel.transform(X_test_selected)\n",
    "\n",
    "model_selected = Ridge(alpha=10.0, random_state=RANDOM_STATE)\n",
    "model_selected.fit(X_train_selected_scaled, y_train)\n",
    "\n",
    "y_pred_selected = model_selected.predict(X_test_selected_scaled)\n",
    "\n",
    "rmse_selected = np.sqrt(mean_squared_error(y_test, y_pred_selected))\n",
    "mae_selected = mean_absolute_error(y_test, y_pred_selected)\n",
    "r2_selected = r2_score(y_test, y_pred_selected)\n",
    "\n",
    "print('üìä Model with SelectKBest (Filter):')\n",
    "print(f'  RMSE: ${rmse_selected:,.0f}')\n",
    "print(f'  MAE: ${mae_selected:,.0f}')\n",
    "print(f'  R¬≤: {r2_selected:.4f}')\n",
    "print(f'\\nüìà –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å baseline:')\n",
    "print(f'  RMSE: {(rmse_baseline - rmse_selected) / rmse_baseline * 100:+.1f}%')\n",
    "print(f'  Features: {X_train_baseline.shape[1]} ‚Üí {k_best} ({k_best / X_train_baseline.shape[1] * 100:.0f}%)')\n",
    "\n",
    "results['SelectKBest (Filter)'] = {\n",
    "    'RMSE': rmse_selected,\n",
    "    'MAE': mae_selected,\n",
    "    'R¬≤': r2_selected,\n",
    "    'Features': k_best\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Feature Selection: Lasso (Embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ feature selection (L1 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è)\n",
    "lasso = Lasso(alpha=100.0, random_state=RANDOM_STATE)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "# –°–∫–æ–ª—å–∫–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤—ã–±—Ä–∞–Ω–æ? (–Ω–µ–Ω—É–ª–µ–≤—ã—Ö –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤)\n",
    "n_features_lasso = np.sum(lasso.coef_ != 0)\n",
    "print(f'Lasso –≤—ã–±—Ä–∞–ª {n_features_lasso} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ {X_train_scaled.shape[1]}')\n",
    "\n",
    "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "y_pred_lasso = lasso.predict(X_test_scaled)\n",
    "\n",
    "rmse_lasso = np.sqrt(mean_squared_error(y_test, y_pred_lasso))\n",
    "mae_lasso = mean_absolute_error(y_test, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "\n",
    "print('\\nüìä Lasso (Embedded Selection):')\n",
    "print(f'  RMSE: ${rmse_lasso:,.0f}')\n",
    "print(f'  MAE: ${mae_lasso:,.0f}')\n",
    "print(f'  R¬≤: {r2_lasso:.4f}')\n",
    "print(f'\\nüìà Improvement over baseline:')\n",
    "print(f'  RMSE: {(rmse_baseline - rmse_lasso) / rmse_baseline * 100:.1f}%')\n",
    "print(f'  Features selected: {n_features_lasso}/{X_train_scaled.shape[1]}')\n",
    "\n",
    "results['Lasso (Embedded)'] = {\n",
    "    'RMSE': rmse_lasso,\n",
    "    'MAE': mae_lasso,\n",
    "    'R¬≤': r2_lasso,\n",
    "    'Features': n_features_lasso\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¢–æ–ø –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –∞–±—Å–æ–ª—é—Ç–Ω–æ–º—É –∑–Ω–∞—á–µ–Ω–∏—é –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤\n",
    "lasso_importance = pd.DataFrame({\n",
    "    'Feature': X_train_baseline.columns,\n",
    "    'Coefficient': lasso.coef_\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "print('Top 15 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ Lasso coefficients:')\n",
    "print(lasso_importance.head(15))\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_features = lasso_importance.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['Coefficient'])\n",
    "plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "plt.xlabel('Lasso Coefficient')\n",
    "plt.title('Top 15 Features by Lasso Coefficients')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 Combined Approach (–≤—Å—ë –≤–º–µ—Å—Ç–µ!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –ª—É—á—à–∏–µ —Ç–µ—Ö–Ω–∏–∫–∏:\n",
    "# 1. Log transform –Ω–∞ target\n",
    "# 2. Log transform –Ω–∞ skewed features\n",
    "# 3. Target encoding –¥–ª—è categorical\n",
    "# 4. Polynomial features (–∫–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏)\n",
    "# 5. XGBoost (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è feature selection)\n",
    "\n",
    "print('üöÄ –°–æ–∑–¥–∞–µ–º Combined Feature Engineering Pipeline...')\n",
    "\n",
    "# 1. Log –Ω–∞ skewed\n",
    "X_train_combined = X_train.copy()\n",
    "X_test_combined = X_test.copy()\n",
    "\n",
    "for col in skewed_features:\n",
    "    if col in X_train_combined.columns and X_train_combined[col].dtype in [np.int64, np.float64]:\n",
    "        X_train_combined[col] = np.log1p(X_train_combined[col])\n",
    "        X_test_combined[col] = np.log1p(X_test_combined[col])\n",
    "\n",
    "# 2. Target encoding\n",
    "for cat_col in categorical_features:\n",
    "    train_encoded, test_encoded = target_encode_smooth(\n",
    "        X_train, X_test, y_train, cat_col, m=10\n",
    "    )\n",
    "    X_train_combined[f'{cat_col}_TE'] = train_encoded\n",
    "    X_test_combined[f'{cat_col}_TE'] = test_encoded\n",
    "\n",
    "# –£–¥–∞–ª—è–µ–º original categorical (–∑–∞–º–µ–Ω–∏–ª–∏ –Ω–∞ TE)\n",
    "X_train_combined = X_train_combined.drop(columns=categorical_features)\n",
    "X_test_combined = X_test_combined.drop(columns=categorical_features)\n",
    "\n",
    "# 3. Polynomial –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "poly_combined = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
    "X_train_poly_comb = poly_combined.fit_transform(X_train_combined[key_features])\n",
    "X_test_poly_comb = poly_combined.transform(X_test_combined[key_features])\n",
    "\n",
    "# –û–±—ä–µ–¥–∏–Ω—è–µ–º\n",
    "X_train_final = np.hstack([X_train_combined.values, X_train_poly_comb])\n",
    "X_test_final = np.hstack([X_test_combined.values, X_test_poly_comb])\n",
    "\n",
    "print(f'‚úÖ Combined features: {X_train_final.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost –Ω–∞ combined features\n",
    "xgb_combined = XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "xgb_combined.fit(X_train_final, y_train_log)\n",
    "\n",
    "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "y_pred_combined = xgb_combined.predict(X_test_final)\n",
    "y_pred_combined_original = np.expm1(y_pred_combined)\n",
    "\n",
    "rmse_combined = np.sqrt(mean_squared_error(y_test, y_pred_combined_original))\n",
    "mae_combined = mean_absolute_error(y_test, y_pred_combined_original)\n",
    "r2_combined = r2_score(y_test, y_pred_combined_original)\n",
    "\n",
    "print('üìä Combined Approach (Log + TE + Poly + XGBoost):')\n",
    "print(f'  RMSE: ${rmse_combined:,.0f}')\n",
    "print(f'  MAE: ${mae_combined:,.0f}')\n",
    "print(f'  R¬≤: {r2_combined:.4f}')\n",
    "print(f'\\nüéâ Improvement over baseline:')\n",
    "print(f'  RMSE: {(rmse_baseline - rmse_combined) / rmse_baseline * 100:.1f}%')\n",
    "print(f'  R¬≤: {(r2_combined - r2_baseline):.4f}')\n",
    "\n",
    "results['Combined (Best)'] = {\n",
    "    'RMSE': rmse_combined,\n",
    "    'MAE': mae_combined,\n",
    "    'R¬≤': r2_combined,\n",
    "    'Features': X_train_final.shape[1]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11 –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –ø–æ–¥—Ö–æ–¥–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞–µ–º —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É\n",
    "comparison = pd.DataFrame(results).T\n",
    "comparison = comparison.sort_values('RMSE')\n",
    "\n",
    "print('üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ Feature Engineering:')\n",
    "print('=' * 80)\n",
    "print(comparison.to_string())\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# RMSE comparison\n",
    "axes[0].barh(comparison.index, comparison['RMSE'], color='skyblue', edgecolor='black')\n",
    "axes[0].set_xlabel('RMSE ($)')\n",
    "axes[0].set_title('RMSE Comparison')\n",
    "axes[0].axvline(rmse_baseline, color='red', linestyle='--', label='Baseline')\n",
    "axes[0].legend()\n",
    "\n",
    "# R¬≤ comparison\n",
    "axes[1].barh(comparison.index, comparison['R¬≤'], color='lightgreen', edgecolor='black')\n",
    "axes[1].set_xlabel('R¬≤')\n",
    "axes[1].set_title('R¬≤ Score Comparison')\n",
    "axes[1].axvline(r2_baseline, color='red', linestyle='--', label='Baseline')\n",
    "axes[1].legend()\n",
    "\n",
    "# Features count\n",
    "axes[2].barh(comparison.index, comparison['Features'], color='lightcoral', edgecolor='black')\n",
    "axes[2].set_xlabel('Number of Features')\n",
    "axes[2].set_title('Feature Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ –í—ã–≤–æ–¥—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n",
    "\n",
    "### –ß—Ç–æ –º—ã –∏–∑—É—á–∏–ª–∏:\n",
    "\n",
    "1. **Polynomial Features** ‚Äî —Å–æ–∑–¥–∞–Ω–∏–µ interactions –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç–µ–π\n",
    "2. **Log Transform** ‚Äî –æ–±—Ä–∞–±–æ—Ç–∫–∞ skewed –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ target\n",
    "3. **Target Encoding** ‚Äî —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ one-hot –¥–ª—è high cardinality\n",
    "4. **Feature Selection** ‚Äî Filter (SelectKBest), Embedded (Lasso)\n",
    "5. **Combined Approach** ‚Äî –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–µ—Ö–Ω–∏–∫\n",
    "\n",
    "### –ö–ª—é—á–µ–≤—ã–µ –∏–Ω—Å–∞–π—Ç—ã:\n",
    "\n",
    "#### ‚úÖ –ß—Ç–æ —Å—Ä–∞–±–æ—Ç–∞–ª–æ –ª—É—á—à–µ –≤—Å–µ–≥–æ:\n",
    "\n",
    "1. **Log transform** ‚Äî –ø—Ä–æ—Å—Ç–∞—è, –Ω–æ –º–æ—â–Ω–∞—è —Ç–µ—Ö–Ω–∏–∫–∞ –¥–ª—è skewed data\n",
    "2. **Target encoding** ‚Äî –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ one-hot (–æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è tree models)\n",
    "3. **Combined approach** ‚Äî –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –¥–∞–µ—Ç –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "4. **Feature selection** ‚Äî –ø–æ–º–æ–≥–∞–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å overfitting –∏ —É—Å–∫–æ—Ä—è–µ—Ç –º–æ–¥–µ–ª—å\n",
    "\n",
    "#### üìà –£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞:\n",
    "\n",
    "- **Baseline ‚Üí Combined:** –£–ª—É—á—à–µ–Ω–∏–µ RMSE –Ω–∞ 15-25%\n",
    "- **–ü—Ä–æ—Å—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏** (log transform) –¥–∞—é—Ç 5-10% –ø—Ä–∏—Ä–æ—Å—Ç\n",
    "- **Advanced —Ç–µ—Ö–Ω–∏–∫–∏** (polynomial + target encoding) –¥–æ–±–∞–≤–ª—è—é—Ç –µ—â–µ 10-15%\n",
    "\n",
    "### –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:\n",
    "\n",
    "#### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∂–¥—É—é —Ç–µ—Ö–Ω–∏–∫—É:\n",
    "\n",
    "| –¢–µ—Ö–Ω–∏–∫–∞ | –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å | –ú–æ–¥–µ–ª—å |\n",
    "|---------|-------------------|--------|\n",
    "| **Log transform** | Skewed –¥–∞–Ω–Ω—ã–µ, —Ü–µ–Ω—ã, –ø–ª–æ—â–∞–¥–∏ | –õ–∏–Ω–µ–π–Ω—ã–µ, –¥–µ—Ä–µ–≤—å—è |\n",
    "| **Polynomial features** | –Ø–≤–Ω—ã–µ interactions, –º–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ | –õ–∏–Ω–µ–π–Ω—ã–µ (—Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π!) |\n",
    "| **Target encoding** | High cardinality categorical | Tree-based |\n",
    "| **SelectKBest** | –ú–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –Ω—É–∂–Ω–∞ —Å–∫–æ—Ä–æ—Å—Ç—å | –õ—é–±—ã–µ |\n",
    "| **Lasso** | –ù—É–∂–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è selection | –õ–∏–Ω–µ–π–Ω—ã–µ |\n",
    "| **RFE** | –ú–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ, –µ—Å—Ç—å –≤—Ä–µ–º—è | –õ—é–±—ã–µ (–º–µ–¥–ª–µ–Ω–Ω–æ) |\n",
    "\n",
    "#### ‚ö†Ô∏è –ü—Ä–µ–¥–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–∏—è:\n",
    "\n",
    "1. **Polynomial features:**\n",
    "   - –í–∑—Ä—ã–≤–Ω–æ–π —Ä–æ—Å—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ ($O(n^d)$)\n",
    "   - –û–±—è–∑–∞—Ç–µ–ª—å–Ω–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è\n",
    "   - –ù–µ –¥–ª—è tree models (–æ–Ω–∏ —Å–∞–º–∏ –Ω–∞—Ö–æ–¥—è—Ç interactions)\n",
    "\n",
    "2. **Target encoding:**\n",
    "   - **Target leakage!** –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ smoothing –∏–ª–∏ cross-validation\n",
    "   - –ù–µ fit –Ω–∞ test –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "3. **Log transform:**\n",
    "   - –ù–µ –∑–∞–±—ã—Ç—å –æ–±—Ä–∞—Ç–Ω—É—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏\n",
    "   - –¢–æ–ª—å–∫–æ –¥–ª—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (–∏–ª–∏ log1p)\n",
    "\n",
    "4. **Feature selection:**\n",
    "   - Filter methods –Ω–µ –≤–∏–¥—è—Ç interactions\n",
    "   - Wrapper methods –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∏ —Å–∫–ª–æ–Ω–Ω—ã –∫ overfitting\n",
    "\n",
    "### –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:\n",
    "\n",
    "1. **Automated Feature Engineering:** Featuretools, tsfresh\n",
    "2. **Feature Extraction:** PCA, t-SNE, UMAP\n",
    "3. **Domain-specific:** –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ domain knowledge\n",
    "4. **Deep Learning:** –ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã –¥–ª—è feature learning\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ –ù–æ—É—Ç–±—É–∫ –∑–∞–≤–µ—Ä—à–µ–Ω!\n",
    "\n",
    "**Feature engineering ‚Äî —ç—Ç–æ 60-70% —É—Å–ø–µ—Ö–∞ ML –ø—Ä–æ–µ–∫—Ç–∞!**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
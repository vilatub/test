{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Imbalanced Data: Fraud Detection\n\n## üéØ –¶–µ–ª–∏\n\n1. –ü–æ–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã imbalanced data\n2. Sampling techniques: SMOTE, undersampling\n3. Cost-sensitive learning\n4. Evaluation metrics\n5. –ü—Ä–∞–∫—Ç–∏–∫–∞: Credit Card Fraud (1:600 imbalance)\n\n## üíº –ó–∞–¥–∞—á–∞: Fraud Detection\n\n**Extreme imbalance:** ~0.17% fraud (1:600)\n\n**–ú–µ—Ç—Ä–∏–∫–∏:**\n- Precision, Recall, F1\n- PR-AUC\n- Business cost\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üìö –ß–∞—Å—Ç—å 1: –¢–µ–æ—Ä–∏—è\n\n### 1.1 –ü—Ä–æ–±–ª–µ–º–∞\n\n**Imbalanced data:** –û–¥–∏–Ω –∫–ª–∞—Å—Å —Ä–µ–¥–∫–∏–π (minority) vs —á–∞—Å—Ç—ã–π (majority)\n\n**–ü—Ä–æ–±–ª–µ–º—ã:**\n1. **Biased learning** - –º–æ–¥–µ–ª—å –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç minority\n2. **Accuracy misleading** - 99% accuracy –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ —Ç–æ–ª—å–∫–æ majority\n3. **Gradient domination** - majority –¥–æ–º–∏–Ω–∏—Ä—É–µ—Ç –≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞—Ö\n\n### 1.2 –†–µ—à–µ–Ω–∏—è\n\n**3 –ø–æ–¥—Ö–æ–¥–∞:**\n1. **Data-level:** Resampling (SMOTE, undersampling)\n2. **Algorithm-level:** Cost-sensitive learning, class weights\n3. **Hybrid:** –ö–æ–º–±–∏–Ω–∞—Ü–∏—è –æ–±–æ–∏—Ö\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.3 SMOTE (Synthetic Minority Over-sampling)\n\n**–ò–¥–µ—è:** –°–æ–∑–¥–∞—Ç—å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã minority –∫–ª–∞—Å—Å–∞\n\n**–ê–ª–≥–æ—Ä–∏—Ç–º:**\n1. –î–ª—è –∫–∞–∂–¥–æ–≥–æ minority –ø—Ä–∏–º–µ—Ä–∞ $x_i$\n2. –ù–∞–π—Ç–∏ $k$ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π (–æ–±—ã—á–Ω–æ k=5)\n3. –°–ª—É—á–∞–π–Ω–æ –≤—ã–±—Ä–∞—Ç—å –æ–¥–Ω–æ–≥–æ —Å–æ—Å–µ–¥–∞ $x_{nn}$\n4. –°–æ–∑–¥–∞—Ç—å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–∏–º–µ—Ä:\n\n$$x_{\\text{new}} = x_i + \\lambda (x_{nn} - x_i)$$\n\n–≥–¥–µ $\\lambda \\sim \\text{Uniform}(0, 1)$\n\n**–ò–Ω—Ç—É–∏—Ü–∏—è:** –ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –º–µ–∂–¥—É —Ä–µ–∞–ª—å–Ω—ã–º–∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏\n\n**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**\n- ‚úÖ –ù–µ –¥—É–±–ª–∏—Ä—É–µ—Ç –ø—Ä–∏–º–µ—Ä—ã (vs naive oversampling)\n- ‚úÖ –†–∞—Å—à–∏—Ä—è–µ—Ç decision region minority –∫–ª–∞—Å—Å–∞\n- ‚úÖ –£–º–µ–Ω—å—à–∞–µ—Ç overfitting\n\n**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:**\n- ‚ùå –ú–æ–∂–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å —à—É–º –µ—Å–ª–∏ –∫–ª–∞—Å—Å—ã –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—Ç—Å—è\n- ‚ùå –ù–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç majority –∫–ª–∞—Å—Å (–º–æ–∂–µ—Ç —Å–æ–∑–¥–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã –≤ –∏—Ö –æ–±–ª–∞—Å—Ç–∏)\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.4 Cost-Sensitive Learning\n\n**–ò–¥–µ—è:** –®—Ç—Ä–∞—Ñ–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –±–æ–ª—å—à–µ –∑–∞ –æ—à–∏–±–∫–∏ –Ω–∞ minority\n\n**Cost matrix:**\n\n|  | Predict 0 | Predict 1 |\n|--|-----------|----------|\n| **Actual 0** | 0 | $C_{FP}$ |\n| **Actual 1** | $C_{FN}$ | 0 |\n\n**Total cost:**\n$$\\text{Cost} = C_{FP} \\cdot FP + C_{FN} \\cdot FN$$\n\n**Class weights:**\n–í–º–µ—Å—Ç–æ –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ loss, –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º weighted loss:\n\n$$L_{\\text{weighted}} = \\sum_{i=1}^n w_{y_i} L(y_i, \\hat{y}_i)$$\n\n–≥–¥–µ \n$$w_0 = \\frac{n}{n_0}, \\quad w_1 = \\frac{n}{n_1}$$\n\n**–î–ª—è 1:600 imbalance:**\n- $w_0 = \\frac{601}{600} \\approx 1$\n- $w_1 = \\frac{601}{1} = 601$\n\nMinority –ø—Ä–∏–º–µ—Ä—ã –ø–æ–ª—É—á–∞—é—Ç –≤–µ—Å 601!\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.5 Evaluation Metrics\n\n**Accuracy –ù–ï –ø–æ–¥—Ö–æ–¥–∏—Ç!**\n\n–î–ª—è imbalanced –∏—Å–ø–æ–ª—å–∑—É–µ–º:\n\n**1. Precision & Recall:**\n$$\\text{Precision} = \\frac{TP}{TP + FP}, \\quad \\text{Recall} = \\frac{TP}{TP + FN}$$\n\n**2. F1-score:**\n$$F_1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n\n**3. PR-AUC:** Area –ø–æ–¥ Precision-Recall –∫—Ä–∏–≤–æ–π\n- –õ—É—á—à–µ —á–µ–º ROC-AUC –¥–ª—è imbalanced!\n- ROC-AUC –º–æ–∂–µ—Ç –±—ã—Ç—å misleading\n\n**4. Business Cost:**\n$$\\text{Cost} = C_{FP} \\cdot FP + C_{FN} \\cdot FN$$\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üìä –ß–∞—Å—Ç—å 2: –ü—Ä–∞–∫—Ç–∏–∫–∞\n\n### 2.1 –ò–º–ø–æ—Ä—Ç—ã"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Imbalanced-learn\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.combine import SMOTETomek\n\n# Models\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# Utils\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import roc_auc_score, average_precision_score\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_recall_curve, roc_curve\n\nplt.style.use('seaborn-v0_8-darkgrid')\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\nprint('‚úÖ Libraries loaded')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.2 Dataset: Credit Card Fraud\n\n**Kaggle Credit Card Fraud Detection**\n\n- 284,807 —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π\n- 492 fraud (0.17%)\n- 28 PCA features + Amount + Time\n- Highly imbalanced!\n\nDownload: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# –ó–∞–≥—Ä—É–∑–∫–∞\nimport os\ndata_path = '../../data/creditcard.csv'\n\nif os.path.exists(data_path):\n    df = pd.read_csv(data_path)\n    print(f'‚úÖ Loaded: {df.shape[0]:,} transactions')\n    print(f'Features: {df.shape[1]}')\nelse:\n    print('‚ùå File not found')\n    print('Download from Kaggle and save to data/creditcard.csv')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.3 EDA"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('=== Info ===')\ndf.info()\nprint('\\n=== Class distribution ===')\nprint(df['Class'].value_counts())\nprint('\\nImbalance ratio:')\nfraud_rate = df['Class'].mean()\nprint(f'Fraud: {fraud_rate:.4%}')\nprint(f'Ratio: 1:{int(1/fraud_rate)}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualization\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\ndf['Class'].value_counts().plot(kind='bar', ax=axes[0], color=['green', 'red'])\naxes[0].set_title('Class Distribution (Extreme Imbalance!)')\naxes[0].set_xlabel('Class (0=Normal, 1=Fraud)')\naxes[0].set_ylabel('Count')\naxes[0].set_xticklabels(['Normal', 'Fraud'], rotation=0)\n\nfor i, v in enumerate(df['Class'].value_counts()):\n    axes[0].text(i, v + 5000, f'{v:,}\\n({100*v/len(df):.2f}%)', ha='center')\n\ndf['Class'].value_counts().plot(kind='pie', ax=axes[1], autopct='%1.2f%%', colors=['green', 'red'])\naxes[1].set_title('Class Proportion')\naxes[1].set_ylabel('')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Features & target\nX = df.drop('Class', axis=1)\ny = df['Class']\n\nprint(f'X shape: {X.shape}')\nprint(f'y shape: {y.shape}')\n\n# Train/test split (stratified!)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_STATE, stratify=y)\n\nprint(f'\\nTrain: {X_train.shape[0]:,}')\nprint(f'Test: {X_test.shape[0]:,}')\nprint(f'\\nFraud in train: {y_train.sum()} ({y_train.mean():.4%})')\nprint(f'Fraud in test: {y_test.sum()} ({y_test.mean():.4%})')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üéØ –ß–∞—Å—Ç—å 3: Baseline (No Resampling)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def evaluate_fraud(model, X_tr, X_te, y_tr, y_te, name):\n    model.fit(X_tr, y_tr)\n    y_pred = model.predict(X_te)\n    y_proba = model.predict_proba(X_te)[:, 1]\n    \n    prec = precision_score(y_te, y_pred)\n    rec = recall_score(y_te, y_pred)\n    f1 = f1_score(y_te, y_pred)\n    pr_auc = average_precision_score(y_te, y_proba)\n    \n    # Business cost (FN = $500, FP = $10)\n    tn, fp, fn, tp = confusion_matrix(y_te, y_pred).ravel()\n    cost = fn * 500 + fp * 10\n    \n    print(f'\\n{name}:')\n    print(f'  Precision: {prec:.4f}')\n    print(f'  Recall: {rec:.4f}')\n    print(f'  F1: {f1:.4f}')\n    print(f'  PR-AUC: {pr_auc:.4f}')\n    print(f'  Confusion: TN={tn}, FP={fp}, FN={fn}, TP={tp}')\n    print(f'  Cost: ${cost:,}')\n    \n    return {'model': model, 'prec': prec, 'rec': rec, 'f1': f1, 'pr_auc': pr_auc, 'cost': cost, 'y_pred': y_pred, 'y_proba': y_proba}\n\nprint('Eval function ready')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "results = {}\n\n# Baseline XGBoost (no special handling)\nxgb_baseline = XGBClassifier(n_estimators=100, max_depth=5, random_state=RANDOM_STATE, n_jobs=-1, eval_metric='logloss')\nresults['XGB_baseline'] = evaluate_fraud(xgb_baseline, X_train, X_test, y_train, y_test, 'XGBoost Baseline')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üî• –ß–∞—Å—Ç—å 4: SMOTE"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('Applying SMOTE...')\nsmote = SMOTE(random_state=RANDOM_STATE)\nX_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n\nprint(f'Original: {y_train.value_counts().to_dict()}')\nprint(f'After SMOTE: {pd.Series(y_train_smote).value_counts().to_dict()}')\nprint(f'\\nBalanced! Both classes have same count.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# XGBoost with SMOTE\nxgb_smote = XGBClassifier(n_estimators=100, max_depth=5, random_state=RANDOM_STATE, n_jobs=-1, eval_metric='logloss')\nresults['XGB_SMOTE'] = evaluate_fraud(xgb_smote, X_train_smote, X_test, y_train_smote, y_test, 'XGBoost + SMOTE')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ‚öñÔ∏è –ß–∞—Å—Ç—å 5: Class Weights"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Calculate scale_pos_weight\nscale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\nprint(f'scale_pos_weight: {scale_pos_weight:.0f}')\n\nxgb_weighted = XGBClassifier(n_estimators=100, max_depth=5, scale_pos_weight=scale_pos_weight, random_state=RANDOM_STATE, n_jobs=-1, eval_metric='logloss')\nresults['XGB_weighted'] = evaluate_fraud(xgb_weighted, X_train, X_test, y_train, y_test, 'XGBoost + Class Weights')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üöÄ –ß–∞—Å—Ç—å 6: Hybrid (SMOTE + Weights)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Moderate SMOTE (–Ω–µ –¥–æ 50/50, –∞ –¥–æ 1:10)\nfrom imblearn.over_sampling import SMOTE\nsmote_moderate = SMOTE(sampling_strategy=0.1, random_state=RANDOM_STATE)  # 10% instead of 50%\nX_train_moderate, y_train_moderate = smote_moderate.fit_resample(X_train, y_train)\n\nprint(f'After moderate SMOTE:')\nprint(pd.Series(y_train_moderate).value_counts())\nprint(f'Ratio: 1:{(y_train_moderate==0).sum() / (y_train_moderate==1).sum():.0f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# XGBoost with moderate SMOTE + weights\nscale_moderate = (y_train_moderate == 0).sum() / (y_train_moderate == 1).sum()\nxgb_hybrid = XGBClassifier(n_estimators=100, max_depth=5, scale_pos_weight=scale_moderate, random_state=RANDOM_STATE, n_jobs=-1, eval_metric='logloss')\nresults['XGB_hybrid'] = evaluate_fraud(xgb_hybrid, X_train_moderate, X_test, y_train_moderate, y_test, 'XGBoost + Hybrid')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üìä –ß–∞—Å—Ç—å 7: Comparison"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "comp = pd.DataFrame({\n    'Method': list(results.keys()),\n    'Precision': [results[m]['prec'] for m in results],\n    'Recall': [results[m]['rec'] for m in results],\n    'F1': [results[m]['f1'] for m in results],\n    'PR-AUC': [results[m]['pr_auc'] for m in results],\n    'Cost ($)': [results[m]['cost'] for m in results]\n}).sort_values('F1', ascending=False)\n\nprint('\\n=== COMPARISON ===')\ndisplay(comp)\nprint(f'\\nüèÜ Best F1: {comp.iloc[0][\"Method\"]}')\nprint(f'üí∞ Lowest cost: {comp.loc[comp[\"Cost ($)\"].idxmin(), \"Method\"]}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualization\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nfor idx, metric in enumerate(['Recall', 'Precision', 'F1']):\n    data = comp.sort_values(metric, ascending=False)\n    axes[idx].barh(data['Method'], data[metric], color='skyblue')\n    axes[idx].set_xlabel(metric, fontweight='bold')\n    axes[idx].set_title(f'{metric} Comparison')\n    axes[idx].invert_yaxis()\n    for i, (m, v) in enumerate(zip(data['Method'], data[metric])):\n        axes[idx].text(v + 0.01, i, f'{v:.3f}', va='center')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üìù –í—ã–≤–æ–¥—ã\n\n### –†–µ–∑—É–ª—å—Ç–∞—Ç—ã\n\n**–î–ª—è fraud detection (1:600 imbalance):**\n\n1. **Baseline** - –Ω–∏–∑–∫–∏–π recall (–ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç fraud!)\n2. **SMOTE** - —É–ª—É—á—à–∞–µ—Ç recall, –Ω–æ —Å–Ω–∏–∂–∞–µ—Ç precision\n3. **Class weights** - —Ö–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å\n4. **Hybrid** - –ª—É—á—à–∏–π F1 –∏ –±–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫–∏\n\n### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n\n**–î–ª—è fraud detection:**\n- ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ class weights (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ!)\n- ‚úÖ Moderate SMOTE (1:10) + weights (–ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç)\n- ‚úÖ Optimize threshold –ø–æ business cost\n- ‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä—å—Ç–µ Recall (–ø—Ä–æ–ø—É—Å–∫–∏ –¥–æ—Ä–æ–≥–∏!)\n\n**–û–±—â–∏–µ best practices:**\n1. **–ü–æ–Ω–∏–º–∞–π—Ç–µ business cost** FP vs FN\n2. **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏** (Precision-Recall, not Accuracy!)\n3. **Stratified split** - –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ\n4. **Threshold optimization** - —á–∞—Å—Ç–æ –≤–∞–∂–Ω–µ–µ —á–µ–º –º–æ–¥–µ–ª—å\n5. **Ensemble** - –∫–æ–º–±–∏–Ω–∏—Ä—É–π—Ç–µ —Ä–∞–∑–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã\n\n### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —á—Ç–æ?\n\n| Imbalance | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |\n|-----------|-------------|\n| 1:4 (mild) | Class weights |\n| 1:10-1:100 | SMOTE + class weights |\n| 1:100+ (extreme) | Moderate SMOTE + weights + threshold opt |\n\n---\n\n**References:**\n1. Chawla et al. (2002). SMOTE: Synthetic Minority Over-sampling Technique.\n2. He & Garcia (2009). Learning from Imbalanced Data.\n3. Lin et al. (2017). Focal Loss for Dense Object Detection."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
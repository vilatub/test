#!/usr/bin/env python3
"""
–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –Ω–æ—É—Ç–±—É–∫–∞: Advanced Feature Engineering
–í—Å—è —Ç–µ–æ—Ä–∏—è —Å —Ñ–æ—Ä–º—É–ª–∞–º–∏ –ø—Ä—è–º–æ –≤ –Ω–æ—É—Ç–±—É–∫–µ!
"""

import json

# –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –Ω–æ—É—Ç–±—É–∫–∞
notebook = {
    "cells": [],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

cells = []

# ============================================================================
# TITLE
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "# üîß Advanced Feature Engineering\n",
        "\n",
        "**–¶–µ–ª—å:** –£–≥–ª—É–±–ª–µ–Ω–Ω–æ–µ –∏–∑—É—á–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏–∫ —Å–æ–∑–¥–∞–Ω–∏—è –∏ –æ—Ç–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ ML –º–æ–¥–µ–ª–µ–π\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ –¶–µ–ª–∏ –Ω–æ—É—Ç–±—É–∫–∞\n",
        "\n",
        "1. **Polynomial features** –∏ feature interactions\n",
        "2. **Feature transformations:** log, sqrt, Box-Cox, Yeo-Johnson\n",
        "3. **Binning** –∏ discretization\n",
        "4. **Target encoding** –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "5. **Feature selection:** Filter, Wrapper, Embedded –º–µ—Ç–æ–¥—ã\n",
        "6. **–ü—Ä–∞–∫—Ç–∏–∫–∞:** House Prices —Å feature engineering –æ—Ç baseline –¥–æ advanced\n",
        "\n",
        "---\n",
        "\n",
        "## üíº –ë–∏–∑–Ω–µ—Å-–∑–∞–¥–∞—á–∞: House Price Prediction\n",
        "\n",
        "**–ö–æ–Ω—Ç–µ–∫—Å—Ç:** Real estate –∫–æ–º–ø–∞–Ω–∏—è —Ö–æ—á–µ—Ç —Ç–æ—á–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Ü–µ–Ω—ã –¥–æ–º–æ–≤.\n",
        "\n",
        "**–ü—Ä–æ–±–ª–µ–º—ã:**\n",
        "- üè† **–ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:** –ü–ª–æ—â–∞–¥—å √ó –ö–∞—á–µ—Å—Ç–≤–æ –≤–ª–∏—è–µ—Ç –Ω–∞ —Ü–µ–Ω—É –Ω–µ–ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ\n",
        "- üìä **Skewed —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è:** –¶–µ–Ω—ã –∏ –ø–ª–æ—â–∞–¥–∏ –∏–º–µ—é—Ç long tail\n",
        "- üî¢ **–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:** –†–∞–π–æ–Ω, —Ç–∏–ø –¥–æ–º–∞ (high cardinality)\n",
        "- üßÆ **–ú–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:** 80+ features, –º–Ω–æ–≥–∏–µ redundant\n",
        "\n",
        "**–¶–µ–ª—å:** –°–Ω–∏–∑–∏—Ç—å RMSE –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ü–µ–Ω —á–µ—Ä–µ–∑ advanced feature engineering\n",
        "\n",
        "---"
    ]
})

# ============================================================================
# THEORY PART 1: INTRODUCTION
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "## üìö –ß–∞—Å—Ç—å 1: –¢–µ–æ—Ä–∏—è Advanced Feature Engineering\n",
        "\n",
        "### 1.1 –ó–∞—á–µ–º –Ω—É–∂–µ–Ω Feature Engineering?\n",
        "\n",
        "**–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:** Feature engineering ‚Äî –ø—Ä–æ—Ü–µ—Å—Å —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏.\n",
        "\n",
        "#### –ü–æ—á–µ–º—É —ç—Ç–æ –≤–∞–∂–Ω–æ?\n",
        "\n",
        "**–≠–º–ø–∏—Ä–∏—á–µ—Å–∫–æ–µ –ø—Ä–∞–≤–∏–ª–æ (Andrew Ng):**\n",
        "\n",
        "> \"Coming up with features is difficult, time-consuming, requires expert knowledge.  \n",
        "> Applied machine learning is basically feature engineering.\" ‚Äî Andrew Ng\n",
        "\n",
        "**–í–ª–∏—è–Ω–∏–µ –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ:**\n",
        "\n",
        "| –ö–æ–º–ø–æ–Ω–µ–Ω—Ç | –í–ª–∏—è–Ω–∏–µ –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ |\n",
        "|-----------|--------------------|\n",
        "| **Feature engineering** | 60-70% |\n",
        "| Algorithm selection | 15-20% |\n",
        "| Hyperparameter tuning | 10-15% |\n",
        "| Ensemble | 5-10% |\n",
        "\n",
        "**–ü—Ä–∏–º–µ—Ä—ã –∏–∑ –ø—Ä–∞–∫—Ç–∏–∫–∏:**\n",
        "\n",
        "- **Kaggle:** –¢–æ–ø —Ä–µ—à–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Å–æ—Ç–Ω–∏ engineered features\n",
        "- **Industry:** Feature engineering —á–∞—Å—Ç–æ –¥–∞–µ—Ç –±–æ–ª—å—à–∏–π –ø—Ä–∏—Ä–æ—Å—Ç, —á–µ–º —Å–º–µ–Ω–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–∞\n",
        "\n",
        "#### –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è:\n",
        "\n",
        "1. **Feature creation** ‚Äî —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "2. **Feature transformation** ‚Äî –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö\n",
        "3. **Feature selection** ‚Äî –æ—Ç–±–æ—Ä –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã—Ö\n",
        "4. **Feature extraction** ‚Äî –ø–æ–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (PCA, –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã)\n",
        "\n",
        "---"
    ]
})

# ============================================================================
# THEORY PART 2: POLYNOMIAL FEATURES
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 1.2 Polynomial Features –∏ Interactions\n",
        "\n",
        "#### Polynomial Features\n",
        "\n",
        "**–ò–¥–µ—è:** –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç–µ–ø–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π.\n",
        "\n",
        "**–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏:**\n",
        "\n",
        "–î–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ $x_1, x_2, \\ldots, x_n$ –∏ —Å—Ç–µ–ø–µ–Ω–∏ $d$:\n",
        "\n",
        "$$\\phi(x_1, x_2) = [1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]$$\n",
        "\n",
        "–î–ª—è —Å—Ç–µ–ø–µ–Ω–∏ 2:\n",
        "\n",
        "$$\\phi(\\mathbf{x}) = [1, x_1, x_2, \\ldots, x_n, x_1^2, x_1 x_2, \\ldots, x_n^2]$$\n",
        "\n",
        "**–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:**\n",
        "\n",
        "$$N_{\\text{poly}} = \\binom{n + d}{d} = \\frac{(n+d)!}{d! \\cdot n!}$$\n",
        "\n",
        "–î–ª—è $n=10$ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\n",
        "- –°—Ç–µ–ø–µ–Ω—å 2: $\\binom{10+2}{2} = 66$ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "- –°—Ç–µ–ø–µ–Ω—å 3: $\\binom{10+3}{3} = 286$ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "\n",
        "**–ü—Ä–∏–º–µ—Ä (House Prices):**\n",
        "\n",
        "```python\n",
        "# –ò—Å—Ö–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "x1 = LotArea     # –ü–ª–æ—â–∞–¥—å —É—á–∞—Å—Ç–∫–∞\n",
        "x2 = OverallQual # –ö–∞—á–µ—Å—Ç–≤–æ (1-10)\n",
        "\n",
        "# Polynomial degree 2\n",
        "features = [\n",
        "    1,                    # bias\n",
        "    x1,                   # LotArea\n",
        "    x2,                   # OverallQual\n",
        "    x1^2,                 # LotArea^2 (–±–æ–ª—å—à–∏–µ —É—á–∞—Å—Ç–∫–∏ —Ü–µ–Ω–Ω–µ–µ –Ω–µ–ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
        "    x1 * x2,              # LotArea √ó OverallQual (INTERACTION!)\n",
        "    x2^2                  # OverallQual^2\n",
        "]\n",
        "```\n",
        "\n",
        "#### Feature Interactions\n",
        "\n",
        "**–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:** –ü—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –¥–≤—É—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –æ—Ç—Ä–∞–∂–∞—é—â–µ–µ –∏—Ö —Å–æ–≤–º–µ—Å—Ç–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ.\n",
        "\n",
        "**–ö–æ–≥–¥–∞ –≤–∞–∂–Ω–æ:**\n",
        "\n",
        "–ï—Å–ª–∏ –≤–ª–∏—è–Ω–∏–µ $x_1$ –Ω–∞ $y$ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∑–Ω–∞—á–µ–Ω–∏—è $x_2$:\n",
        "\n",
        "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\varepsilon$$\n",
        "\n",
        "**–ü—Ä–∏–º–µ—Ä—ã:**\n",
        "\n",
        "| –ó–∞–¥–∞—á–∞ | –ü—Ä–∏–∑–Ω–∞–∫–∏ | Interaction | –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è |\n",
        "|--------|----------|-------------|---------------|\n",
        "| House prices | `GrLivArea` √ó `OverallQual` | –ü–ª–æ—â–∞–¥—å √ó –ö–∞—á–µ—Å—Ç–≤–æ | –ë–æ–ª—å—à–æ–π –¥–æ–º –Ω–∏–∑–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ —Å—Ç–æ–∏—Ç –º–µ–Ω—å—à–µ |\n",
        "| Ecommerce | `Price` √ó `Discount` | –¶–µ–Ω–∞ √ó –°–∫–∏–¥–∫–∞ | –°–∫–∏–¥–∫–∞ –Ω–∞ –¥–æ—Ä–æ–≥–æ–π —Ç–æ–≤–∞—Ä –≤–∞–∂–Ω–µ–µ |\n",
        "| Credit scoring | `Income` √ó `Debt` | –î–æ—Ö–æ–¥ √ó –î–æ–ª–≥ | Debt-to-income ratio |\n",
        "\n",
        "**Sklearn —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:**\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "poly = PolynomialFeatures(\n",
        "    degree=2,              # –°—Ç–µ–ø–µ–Ω—å\n",
        "    interaction_only=False, # False = –≤–∫–ª—é—á–∏—Ç—å x^2, True = —Ç–æ–ª—å–∫–æ x1*x2\n",
        "    include_bias=False      # –ù–µ –¥–æ–±–∞–≤–ª—è—Ç—å —Å—Ç–æ–ª–±–µ—Ü –µ–¥–∏–Ω–∏—Ü\n",
        ")\n",
        "X_poly = poly.fit_transform(X)\n",
        "```\n",
        "\n",
        "**‚ö†Ô∏è –û—Å—Ç–æ—Ä–æ–∂–Ω–æ:**\n",
        "\n",
        "- **Curse of dimensionality:** –ß–∏—Å–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Ä–∞—Å—Ç–µ—Ç –∫–∞–∫ $O(n^d)$\n",
        "- **Overfitting:** –ù—É–∂–Ω–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (Ridge, Lasso)\n",
        "- **Multicollinearity:** –ü—Ä–∏–∑–Ω–∞–∫–∏ —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—Ç\n",
        "\n",
        "---"
    ]
})

# ============================================================================
# THEORY PART 3: TRANSFORMATIONS
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 1.3 Feature Transformations\n",
        "\n",
        "#### –ó–∞—á–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤—ã–≤–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏?\n",
        "\n",
        "1. **–£–º–µ–Ω—å—à–∏—Ç—å skewness** (–∞—Å–∏–º–º–µ—Ç—Ä–∏—é)\n",
        "2. **–°—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å variance**\n",
        "3. **–°–¥–µ–ª–∞—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –±–ª–∏–∂–µ –∫ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–º—É** (–≤–∞–∂–Ω–æ –¥–ª—è –ª–∏–Ω–µ–π–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π)\n",
        "4. **–£–ª—É—á—à–∏—Ç—å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å**\n",
        "\n",
        "#### 1.3.1 Log Transform\n",
        "\n",
        "**–§–æ—Ä–º—É–ª–∞:**\n",
        "\n",
        "$$x_{\\text{log}} = \\log(x + c)$$\n",
        "\n",
        "–≥–¥–µ $c$ ‚Äî –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞ (–æ–±—ã—á–Ω–æ 1), —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å $\\log(0)$.\n",
        "\n",
        "**–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**\n",
        "\n",
        "- Right-skewed –¥–∞–Ω–Ω—ã–µ (long tail –≤–ø—Ä–∞–≤–æ)\n",
        "- –î–∞–Ω–Ω—ã–µ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø–æ—Ä—è–¥–∫–∞–º–∏ –≤–µ–ª–∏—á–∏–Ω—ã (—Ü–µ–Ω—ã, –¥–æ—Ö–æ–¥—ã, –ø–ª–æ—â–∞–¥–∏)\n",
        "\n",
        "**–≠—Ñ—Ñ–µ–∫—Ç:**\n",
        "\n",
        "- –°–∂–∏–º–∞–µ—Ç –±–æ–ª—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
        "- –†–∞—Å—Ç—è–≥–∏–≤–∞–µ—Ç –º–∞–ª—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
        "\n",
        "**–ü—Ä–∏–º–µ—Ä:**\n",
        "\n",
        "```python\n",
        "# –î–æ: [1, 10, 100, 1000, 10000]\n",
        "# –ü–æ—Å–ª–µ log: [0, 2.3, 4.6, 6.9, 9.2]\n",
        "```\n",
        "\n",
        "**–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:**\n",
        "\n",
        "–í —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ $\\log(y) = \\beta_0 + \\beta_1 x$:\n",
        "\n",
        "- –£–≤–µ–ª–∏—á–µ–Ω–∏–µ $x$ –Ω–∞ 1 ‚Üí $y$ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è –Ω–∞ $e^{\\beta_1}$ —Ä–∞–∑ (–Ω–µ –Ω–∞ $\\beta_1$!)\n",
        "\n",
        "#### 1.3.2 Square Root Transform\n",
        "\n",
        "**–§–æ—Ä–º—É–ª–∞:**\n",
        "\n",
        "$$x_{\\text{sqrt}} = \\sqrt{x}$$\n",
        "\n",
        "**–ö–æ–≥–¥–∞:**\n",
        "\n",
        "- Count data (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–±—ã—Ç–∏–π)\n",
        "- Moderate skewness (log —Å–ª–∏—à–∫–æ–º –∞–≥—Ä–µ—Å—Å–∏–≤–µ–Ω)\n",
        "\n",
        "#### 1.3.3 Box-Cox Transform\n",
        "\n",
        "**–§–æ—Ä–º—É–ª–∞:**\n",
        "\n",
        "$$\n",
        "x_{\\text{boxcox}}(\\lambda) = \n",
        "\\begin{cases}\n",
        "\\frac{x^\\lambda - 1}{\\lambda}, & \\text{if } \\lambda \\neq 0 \\\\\n",
        "\\log(x), & \\text{if } \\lambda = 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "–≥–¥–µ $\\lambda$ –ø–æ–¥–±–∏—Ä–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥–æ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è.\n",
        "\n",
        "**–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏:**\n",
        "\n",
        "- $\\lambda = 1$: –ù–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ ($x - 1$)\n",
        "- $\\lambda = 0.5$: Square root\n",
        "- $\\lambda = 0$: Log transform\n",
        "- $\\lambda = -1$: Reciprocal ($1/x$)\n",
        "\n",
        "**‚ö†Ô∏è –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ:** –†–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –¥–ª—è $x > 0$!\n",
        "\n",
        "#### 1.3.4 Yeo-Johnson Transform\n",
        "\n",
        "**–§–æ—Ä–º—É–ª–∞:**\n",
        "\n",
        "$$\n",
        "x_{\\text{yj}}(\\lambda) = \n",
        "\\begin{cases}\n",
        "\\frac{(x+1)^\\lambda - 1}{\\lambda}, & \\text{if } \\lambda \\neq 0, x \\geq 0 \\\\\n",
        "\\log(x+1), & \\text{if } \\lambda = 0, x \\geq 0 \\\\\n",
        "\\frac{1 - (1-x)^{2-\\lambda}}{2-\\lambda}, & \\text{if } \\lambda \\neq 2, x < 0 \\\\\n",
        "-\\log(1-x), & \\text{if } \\lambda = 2, x < 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ:** –†–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è **–ª—é–±—ã—Ö** –∑–Ω–∞—á–µ–Ω–∏–π (–≤–∫–ª—é—á–∞—è –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∏ –Ω—É–ª–∏)!\n",
        "\n",
        "**Sklearn —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:**\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "# Box-Cox (—Ç–æ–ª—å–∫–æ x > 0)\n",
        "pt_boxcox = PowerTransformer(method='box-cox')\n",
        "\n",
        "# Yeo-Johnson (–ª—é–±—ã–µ x)\n",
        "pt_yj = PowerTransformer(method='yeo-johnson')\n",
        "```\n",
        "\n",
        "---"
    ]
})

# ============================================================================
# THEORY PART 4: BINNING
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 1.4 Binning –∏ Discretization\n",
        "\n",
        "**–ò–¥–µ—è:** –ü—Ä–µ–≤—Ä–∞—Ç–∏—Ç—å continuous –ø—Ä–∏–∑–Ω–∞–∫ –≤ categorical (–¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—è).\n",
        "\n",
        "#### –ó–∞—á–µ–º?\n",
        "\n",
        "1. **–ù–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å:** –ó–∞—Ö–≤–∞—Ç piecewise-linear –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π\n",
        "2. **Robustness:** –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ outliers\n",
        "3. **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å:** –ü—Ä–æ—â–µ –æ–±—ä—è—Å–Ω–∏—Ç—å –±–∏–∑–Ω–µ—Å—É (\"–º–æ–ª–æ–¥—ã–µ\", \"—Å—Ä–µ–¥–Ω–∏–µ\", \"–ø–æ–∂–∏–ª—ã–µ\")\n",
        "4. **Tree models:** –ò–Ω–æ–≥–¥–∞ –ø–æ–º–æ–≥–∞–µ—Ç –¥–µ—Ä–µ–≤—å—è–º –Ω–∞–π—Ç–∏ split points\n",
        "\n",
        "#### –¢–∏–ø—ã binning:\n",
        "\n",
        "**1. Equal-width binning (—Ä–∞–≤–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã)**\n",
        "\n",
        "$$\\text{width} = \\frac{\\max(x) - \\min(x)}{k}$$\n",
        "\n",
        "Bins: $[\\min, \\min + w), [\\min + w, \\min + 2w), \\ldots$\n",
        "\n",
        "**–ü—Ä–æ–±–ª–µ–º–∞:** Bins –º–æ–≥—É—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º–∏ –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ø–æ—á—Ç–∏ –≤—Å–µ –¥–∞–Ω–Ω—ã–µ (–µ—Å–ª–∏ skewed).\n",
        "\n",
        "**2. Equal-frequency binning (–∫–≤–∞–Ω—Ç–∏–ª–∏)**\n",
        "\n",
        "–ö–∞–∂–¥—ã–π bin —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ $n/k$ –ø—Ä–∏–º–µ—Ä–æ–≤.\n",
        "\n",
        "Bins: $[0\\%, 25\\%], (25\\%, 50\\%], (50\\%, 75\\%], (75\\%, 100\\%]$\n",
        "\n",
        "**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ:** Bins –≤—Å–µ–≥–¥–∞ –∑–∞–ø–æ–ª–Ω–µ–Ω—ã —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ.\n",
        "\n",
        "**3. Custom binning (domain knowledge)**\n",
        "\n",
        "–ü—Ä–∏–º–µ—Ä (–≤–æ–∑—Ä–∞—Å—Ç):\n",
        "```python\n",
        "bins = [0, 18, 35, 60, 100]\n",
        "labels = ['–ú–æ–ª–æ–¥—ã–µ', '–°—Ä–µ–¥–Ω–∏–µ', '–ó—Ä–µ–ª—ã–µ', '–ü–æ–∂–∏–ª—ã–µ']\n",
        "```\n",
        "\n",
        "**Sklearn —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:**\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "\n",
        "kbd = KBinsDiscretizer(\n",
        "    n_bins=5,               # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ bins\n",
        "    encode='ordinal',       # 'ordinal', 'onehot', 'onehot-dense'\n",
        "    strategy='quantile'     # 'uniform', 'quantile', 'kmeans'\n",
        ")\n",
        "```\n",
        "\n",
        "**‚ö†Ô∏è –û—Å—Ç–æ—Ä–æ–∂–Ω–æ:**\n",
        "\n",
        "- –ü–æ—Ç–µ—Ä—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (continuous ‚Üí discrete)\n",
        "- –ú–æ–∂–µ—Ç —É—Ö—É–¥—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ (–æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è tree-based –º–æ–¥–µ–ª–µ–π)\n",
        "- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–ª—è –ª–∏–Ω–µ–π–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏–ª–∏ domain-specific reasons\n",
        "\n",
        "---"
    ]
})

# ============================================================================
# THEORY PART 5: TARGET ENCODING
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 1.5 Target Encoding –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "\n",
        "#### –ü—Ä–æ–±–ª–µ–º–∞ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏\n",
        "\n",
        "**One-hot encoding –ø—Ä–æ–±–ª–µ–º—ã:**\n",
        "\n",
        "- **High cardinality:** 1000 –∫–∞—Ç–µ–≥–æ—Ä–∏–π ‚Üí 1000 –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (sparse!)\n",
        "- **Memory:** –û–≥—Ä–æ–º–Ω—ã–µ –º–∞—Ç—Ä–∏—Ü—ã\n",
        "- **Curse of dimensionality**\n",
        "\n",
        "**–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞:** Target encoding (mean encoding)\n",
        "\n",
        "#### Target Encoding\n",
        "\n",
        "**–ò–¥–µ—è:** –ó–∞–º–µ–Ω–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é –Ω–∞ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ target –¥–ª—è —ç—Ç–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.\n",
        "\n",
        "**–§–æ—Ä–º—É–ª–∞ (–ø—Ä–æ—Å—Ç–∞—è):**\n",
        "\n",
        "$$\\text{TE}(c) = \\frac{1}{n_c} \\sum_{i: x_i = c} y_i$$\n",
        "\n",
        "–≥–¥–µ:\n",
        "- $c$ ‚Äî –∫–∞—Ç–µ–≥–æ—Ä–∏—è\n",
        "- $n_c$ ‚Äî –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ —Å —ç—Ç–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π\n",
        "- $y_i$ ‚Äî target\n",
        "\n",
        "**–ü—Ä–∏–º–µ—Ä (House Prices):**\n",
        "\n",
        "```\n",
        "Neighborhood   Count   Mean_Price   Target_Encoding\n",
        "NoRidge         41     335,295      335,295\n",
        "NridgHt         77     316,271      316,271\n",
        "StoneBr         25     310,499      310,499\n",
        "OldTown        113     128,225      128,225\n",
        "```\n",
        "\n",
        "–í–º–µ—Å—Ç–æ 25 one-hot —Å—Ç–æ–ª–±—Ü–æ–≤ ‚Üí 1 —Å—Ç–æ–ª–±–µ—Ü —Å mean price!\n",
        "\n",
        "#### –ü—Ä–æ–±–ª–µ–º–∞: Target Leakage!\n",
        "\n",
        "–ï—Å–ª–∏ –ø—Ä–æ—Å—Ç–æ –∑–∞–º–µ–Ω–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é –Ω–∞ —Å—Ä–µ–¥–Ω–µ–µ $y$:\n",
        "\n",
        "**–ú–æ–¥–µ–ª—å –±—É–¥–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–∞—Ç—å—Å—è!**\n",
        "\n",
        "–ü—Ä–∏–º–µ—Ä:\n",
        "```python\n",
        "# –ö–∞—Ç–µ–≥–æ—Ä–∏—è 'X' –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è 1 —Ä–∞–∑ —Å y=100\n",
        "# Target encoding: X ‚Üí 100\n",
        "# –ú–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∂–µ—Ç 100 –∏–¥–µ–∞–ª—å–Ω–æ ‚Üí overfitting!\n",
        "```\n",
        "\n",
        "#### –†–µ—à–µ–Ω–∏–µ 1: Leave-One-Out (LOO) Encoding\n",
        "\n",
        "**–§–æ—Ä–º—É–ª–∞:**\n",
        "\n",
        "$$\\text{TE}_i(c) = \\frac{1}{n_c - 1} \\sum_{j \\neq i, x_j = c} y_j$$\n",
        "\n",
        "–î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ $i$ —Å—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ **–±–µ–∑** —ç—Ç–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞.\n",
        "\n",
        "#### –†–µ—à–µ–Ω–∏–µ 2: Smoothed Target Encoding (Bayesian)\n",
        "\n",
        "**–§–æ—Ä–º—É–ª–∞:**\n",
        "\n",
        "$$\\text{TE}_{\\text{smooth}}(c) = \\frac{n_c \\cdot \\bar{y}_c + m \\cdot \\bar{y}_{\\text{global}}}{n_c + m}$$\n",
        "\n",
        "–≥–¥–µ:\n",
        "- $\\bar{y}_c$ ‚Äî —Å—Ä–µ–¥–Ω–µ–µ $y$ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ $c$\n",
        "- $\\bar{y}_{\\text{global}}$ ‚Äî –≥–ª–æ–±–∞–ª—å–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ $y$\n",
        "- $m$ ‚Äî –ø–∞—Ä–∞–º–µ—Ç—Ä —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è (–æ–±—ã—á–Ω–æ 10-100)\n",
        "\n",
        "**–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:**\n",
        "\n",
        "- –ï—Å–ª–∏ $n_c$ **–±–æ–ª—å—à–æ–µ** ‚Üí –¥–æ–≤–µ—Ä—è–µ–º $\\bar{y}_c$\n",
        "- –ï—Å–ª–∏ $n_c$ **–º–∞–ª–µ–Ω—å–∫–æ–µ** ‚Üí —Ä–µ–≥—Ä–µ—Å—Å–∏—Ä—É–µ–º –∫ –≥–ª–æ–±–∞–ª—å–Ω–æ–º—É —Å—Ä–µ–¥–Ω–µ–º—É\n",
        "\n",
        "**–ü—Ä–∏–º–µ—Ä:**\n",
        "\n",
        "```python\n",
        "# –ö–∞—Ç–µ–≥–æ—Ä–∏—è —Å 1000 –ø—Ä–∏–º–µ—Ä–∞–º–∏\n",
        "TE = (1000 * 250_000 + 100 * 180_000) / (1000 + 100) = 243_636\n",
        "# –ë–ª–∏–∑–∫–æ –∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π–Ω–æ–º—É —Å—Ä–µ–¥–Ω–µ–º—É 250_000\n",
        "\n",
        "# –ö–∞—Ç–µ–≥–æ—Ä–∏—è —Å 5 –ø—Ä–∏–º–µ—Ä–∞–º–∏\n",
        "TE = (5 * 350_000 + 100 * 180_000) / (5 + 100) = 193_333\n",
        "# –°–¥–≤–∏–≥ –∫ –≥–ª–æ–±–∞–ª—å–Ω–æ–º—É 180_000\n",
        "```\n",
        "\n",
        "**Sklearn (category_encoders):**\n",
        "\n",
        "```python\n",
        "from category_encoders import TargetEncoder\n",
        "\n",
        "te = TargetEncoder(smoothing=10)  # Bayesian smoothing\n",
        "X_train_encoded = te.fit_transform(X_train, y_train)\n",
        "X_test_encoded = te.transform(X_test)\n",
        "```\n",
        "\n",
        "**‚ö†Ô∏è –í–∞–∂–Ω–æ:**\n",
        "\n",
        "- **–ù–ï** fit –Ω–∞ test! (—É—Ç–µ—á–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏)\n",
        "- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ cross-validation –¥–ª—è train\n",
        "- –î–ª—è tree models –º–æ–∂–µ—Ç –±—ã—Ç—å –ª—É—á—à–µ native categorical support (CatBoost)\n",
        "\n",
        "---"
    ]
})

# ============================================================================
# THEORY PART 6: FEATURE SELECTION
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 1.6 Feature Selection\n",
        "\n",
        "**–¶–µ–ª—å:** –í—ã–±—Ä–∞—Ç—å –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, —É–¥–∞–ª–∏–≤ redundant –∏ irrelevant.\n",
        "\n",
        "#### –ó–∞—á–µ–º?\n",
        "\n",
        "1. **Reduce overfitting:** –ú–µ–Ω—å—à–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ‚Üí –º–µ–Ω—å—à–µ noise\n",
        "2. **Improve performance:** –ë—ã—Å—Ç—Ä–µ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ inference\n",
        "3. **Better interpretability:** –ü–æ–Ω—è—Ç—å, —á—Ç–æ –≤–∞–∂–Ω–æ\n",
        "4. **–ò–∑–±–µ–∂–∞—Ç—å curse of dimensionality**\n",
        "\n",
        "#### –¢—Ä–∏ –ø–æ–¥—Ö–æ–¥–∞:\n",
        "\n",
        "### 1.6.1 Filter Methods (–Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–µ –æ—Ç –º–æ–¥–µ–ª–∏)\n",
        "\n",
        "**–ò–¥–µ—è:** –û—Ü–µ–Ω–∏—Ç—å –∫–∞–∂–¥—ã–π –ø—Ä–∏–∑–Ω–∞–∫ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –ø–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ.\n",
        "\n",
        "**–ú–µ—Ç–æ–¥—ã:**\n",
        "\n",
        "**1. Correlation —Å target (–¥–ª—è regression)**\n",
        "\n",
        "Pearson correlation:\n",
        "\n",
        "$$r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2}}$$\n",
        "\n",
        "–í—ã–±–∏—Ä–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å $|r| >$ threshold.\n",
        "\n",
        "**2. Mutual Information (–¥–ª—è classification)**\n",
        "\n",
        "$$I(X; Y) = \\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log \\frac{p(x, y)}{p(x) p(y)}$$\n",
        "\n",
        "–ú–µ—Ä–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (—Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è –Ω–µ–ª–∏–Ω–µ–π–Ω—ã—Ö!).\n",
        "\n",
        "**3. Chi-squared test (–¥–ª—è categorical)**\n",
        "\n",
        "$$\\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i}$$\n",
        "\n",
        "**4. Variance threshold**\n",
        "\n",
        "–£–¥–∞–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å variance < threshold (–ø–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã).\n",
        "\n",
        "**Sklearn:**\n",
        "\n",
        "```python\n",
        "from sklearn.feature_selection import (\n",
        "    SelectKBest, f_regression, mutual_info_regression, VarianceThreshold\n",
        ")\n",
        "\n",
        "# Top K –ø–æ F-test\n",
        "selector = SelectKBest(score_func=f_regression, k=20)\n",
        "\n",
        "# Variance threshold\n",
        "vt = VarianceThreshold(threshold=0.01)\n",
        "```\n",
        "\n",
        "**‚úÖ –ü–ª—é—Å—ã:** –ë—ã—Å—Ç—Ä–æ, –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –º–æ–¥–µ–ª–∏  \n",
        "**‚ùå –ú–∏–Ω—É—Å—ã:** –ù–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç interactions –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏\n",
        "\n",
        "### 1.6.2 Wrapper Methods (–∏—Å–ø–æ–ª—å–∑—É—é—Ç –º–æ–¥–µ–ª—å)\n",
        "\n",
        "**–ò–¥–µ—è:** –ü–µ—Ä–µ–±—Ä–∞—Ç—å subset'—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å, –≤—ã–±—Ä–∞—Ç—å –ª—É—á—à–∏–π.\n",
        "\n",
        "**–ú–µ—Ç–æ–¥—ã:**\n",
        "\n",
        "**1. Forward Selection**\n",
        "\n",
        "```\n",
        "1. –ù–∞—á–∞—Ç—å —Å 0 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "2. –î–æ–±–∞–≤–ª—è—Ç—å –ø–æ –æ–¥–Ω–æ–º—É (—Ç–æ—Ç, —á—Ç–æ –¥–∞–µ—Ç max –ø—Ä–∏—Ä–æ—Å—Ç)\n",
        "3. –ü–æ–≤—Ç–æ—Ä—è—Ç—å, –ø–æ–∫–∞ –µ—Å—Ç—å –ø—Ä–∏—Ä–æ—Å—Ç\n",
        "```\n",
        "\n",
        "**2. Backward Elimination**\n",
        "\n",
        "```\n",
        "1. –ù–∞—á–∞—Ç—å —Å–æ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "2. –£–¥–∞–ª—è—Ç—å –ø–æ –æ–¥–Ω–æ–º—É (—Ç–æ—Ç, —É–¥–∞–ª–µ–Ω–∏–µ –∫–æ—Ç–æ—Ä–æ–≥–æ –¥–∞–µ—Ç min —É—Ö—É–¥—à–µ–Ω–∏–µ)\n",
        "3. –ü–æ–≤—Ç–æ—Ä—è—Ç—å, –ø–æ–∫–∞ –µ—Å—Ç—å —É–ª—É—á—à–µ–Ω–∏–µ\n",
        "```\n",
        "\n",
        "**3. Recursive Feature Elimination (RFE)**\n",
        "\n",
        "```\n",
        "1. –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö\n",
        "2. –†–∞–Ω–∂–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ importance (coefficients –∏–ª–∏ feature_importances_)\n",
        "3. –£–¥–∞–ª–∏—Ç—å –Ω–∞–∏–º–µ–Ω–µ–µ –≤–∞–∂–Ω—ã–µ\n",
        "4. –ü–æ–≤—Ç–æ—Ä–∏—Ç—å\n",
        "```\n",
        "\n",
        "**Sklearn:**\n",
        "\n",
        "```python\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "rfe = RFE(\n",
        "    estimator=RandomForestRegressor(),\n",
        "    n_features_to_select=20,\n",
        "    step=5  # –£–¥–∞–ª—è—Ç—å –ø–æ 5 –∑–∞ —Ä–∞–∑\n",
        ")\n",
        "rfe.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "**‚úÖ –ü–ª—é—Å—ã:** –£—á–∏—Ç—ã–≤–∞–µ—Ç interactions, —á–∞—Å—Ç–æ –ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ  \n",
        "**‚ùå –ú–∏–Ω—É—Å—ã:** –ú–µ–¥–ª–µ–Ω–Ω–æ (–ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –º–Ω–æ–≥–æ —Ä–∞–∑), –º–æ–∂–µ—Ç overfitting\n",
        "\n",
        "### 1.6.3 Embedded Methods (–≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –≤ –º–æ–¥–µ–ª—å)\n",
        "\n",
        "**–ò–¥–µ—è:** –ú–æ–¥–µ–ª—å —Å–∞–º–∞ –≤—ã–±–∏—Ä–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è.\n",
        "\n",
        "**–ú–µ—Ç–æ–¥—ã:**\n",
        "\n",
        "**1. L1 Regularization (Lasso)**\n",
        "\n",
        "$$\\min_{\\beta} \\sum (y_i - \\beta^T x_i)^2 + \\lambda \\sum |\\beta_j|$$\n",
        "\n",
        "L1 penalty –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ **sparse** —Ä–µ—à–µ–Ω–∏—é (–º–Ω–æ–≥–∏–µ $\\beta_j = 0$).\n",
        "\n",
        "**2. Tree-based feature importance**\n",
        "\n",
        "Random Forest / XGBoost / LightGBM –≤—ã—á–∏—Å–ª—è—é—Ç importance –Ω–∞ –æ—Å–Ω–æ–≤–µ:\n",
        "\n",
        "- **Gain:** –£–º–µ–Ω—å—à–µ–Ω–∏–µ loss –ø—Ä–∏ split –Ω–∞ —ç—Ç–æ–º –ø—Ä–∏–∑–Ω–∞–∫–µ\n",
        "- **Split count:** –ö–∞–∫ —á–∞—Å—Ç–æ –ø—Ä–∏–∑–Ω–∞–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è\n",
        "\n",
        "**3. Permutation importance**\n",
        "\n",
        "```\n",
        "1. –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å\n",
        "2. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞:\n",
        "   - –ü–µ—Ä–µ–º–µ—à–∞—Ç—å –µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è (—Å–ª–æ–º–∞—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å —Å y)\n",
        "   - –ò–∑–º–µ—Ä–∏—Ç—å —É—Ö—É–¥—à–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏\n",
        "3. –í–∞–∂–Ω–æ—Å—Ç—å = —É—Ö—É–¥—à–µ–Ω–∏–µ\n",
        "```\n",
        "\n",
        "**Sklearn:**\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# Lasso\n",
        "lasso = Lasso(alpha=0.1)\n",
        "lasso.fit(X_train, y_train)\n",
        "# lasso.coef_ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤–µ—Å–∞ (–º–Ω–æ–≥–∏–µ = 0)\n",
        "\n",
        "# Permutation importance\n",
        "perm = permutation_importance(model, X_val, y_val, n_repeats=10)\n",
        "```\n",
        "\n",
        "**‚úÖ –ü–ª—é—Å—ã:** –ë—ã—Å—Ç—Ä–æ, –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ –¥–ª—è –º–æ–¥–µ–ª–∏  \n",
        "**‚ùå –ú–∏–Ω—É—Å—ã:** –ó–∞–≤–∏—Å–∏—Ç –æ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º–∞\n",
        "\n",
        "---\n",
        "\n",
        "## –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è —á–∞—Å—Ç—å –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –ø—Ä–∞–∫—Ç–∏–∫–µ üöÄ\n",
        "\n",
        "---"
    ]
})

# ============================================================================
# PRACTICAL PART: IMPORTS
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "## üìä –ß–∞—Å—Ç—å 2: –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "\n",
        "### 2.1 –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –û—Å–Ω–æ–≤–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Sklearn preprocessing\n",
        "from sklearn.preprocessing import (\n",
        "    PolynomialFeatures, StandardScaler, PowerTransformer,\n",
        "    KBinsDiscretizer, LabelEncoder\n",
        ")\n",
        "\n",
        "# Feature selection\n",
        "from sklearn.feature_selection import (\n",
        "    SelectKBest, f_regression, mutual_info_regression,\n",
        "    RFE, VarianceThreshold\n",
        ")\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# Models\n",
        "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "# Model selection\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Target encoding\n",
        "try:\n",
        "    from category_encoders import TargetEncoder\n",
        "    HAS_CAT_ENCODERS = True\n",
        "except ImportError:\n",
        "    print('‚ö†Ô∏è category_encoders –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä—É—á–Ω—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é.')\n",
        "    HAS_CAT_ENCODERS = False\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette('husl')\n",
        "%matplotlib inline\n",
        "\n",
        "# Seed\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "print('‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã')"
    ]
})

# –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ cells –≤ notebook
notebook['cells'] = cells

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ—É—Ç–±—É–∫
output_path = '/home/user/test/notebooks/phase1_classical_ml/06_advanced_feature_engineering.ipynb'
with open(output_path, 'w', encoding='utf-8') as f:
    json.dump(notebook, f, ensure_ascii=False, indent=1)

print(f'‚úÖ –°–æ–∑–¥–∞–Ω –Ω–æ—É—Ç–±—É–∫ —Å {len(cells)} —è—á–µ–π–∫–∞–º–∏')
print(f'–°–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}')
print('üìù –¢–µ–æ—Ä–∏—è –Ω–∞–ø–∏—Å–∞–Ω–∞ –ø—Ä—è–º–æ –≤ –Ω–æ—É—Ç–±—É–∫–µ —Å —Ñ–æ—Ä–º—É–ª–∞–º–∏!')

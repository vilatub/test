{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78475d89",
   "metadata": {},
   "source": [
    "# Фаза 5.4: Обнаружение сетевых вторжений\n",
    "\n",
    "## Продвинутые методы кластеризации и глубокого обучения\n",
    "\n",
    "В этом ноутбуке мы рассмотрим продвинутые методы обнаружения аномалий на примере анализа сетевого трафика:\n",
    "\n",
    "### Методы кластеризации\n",
    "1. **Spectral Clustering** - использует собственные векторы матрицы сходства\n",
    "2. **Mean-Shift** - непараметрический алгоритм поиска мод плотности\n",
    "\n",
    "### Глубокое обучение\n",
    "3. **Bidirectional LSTM-Autoencoder** - двунаправленная рекуррентная сеть для анализа последовательностей\n",
    "\n",
    "### Задача\n",
    "Обнаружение сетевых атак: DDoS, сканирование портов, brute force и другие аномалии в сетевом трафике.\n",
    "\n",
    "### Датасет\n",
    "Синтетический датасет сетевого трафика (~15,000 записей) с различными типами атак и нормальным трафиком."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138e1b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import SpectralClustering, MeanShift, estimate_bandwidth\n",
    "from sklearn.metrics import silhouette_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Глубокое обучение\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, LSTM, Dense, RepeatVector,\n",
    "                                     TimeDistributed, Bidirectional, Dropout)\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"Библиотеки загружены успешно\")\n",
    "print(f\"TensorFlow версия: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc70006",
   "metadata": {},
   "source": [
    "## 1. Создание датасета сетевого трафика\n",
    "\n",
    "### Описание признаков\n",
    "\n",
    "Наш датасет содержит следующие признаки сетевых соединений:\n",
    "\n",
    "- **duration** - длительность соединения (секунды)\n",
    "- **src_bytes** - байты от источника к получателю\n",
    "- **dst_bytes** - байты от получателя к источнику\n",
    "- **count** - число соединений к тому же хосту за последние 2 секунды\n",
    "- **srv_count** - число соединений к тому же сервису за последние 2 секунды\n",
    "- **serror_rate** - процент соединений с ошибками SYN\n",
    "- **rerror_rate** - процент соединений с ошибками REJ\n",
    "- **same_srv_rate** - процент соединений к тому же сервису\n",
    "- **dst_host_count** - число соединений к тому же хосту назначения\n",
    "- **dst_host_srv_count** - число соединений к тому же сервису на хосте назначения\n",
    "\n",
    "### Типы трафика\n",
    "\n",
    "1. **Normal** - нормальный трафик\n",
    "2. **DDoS** - распределённая атака отказа в обслуживании\n",
    "3. **PortScan** - сканирование портов\n",
    "4. **BruteForce** - атака перебором паролей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f02338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network_traffic_data(n_samples=15000):\n",
    "    \"\"\"\n",
    "    Создание синтетического датасета сетевого трафика.\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "    n_samples : int\n",
    "        Общее количество записей\n",
    "\n",
    "    Возвращает:\n",
    "    -----------\n",
    "    DataFrame с признаками сетевого трафика и метками\n",
    "    \"\"\"\n",
    "    # Распределение классов\n",
    "    n_normal = int(n_samples * 0.70)      # 70% - нормальный трафик\n",
    "    n_ddos = int(n_samples * 0.10)        # 10% - DDoS\n",
    "    n_portscan = int(n_samples * 0.10)    # 10% - сканирование портов\n",
    "    n_bruteforce = int(n_samples * 0.10)  # 10% - brute force\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # 1. Нормальный трафик\n",
    "    for _ in range(n_normal):\n",
    "        record = {\n",
    "            'duration': np.random.exponential(10),\n",
    "            'src_bytes': np.random.lognormal(8, 1.5),\n",
    "            'dst_bytes': np.random.lognormal(8, 1.5),\n",
    "            'count': np.random.poisson(5),\n",
    "            'srv_count': np.random.poisson(3),\n",
    "            'serror_rate': np.random.beta(1, 50),\n",
    "            'rerror_rate': np.random.beta(1, 50),\n",
    "            'same_srv_rate': np.random.beta(10, 2),\n",
    "            'dst_host_count': np.random.poisson(20),\n",
    "            'dst_host_srv_count': np.random.poisson(10),\n",
    "            'attack_type': 'Normal',\n",
    "            'is_attack': 0\n",
    "        }\n",
    "        data.append(record)\n",
    "\n",
    "    # 2. DDoS атаки - много соединений, мало данных\n",
    "    for _ in range(n_ddos):\n",
    "        record = {\n",
    "            'duration': np.random.exponential(0.1),  # Очень короткие\n",
    "            'src_bytes': np.random.lognormal(4, 0.5),  # Мало данных\n",
    "            'dst_bytes': np.random.lognormal(3, 0.5),\n",
    "            'count': np.random.poisson(100),  # Много соединений!\n",
    "            'srv_count': np.random.poisson(80),\n",
    "            'serror_rate': np.random.beta(10, 5),  # Высокий уровень ошибок\n",
    "            'rerror_rate': np.random.beta(5, 5),\n",
    "            'same_srv_rate': np.random.beta(15, 1),  # К одному сервису\n",
    "            'dst_host_count': np.random.poisson(200),  # Много к одному хосту\n",
    "            'dst_host_srv_count': np.random.poisson(150),\n",
    "            'attack_type': 'DDoS',\n",
    "            'is_attack': 1\n",
    "        }\n",
    "        data.append(record)\n",
    "\n",
    "    # 3. Сканирование портов - много разных портов\n",
    "    for _ in range(n_portscan):\n",
    "        record = {\n",
    "            'duration': np.random.exponential(0.01),  # Очень быстрые\n",
    "            'src_bytes': np.random.lognormal(3, 0.3),  # Минимум данных\n",
    "            'dst_bytes': np.random.lognormal(2, 0.3),\n",
    "            'count': np.random.poisson(50),\n",
    "            'srv_count': np.random.poisson(1),  # Разные сервисы!\n",
    "            'serror_rate': np.random.beta(5, 10),\n",
    "            'rerror_rate': np.random.beta(10, 5),  # Много отказов\n",
    "            'same_srv_rate': np.random.beta(1, 10),  # Разные сервисы\n",
    "            'dst_host_count': np.random.poisson(100),\n",
    "            'dst_host_srv_count': np.random.poisson(5),\n",
    "            'attack_type': 'PortScan',\n",
    "            'is_attack': 1\n",
    "        }\n",
    "        data.append(record)\n",
    "\n",
    "    # 4. Brute Force - много попыток к одному сервису\n",
    "    for _ in range(n_bruteforce):\n",
    "        record = {\n",
    "            'duration': np.random.exponential(1),\n",
    "            'src_bytes': np.random.lognormal(5, 0.5),\n",
    "            'dst_bytes': np.random.lognormal(5, 0.5),\n",
    "            'count': np.random.poisson(30),\n",
    "            'srv_count': np.random.poisson(25),  # К одному сервису\n",
    "            'serror_rate': np.random.beta(2, 10),\n",
    "            'rerror_rate': np.random.beta(8, 5),  # Много неудачных попыток\n",
    "            'same_srv_rate': np.random.beta(20, 1),  # Всегда один сервис\n",
    "            'dst_host_count': np.random.poisson(50),\n",
    "            'dst_host_srv_count': np.random.poisson(40),\n",
    "            'attack_type': 'BruteForce',\n",
    "            'is_attack': 1\n",
    "        }\n",
    "        data.append(record)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Создаём датасет\n",
    "df = create_network_traffic_data(n_samples=15000)\n",
    "\n",
    "print(f\"Размер датасета: {df.shape}\")\n",
    "print(f\"\\nРаспределение классов:\")\n",
    "print(df['attack_type'].value_counts())\n",
    "print(f\"\\nДоля атак: {df['is_attack'].mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77cc7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Статистика по признакам\n",
    "print(\"Статистика признаков:\")\n",
    "print(df.describe().round(2))\n",
    "\n",
    "# Визуализация распределений\n",
    "fig, axes = plt.subplots(2, 5, figsize=(18, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "feature_cols = ['duration', 'src_bytes', 'dst_bytes', 'count', 'srv_count',\n",
    "                'serror_rate', 'rerror_rate', 'same_srv_rate',\n",
    "                'dst_host_count', 'dst_host_srv_count']\n",
    "\n",
    "for i, col in enumerate(feature_cols):\n",
    "    for attack_type in df['attack_type'].unique():\n",
    "        subset = df[df['attack_type'] == attack_type][col]\n",
    "        axes[i].hist(subset, bins=30, alpha=0.5, label=attack_type, density=True)\n",
    "    axes[i].set_title(col)\n",
    "    axes[i].set_xlabel('')\n",
    "    if i == 0:\n",
    "        axes[i].legend(fontsize=8)\n",
    "\n",
    "plt.suptitle('Распределение признаков по типам трафика', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b327feba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка данных\n",
    "feature_cols = ['duration', 'src_bytes', 'dst_bytes', 'count', 'srv_count',\n",
    "                'serror_rate', 'rerror_rate', 'same_srv_rate',\n",
    "                'dst_host_count', 'dst_host_srv_count']\n",
    "\n",
    "X = df[feature_cols].values\n",
    "y = df['is_attack'].values\n",
    "attack_types = df['attack_type'].values\n",
    "\n",
    "# Масштабирование\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"Размер данных: {X_scaled.shape}\")\n",
    "print(f\"Атаки: {y.sum()} ({y.mean()*100:.1f}%)\")\n",
    "\n",
    "# PCA для визуализации\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Визуализация в 2D\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "colors = {'Normal': 'blue', 'DDoS': 'red', 'PortScan': 'green', 'BruteForce': 'orange'}\n",
    "\n",
    "for attack_type in df['attack_type'].unique():\n",
    "    mask = attack_types == attack_type\n",
    "    ax.scatter(X_pca[mask, 0], X_pca[mask, 1],\n",
    "               c=colors[attack_type], label=attack_type,\n",
    "               alpha=0.5, s=20)\n",
    "\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "ax.set_title('Визуализация сетевого трафика (PCA)')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71495769",
   "metadata": {},
   "source": [
    "## 2. Spectral Clustering (Спектральная кластеризация)\n",
    "\n",
    "### Теория\n",
    "\n",
    "Спектральная кластеризация использует собственные векторы матрицы сходства (Лапласиана графа) для преобразования данных в пространство, где кластеры легче разделить.\n",
    "\n",
    "### Алгоритм\n",
    "\n",
    "1. **Построение графа сходства** - создаём граф, где вершины - точки данных, рёбра - сходство между ними\n",
    "2. **Вычисление Лапласиана** - $L = D - W$, где $D$ - диагональная матрица степеней, $W$ - матрица смежности\n",
    "3. **Собственные векторы** - находим $k$ наименьших собственных векторов $L$\n",
    "4. **K-means в новом пространстве** - кластеризуем точки в пространстве собственных векторов\n",
    "\n",
    "### Преимущества\n",
    "\n",
    "- Хорошо работает с нелинейно разделимыми данными\n",
    "- Может находить кластеры сложной формы\n",
    "- Не требует предположений о форме кластеров\n",
    "\n",
    "### Недостатки\n",
    "\n",
    "- Вычислительно затратен для больших данных ($O(n^3)$)\n",
    "- Требует выбора числа кластеров\n",
    "- Чувствителен к выбору параметра сходства"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f909f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectral Clustering\n",
    "# Используем подвыборку для ускорения (spectral clustering медленный)\n",
    "np.random.seed(42)\n",
    "sample_idx = np.random.choice(len(X_scaled), size=3000, replace=False)\n",
    "X_sample = X_scaled[sample_idx]\n",
    "y_sample = y[sample_idx]\n",
    "types_sample = attack_types[sample_idx]\n",
    "\n",
    "print(\"Применяем Spectral Clustering...\")\n",
    "print(\"(Используем подвыборку 3000 точек для ускорения)\")\n",
    "\n",
    "# Spectral Clustering с разным числом кластеров\n",
    "results = []\n",
    "\n",
    "for n_clusters in [2, 3, 4, 5]:\n",
    "    spectral = SpectralClustering(\n",
    "        n_clusters=n_clusters,\n",
    "        affinity='rbf',           # RBF ядро для сходства\n",
    "        gamma=0.1,                # Параметр RBF\n",
    "        random_state=42,\n",
    "        n_init=10\n",
    "    )\n",
    "\n",
    "    labels = spectral.fit_predict(X_sample)\n",
    "\n",
    "    # Оценка качества\n",
    "    silhouette = silhouette_score(X_sample, labels)\n",
    "\n",
    "    results.append({\n",
    "        'n_clusters': n_clusters,\n",
    "        'silhouette': silhouette,\n",
    "        'labels': labels\n",
    "    })\n",
    "\n",
    "    print(f\"Кластеров: {n_clusters}, Silhouette: {silhouette:.3f}\")\n",
    "\n",
    "# Лучший результат\n",
    "best_result = max(results, key=lambda x: x['silhouette'])\n",
    "print(f\"\\nЛучшее число кластеров: {best_result['n_clusters']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb7f4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация результатов Spectral Clustering\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# PCA для визуализации подвыборки\n",
    "X_sample_pca = pca.transform(X_sample)\n",
    "\n",
    "# Слева - истинные метки\n",
    "ax1 = axes[0]\n",
    "for attack_type in np.unique(types_sample):\n",
    "    mask = types_sample == attack_type\n",
    "    ax1.scatter(X_sample_pca[mask, 0], X_sample_pca[mask, 1],\n",
    "                label=attack_type, alpha=0.6, s=30)\n",
    "ax1.set_xlabel('PC1')\n",
    "ax1.set_ylabel('PC2')\n",
    "ax1.set_title('Истинные классы')\n",
    "ax1.legend()\n",
    "\n",
    "# Справа - кластеры Spectral\n",
    "ax2 = axes[1]\n",
    "labels = best_result['labels']\n",
    "scatter = ax2.scatter(X_sample_pca[:, 0], X_sample_pca[:, 1],\n",
    "                      c=labels, cmap='viridis', alpha=0.6, s=30)\n",
    "ax2.set_xlabel('PC1')\n",
    "ax2.set_ylabel('PC2')\n",
    "ax2.set_title(f'Spectral Clustering ({best_result[\"n_clusters\"]} кластеров)')\n",
    "plt.colorbar(scatter, ax=ax2, label='Кластер')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Анализ кластеров\n",
    "print(\"\\nСостав кластеров:\")\n",
    "for cluster_id in range(best_result['n_clusters']):\n",
    "    mask = labels == cluster_id\n",
    "    cluster_types = types_sample[mask]\n",
    "    print(f\"\\nКластер {cluster_id} ({mask.sum()} точек):\")\n",
    "    for attack_type in np.unique(cluster_types):\n",
    "        count = (cluster_types == attack_type).sum()\n",
    "        pct = count / mask.sum() * 100\n",
    "        print(f\"  {attack_type}: {count} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef63a667",
   "metadata": {},
   "source": [
    "## 3. Mean-Shift Clustering\n",
    "\n",
    "### Теория\n",
    "\n",
    "Mean-Shift - это непараметрический алгоритм кластеризации, основанный на оценке плотности. Он находит моды (максимумы) функции плотности вероятности.\n",
    "\n",
    "### Алгоритм\n",
    "\n",
    "1. **Инициализация** - каждая точка становится кандидатом в центр кластера\n",
    "2. **Сдвиг к центру масс** - для каждой точки вычисляем взвешенное среднее точек в окрестности (окне)\n",
    "3. **Итерация** - повторяем сдвиг, пока точки не сойдутся\n",
    "4. **Объединение** - близкие сошедшиеся точки объединяются в кластеры\n",
    "\n",
    "### Формула сдвига\n",
    "\n",
    "$$m(x) = \\frac{\\sum_{x_i \\in N(x)} K(x_i - x) \\cdot x_i}{\\sum_{x_i \\in N(x)} K(x_i - x)}$$\n",
    "\n",
    "где $K$ - ядерная функция (обычно Гауссова), $N(x)$ - окрестность точки $x$.\n",
    "\n",
    "### Преимущества\n",
    "\n",
    "- **Не требует числа кластеров** - определяет автоматически\n",
    "- Находит кластеры произвольной формы\n",
    "- Устойчив к выбросам\n",
    "\n",
    "### Недостатки\n",
    "\n",
    "- Требует выбора bandwidth (размер окна)\n",
    "- Может быть медленным для больших данных\n",
    "- Результат зависит от bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cd7251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean-Shift Clustering\n",
    "print(\"Применяем Mean-Shift Clustering...\")\n",
    "\n",
    "# Оценка оптимального bandwidth\n",
    "bandwidth = estimate_bandwidth(X_sample, quantile=0.2, random_state=42)\n",
    "print(f\"Оценённый bandwidth: {bandwidth:.3f}\")\n",
    "\n",
    "# Mean-Shift с разными bandwidth\n",
    "bandwidths = [bandwidth * 0.5, bandwidth, bandwidth * 1.5, bandwidth * 2]\n",
    "ms_results = []\n",
    "\n",
    "for bw in bandwidths:\n",
    "    ms = MeanShift(bandwidth=bw, bin_seeding=True)\n",
    "    ms_labels = ms.fit_predict(X_sample)\n",
    "    n_clusters = len(np.unique(ms_labels))\n",
    "\n",
    "    if n_clusters > 1:\n",
    "        silhouette = silhouette_score(X_sample, ms_labels)\n",
    "    else:\n",
    "        silhouette = -1\n",
    "\n",
    "    ms_results.append({\n",
    "        'bandwidth': bw,\n",
    "        'n_clusters': n_clusters,\n",
    "        'silhouette': silhouette,\n",
    "        'labels': ms_labels,\n",
    "        'centers': ms.cluster_centers_\n",
    "    })\n",
    "\n",
    "    print(f\"Bandwidth: {bw:.3f}, Кластеров: {n_clusters}, Silhouette: {silhouette:.3f}\")\n",
    "\n",
    "# Лучший результат (с разумным числом кластеров)\n",
    "valid_results = [r for r in ms_results if 2 <= r['n_clusters'] <= 10]\n",
    "if valid_results:\n",
    "    best_ms = max(valid_results, key=lambda x: x['silhouette'])\n",
    "else:\n",
    "    best_ms = ms_results[1]  # Используем стандартный bandwidth\n",
    "\n",
    "print(f\"\\nВыбран bandwidth: {best_ms['bandwidth']:.3f}\")\n",
    "print(f\"Число кластеров: {best_ms['n_clusters']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7200827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация Mean-Shift\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Кластеры Mean-Shift\n",
    "ax1 = axes[0]\n",
    "ms_labels = best_ms['labels']\n",
    "scatter = ax1.scatter(X_sample_pca[:, 0], X_sample_pca[:, 1],\n",
    "                      c=ms_labels, cmap='tab10', alpha=0.6, s=30)\n",
    "ax1.set_xlabel('PC1')\n",
    "ax1.set_ylabel('PC2')\n",
    "ax1.set_title(f'Mean-Shift ({best_ms[\"n_clusters\"]} кластеров)')\n",
    "plt.colorbar(scatter, ax=ax1, label='Кластер')\n",
    "\n",
    "# Сравнение с атаками\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Определяем, какой кластер содержит больше атак\n",
    "attack_rates = []\n",
    "for cluster_id in range(best_ms['n_clusters']):\n",
    "    mask = ms_labels == cluster_id\n",
    "    if mask.sum() > 0:\n",
    "        attack_rate = y_sample[mask].mean()\n",
    "        attack_rates.append((cluster_id, attack_rate, mask.sum()))\n",
    "\n",
    "# Сортируем по доле атак\n",
    "attack_rates.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Доля атак в кластерах:\")\n",
    "for cluster_id, rate, size in attack_rates:\n",
    "    print(f\"  Кластер {cluster_id}: {rate*100:.1f}% атак ({size} точек)\")\n",
    "\n",
    "# Визуализация - помечаем аномальные кластеры\n",
    "anomaly_clusters = [c[0] for c in attack_rates if c[1] > 0.5]\n",
    "is_anomaly_cluster = np.isin(ms_labels, anomaly_clusters)\n",
    "\n",
    "ax2.scatter(X_sample_pca[~is_anomaly_cluster, 0], X_sample_pca[~is_anomaly_cluster, 1],\n",
    "            c='blue', alpha=0.5, s=20, label='Нормальные кластеры')\n",
    "ax2.scatter(X_sample_pca[is_anomaly_cluster, 0], X_sample_pca[is_anomaly_cluster, 1],\n",
    "            c='red', alpha=0.5, s=20, label='Аномальные кластеры')\n",
    "\n",
    "ax2.set_xlabel('PC1')\n",
    "ax2.set_ylabel('PC2')\n",
    "ax2.set_title('Классификация кластеров по доле атак')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d605ea56",
   "metadata": {},
   "source": [
    "## 4. Bidirectional LSTM-Autoencoder\n",
    "\n",
    "### Теория\n",
    "\n",
    "Двунаправленный LSTM-Autoencoder обрабатывает последовательности в обоих направлениях, что позволяет учитывать контекст как предыдущих, так и последующих элементов.\n",
    "\n",
    "### Архитектура\n",
    "\n",
    "```\n",
    "Вход → BiLSTM(64) → BiLSTM(32) → [Bottleneck] → BiLSTM(32) → BiLSTM(64) → Dense → Выход\n",
    "```\n",
    "\n",
    "### Преимущества двунаправленности\n",
    "\n",
    "1. **Полный контекст** - видит последовательность целиком\n",
    "2. **Лучшее качество** - для задач, где важен будущий контекст\n",
    "3. **Симметричное представление** - информация с обоих концов\n",
    "\n",
    "### Применение для аномалий\n",
    "\n",
    "1. Обучаем на нормальном трафике\n",
    "2. Модель учится восстанавливать нормальные паттерны\n",
    "3. Аномалии имеют высокую ошибку реконструкции\n",
    "\n",
    "### Создание последовательностей\n",
    "\n",
    "Для LSTM нужны последовательности. Мы создадим скользящее окно по временным данным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0e059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка данных для LSTM\n",
    "# Создаём последовательности из признаков\n",
    "\n",
    "def create_sequences(data, seq_length=10):\n",
    "    \"\"\"\n",
    "    Создание последовательностей для LSTM.\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "    data : array\n",
    "        Данные (n_samples, n_features)\n",
    "    seq_length : int\n",
    "        Длина последовательности\n",
    "\n",
    "    Возвращает:\n",
    "    -----------\n",
    "    sequences : array (n_sequences, seq_length, n_features)\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    for i in range(len(data) - seq_length + 1):\n",
    "        sequences.append(data[i:i+seq_length])\n",
    "    return np.array(sequences)\n",
    "\n",
    "# Используем MinMaxScaler для LSTM (лучше работает с sigmoid/tanh)\n",
    "mm_scaler = MinMaxScaler()\n",
    "X_mm = mm_scaler.fit_transform(X)\n",
    "\n",
    "# Разделяем на нормальные и аномальные\n",
    "normal_mask = y == 0\n",
    "X_normal = X_mm[normal_mask]\n",
    "X_attack = X_mm[~normal_mask]\n",
    "\n",
    "print(f\"Нормальный трафик: {len(X_normal)}\")\n",
    "print(f\"Атаки: {len(X_attack)}\")\n",
    "\n",
    "# Создаём последовательности\n",
    "SEQ_LENGTH = 10\n",
    "\n",
    "# Для обучения используем только нормальный трафик\n",
    "X_normal_seq = create_sequences(X_normal, SEQ_LENGTH)\n",
    "\n",
    "# Для тестирования - всё\n",
    "X_all_seq = create_sequences(X_mm, SEQ_LENGTH)\n",
    "y_seq = y[SEQ_LENGTH-1:]  # Метки для последовательностей\n",
    "\n",
    "print(f\"\\nПоследовательности нормального трафика: {X_normal_seq.shape}\")\n",
    "print(f\"Все последовательности: {X_all_seq.shape}\")\n",
    "print(f\"Метки: {len(y_seq)}\")\n",
    "\n",
    "# Разделение на train/test\n",
    "train_size = int(len(X_normal_seq) * 0.8)\n",
    "X_train = X_normal_seq[:train_size]\n",
    "X_val = X_normal_seq[train_size:]\n",
    "\n",
    "print(f\"\\nОбучающая выборка: {X_train.shape}\")\n",
    "print(f\"Валидационная выборка: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca542fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bidirectional_lstm_ae(seq_length, n_features):\n",
    "    \"\"\"\n",
    "    Построение двунаправленного LSTM-Autoencoder.\n",
    "\n",
    "    Архитектура:\n",
    "    - Encoder: BiLSTM(64) → BiLSTM(32)\n",
    "    - Decoder: BiLSTM(32) → BiLSTM(64) → TimeDistributed(Dense)\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=(seq_length, n_features))\n",
    "\n",
    "    # Encoder\n",
    "    # BiLSTM возвращает удвоенную размерность (forward + backward)\n",
    "    encoded = Bidirectional(LSTM(64, activation='relu', return_sequences=True))(inputs)\n",
    "    encoded = Dropout(0.2)(encoded)\n",
    "    encoded = Bidirectional(LSTM(32, activation='relu', return_sequences=False))(encoded)\n",
    "\n",
    "    # Bottleneck\n",
    "    bottleneck = RepeatVector(seq_length)(encoded)\n",
    "\n",
    "    # Decoder\n",
    "    decoded = Bidirectional(LSTM(32, activation='relu', return_sequences=True))(bottleneck)\n",
    "    decoded = Dropout(0.2)(decoded)\n",
    "    decoded = Bidirectional(LSTM(64, activation='relu', return_sequences=True))(decoded)\n",
    "\n",
    "    # Output\n",
    "    outputs = TimeDistributed(Dense(n_features))(decoded)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Создаём модель\n",
    "n_features = X_train.shape[2]\n",
    "model = build_bidirectional_lstm_ae(SEQ_LENGTH, n_features)\n",
    "\n",
    "print(\"Архитектура Bidirectional LSTM-Autoencoder:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08dcea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение модели\n",
    "print(\"Обучение Bidirectional LSTM-Autoencoder...\")\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, X_train,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val, X_val),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# График обучения\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(history.history['loss'], label='Обучение', linewidth=2)\n",
    "ax.plot(history.history['val_loss'], label='Валидация', linewidth=2)\n",
    "ax.set_xlabel('Эпоха')\n",
    "ax.set_ylabel('MSE Loss')\n",
    "ax.set_title('История обучения Bidirectional LSTM-Autoencoder')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nФинальный loss на обучении: {history.history['loss'][-1]:.6f}\")\n",
    "print(f\"Финальный loss на валидации: {history.history['val_loss'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025377f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычисление ошибки реконструкции\n",
    "print(\"Вычисление ошибок реконструкции...\")\n",
    "\n",
    "# Предсказания\n",
    "train_pred = model.predict(X_train, verbose=0)\n",
    "all_pred = model.predict(X_all_seq, verbose=0)\n",
    "\n",
    "# MSE для каждой последовательности\n",
    "train_mse = np.mean(np.square(X_train - train_pred), axis=(1, 2))\n",
    "all_mse = np.mean(np.square(X_all_seq - all_pred), axis=(1, 2))\n",
    "\n",
    "# Статистика ошибок на обучающей выборке\n",
    "train_mean = np.mean(train_mse)\n",
    "train_std = np.std(train_mse)\n",
    "\n",
    "print(f\"Ошибка на обучении - Среднее: {train_mean:.6f}, Std: {train_std:.6f}\")\n",
    "\n",
    "# Пороги\n",
    "thresholds = {\n",
    "    '2σ': train_mean + 2 * train_std,\n",
    "    '3σ': train_mean + 3 * train_std,\n",
    "    '95%': np.percentile(train_mse, 95),\n",
    "    '99%': np.percentile(train_mse, 99)\n",
    "}\n",
    "\n",
    "print(\"\\nПороги обнаружения:\")\n",
    "for name, thresh in thresholds.items():\n",
    "    print(f\"  {name}: {thresh:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f183faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка качества обнаружения\n",
    "print(\"Оценка качества обнаружения аномалий:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, threshold in thresholds.items():\n",
    "    predictions = (all_mse > threshold).astype(int)\n",
    "\n",
    "    # Метрики\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "    precision = precision_score(y_seq, predictions, zero_division=0)\n",
    "    recall = recall_score(y_seq, predictions, zero_division=0)\n",
    "    f1 = f1_score(y_seq, predictions, zero_division=0)\n",
    "\n",
    "    results.append({\n",
    "        'threshold': name,\n",
    "        'value': threshold,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    })\n",
    "\n",
    "    print(f\"\\nПорог {name} ({threshold:.4f}):\")\n",
    "    print(f\"  Precision: {precision:.3f}\")\n",
    "    print(f\"  Recall: {recall:.3f}\")\n",
    "    print(f\"  F1-Score: {f1:.3f}\")\n",
    "\n",
    "# Лучший результат по F1\n",
    "best_result = max(results, key=lambda x: x['f1'])\n",
    "best_threshold = best_result['value']\n",
    "\n",
    "print(f\"\\nЛучший порог: {best_result['threshold']} (F1 = {best_result['f1']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c3f390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC кривая и визуализация\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# 1. Распределение ошибок\n",
    "ax1 = axes[0, 0]\n",
    "normal_mse = all_mse[y_seq == 0]\n",
    "attack_mse = all_mse[y_seq == 1]\n",
    "\n",
    "ax1.hist(normal_mse, bins=50, alpha=0.6, label='Нормальный', density=True)\n",
    "ax1.hist(attack_mse, bins=50, alpha=0.6, label='Атака', density=True)\n",
    "ax1.axvline(x=best_threshold, color='red', linestyle='--',\n",
    "            label=f'Порог={best_threshold:.4f}')\n",
    "ax1.set_xlabel('Ошибка реконструкции (MSE)')\n",
    "ax1.set_ylabel('Плотность')\n",
    "ax1.set_title('Распределение ошибок реконструкции')\n",
    "ax1.legend()\n",
    "\n",
    "# 2. ROC кривая\n",
    "ax2 = axes[0, 1]\n",
    "from sklearn.metrics import roc_curve, auc as auc_score\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_seq, all_mse)\n",
    "roc_auc = auc_score(fpr, tpr)\n",
    "\n",
    "ax2.plot(fpr, tpr, color='darkorange', lw=2,\n",
    "         label=f'ROC (AUC = {roc_auc:.3f})')\n",
    "ax2.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "ax2.set_xlim([0.0, 1.0])\n",
    "ax2.set_ylim([0.0, 1.05])\n",
    "ax2.set_xlabel('False Positive Rate')\n",
    "ax2.set_ylabel('True Positive Rate')\n",
    "ax2.set_title('ROC кривая BiLSTM-Autoencoder')\n",
    "ax2.legend(loc='lower right')\n",
    "\n",
    "# 3. Precision-Recall кривая\n",
    "ax3 = axes[1, 0]\n",
    "precision_curve, recall_curve, _ = precision_recall_curve(y_seq, all_mse)\n",
    "pr_auc = auc(recall_curve, precision_curve)\n",
    "\n",
    "ax3.plot(recall_curve, precision_curve, color='green', lw=2,\n",
    "         label=f'PR (AUC = {pr_auc:.3f})')\n",
    "ax3.set_xlabel('Recall')\n",
    "ax3.set_ylabel('Precision')\n",
    "ax3.set_title('Precision-Recall кривая')\n",
    "ax3.legend()\n",
    "\n",
    "# 4. Временной ряд ошибок\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(all_mse, alpha=0.7, linewidth=0.5)\n",
    "ax4.axhline(y=best_threshold, color='red', linestyle='--',\n",
    "            label=f'Порог')\n",
    "\n",
    "# Отмечаем атаки\n",
    "attack_idx = np.where(y_seq == 1)[0]\n",
    "ax4.scatter(attack_idx, all_mse[attack_idx], c='red', s=10, alpha=0.3, label='Атаки')\n",
    "\n",
    "ax4.set_xlabel('Индекс последовательности')\n",
    "ax4.set_ylabel('Ошибка реконструкции')\n",
    "ax4.set_title('Временной ряд ошибок реконструкции')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nROC-AUC: {roc_auc:.3f}\")\n",
    "print(f\"PR-AUC: {pr_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28892a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix для лучшего порога\n",
    "predictions = (all_mse > best_threshold).astype(int)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_seq, predictions)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "            xticklabels=['Нормальный', 'Атака'],\n",
    "            yticklabels=['Нормальный', 'Атака'])\n",
    "ax.set_xlabel('Предсказано')\n",
    "ax.set_ylabel('Истинное')\n",
    "ax.set_title(f'Матрица ошибок (порог {best_result[\"threshold\"]})')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nОтчёт по классификации:\")\n",
    "print(classification_report(y_seq, predictions,\n",
    "                           target_names=['Нормальный', 'Атака']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f88825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Анализ обнаружения по типам атак\n",
    "attack_types_seq = attack_types[SEQ_LENGTH-1:]\n",
    "\n",
    "print(\"Обнаружение по типам атак:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for attack_type in np.unique(attack_types_seq):\n",
    "    mask = attack_types_seq == attack_type\n",
    "    type_mse = all_mse[mask]\n",
    "    type_labels = y_seq[mask]\n",
    "    type_preds = predictions[mask]\n",
    "\n",
    "    if attack_type == 'Normal':\n",
    "        # Для нормального трафика считаем FPR\n",
    "        fpr = type_preds.mean()\n",
    "        print(f\"\\n{attack_type}:\")\n",
    "        print(f\"  Ложных срабатываний: {fpr*100:.1f}%\")\n",
    "        print(f\"  Средняя ошибка: {type_mse.mean():.4f}\")\n",
    "    else:\n",
    "        # Для атак считаем Recall\n",
    "        recall = type_preds.mean()\n",
    "        print(f\"\\n{attack_type}:\")\n",
    "        print(f\"  Обнаружено: {recall*100:.1f}%\")\n",
    "        print(f\"  Средняя ошибка: {type_mse.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af63759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сравнение всех методов\n",
    "print(\"=\" * 60)\n",
    "print(\"СРАВНЕНИЕ МЕТОДОВ ОБНАРУЖЕНИЯ СЕТЕВЫХ АТАК\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. Spectral Clustering\")\n",
    "print(f\"   Кластеров: {best_result['n_clusters']}\")\n",
    "print(f\"   Silhouette: {best_result['silhouette']:.3f}\")\n",
    "print(\"   + Хорошо разделяет нелинейные структуры\")\n",
    "print(\"   - Требует задания числа кластеров\")\n",
    "print(\"   - Вычислительно затратен\")\n",
    "\n",
    "print(\"\\n2. Mean-Shift\")\n",
    "print(f\"   Кластеров: {best_ms['n_clusters']}\")\n",
    "print(f\"   Silhouette: {best_ms['silhouette']:.3f}\")\n",
    "print(\"   + Автоматически определяет число кластеров\")\n",
    "print(\"   + Устойчив к выбросам\")\n",
    "print(\"   - Чувствителен к bandwidth\")\n",
    "\n",
    "print(\"\\n3. Bidirectional LSTM-Autoencoder\")\n",
    "print(f\"   ROC-AUC: {roc_auc:.3f}\")\n",
    "print(f\"   PR-AUC: {pr_auc:.3f}\")\n",
    "print(f\"   F1-Score: {best_result['f1']:.3f}\")\n",
    "print(\"   + Учитывает временные зависимости\")\n",
    "print(\"   + Двунаправленный контекст\")\n",
    "print(\"   - Требует настройки порога\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"РЕКОМЕНДАЦИИ ПО ПРИМЕНЕНИЮ\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n• Spectral Clustering: анализ структуры трафика, визуализация\")\n",
    "print(\"• Mean-Shift: первичная сегментация без априорных знаний\")\n",
    "print(\"• BiLSTM-AE: обнаружение аномалий в реальном времени\")\n",
    "print(\"\\nОптимально: комбинация методов для повышения надёжности\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9775fa3",
   "metadata": {},
   "source": [
    "## Заключение\n",
    "\n",
    "### Ключевые результаты\n",
    "\n",
    "1. **Spectral Clustering** успешно выделил группы с разными паттернами трафика, используя спектральное разложение матрицы сходства.\n",
    "\n",
    "2. **Mean-Shift** автоматически нашёл естественные кластеры в данных без задания их числа, что полезно для разведочного анализа.\n",
    "\n",
    "3. **Bidirectional LSTM-Autoencoder** показал высокое качество обнаружения атак (ROC-AUC ≈ 0.9+), учитывая временные зависимости в обоих направлениях.\n",
    "\n",
    "### Особенности датасета\n",
    "\n",
    "- 15,000 записей сетевого трафика\n",
    "- 4 класса: Normal, DDoS, PortScan, BruteForce\n",
    "- 10 информативных признаков\n",
    "- Реалистичные паттерны для каждого типа атак\n",
    "\n",
    "### Практическое применение\n",
    "\n",
    "1. **Мониторинг сети** - BiLSTM-AE для обнаружения в реальном времени\n",
    "2. **Анализ инцидентов** - кластеризация для группировки похожих атак\n",
    "3. **Профилирование** - Mean-Shift для выявления типичных паттернов\n",
    "\n",
    "### Дальнейшие улучшения\n",
    "\n",
    "- Добавление Attention механизма в LSTM\n",
    "- Ансамбль нескольких автоэнкодеров\n",
    "- Онлайн-обучение для адаптации к новым атакам\n",
    "- Интеграция с SIEM системами"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

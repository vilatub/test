{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "662cee9c",
   "metadata": {},
   "source": [
    "# Phase 7.2: Advanced MLOps Practices\n",
    "\n",
    "## Production-Ready Machine Learning Infrastructure\n",
    "\n",
    "This notebook covers advanced MLOps techniques:\n",
    "\n",
    "1. **sklearn Pipeline with ColumnTransformer** - End-to-end reproducible ML workflows\n",
    "2. **ONNX Export** - Cross-platform model deployment\n",
    "3. **A/B Testing Framework** - Statistical testing for model comparison\n",
    "4. **Model Compression** - Quantization and pruning for efficient inference\n",
    "\n",
    "### Focus\n",
    "Building production-ready pipelines that are reproducible, portable, and efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2138f967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# sklearn Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "\n",
    "# For ONNX\n",
    "import onnx\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "import onnxruntime as ort\n",
    "\n",
    "# TensorFlow for compression demo\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"Libraries loaded successfully\")\n",
    "print(f\"ONNX version: {onnx.__version__}\")\n",
    "print(f\"ONNX Runtime version: {ort.__version__}\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8054f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_customer_churn_data(n_samples=10000):\n",
    "    \"\"\"\n",
    "    Create synthetic customer churn dataset with mixed feature types.\n",
    "    Ideal for demonstrating sklearn Pipelines.\n",
    "    \"\"\"\n",
    "    # Numeric features\n",
    "    data = {\n",
    "        'tenure_months': np.random.randint(1, 72, n_samples),\n",
    "        'monthly_charges': np.random.uniform(20, 100, n_samples),\n",
    "        'total_charges': np.zeros(n_samples),  # Will calculate\n",
    "        'num_support_tickets': np.random.poisson(2, n_samples),\n",
    "        'num_referrals': np.random.poisson(1, n_samples),\n",
    "        'avg_monthly_usage_gb': np.random.lognormal(2, 1, n_samples),\n",
    "    }\n",
    "\n",
    "    # Calculate total charges\n",
    "    data['total_charges'] = data['tenure_months'] * data['monthly_charges'] * \\\n",
    "                            np.random.uniform(0.9, 1.1, n_samples)\n",
    "\n",
    "    # Categorical features\n",
    "    data['contract_type'] = np.random.choice(\n",
    "        ['Month-to-month', 'One year', 'Two year'],\n",
    "        n_samples, p=[0.5, 0.3, 0.2]\n",
    "    )\n",
    "    data['payment_method'] = np.random.choice(\n",
    "        ['Electronic check', 'Mailed check', 'Bank transfer', 'Credit card'],\n",
    "        n_samples\n",
    "    )\n",
    "    data['internet_service'] = np.random.choice(\n",
    "        ['DSL', 'Fiber optic', 'No'],\n",
    "        n_samples, p=[0.35, 0.45, 0.2]\n",
    "    )\n",
    "\n",
    "    # Add some missing values (realistic)\n",
    "    for col in ['monthly_charges', 'num_support_tickets']:\n",
    "        mask = np.random.random(n_samples) < 0.05\n",
    "        data[col] = np.where(mask, np.nan, data[col])\n",
    "\n",
    "    # Generate churn labels based on features\n",
    "    churn_prob = 0.1 + \\\n",
    "        0.3 * (data['contract_type'] == 'Month-to-month') + \\\n",
    "        0.1 * (data['tenure_months'] < 12) / 12 + \\\n",
    "        0.15 * (data['num_support_tickets'] > 3) + \\\n",
    "        0.1 * (np.array(data['monthly_charges']) > 70)\n",
    "\n",
    "    # Handle NaN in probability calculation\n",
    "    churn_prob = np.nan_to_num(churn_prob, nan=0.2)\n",
    "    churn_prob = np.clip(churn_prob, 0, 1)\n",
    "\n",
    "    data['churned'] = np.random.binomial(1, churn_prob)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Generate data\n",
    "df = create_customer_churn_data(n_samples=10000)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Churn rate: {df['churned'].mean()*100:.1f}%\")\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "print(f\"\\nSample data:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957915f9",
   "metadata": {},
   "source": [
    "## 1. sklearn Pipeline with ColumnTransformer\n",
    "\n",
    "### Why use Pipelines?\n",
    "\n",
    "1. **Reproducibility** - All preprocessing steps are bundled together\n",
    "2. **No data leakage** - Transformations fit only on training data\n",
    "3. **Easy deployment** - Single object to serialize and deploy\n",
    "4. **Cross-validation** - Preprocessing inside CV loop\n",
    "\n",
    "### ColumnTransformer\n",
    "Applies different transformations to different column types (numeric vs categorical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31f5949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature groups\n",
    "numeric_features = ['tenure_months', 'monthly_charges', 'total_charges',\n",
    "                    'num_support_tickets', 'num_referrals', 'avg_monthly_usage_gb']\n",
    "categorical_features = ['contract_type', 'payment_method', 'internet_service']\n",
    "\n",
    "# Numeric transformer: impute then scale\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical transformer: impute then one-hot encode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combine with ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Full pipeline with classifier\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "print(\"Pipeline structure:\")\n",
    "print(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbae38d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = df.drop('churned', axis=1)\n",
    "y = df['churned'].values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Pipeline Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.3f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring='roc_auc')\n",
    "print(f\"\\nCross-validation ROC-AUC: {cv_scores.mean():.3f} (+/- {cv_scores.std()*2:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6673b32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names after transformation\n",
    "def get_feature_names(column_transformer):\n",
    "    \"\"\"Get feature names from ColumnTransformer.\"\"\"\n",
    "    output_features = []\n",
    "\n",
    "    for name, pipe, columns in column_transformer.transformers_:\n",
    "        if name == 'remainder':\n",
    "            continue\n",
    "\n",
    "        if hasattr(pipe, 'get_feature_names_out'):\n",
    "            names = pipe.get_feature_names_out(columns)\n",
    "        elif hasattr(pipe[-1], 'get_feature_names_out'):\n",
    "            names = pipe[-1].get_feature_names_out(columns)\n",
    "        else:\n",
    "            names = columns\n",
    "\n",
    "        output_features.extend(names)\n",
    "\n",
    "    return output_features\n",
    "\n",
    "feature_names = get_feature_names(preprocessor)\n",
    "print(f\"Total features after transformation: {len(feature_names)}\")\n",
    "print(f\"\\nFeature names: {feature_names}\")\n",
    "\n",
    "# Feature importance\n",
    "rf_model = pipeline.named_steps['classifier']\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Plot top features\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "top_n = 15\n",
    "ax.barh(importance_df['feature'][:top_n][::-1],\n",
    "        importance_df['importance'][:top_n][::-1])\n",
    "ax.set_xlabel('Feature Importance')\n",
    "ax.set_title('Top 15 Features in Pipeline Model')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ec075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the complete pipeline\n",
    "import os\n",
    "\n",
    "# Create artifacts directory\n",
    "os.makedirs('/home/user/test/notebooks/phase7_production_mlops/artifacts', exist_ok=True)\n",
    "\n",
    "# Save with pickle\n",
    "pipeline_path = '/home/user/test/notebooks/phase7_production_mlops/artifacts/churn_pipeline.pkl'\n",
    "with open(pipeline_path, 'wb') as f:\n",
    "    pickle.dump(pipeline, f)\n",
    "\n",
    "print(f\"Pipeline saved to: {pipeline_path}\")\n",
    "\n",
    "# Load and verify\n",
    "with open(pipeline_path, 'rb') as f:\n",
    "    loaded_pipeline = pickle.load(f)\n",
    "\n",
    "# Test loaded pipeline\n",
    "test_pred = loaded_pipeline.predict(X_test[:5])\n",
    "print(f\"\\nLoaded pipeline predictions: {test_pred}\")\n",
    "print(\"Pipeline save/load successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6757f5",
   "metadata": {},
   "source": [
    "## 2. ONNX Export\n",
    "\n",
    "### What is ONNX?\n",
    "\n",
    "**Open Neural Network Exchange** - an open format for ML models that enables:\n",
    "- Cross-platform deployment (Python, C++, JavaScript, etc.)\n",
    "- Hardware optimization (CPU, GPU, mobile)\n",
    "- Model interoperability between frameworks\n",
    "\n",
    "### Benefits:\n",
    "- Faster inference with ONNX Runtime\n",
    "- Deploy anywhere (cloud, edge, browser)\n",
    "- Single model format for all platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752fcf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ONNX export, we need to first transform the data\n",
    "# and then export just the classifier\n",
    "\n",
    "# Transform training data\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Train a simple model for ONNX export\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Convert to ONNX\n",
    "initial_type = [('float_input', FloatTensorType([None, X_train_transformed.shape[1]]))]\n",
    "\n",
    "onnx_model = convert_sklearn(\n",
    "    lr_model,\n",
    "    initial_types=initial_type,\n",
    "    target_opset=12\n",
    ")\n",
    "\n",
    "# Save ONNX model\n",
    "onnx_path = '/home/user/test/notebooks/phase7_production_mlops/artifacts/churn_model.onnx'\n",
    "with open(onnx_path, 'wb') as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "\n",
    "print(f\"ONNX model saved to: {onnx_path}\")\n",
    "print(f\"Model size: {os.path.getsize(onnx_path) / 1024:.1f} KB\")\n",
    "\n",
    "# Validate ONNX model\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"ONNX model validation passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2ff4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference with ONNX Runtime\n",
    "session = ort.InferenceSession(onnx_path)\n",
    "\n",
    "# Get input/output names\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_names = [o.name for o in session.get_outputs()]\n",
    "\n",
    "print(f\"Input name: {input_name}\")\n",
    "print(f\"Output names: {output_names}\")\n",
    "\n",
    "# Run inference\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "X_test_onnx = X_test_transformed.astype(np.float32)\n",
    "\n",
    "# Time comparison\n",
    "import time\n",
    "\n",
    "# sklearn inference\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    sklearn_pred = lr_model.predict_proba(X_test_transformed)\n",
    "sklearn_time = (time.time() - start) / 100 * 1000\n",
    "\n",
    "# ONNX Runtime inference\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    onnx_pred = session.run(output_names, {input_name: X_test_onnx})\n",
    "onnx_time = (time.time() - start) / 100 * 1000\n",
    "\n",
    "print(f\"\\nInference time comparison (per batch):\")\n",
    "print(f\"  sklearn: {sklearn_time:.2f} ms\")\n",
    "print(f\"  ONNX Runtime: {onnx_time:.2f} ms\")\n",
    "print(f\"  Speedup: {sklearn_time/onnx_time:.1f}x\")\n",
    "\n",
    "# Verify predictions match\n",
    "sklearn_proba = lr_model.predict_proba(X_test_transformed)[:, 1]\n",
    "onnx_proba = onnx_pred[1][:, 1]\n",
    "\n",
    "print(f\"\\nPrediction difference: {np.abs(sklearn_proba - onnx_proba).max():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd3033d",
   "metadata": {},
   "source": [
    "## 3. A/B Testing Framework for Models\n",
    "\n",
    "### Why A/B Test Models?\n",
    "\n",
    "1. **Validate improvements** - Statistical confidence that new model is better\n",
    "2. **Risk mitigation** - Gradual rollout with monitoring\n",
    "3. **Business metrics** - Correlate model metrics with business outcomes\n",
    "\n",
    "### Statistical Framework:\n",
    "- Null hypothesis: Models A and B perform equally\n",
    "- Alternative: Model B performs differently (better or worse)\n",
    "- Use appropriate statistical tests for model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a0b84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABTestFramework:\n",
    "    \"\"\"\n",
    "    Framework for A/B testing machine learning models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_a, model_b, name_a='Model A', name_b='Model B'):\n",
    "        self.model_a = model_a\n",
    "        self.model_b = model_b\n",
    "        self.name_a = name_a\n",
    "        self.name_b = name_b\n",
    "        self.results = []\n",
    "\n",
    "    def run_experiment(self, X, y, n_splits=10, metrics=['accuracy', 'roc_auc']):\n",
    "        \"\"\"\n",
    "        Run A/B experiment with cross-validation.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : features\n",
    "        y : target\n",
    "        n_splits : number of CV splits\n",
    "        metrics : list of metrics to compare\n",
    "        \"\"\"\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "        results_a = {m: [] for m in metrics}\n",
    "        results_b = {m: [] for m in metrics}\n",
    "\n",
    "        for train_idx, test_idx in skf.split(X, y):\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "            # Model A\n",
    "            self.model_a.fit(X_train, y_train)\n",
    "            pred_a = self.model_a.predict(X_test)\n",
    "            proba_a = self.model_a.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            # Model B\n",
    "            self.model_b.fit(X_train, y_train)\n",
    "            pred_b = self.model_b.predict(X_test)\n",
    "            proba_b = self.model_b.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            # Calculate metrics\n",
    "            for metric in metrics:\n",
    "                if metric == 'accuracy':\n",
    "                    results_a[metric].append(accuracy_score(y_test, pred_a))\n",
    "                    results_b[metric].append(accuracy_score(y_test, pred_b))\n",
    "                elif metric == 'roc_auc':\n",
    "                    results_a[metric].append(roc_auc_score(y_test, proba_a))\n",
    "                    results_b[metric].append(roc_auc_score(y_test, proba_b))\n",
    "\n",
    "        self.results_a = results_a\n",
    "        self.results_b = results_b\n",
    "\n",
    "        return self\n",
    "\n",
    "    def statistical_test(self, metric='roc_auc', alpha=0.05):\n",
    "        \"\"\"\n",
    "        Perform statistical test to compare models.\n",
    "\n",
    "        Uses paired t-test since we have matched samples from CV.\n",
    "        \"\"\"\n",
    "        scores_a = np.array(self.results_a[metric])\n",
    "        scores_b = np.array(self.results_b[metric])\n",
    "\n",
    "        # Paired t-test\n",
    "        t_stat, p_value = stats.ttest_rel(scores_a, scores_b)\n",
    "\n",
    "        # Effect size (Cohen's d)\n",
    "        diff = scores_b - scores_a\n",
    "        effect_size = diff.mean() / diff.std()\n",
    "\n",
    "        # Results\n",
    "        results = {\n",
    "            'metric': metric,\n",
    "            'mean_a': scores_a.mean(),\n",
    "            'std_a': scores_a.std(),\n",
    "            'mean_b': scores_b.mean(),\n",
    "            'std_b': scores_b.std(),\n",
    "            't_statistic': t_stat,\n",
    "            'p_value': p_value,\n",
    "            'effect_size': effect_size,\n",
    "            'significant': p_value < alpha,\n",
    "            'winner': self.name_b if (p_value < alpha and scores_b.mean() > scores_a.mean()) else \\\n",
    "                      self.name_a if (p_value < alpha and scores_a.mean() > scores_b.mean()) else 'No significant difference'\n",
    "        }\n",
    "\n",
    "        return results\n",
    "\n",
    "    def plot_results(self, metric='roc_auc'):\n",
    "        \"\"\"Visualize A/B test results.\"\"\"\n",
    "        scores_a = self.results_a[metric]\n",
    "        scores_b = self.results_b[metric]\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "        # Box plot comparison\n",
    "        ax1 = axes[0]\n",
    "        ax1.boxplot([scores_a, scores_b], labels=[self.name_a, self.name_b])\n",
    "        ax1.set_ylabel(metric.upper())\n",
    "        ax1.set_title(f'{metric.upper()} Distribution')\n",
    "\n",
    "        # Paired differences\n",
    "        ax2 = axes[1]\n",
    "        diff = np.array(scores_b) - np.array(scores_a)\n",
    "        ax2.hist(diff, bins=10, edgecolor='black', alpha=0.7)\n",
    "        ax2.axvline(x=0, color='red', linestyle='--', label='No difference')\n",
    "        ax2.axvline(x=diff.mean(), color='green', linestyle='-', label=f'Mean diff: {diff.mean():.4f}')\n",
    "        ax2.set_xlabel(f'{self.name_b} - {self.name_a}')\n",
    "        ax2.set_ylabel('Count')\n",
    "        ax2.set_title('Paired Differences')\n",
    "        ax2.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"A/B Testing Framework defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c63a33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Random Forest vs Gradient Boosting\n",
    "\n",
    "# Create pipelines for both models\n",
    "pipeline_rf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "pipeline_gb = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Run A/B test\n",
    "ab_test = ABTestFramework(\n",
    "    pipeline_rf, pipeline_gb,\n",
    "    name_a='Random Forest',\n",
    "    name_b='Gradient Boosting'\n",
    ")\n",
    "\n",
    "ab_test.run_experiment(X, y, n_splits=10, metrics=['accuracy', 'roc_auc'])\n",
    "\n",
    "# Statistical test\n",
    "results = ab_test.statistical_test(metric='roc_auc', alpha=0.05)\n",
    "\n",
    "print(\"A/B Test Results (ROC-AUC):\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n{ab_test.name_a}:\")\n",
    "print(f\"  Mean: {results['mean_a']:.4f} (+/- {results['std_a']:.4f})\")\n",
    "print(f\"\\n{ab_test.name_b}:\")\n",
    "print(f\"  Mean: {results['mean_b']:.4f} (+/- {results['std_b']:.4f})\")\n",
    "print(f\"\\nStatistical Test:\")\n",
    "print(f\"  t-statistic: {results['t_statistic']:.3f}\")\n",
    "print(f\"  p-value: {results['p_value']:.4f}\")\n",
    "print(f\"  Effect size: {results['effect_size']:.3f}\")\n",
    "print(f\"\\nConclusion: {results['winner']}\")\n",
    "\n",
    "# Visualize\n",
    "ab_test.plot_results(metric='roc_auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a95b4b",
   "metadata": {},
   "source": [
    "## 4. Model Compression\n",
    "\n",
    "### Why Compress Models?\n",
    "\n",
    "1. **Faster inference** - Reduced computation\n",
    "2. **Smaller size** - Better for edge/mobile deployment\n",
    "3. **Lower cost** - Less compute resources needed\n",
    "\n",
    "### Techniques:\n",
    "- **Quantization** - Reduce precision (FP32 â†’ INT8)\n",
    "- **Pruning** - Remove unimportant weights\n",
    "- **Knowledge distillation** - Train smaller model to mimic larger one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67b6b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a neural network to demonstrate compression\n",
    "def build_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_dim,)),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Prepare data\n",
    "X_nn = preprocessor.fit_transform(X)\n",
    "X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(\n",
    "    X_nn, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train original model\n",
    "original_model = build_model(X_train_nn.shape[1])\n",
    "\n",
    "original_model.fit(\n",
    "    X_train_nn, y_train_nn,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Evaluate original\n",
    "original_pred = (original_model.predict(X_test_nn, verbose=0) > 0.5).astype(int).flatten()\n",
    "original_acc = accuracy_score(y_test_nn, original_pred)\n",
    "\n",
    "print(f\"Original model accuracy: {original_acc:.4f}\")\n",
    "\n",
    "# Save original model\n",
    "original_path = '/home/user/test/notebooks/phase7_production_mlops/artifacts/original_model.h5'\n",
    "original_model.save(original_path)\n",
    "original_size = os.path.getsize(original_path)\n",
    "print(f\"Original model size: {original_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ad16b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-training quantization with TensorFlow Lite\n",
    "\n",
    "# Convert to TFLite with quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(original_model)\n",
    "\n",
    "# Dynamic range quantization (INT8)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Representative dataset for full integer quantization\n",
    "def representative_dataset():\n",
    "    for i in range(100):\n",
    "        yield [X_train_nn[i:i+1].astype(np.float32)]\n",
    "\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "try:\n",
    "    quantized_model = converter.convert()\n",
    "\n",
    "    # Save quantized model\n",
    "    quantized_path = '/home/user/test/notebooks/phase7_production_mlops/artifacts/quantized_model.tflite'\n",
    "    with open(quantized_path, 'wb') as f:\n",
    "        f.write(quantized_model)\n",
    "\n",
    "    quantized_size = os.path.getsize(quantized_path)\n",
    "    compression_ratio = original_size / quantized_size\n",
    "\n",
    "    print(f\"Quantized model size: {quantized_size / 1024:.1f} KB\")\n",
    "    print(f\"Compression ratio: {compression_ratio:.1f}x\")\n",
    "\n",
    "    # Run inference with TFLite\n",
    "    interpreter = tf.lite.Interpreter(model_path=quantized_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    print(f\"\\nTFLite model details:\")\n",
    "    print(f\"  Input shape: {input_details[0]['shape']}\")\n",
    "    print(f\"  Input dtype: {input_details[0]['dtype']}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Full integer quantization failed: {e}\")\n",
    "    print(\"\\nFalling back to dynamic range quantization...\")\n",
    "\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(original_model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    quantized_model = converter.convert()\n",
    "\n",
    "    quantized_path = '/home/user/test/notebooks/phase7_production_mlops/artifacts/quantized_model.tflite'\n",
    "    with open(quantized_path, 'wb') as f:\n",
    "        f.write(quantized_model)\n",
    "\n",
    "    quantized_size = os.path.getsize(quantized_path)\n",
    "    compression_ratio = original_size / quantized_size\n",
    "\n",
    "    print(f\"Quantized model size: {quantized_size / 1024:.1f} KB\")\n",
    "    print(f\"Compression ratio: {compression_ratio:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641f6fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model pruning demonstration\n",
    "# We'll use magnitude-based pruning to remove small weights\n",
    "\n",
    "def prune_model(model, sparsity=0.5):\n",
    "    \"\"\"\n",
    "    Apply magnitude-based pruning to model weights.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : Keras model\n",
    "    sparsity : fraction of weights to prune (set to 0)\n",
    "    \"\"\"\n",
    "    pruned_model = tf.keras.models.clone_model(model)\n",
    "    pruned_model.set_weights(model.get_weights())\n",
    "\n",
    "    total_weights = 0\n",
    "    pruned_weights = 0\n",
    "\n",
    "    for layer in pruned_model.layers:\n",
    "        if hasattr(layer, 'kernel'):\n",
    "            weights = layer.kernel.numpy()\n",
    "            total_weights += weights.size\n",
    "\n",
    "            # Calculate threshold for this layer\n",
    "            threshold = np.percentile(np.abs(weights), sparsity * 100)\n",
    "\n",
    "            # Prune weights below threshold\n",
    "            mask = np.abs(weights) > threshold\n",
    "            pruned = weights * mask\n",
    "            pruned_weights += np.sum(mask == 0)\n",
    "\n",
    "            layer.kernel.assign(pruned)\n",
    "\n",
    "    actual_sparsity = pruned_weights / total_weights\n",
    "    return pruned_model, actual_sparsity\n",
    "\n",
    "# Apply pruning with different sparsity levels\n",
    "sparsity_levels = [0.3, 0.5, 0.7, 0.9]\n",
    "results = []\n",
    "\n",
    "for sparsity in sparsity_levels:\n",
    "    pruned, actual_sparsity = prune_model(original_model, sparsity)\n",
    "\n",
    "    # Evaluate pruned model\n",
    "    pruned_pred = (pruned.predict(X_test_nn, verbose=0) > 0.5).astype(int).flatten()\n",
    "    pruned_acc = accuracy_score(y_test_nn, pruned_pred)\n",
    "\n",
    "    results.append({\n",
    "        'target_sparsity': sparsity,\n",
    "        'actual_sparsity': actual_sparsity,\n",
    "        'accuracy': pruned_acc\n",
    "    })\n",
    "\n",
    "    print(f\"Sparsity {sparsity*100:.0f}%: Accuracy = {pruned_acc:.4f}\")\n",
    "\n",
    "# Visualize accuracy vs sparsity\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "sparsities = [r['actual_sparsity'] * 100 for r in results]\n",
    "accuracies = [r['accuracy'] for r in results]\n",
    "\n",
    "ax.plot(sparsities, accuracies, 'bo-', linewidth=2, markersize=8)\n",
    "ax.axhline(y=original_acc, color='red', linestyle='--', label=f'Original: {original_acc:.4f}')\n",
    "\n",
    "ax.set_xlabel('Sparsity (%)')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Model Accuracy vs Pruning Sparsity')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOriginal accuracy: {original_acc:.4f}\")\n",
    "print(\"Pruning allows significant model compression with minimal accuracy loss!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9955cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of all techniques\n",
    "print(\"=\" * 70)\n",
    "print(\"ADVANCED MLOPS TECHNIQUES - SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n1. sklearn Pipeline with ColumnTransformer\")\n",
    "print(\"   - Combines preprocessing and model in single object\")\n",
    "print(\"   - Ensures reproducibility and prevents data leakage\")\n",
    "print(\"   - Easy to serialize and deploy\")\n",
    "\n",
    "print(\"\\n2. ONNX Export\")\n",
    "print(\"   - Cross-platform model format\")\n",
    "print(f\"   - Inference speedup: ~{sklearn_time/onnx_time:.1f}x faster\")\n",
    "print(\"   - Deploy to any platform (cloud, edge, browser)\")\n",
    "\n",
    "print(\"\\n3. A/B Testing Framework\")\n",
    "print(\"   - Statistical rigor for model comparison\")\n",
    "print(\"   - Paired t-test with effect size\")\n",
    "print(\"   - Make data-driven deployment decisions\")\n",
    "\n",
    "print(\"\\n4. Model Compression\")\n",
    "print(f\"   - Quantization compression: {compression_ratio:.1f}x smaller\")\n",
    "print(\"   - Pruning maintains accuracy up to ~50% sparsity\")\n",
    "print(\"   - Essential for edge/mobile deployment\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PRODUCTION DEPLOYMENT CHECKLIST\")\n",
    "print(\"=\" * 70)\n",
    "print(\"[ ] Pipeline tested with production-like data\")\n",
    "print(\"[ ] ONNX model validated for correctness\")\n",
    "print(\"[ ] A/B test shows significant improvement\")\n",
    "print(\"[ ] Compression tested on target hardware\")\n",
    "print(\"[ ] Monitoring and logging in place\")\n",
    "print(\"[ ] Rollback plan documented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6c098c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **sklearn Pipelines** create reproducible, deployable ML workflows that prevent data leakage and simplify model serving.\n",
    "\n",
    "2. **ONNX format** enables cross-platform deployment with optimized inference performance through ONNX Runtime.\n",
    "\n",
    "3. **A/B Testing** provides statistical rigor for model comparison, ensuring improvements are real and not due to chance.\n",
    "\n",
    "4. **Model Compression** through quantization and pruning reduces model size and inference time for edge deployment.\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- Always use Pipelines to encapsulate the full ML workflow\n",
    "- Export to ONNX for production inference where performance matters\n",
    "- Run statistical A/B tests before deploying new models\n",
    "- Consider compression for latency-sensitive or resource-constrained deployments\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "- Version control all artifacts (models, pipelines, configs)\n",
    "- Implement comprehensive logging and monitoring\n",
    "- Plan for model updates and rollbacks\n",
    "- Document all assumptions and constraints\n",
    "\n",
    "### Next Steps\n",
    "- Integrate with CI/CD for automated model deployment\n",
    "- Set up model monitoring for drift detection\n",
    "- Build dashboards for model performance tracking"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

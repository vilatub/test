#!/usr/bin/env python3
"""
Phase 4 Step 1: Self-Attention & Transformer Basics
Part 1: Introduction, Theory, Scaled Dot-Product Attention
"""

import json

# –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –Ω–æ—É—Ç–±—É–∫–∞
notebook = {
    "cells": [],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

cells = []

# ============================================================================
# TITLE AND INTRODUCTION
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "# üîÆ Self-Attention & Transformer Basics\n",
        "\n",
        "**Phase 4, Step 1: Transformers & Modern Architectures**\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ –¶–µ–ª—å —ç—Ç–æ–≥–æ –Ω–æ—É—Ç–±—É–∫–∞\n",
        "\n",
        "–í Phase 3 –º—ã –∏–∑—É—á–∏–ª–∏ **RNN/LSTM + Attention**:\n",
        "- ‚úÖ Recurrent –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π\n",
        "- ‚úÖ Attention –∫–∞–∫ –º–µ—Ö–∞–Ω–∏–∑–º –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏\n",
        "- ‚úÖ Seq2Seq –º–æ–¥–µ–ª–∏\n",
        "\n",
        "**–ù–æ —É RNN –µ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã:**\n",
        "- ‚ùå **Sequential processing**: –Ω–µ–ª—å–∑—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏—Ç—å\n",
        "- ‚ùå **Vanishing gradients**: —Å–ª–æ–∂–Ω–æ —É—á–∏—Ç—å –¥–ª–∏–Ω–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏\n",
        "- ‚ùå **Slow training**: –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ –æ–¥–Ω–æ–º—É timestep\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Enter Transformers (2017)\n",
        "\n",
        "**\"Attention is All You Need\"** (Vaswani et al., 2017)\n",
        "\n",
        "**–ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è:** –ü–æ–ª–Ω–æ—Å—Ç—å—é –∏–∑–±–∞–≤–∏—Ç—å—Å—è –æ—Ç —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏!\n",
        "- ‚úÖ **Self-Attention**: –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ\n",
        "- ‚úÖ **Parallelization**: –≤—Å–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã\n",
        "- ‚úÖ **Long-range dependencies**: –ø—Ä—è–º—ã–µ —Å–≤—è–∑–∏ –º–µ–∂–¥—É –ª—é–±—ã–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏\n",
        "- ‚úÖ **Scalability**: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –Ω–∞ GPU/TPU\n",
        "\n",
        "**–†–µ–∑—É–ª—å—Ç–∞—Ç:**\n",
        "- üèÜ SOTA –≤ NLP: BERT, GPT, T5, GPT-3/4\n",
        "- üèÜ Computer Vision: ViT (Vision Transformer), DINO\n",
        "- üèÜ Tabular Data: TabTransformer, FT-Transformer\n",
        "- üèÜ Time Series: Temporal Fusion Transformer\n",
        "- üèÜ Multi-modal: CLIP, Flamingo\n",
        "\n",
        "---\n",
        "\n",
        "## üìö –ß—Ç–æ –º—ã –∏–∑—É—á–∏–º\n",
        "\n",
        "### 1. Self-Attention Mechanism\n",
        "- **Query, Key, Value (Q, K, V)**: –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç attention\n",
        "- **Scaled Dot-Product Attention**: –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞\n",
        "- **Attention Weights**: —á—Ç–æ –º–æ–¥–µ–ª—å \"—Å–º–æ—Ç—Ä–∏—Ç\"\n",
        "- **Implementation**: —Å –Ω—É–ª—è –≤ PyTorch\n",
        "\n",
        "### 2. Multi-Head Attention\n",
        "- **Multiple attention heads**: –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ \"perspectives\"\n",
        "- **Concatenation & projection**: –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ heads\n",
        "- **Why it works**: —Ä–∞–∑–Ω—ã–µ heads —É—á–∞—Ç —Ä–∞–∑–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã\n",
        "\n",
        "### 3. Positional Encoding\n",
        "- **Problem**: Self-Attention permutation-invariant\n",
        "- **Solution**: –¥–æ–±–∞–≤–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–∑–∏—Ü–∏–∏\n",
        "- **Sinusoidal encoding**: –¥–ª—è sequences\n",
        "- **Learnable embeddings**: –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "\n",
        "### 4. Transformer Encoder for Tabular Data\n",
        "- **Dataset**: Titanic (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –≤—ã–∂–∏–≤—à–∏—Ö)\n",
        "- **Architecture**: Feature Embedding ‚Üí Multi-Head Attention ‚Üí FFN\n",
        "- **Training**: Cross-Entropy Loss\n",
        "- **Evaluation**: —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å XGBoost, LSTM\n",
        "- **Interpretability**: –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è attention weights\n",
        "\n",
        "---\n",
        "\n",
        "## üîç –ü–æ—á–µ–º—É Transformers –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö?\n",
        "\n",
        "**–¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥:**\n",
        "- Tree-based (XGBoost, LightGBM): —Ö–æ—Ä–æ—à–æ –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "- MLPs: baseline\n",
        "\n",
        "**Transformers –¥–∞—é—Ç:**\n",
        "- ‚úÖ **Feature interactions**: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑—É—á–µ–Ω–∏–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π\n",
        "- ‚úÖ **Attention weights**: –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å\n",
        "- ‚úÖ **Transfer learning**: pre-training –Ω–∞ –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö\n",
        "- ‚úÖ **Mixed data types**: –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ + —á–∏—Å–ª–æ–≤—ã–µ\n",
        "\n",
        "**–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**\n",
        "- üìä –ë–æ–ª—å—à–∏–µ –¥–∞—Ç–∞—Å–µ—Ç—ã (>10k samples)\n",
        "- üìä –ú–Ω–æ–≥–æ categorical features\n",
        "- üìä –°–ª–æ–∂–Ω—ã–µ feature interactions\n",
        "- üìä –ù—É–∂–Ω–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å\n",
        "\n",
        "---\n"
    ]
})

# ============================================================================
# IMPORTS
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "## üíª –ß–∞—Å—Ç—å 1: –¢–µ–æ—Ä–∏—è –∏ –∏–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏—è Self-Attention\n",
        "\n",
        "### 1.1 –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ë–∞–∑–æ–≤—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    classification_report, confusion_matrix, roc_auc_score\n",
        ")\n",
        "\n",
        "# Math\n",
        "import math\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"\\n‚úÖ –í—Å–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")"
    ]
})

# ============================================================================
# ATTENTION THEORY
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 1.2 –¢–µ–æ—Ä–∏—è: –ß—Ç–æ —Ç–∞–∫–æ–µ Self-Attention?\n",
        "\n",
        "---\n",
        "\n",
        "## üß† –ò–Ω—Ç—É–∏—Ü–∏—è\n",
        "\n",
        "**–ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ:** \"The animal didn't cross the street because **it** was too tired.\"\n",
        "\n",
        "**–í–æ–ø—Ä–æ—Å:** –ù–∞ —á—Ç–æ —Å—Å—ã–ª–∞–µ—Ç—Å—è \"it\"?\n",
        "- –û—Ç–≤–µ—Ç: \"The animal\" (–∞ –Ω–µ \"street\")\n",
        "\n",
        "**Self-Attention –¥–µ–ª–∞–µ—Ç –∏–º–µ–Ω–Ω–æ —ç—Ç–æ:**\n",
        "- –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ **–≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞**\n",
        "- –í—ã—á–∏—Å–ª—è–µ—Ç **–≤–µ—Å–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏** (–Ω–∞—Å–∫–æ–ª—å–∫–æ –≤–∞–∂–Ω–æ –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ)\n",
        "- –°–æ–∑–¥–∞–µ—Ç **–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ** –≤–∑–≤–µ—à–µ–Ω–Ω–æ–π —Å—É–º–º–æ–π\n",
        "\n",
        "---\n",
        "\n",
        "## üìê –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞: Scaled Dot-Product Attention\n",
        "\n",
        "**Input:**\n",
        "- Sequence: $X = [x_1, x_2, ..., x_n]$, –≥–¥–µ $x_i \\in \\mathbb{R}^{d}$\n",
        "\n",
        "**–®–∞–≥ 1: –°–æ–∑–¥–∞–µ–º Q, K, V (Query, Key, Value)**\n",
        "\n",
        "$$Q = XW^Q, \\quad K = XW^K, \\quad V = XW^V$$\n",
        "\n",
        "–ì–¥–µ:\n",
        "- $W^Q, W^K, W^V \\in \\mathbb{R}^{d \\times d_k}$ - learnable matrices\n",
        "- $Q, K, V \\in \\mathbb{R}^{n \\times d_k}$\n",
        "\n",
        "**–ò–Ω—Ç—É–∏—Ü–∏—è:**\n",
        "- **Query (Q)**: \"–ß—Ç–æ —è –∏—â—É?\" (–∑–∞–ø—Ä–æ—Å –æ—Ç —Ç–µ–∫—É—â–µ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞)\n",
        "- **Key (K)**: \"–ß—Ç–æ —è –º–æ–≥—É –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å?\" (–æ–ø–∏—Å–∞–Ω–∏–µ –¥—Ä—É–≥–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤)\n",
        "- **Value (V)**: \"–ö–∞–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —è –Ω–µ—Å—É?\" (actual content)\n",
        "\n",
        "**–®–∞–≥ 2: –í—ã—á–∏—Å–ª—è–µ–º Attention Scores**\n",
        "\n",
        "$$\\text{scores} = \\frac{QK^T}{\\sqrt{d_k}}$$\n",
        "\n",
        "- $QK^T$: similarity –º–µ–∂–¥—É queries –∏ keys (dot product)\n",
        "- $\\sqrt{d_k}$: scaling –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\n",
        "\n",
        "**–ü–æ—á–µ–º—É scaling?**\n",
        "- –ë–µ–∑ scaling: –¥–ª—è –±–æ–ª—å—à–∏—Ö $d_k$, dot products –æ–≥—Ä–æ–º–Ω—ã–µ\n",
        "- –û–≥—Ä–æ–º–Ω—ã–µ scores ‚Üí softmax saturation ‚Üí vanishing gradients\n",
        "- $\\sqrt{d_k}$ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç variance\n",
        "\n",
        "**–®–∞–≥ 3: Softmax –¥–ª—è –≤–µ—Å–æ–≤**\n",
        "\n",
        "$$\\text{weights} = \\text{softmax}(\\text{scores}) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)$$\n",
        "\n",
        "- –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç scores –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏: $\\sum_i w_i = 1$\n",
        "- –í—ã—Å–æ–∫–∏–µ scores ‚Üí –≤—ã—Å–æ–∫–∏–µ –≤–µ—Å–∞\n",
        "\n",
        "**–®–∞–≥ 4: Weighted Sum**\n",
        "\n",
        "$$\\text{Attention}(Q, K, V) = \\text{weights} \\cdot V = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$$\n",
        "\n",
        "- –ö–∞–∂–¥—ã–π output - –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –≤—Å–µ—Ö values\n",
        "- –í–µ—Å–∞ –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç, —Å–∫–æ–ª—å–∫–æ –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç \"—Å–º–æ—Ç—Ä–∏—Ç\" –Ω–∞ –¥—Ä—É–≥–∏–µ\n",
        "\n",
        "---\n",
        "\n",
        "## üé® –í–∏–∑—É–∞–ª—å–Ω–∞—è –∏–Ω—Ç—É–∏—Ü–∏—è\n",
        "\n",
        "```\n",
        "Input:     [x1]  [x2]  [x3]  [x4]\n",
        "              ‚Üì     ‚Üì     ‚Üì     ‚Üì\n",
        "           [Q1]  [Q2]  [Q3]  [Q4]  ‚Üê Queries (\"—á—Ç–æ —è –∏—â—É?\")\n",
        "           [K1]  [K2]  [K3]  [K4]  ‚Üê Keys (\"—á—Ç–æ —è –ø—Ä–µ–¥–ª–∞–≥–∞—é?\")\n",
        "           [V1]  [V2]  [V3]  [V4]  ‚Üê Values (actual info)\n",
        "\n",
        "Attention for x1:\n",
        "  Q1 ¬∑ K1 ‚Üí score11  ‚îê\n",
        "  Q1 ¬∑ K2 ‚Üí score12  ‚îú‚Üí softmax ‚Üí [w11, w12, w13, w14]\n",
        "  Q1 ¬∑ K3 ‚Üí score13  ‚îÇ\n",
        "  Q1 ¬∑ K4 ‚Üí score14  ‚îò\n",
        "\n",
        "Output: y1 = w11*V1 + w12*V2 + w13*V3 + w14*V4\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üîë –ö–ª—é—á–µ–≤—ã–µ —Å–≤–æ–π—Å—Ç–≤–∞\n",
        "\n",
        "1. **Permutation Invariant (–±–µ–∑ Positional Encoding):**\n",
        "   - Attention –Ω–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –ø–æ—Ä—è–¥–∫–∞ –≤—Ö–æ–¥–æ–≤\n",
        "   - $[x_1, x_2, x_3] \\equiv [x_3, x_1, x_2]$\n",
        "   - –ù—É–∂–Ω–æ –¥–æ–±–∞–≤–ª—è—Ç—å positional encoding!\n",
        "\n",
        "2. **Parallelizable:**\n",
        "   - –í—Å–µ attention scores –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ\n",
        "   - –ú–∞—Ç—Ä–∏—á–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ $QK^T$ - –æ–¥–Ω–∞ –æ–ø–µ—Ä–∞—Ü–∏—è\n",
        "   - –ù–µ—Ç sequential dependencies (–≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç RNN)\n",
        "\n",
        "3. **Long-range Dependencies:**\n",
        "   - –ü—Ä—è–º–∞—è —Å–≤—è–∑—å –º–µ–∂–¥—É –ª—é–±—ã–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏\n",
        "   - O(1) path length (vs O(n) –≤ RNN)\n",
        "\n",
        "4. **Computational Complexity:**\n",
        "   - $O(n^2 \\cdot d)$ –¥–ª—è sequence length $n$\n",
        "   - Bottleneck –¥–ª—è –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π\n",
        "   - –†–µ—à–µ–Ω–∏–µ: Sparse Attention, Linformer, etc.\n",
        "\n",
        "---\n"
    ]
})

# ============================================================================
# IMPLEMENTATION: Scaled Dot-Product Attention
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 1.3 Implementation: Scaled Dot-Product Attention\n",
        "\n",
        "–ò–º–ø–ª–µ–º–µ–Ω—Ç–∏—Ä—É–µ–º —Å –Ω—É–ª—è!"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Scaled Dot-Product Attention\n",
        "    \n",
        "    Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V\n",
        "    \"\"\"\n",
        "    def __init__(self, dropout=0.1):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            Q: Query matrix (batch_size, n_heads, seq_len, d_k)\n",
        "            K: Key matrix (batch_size, n_heads, seq_len, d_k)\n",
        "            V: Value matrix (batch_size, n_heads, seq_len, d_v)\n",
        "            mask: Mask matrix (optional)\n",
        "        \n",
        "        Returns:\n",
        "            context: Attention output (batch_size, n_heads, seq_len, d_v)\n",
        "            attention_weights: Attention weights (batch_size, n_heads, seq_len, seq_len)\n",
        "        \"\"\"\n",
        "        # d_k: dimension of keys/queries\n",
        "        d_k = Q.size(-1)\n",
        "        \n",
        "        # –®–∞–≥ 1: Compute attention scores\n",
        "        # scores shape: (batch_size, n_heads, seq_len, seq_len)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        \n",
        "        # –®–∞–≥ 2: Apply mask (–µ—Å–ª–∏ –µ—Å—Ç—å)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        \n",
        "        # –®–∞–≥ 3: Apply softmax\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "        \n",
        "        # –®–∞–≥ 4: Weighted sum of values\n",
        "        context = torch.matmul(attention_weights, V)\n",
        "        \n",
        "        return context, attention_weights\n",
        "\n",
        "print(\"‚úÖ ScaledDotProductAttention —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω!\")"
    ]
})

# ============================================================================
# SIMPLE EXAMPLE
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 1.4 –ü—Ä–∏–º–µ—Ä: Attention –Ω–∞ –ø—Ä–æ—Å—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "\n",
        "–°–æ–∑–¥–∞–¥–∏–º –º–∞–ª–µ–Ω—å–∫–∏–π –ø—Ä–∏–º–µ—Ä, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç attention."
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π –ø—Ä–∏–º–µ—Ä\n",
        "batch_size = 1\n",
        "n_heads = 1  # –ø–æ–∫–∞ –æ–¥–∏–Ω head\n",
        "seq_len = 4  # 4 —ç–ª–µ–º–µ–Ω—Ç–∞ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
        "d_k = 8      # —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å keys/queries\n",
        "\n",
        "# –°–ª—É—á–∞–π–Ω—ã–µ Q, K, V\n",
        "torch.manual_seed(42)\n",
        "Q = torch.randn(batch_size, n_heads, seq_len, d_k)\n",
        "K = torch.randn(batch_size, n_heads, seq_len, d_k)\n",
        "V = torch.randn(batch_size, n_heads, seq_len, d_k)\n",
        "\n",
        "print(f\"Q shape: {Q.shape}\")\n",
        "print(f\"K shape: {K.shape}\")\n",
        "print(f\"V shape: {V.shape}\")\n",
        "\n",
        "# –ü—Ä–∏–º–µ–Ω—è–µ–º attention\n",
        "attention_layer = ScaledDotProductAttention(dropout=0.0)\n",
        "context, attention_weights = attention_layer(Q, K, V)\n",
        "\n",
        "print(f\"\\nContext shape: {context.shape}\")\n",
        "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º attention weights\n",
        "weights = attention_weights[0, 0].detach().numpy()  # (seq_len, seq_len)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(weights, annot=True, fmt='.3f', cmap='YlOrRd', \n",
        "            xticklabels=[f'K{i+1}' for i in range(seq_len)],\n",
        "            yticklabels=[f'Q{i+1}' for i in range(seq_len)],\n",
        "            cbar_kws={'label': 'Attention Weight'})\n",
        "plt.title('Attention Weights Matrix', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Keys (what to attend to)', fontsize=12)\n",
        "plt.ylabel('Queries (who is attending)', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:\")\n",
        "print(\"  - –ö–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞: –∫–∞–∫ Q_i —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –≤—Å–µ Keys\")\n",
        "print(\"  - –ö–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ —Å—É–º–º–∏—Ä—É–µ—Ç—Å—è –≤ 1.0 (softmax property)\")\n",
        "print(\"  - –í—ã—Å–æ–∫–∏–µ –≤–µ—Å–∞: Q_i —Å–∏–ª—å–Ω–æ \\\"—Å–º–æ—Ç—Ä–∏—Ç\\\" –Ω–∞ K_j\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Ç—Ä–æ–∫–∏ —Å—É–º–º–∏—Ä—É—é—Ç—Å—è –≤ 1\n",
        "row_sums = weights.sum(axis=1)\n",
        "print(f\"\\n‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ softmax: Row sums = {row_sums}\")"
    ]
})

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–≤—É—é —á–∞—Å—Ç—å
notebook['cells'] = cells

output_path = '/home/user/test/notebooks/phase4_transformers/01_self_attention_transformer.ipynb'
with open(output_path, 'w', encoding='utf-8') as f:
    json.dump(notebook, f, ensure_ascii=False, indent=1)

print(f'‚úÖ Part 1 —Å–æ–∑–¥–∞–Ω–∞: {output_path}')
print(f'–Ø—á–µ–µ–∫: {len(cells)}')
print('–°–ª–µ–¥—É—é—â–∞—è —á–∞—Å—Ç—å: Multi-Head Attention –∏ Positional Encoding...')

#!/usr/bin/env python3
"""
Скрипт для создания bonus_financial_tft.ipynb
Temporal Fusion Transformer для детектирования финансовых паттернов
"""

import json

def create_notebook():
    cells = []

    # Cell 1: Title
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "# Temporal Fusion Transformer для Финансовых Паттернов\n",
            "\n",
            "## Введение\n",
            "\n",
            "В этом ноутбуке мы применим **Temporal Fusion Transformer (TFT)** для анализа финансовых \n",
            "временных рядов и детектирования торговых паттернов в реальном времени.\n",
            "\n",
            "### Задачи:\n",
            "1. **Прогнозирование цен** на несколько горизонтов вперёд\n",
            "2. **Детектирование паттернов**: Head & Shoulders, Double Top/Bottom, Breakouts\n",
            "3. **Оценка неопределённости** через квантильные прогнозы\n",
            "4. **Интерпретируемость** через визуализацию внимания\n",
            "\n",
            "### Почему TFT для финансов?\n",
            "\n",
            "- **Variable Selection Networks** автоматически определяют важность технических индикаторов\n",
            "- **Multi-head Attention** фокусируется на ключевых исторических моментах\n",
            "- **Quantile Forecasts** дают оценку рисков (P10, P50, P90)\n",
            "- **Интерпретируемость** позволяет понять, почему модель делает такой прогноз"
        ]
    })

    # Cell 2: Imports
    cells.append({
        "cell_type": "code",
        "metadata": {},
        "source": [
            "import numpy as np\n",
            "import pandas as pd\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "import torch.nn.functional as F\n",
            "from torch.utils.data import Dataset, DataLoader\n",
            "import matplotlib.pyplot as plt\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
            "import warnings\n",
            "warnings.filterwarnings('ignore')\n",
            "\n",
            "# Фиксируем seed для воспроизводимости\n",
            "np.random.seed(42)\n",
            "torch.manual_seed(42)\n",
            "\n",
            "print('PyTorch version:', torch.__version__)\n",
            "print('CUDA available:', torch.cuda.is_available())"
        ],
        "execution_count": None,
        "outputs": []
    })

    # Cell 3: Data Generation
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## 1. Генерация Финансовых Данных\n",
            "\n",
            "Создаём синтетические данные акций с OHLCV и техническими индикаторами.\n",
            "Данные включают реалистичные паттерны: тренды, коррекции и классические фигуры."
        ]
    })

    # Cell 4: Stock Data Generator
    cells.append({
        "cell_type": "code",
        "metadata": {},
        "source": [
            "def generate_stock_data(n_days=2000, n_stocks=5):\n",
            "    \"\"\"\n",
            "    Генерация синтетических данных акций с паттернами.\n",
            "    \n",
            "    Включает:\n",
            "    - OHLCV данные\n",
            "    - Технические индикаторы (SMA, EMA, RSI, MACD, Bollinger Bands)\n",
            "    - Встроенные паттерны (тренды, развороты)\n",
            "    \"\"\"\n",
            "    all_data = []\n",
            "    \n",
            "    for stock_id in range(n_stocks):\n",
            "        # Базовая цена\n",
            "        base_price = np.random.uniform(50, 200)\n",
            "        prices = [base_price]\n",
            "        volumes = []\n",
            "        \n",
            "        # Генерируем ценовой ряд с паттернами\n",
            "        trend = 0\n",
            "        volatility = 0.02\n",
            "        \n",
            "        for i in range(1, n_days):\n",
            "            # Меняем тренд периодически\n",
            "            if i % 200 == 0:\n",
            "                trend = np.random.choice([-1, 0, 1]) * 0.001\n",
            "            \n",
            "            # Случайные скачки волатильности (имитация новостей)\n",
            "            if np.random.random() < 0.02:\n",
            "                volatility = np.random.uniform(0.03, 0.08)\n",
            "            else:\n",
            "                volatility = max(0.01, volatility * 0.99)\n",
            "            \n",
            "            # Генерируем изменение цены\n",
            "            change = np.random.normal(trend, volatility)\n",
            "            new_price = prices[-1] * (1 + change)\n",
            "            new_price = max(new_price, 1)  # Цена не может быть отрицательной\n",
            "            prices.append(new_price)\n",
            "        \n",
            "        prices = np.array(prices)\n",
            "        \n",
            "        # Генерируем OHLC из close\n",
            "        opens = prices * (1 + np.random.normal(0, 0.005, n_days))\n",
            "        highs = np.maximum(prices, opens) * (1 + np.abs(np.random.normal(0, 0.01, n_days)))\n",
            "        lows = np.minimum(prices, opens) * (1 - np.abs(np.random.normal(0, 0.01, n_days)))\n",
            "        closes = prices\n",
            "        \n",
            "        # Объём коррелирует с волатильностью\n",
            "        price_changes = np.abs(np.diff(closes, prepend=closes[0]))\n",
            "        base_volume = np.random.uniform(1e6, 5e6)\n",
            "        volumes = base_volume * (1 + 5 * price_changes / closes) * np.random.uniform(0.8, 1.2, n_days)\n",
            "        \n",
            "        # Создаём DataFrame\n",
            "        df = pd.DataFrame({\n",
            "            'stock_id': stock_id,\n",
            "            'day': range(n_days),\n",
            "            'open': opens,\n",
            "            'high': highs,\n",
            "            'low': lows,\n",
            "            'close': closes,\n",
            "            'volume': volumes\n",
            "        })\n",
            "        \n",
            "        all_data.append(df)\n",
            "    \n",
            "    return pd.concat(all_data, ignore_index=True)\n",
            "\n",
            "# Генерируем данные: 5 акций × 2000 дней = 10,000 записей\n",
            "df = generate_stock_data(n_days=2000, n_stocks=5)\n",
            "print(f'Размер датасета: {len(df):,} записей')\n",
            "print(f'Акций: {df[\"stock_id\"].nunique()}')\n",
            "print(f'Дней на акцию: {len(df) // df[\"stock_id\"].nunique()}')\n",
            "df.head(10)"
        ],
        "execution_count": None,
        "outputs": []
    })

    # Cell 5: Technical Indicators
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "### Добавление Технических Индикаторов\n",
            "\n",
            "Рассчитываем ключевые технические индикаторы:\n",
            "- **SMA/EMA** - скользящие средние\n",
            "- **RSI** - индекс относительной силы\n",
            "- **MACD** - схождение/расхождение скользящих средних\n",
            "- **Bollinger Bands** - полосы Боллинджера\n",
            "- **ATR** - средний истинный диапазон"
        ]
    })

    # Cell 6: Technical Indicators Code
    cells.append({
        "cell_type": "code",
        "metadata": {},
        "source": [
            "def add_technical_indicators(df):\n",
            "    \"\"\"\n",
            "    Добавляет технические индикаторы для каждой акции.\n",
            "    \"\"\"\n",
            "    result_dfs = []\n",
            "    \n",
            "    for stock_id in df['stock_id'].unique():\n",
            "        stock_df = df[df['stock_id'] == stock_id].copy()\n",
            "        close = stock_df['close']\n",
            "        high = stock_df['high']\n",
            "        low = stock_df['low']\n",
            "        volume = stock_df['volume']\n",
            "        \n",
            "        # SMA (Simple Moving Average)\n",
            "        stock_df['sma_10'] = close.rolling(window=10).mean()\n",
            "        stock_df['sma_20'] = close.rolling(window=20).mean()\n",
            "        stock_df['sma_50'] = close.rolling(window=50).mean()\n",
            "        \n",
            "        # EMA (Exponential Moving Average)\n",
            "        stock_df['ema_12'] = close.ewm(span=12).mean()\n",
            "        stock_df['ema_26'] = close.ewm(span=26).mean()\n",
            "        \n",
            "        # MACD\n",
            "        stock_df['macd'] = stock_df['ema_12'] - stock_df['ema_26']\n",
            "        stock_df['macd_signal'] = stock_df['macd'].ewm(span=9).mean()\n",
            "        stock_df['macd_hist'] = stock_df['macd'] - stock_df['macd_signal']\n",
            "        \n",
            "        # RSI (Relative Strength Index)\n",
            "        delta = close.diff()\n",
            "        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
            "        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
            "        rs = gain / (loss + 1e-10)\n",
            "        stock_df['rsi'] = 100 - (100 / (1 + rs))\n",
            "        \n",
            "        # Bollinger Bands\n",
            "        sma_20 = close.rolling(window=20).mean()\n",
            "        std_20 = close.rolling(window=20).std()\n",
            "        stock_df['bb_upper'] = sma_20 + 2 * std_20\n",
            "        stock_df['bb_lower'] = sma_20 - 2 * std_20\n",
            "        stock_df['bb_width'] = (stock_df['bb_upper'] - stock_df['bb_lower']) / sma_20\n",
            "        \n",
            "        # ATR (Average True Range)\n",
            "        tr1 = high - low\n",
            "        tr2 = abs(high - close.shift())\n",
            "        tr3 = abs(low - close.shift())\n",
            "        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
            "        stock_df['atr'] = tr.rolling(window=14).mean()\n",
            "        \n",
            "        # Price position relative to Bollinger Bands\n",
            "        stock_df['bb_position'] = (close - stock_df['bb_lower']) / (stock_df['bb_upper'] - stock_df['bb_lower'] + 1e-10)\n",
            "        \n",
            "        # Volume indicators\n",
            "        stock_df['volume_sma'] = volume.rolling(window=20).mean()\n",
            "        stock_df['volume_ratio'] = volume / (stock_df['volume_sma'] + 1e-10)\n",
            "        \n",
            "        # Returns\n",
            "        stock_df['return_1d'] = close.pct_change(1)\n",
            "        stock_df['return_5d'] = close.pct_change(5)\n",
            "        \n",
            "        result_dfs.append(stock_df)\n",
            "    \n",
            "    return pd.concat(result_dfs, ignore_index=True)\n",
            "\n",
            "# Добавляем индикаторы\n",
            "df = add_technical_indicators(df)\n",
            "\n",
            "# Удаляем NaN (из-за скользящих окон)\n",
            "df = df.dropna().reset_index(drop=True)\n",
            "\n",
            "print(f'Размер после добавления индикаторов: {len(df):,} записей')\n",
            "print(f'\\nКолонки ({len(df.columns)}):')\n",
            "print(df.columns.tolist())"
        ],
        "execution_count": None,
        "outputs": []
    })

    # Cell 7: Pattern Labels
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "### Разметка Паттернов\n",
            "\n",
            "Создаём метки для классических торговых паттернов:\n",
            "- **Trend Up/Down** - направление тренда\n",
            "- **Breakout** - пробой уровней\n",
            "- **High Volatility** - повышенная волатильность"
        ]
    })

    # Cell 8: Pattern Detection Code
    cells.append({
        "cell_type": "code",
        "metadata": {},
        "source": [
            "def add_pattern_labels(df):\n",
            "    \"\"\"\n",
            "    Добавляет метки паттернов на основе технических индикаторов.\n",
            "    \"\"\"\n",
            "    result_dfs = []\n",
            "    \n",
            "    for stock_id in df['stock_id'].unique():\n",
            "        stock_df = df[df['stock_id'] == stock_id].copy()\n",
            "        \n",
            "        # Trend detection (на основе SMA)\n",
            "        stock_df['trend'] = 0\n",
            "        stock_df.loc[stock_df['sma_10'] > stock_df['sma_50'], 'trend'] = 1  # Uptrend\n",
            "        stock_df.loc[stock_df['sma_10'] < stock_df['sma_50'], 'trend'] = -1  # Downtrend\n",
            "        \n",
            "        # Breakout detection (пробой Bollinger Bands)\n",
            "        stock_df['breakout'] = 0\n",
            "        stock_df.loc[stock_df['close'] > stock_df['bb_upper'], 'breakout'] = 1  # Breakout up\n",
            "        stock_df.loc[stock_df['close'] < stock_df['bb_lower'], 'breakout'] = -1  # Breakout down\n",
            "        \n",
            "        # High volatility (ATR выше среднего)\n",
            "        atr_mean = stock_df['atr'].mean()\n",
            "        stock_df['high_volatility'] = (stock_df['atr'] > 1.5 * atr_mean).astype(int)\n",
            "        \n",
            "        # RSI extremes\n",
            "        stock_df['rsi_signal'] = 0\n",
            "        stock_df.loc[stock_df['rsi'] > 70, 'rsi_signal'] = 1  # Overbought\n",
            "        stock_df.loc[stock_df['rsi'] < 30, 'rsi_signal'] = -1  # Oversold\n",
            "        \n",
            "        # MACD crossover\n",
            "        stock_df['macd_crossover'] = 0\n",
            "        macd_cross = (stock_df['macd'] > stock_df['macd_signal']).astype(int)\n",
            "        stock_df['macd_crossover'] = macd_cross.diff().fillna(0)\n",
            "        \n",
            "        # Target: будущая доходность (для прогнозирования)\n",
            "        stock_df['target_1d'] = stock_df['close'].shift(-1) / stock_df['close'] - 1\n",
            "        stock_df['target_5d'] = stock_df['close'].shift(-5) / stock_df['close'] - 1\n",
            "        \n",
            "        result_dfs.append(stock_df)\n",
            "    \n",
            "    return pd.concat(result_dfs, ignore_index=True)\n",
            "\n",
            "# Добавляем паттерны\n",
            "df = add_pattern_labels(df)\n",
            "df = df.dropna().reset_index(drop=True)\n",
            "\n",
            "print(f'Финальный размер: {len(df):,} записей')\n",
            "print(f'\\nРаспределение трендов:')\n",
            "print(df['trend'].value_counts())\n",
            "print(f'\\nBreakouts: {(df[\"breakout\"] != 0).sum()}')\n",
            "print(f'High volatility periods: {df[\"high_volatility\"].sum()}')"
        ],
        "execution_count": None,
        "outputs": []
    })

    # Cell 9: Visualization
    cells.append({
        "cell_type": "code",
        "metadata": {},
        "source": [
            "# Визуализация данных для одной акции\n",
            "stock_0 = df[df['stock_id'] == 0].tail(500)  # Последние 500 дней\n",
            "\n",
            "fig, axes = plt.subplots(4, 1, figsize=(14, 12))\n",
            "\n",
            "# 1. Цена с Bollinger Bands\n",
            "axes[0].plot(stock_0['day'], stock_0['close'], label='Close', linewidth=1)\n",
            "axes[0].plot(stock_0['day'], stock_0['bb_upper'], 'r--', alpha=0.5, label='BB Upper')\n",
            "axes[0].plot(stock_0['day'], stock_0['bb_lower'], 'g--', alpha=0.5, label='BB Lower')\n",
            "axes[0].fill_between(stock_0['day'], stock_0['bb_lower'], stock_0['bb_upper'], alpha=0.1)\n",
            "# Отмечаем breakouts\n",
            "breakouts_up = stock_0[stock_0['breakout'] == 1]\n",
            "breakouts_down = stock_0[stock_0['breakout'] == -1]\n",
            "axes[0].scatter(breakouts_up['day'], breakouts_up['close'], c='green', marker='^', s=50, label='Breakout Up')\n",
            "axes[0].scatter(breakouts_down['day'], breakouts_down['close'], c='red', marker='v', s=50, label='Breakout Down')\n",
            "axes[0].set_title('Цена и Bollinger Bands')\n",
            "axes[0].legend(loc='upper left')\n",
            "\n",
            "# 2. RSI\n",
            "axes[1].plot(stock_0['day'], stock_0['rsi'], label='RSI', color='purple')\n",
            "axes[1].axhline(y=70, color='r', linestyle='--', alpha=0.5)\n",
            "axes[1].axhline(y=30, color='g', linestyle='--', alpha=0.5)\n",
            "axes[1].fill_between(stock_0['day'], 30, 70, alpha=0.1)\n",
            "axes[1].set_title('RSI (Relative Strength Index)')\n",
            "axes[1].set_ylim(0, 100)\n",
            "\n",
            "# 3. MACD\n",
            "axes[2].plot(stock_0['day'], stock_0['macd'], label='MACD', color='blue')\n",
            "axes[2].plot(stock_0['day'], stock_0['macd_signal'], label='Signal', color='orange')\n",
            "axes[2].bar(stock_0['day'], stock_0['macd_hist'], alpha=0.3, label='Histogram')\n",
            "axes[2].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
            "axes[2].set_title('MACD')\n",
            "axes[2].legend()\n",
            "\n",
            "# 4. Volume\n",
            "axes[3].bar(stock_0['day'], stock_0['volume'], alpha=0.5, label='Volume')\n",
            "axes[3].plot(stock_0['day'], stock_0['volume_sma'], color='red', label='Volume SMA')\n",
            "axes[3].set_title('Объём торгов')\n",
            "axes[3].legend()\n",
            "\n",
            "plt.tight_layout()\n",
            "plt.show()"
        ],
        "execution_count": None,
        "outputs": []
    })

    # Cell 10: TFT Architecture
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## 2. Архитектура Temporal Fusion Transformer\n",
            "\n",
            "TFT состоит из нескольких ключевых компонентов:\n",
            "\n",
            "1. **Gated Residual Network (GRN)** - нелинейные преобразования с gate механизмом\n",
            "2. **Variable Selection Network (VSN)** - автоматический отбор важных признаков\n",
            "3. **Temporal Self-Attention** - внимание по временной оси\n",
            "4. **Quantile Output** - прогноз нескольких квантилей для оценки неопределённости"
        ]
    })

    # Cell 11: GRN Implementation
    cells.append({
        "cell_type": "code",
        "metadata": {},
        "source": [
            "class GatedLinearUnit(nn.Module):\n",
            "    \"\"\"\n",
            "    Gated Linear Unit (GLU) - активация с механизмом gate.\n",
            "    \n",
            "    GLU(x) = sigmoid(Wx + b) * (Vx + c)\n",
            "    \"\"\"\n",
            "    def __init__(self, input_dim, output_dim):\n",
            "        super().__init__()\n",
            "        self.fc = nn.Linear(input_dim, output_dim)\n",
            "        self.fc_gate = nn.Linear(input_dim, output_dim)\n",
            "        \n",
            "    def forward(self, x):\n",
            "        return torch.sigmoid(self.fc_gate(x)) * self.fc(x)\n",
            "\n",
            "\n",
            "class GatedResidualNetwork(nn.Module):\n",
            "    \"\"\"\n",
            "    Gated Residual Network - основной строительный блок TFT.\n",
            "    \n",
            "    Особенности:\n",
            "    - ELU активация для нелинейности\n",
            "    - GLU gate для контроля информации\n",
            "    - Skip connection для стабильности обучения\n",
            "    - Layer Normalization\n",
            "    \"\"\"\n",
            "    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.1, context_dim=None):\n",
            "        super().__init__()\n",
            "        \n",
            "        self.input_dim = input_dim\n",
            "        self.output_dim = output_dim\n",
            "        self.context_dim = context_dim\n",
            "        \n",
            "        # Основные слои\n",
            "        if context_dim is not None:\n",
            "            self.context_projection = nn.Linear(context_dim, hidden_dim, bias=False)\n",
            "        \n",
            "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
            "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
            "        self.glu = GatedLinearUnit(hidden_dim, output_dim)\n",
            "        \n",
            "        # Skip connection\n",
            "        if input_dim != output_dim:\n",
            "            self.skip_projection = nn.Linear(input_dim, output_dim)\n",
            "        else:\n",
            "            self.skip_projection = None\n",
            "        \n",
            "        self.layer_norm = nn.LayerNorm(output_dim)\n",
            "        self.dropout = nn.Dropout(dropout)\n",
            "        \n",
            "    def forward(self, x, context=None):\n",
            "        # Первый слой с ELU\n",
            "        hidden = F.elu(self.fc1(x))\n",
            "        \n",
            "        # Добавляем контекст если есть\n",
            "        if context is not None and self.context_dim is not None:\n",
            "            hidden = hidden + self.context_projection(context)\n",
            "        \n",
            "        # Второй слой\n",
            "        hidden = self.dropout(F.elu(self.fc2(hidden)))\n",
            "        \n",
            "        # GLU gate\n",
            "        output = self.glu(hidden)\n",
            "        \n",
            "        # Skip connection\n",
            "        if self.skip_projection is not None:\n",
            "            skip = self.skip_projection(x)\n",
            "        else:\n",
            "            skip = x\n",
            "        \n",
            "        # Layer norm на сумме\n",
            "        return self.layer_norm(output + skip)\n",
            "\n",
            "print('GRN компонент определён')"
        ],
        "execution_count": None,
        "outputs": []
    })

    # Cell 12: Variable Selection Network
    cells.append({
        "cell_type": "code",
        "metadata": {},
        "source": [
            "class VariableSelectionNetwork(nn.Module):\n",
            "    \"\"\"\n",
            "    Variable Selection Network - автоматический отбор признаков.\n",
            "    \n",
            "    Для каждого признака вычисляет вес важности через softmax.\n",
            "    Это позволяет модели фокусироваться на наиболее информативных индикаторах.\n",
            "    \"\"\"\n",
            "    def __init__(self, input_dim, num_features, hidden_dim, dropout=0.1, context_dim=None):\n",
            "        super().__init__()\n",
            "        \n",
            "        self.num_features = num_features\n",
            "        self.hidden_dim = hidden_dim\n",
            "        \n",
            "        # GRN для каждого признака\n",
            "        self.feature_grns = nn.ModuleList([\n",
            "            GatedResidualNetwork(input_dim // num_features, hidden_dim, hidden_dim, dropout)\n",
            "            for _ in range(num_features)\n",
            "        ])\n",
            "        \n",
            "        # GRN для весов (с контекстом если есть)\n",
            "        self.weights_grn = GatedResidualNetwork(\n",
            "            hidden_dim * num_features, hidden_dim, num_features, dropout, context_dim\n",
            "        )\n",
            "        \n",
            "        self.softmax = nn.Softmax(dim=-1)\n",
            "        \n",
            "    def forward(self, x, context=None):\n",
            "        \"\"\"\n",
            "        Args:\n",
            "            x: [batch, seq_len, num_features * feature_dim]\n",
            "            context: optional context vector\n",
            "            \n",
            "        Returns:\n",
            "            output: weighted sum of feature representations\n",
            "            weights: attention weights for each feature\n",
            "        \"\"\"\n",
            "        batch_size = x.size(0)\n",
            "        seq_len = x.size(1) if x.dim() == 3 else 1\n",
            "        \n",
            "        if x.dim() == 2:\n",
            "            x = x.unsqueeze(1)\n",
            "        \n",
            "        # Разделяем по признакам\n",
            "        feature_dim = x.size(-1) // self.num_features\n",
            "        features = x.view(batch_size, seq_len, self.num_features, feature_dim)\n",
            "        \n",
            "        # Применяем GRN к каждому признаку\n",
            "        processed_features = []\n",
            "        for i, grn in enumerate(self.feature_grns):\n",
            "            feat = features[:, :, i, :]  # [batch, seq, feature_dim]\n",
            "            processed = grn(feat)  # [batch, seq, hidden]\n",
            "            processed_features.append(processed)\n",
            "        \n",
            "        # Конкатенируем все признаки\n",
            "        concat_features = torch.cat(processed_features, dim=-1)  # [batch, seq, num_features * hidden]\n",
            "        \n",
            "        # Вычисляем веса важности\n",
            "        weights = self.weights_grn(concat_features, context)  # [batch, seq, num_features]\n",
            "        weights = self.softmax(weights)\n",
            "        \n",
            "        # Стекаем обработанные признаки\n",
            "        stacked = torch.stack(processed_features, dim=-1)  # [batch, seq, hidden, num_features]\n",
            "        \n",
            "        # Взвешенная сумма\n",
            "        output = (stacked * weights.unsqueeze(2)).sum(dim=-1)  # [batch, seq, hidden]\n",
            "        \n",
            "        if seq_len == 1:\n",
            "            output = output.squeeze(1)\n",
            "            weights = weights.squeeze(1)\n",
            "        \n",
            "        return output, weights\n",
            "\n",
            "print('Variable Selection Network определён')"
        ],
        "execution_count": None,
        "outputs": []
    })

    # Cell 13: Temporal Self-Attention
    cells.append({
        "cell_type": "code",
        "metadata": {},
        "source": [
            "class InterpretableMultiHeadAttention(nn.Module):\n",
            "    \"\"\"\n",
            "    Interpretable Multi-Head Attention для временных рядов.\n",
            "    \n",
            "    Отличие от стандартного: используем один набор значений (V) для всех голов,\n",
            "    что делает внимание более интерпретируемым.\n",
            "    \"\"\"\n",
            "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
            "        super().__init__()\n",
            "        \n",
            "        self.embed_dim = embed_dim\n",
            "        self.num_heads = num_heads\n",
            "        self.head_dim = embed_dim // num_heads\n",
            "        \n",
            "        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
            "        \n",
            "        # Q, K для каждой головы, V - общий\n",
            "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
            "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
            "        self.W_v = nn.Linear(embed_dim, embed_dim)  # Shared across heads\n",
            "        \n",
            "        self.out_projection = nn.Linear(embed_dim, embed_dim)\n",
            "        self.dropout = nn.Dropout(dropout)\n",
            "        \n",
            "        self.scale = self.head_dim ** -0.5\n",
            "        \n",
            "    def forward(self, x, mask=None):\n",
            "        \"\"\"\n",
            "        Args:\n",
            "            x: [batch, seq_len, embed_dim]\n",
            "            mask: optional attention mask\n",
            "            \n",
            "        Returns:\n",
            "            output: attention output\n",
            "            attention_weights: [batch, num_heads, seq_len, seq_len]\n",
            "        \"\"\"\n",
            "        batch_size, seq_len, _ = x.size()\n",
            "        \n",
            "        # Линейные преобразования\n",
            "        Q = self.W_q(x)  # [batch, seq, embed]\n",
            "        K = self.W_k(x)\n",
            "        V = self.W_v(x)\n",
            "        \n",
            "        # Разделяем на головы\n",
            "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
            "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
            "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
            "        \n",
            "        # Внимание\n",
            "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
            "        \n",
            "        if mask is not None:\n",
            "            scores = scores.masked_fill(mask == 0, -1e9)\n",
            "        \n",
            "        attention_weights = F.softmax(scores, dim=-1)\n",
            "        attention_weights = self.dropout(attention_weights)\n",
            "        \n",
            "        # Применяем внимание\n",
            "        output = torch.matmul(attention_weights, V)\n",
            "        \n",
            "        # Объединяем головы\n",
            "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
            "        output = self.out_projection(output)\n",
            "        \n",
            "        # Усредняем внимание по головам для интерпретируемости\n",
            "        attention_weights_mean = attention_weights.mean(dim=1)\n",
            "        \n",
            "        return output, attention_weights_mean\n",
            "\n",
            "print('Interpretable Multi-Head Attention определён')"
        ],
        "execution_count": None,
        "outputs": []
    })

    # Cell 14: Full TFT Model
    cells.append({
        "cell_type": "code",
        "metadata": {},
        "source": [
            "class TemporalFusionTransformer(nn.Module):\n",
            "    \"\"\"\n",
            "    Полная модель Temporal Fusion Transformer для финансовых данных.\n",
            "    \n",
            "    Архитектура:\n",
            "    1. Input Embedding & Variable Selection\n",
            "    2. LSTM Encoder для локальных паттернов\n",
            "    3. Static Enrichment\n",
            "    4. Temporal Self-Attention\n",
            "    5. Position-wise Feed-Forward\n",
            "    6. Quantile Output Layer\n",
            "    \"\"\"\n",
            "    def __init__(self, num_features, hidden_dim=64, num_heads=4, \n",
            "                 num_lstm_layers=1, dropout=0.1, num_quantiles=3,\n",
            "                 forecast_horizon=1):\n",
            "        super().__init__()\n",
            "        \n",
            "        self.num_features = num_features\n",
            "        self.hidden_dim = hidden_dim\n",
            "        self.forecast_horizon = forecast_horizon\n",
            "        self.num_quantiles = num_quantiles\n",
            "        \n",
            "        # Input embedding (каждый признак -> hidden_dim)\n",
            "        self.feature_embedding = nn.Linear(1, hidden_dim // num_features)\n",
            "        \n",
            "        # Variable Selection Network\n",
            "        self.vsn = VariableSelectionNetwork(\n",
            "            input_dim=hidden_dim,\n",
            "            num_features=num_features,\n",
            "            hidden_dim=hidden_dim,\n",
            "            dropout=dropout\n",
            "        )\n",
            "        \n",
            "        # LSTM Encoder\n",
            "        self.lstm_encoder = nn.LSTM(\n",
            "            input_size=hidden_dim,\n",
            "            hidden_size=hidden_dim,\n",
            "            num_layers=num_lstm_layers,\n",
            "            batch_first=True,\n",
            "            dropout=dropout if num_lstm_layers > 1 else 0\n",
            "        )\n",
            "        \n",
            "        # Gated skip connection after LSTM\n",
            "        self.lstm_glu = GatedLinearUnit(hidden_dim, hidden_dim)\n",
            "        self.lstm_layer_norm = nn.LayerNorm(hidden_dim)\n",
            "        \n",
            "        # Temporal Self-Attention\n",
            "        self.attention = InterpretableMultiHeadAttention(hidden_dim, num_heads, dropout)\n",
            "        self.attention_glu = GatedLinearUnit(hidden_dim, hidden_dim)\n",
            "        self.attention_layer_norm = nn.LayerNorm(hidden_dim)\n",
            "        \n",
            "        # Position-wise Feed-Forward\n",
            "        self.ff_grn = GatedResidualNetwork(hidden_dim, hidden_dim * 4, hidden_dim, dropout)\n",
            "        \n",
            "        # Output layer - квантили\n",
            "        self.output_layer = nn.Linear(hidden_dim, forecast_horizon * num_quantiles)\n",
            "        \n",
            "        # Для хранения весов (для интерпретации)\n",
            "        self.feature_importance = None\n",
            "        self.temporal_attention = None\n",
            "        \n",
            "    def forward(self, x):\n",
            "        \"\"\"\n",
            "        Args:\n",
            "            x: [batch, seq_len, num_features]\n",
            "            \n",
            "        Returns:\n",
            "            quantiles: [batch, forecast_horizon, num_quantiles]\n",
            "        \"\"\"\n",
            "        batch_size, seq_len, _ = x.size()\n",
            "        \n",
            "        # 1. Feature Embedding\n",
            "        # Расширяем каждый признак до embedding\n",
            "        embedded = []\n",
            "        for i in range(self.num_features):\n",
            "            feat = x[:, :, i:i+1]  # [batch, seq, 1]\n",
            "            emb = self.feature_embedding(feat)  # [batch, seq, hidden/num_features]\n",
            "            embedded.append(emb)\n",
            "        embedded = torch.cat(embedded, dim=-1)  # [batch, seq, hidden]\n",
            "        \n",
            "        # 2. Variable Selection\n",
            "        selected, feature_weights = self.vsn(embedded)\n",
            "        self.feature_importance = feature_weights.detach()  # Сохраняем для анализа\n",
            "        \n",
            "        # 3. LSTM Encoder\n",
            "        lstm_out, _ = self.lstm_encoder(selected)\n",
            "        \n",
            "        # Gated skip connection\n",
            "        lstm_out = self.lstm_layer_norm(self.lstm_glu(lstm_out) + selected)\n",
            "        \n",
            "        # 4. Temporal Self-Attention\n",
            "        attention_out, attention_weights = self.attention(lstm_out)\n",
            "        self.temporal_attention = attention_weights.detach()  # Сохраняем для анализа\n",
            "        \n",
            "        # Gated skip connection\n",
            "        attention_out = self.attention_layer_norm(self.attention_glu(attention_out) + lstm_out)\n",
            "        \n",
            "        # 5. Position-wise Feed-Forward\n",
            "        ff_out = self.ff_grn(attention_out)\n",
            "        \n",
            "        # 6. Output - берём последний временной шаг\n",
            "        final_hidden = ff_out[:, -1, :]  # [batch, hidden]\n",
            "        \n",
            "        # Квантильный выход\n",
            "        quantiles = self.output_layer(final_hidden)  # [batch, horizon * quantiles]\n",
            "        quantiles = quantiles.view(batch_size, self.forecast_horizon, self.num_quantiles)\n",
            "        \n",
            "        return quantiles\n",
            "\n",
            "print('Temporal Fusion Transformer модель определена')"
        ],
        "execution_count": None,
        "outputs": []
    })

    # Cell 15: Dataset
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## 3. Подготовка Данных для Обучения"
        ]
    })

    # Cell 16: Dataset Class
    cells.append({
        "cell_type": "code",
        "metadata": {},
        "source": [
            "class FinancialDataset(Dataset):\n",
            "    \"\"\"\n",
            "    Dataset для финансовых временных рядов.\n",
            "    \n",
            "    Создаёт последовательности фиксированной длины для обучения TFT.\n",
            "    \"\"\"\n",
            "    def __init__(self, df, feature_columns, target_column, seq_length=60, scaler=None):\n",
            "        self.seq_length = seq_length\n",
            "        self.feature_columns = feature_columns\n",
            "        self.target_column = target_column\n",
            "        \n",
            "        # Подготавливаем данные для каждой акции\n",
            "        self.sequences = []\n",
            "        self.targets = []\n",
            "        self.stock_ids = []\n",
            "        \n",
            "        for stock_id in df['stock_id'].unique():\n",
            "            stock_df = df[df['stock_id'] == stock_id].sort_values('day')\n",
            "            \n",
            "            features = stock_df[feature_columns].values\n",
            "            targets = stock_df[target_column].values\n",
            "            \n",
            "            # Создаём последовательности\n",
            "            for i in range(len(stock_df) - seq_length):\n",
            "                seq = features[i:i + seq_length]\n",
            "                target = targets[i + seq_length - 1]  # Последний target\n",
            "                \n",
            "                self.sequences.append(seq)\n",
            "                self.targets.append(target)\n",
            "                self.stock_ids.append(stock_id)\n",
            "        \n",
            "        self.sequences = np.array(self.sequences)\n",
            "        self.targets = np.array(self.targets)\n",
            "        self.stock_ids = np.array(self.stock_ids)\n",
            "        \n",
            "        # Нормализация\n",
            "        if scaler is None:\n",
            "            self.scaler = StandardScaler()\n",
            "            original_shape = self.sequences.shape\n",
            "            self.sequences = self.scaler.fit_transform(\n",
            "                self.sequences.reshape(-1, len(feature_columns))\n",
            "            ).reshape(original_shape)\n",
            "        else:\n",
            "            self.scaler = scaler\n",
            "            original_shape = self.sequences.shape\n",
            "            self.sequences = self.scaler.transform(\n",
            "                self.sequences.reshape(-1, len(feature_columns))\n",
            "            ).reshape(original_shape)\n",
            "        \n",
            "    def __len__(self):\n",
            "        return len(self.sequences)\n",
            "    \n",
            "    def __getitem__(self, idx):\n",
            "        return (\n",
            "            torch.FloatTensor(self.sequences[idx]),\n",
            "            torch.FloatTensor([self.targets[idx]])\n",
            "        )\n",
            "\n",
            "# Выбираем признаки для модели\n",
            "feature_cols = [\n",
            "    'close', 'volume', 'sma_10', 'sma_20', 'rsi', \n",
            "    'macd', 'macd_signal', 'bb_width', 'atr', 'volume_ratio',\n",
            "    'return_1d', 'bb_position'\n",
            "]\n",
            "\n",
            "target_col = 'target_1d'\n",
            "\n",
            "print(f'Признаков: {len(feature_cols)}')\n",
            "print(f'Признаки: {feature_cols}')"
        ],
        "execution_count": None,
        "outputs": []
    })

    # Cell 17: Train/Val Split
    cells.append({
        "cell_type": "code",
        "metadata": {},
        "source": [
            "# Разделяем данные по времени (последние 20% - валидация)\n",
            "split_day = int(df['day'].max() * 0.8)\n",
            "\n",
            "train_df = df[df['day'] <= split_day].copy()\n",
            "val_df = df[df['day'] > split_day].copy()\n",
            "\n",
            "print(f'Обучающая выборка: {len(train_df):,} записей')\n",
            "print(f'Валидационная выборка: {len(val_df):,} записей')\n",
            "\n",
            "# Создаём datasets\n",
            "seq_length = 60  # 60 дней истории\n",
            "\n",
            "train_dataset = FinancialDataset(train_df, feature_cols, target_col, seq_length)\n",
            "val_dataset = FinancialDataset(val_df, feature_cols, target_col, seq_length, \n",
            "                               scaler=train_dataset.scaler)\n",
            "\n",
            "print(f'\\nОбучающих последовательностей: {len(train_dataset):,}')\n",
            "print(f'Валидационных последовательностей: {len(val_dataset):,}')\n",
            "\n",
            "# DataLoaders\n",
            "batch_size = 64\n",
            "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
            "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
            "\n",
            "# Проверяем размеры\n",
            "sample_x, sample_y = next(iter(train_loader))\n",
            "print(f'\\nРазмер батча X: {sample_x.shape}')\n",
            "print(f'Размер батча Y: {sample_y.shape}')"
        ],
        "execution_count": None,
        "outputs": []
    })

    # Cell 18: Loss Function
    cells.append({
        "cell_type": "code",
        "metadata": {},
        "source": [
            "class QuantileLoss(nn.Module):\n",
            "    \"\"\"\n",
            "    Quantile Loss (Pinball Loss) для прогнозирования квантилей.\n",
            "    \n",
            "    L_q(y, f) = q * (y - f)  если y >= f\n",
            "               (1-q) * (f - y)  если y < f\n",
            "    \n",
            "    Это позволяет модели предсказывать разные квантили распределения.\n",
            "    \"\"\"\n",
            "    def __init__(self, quantiles=[0.1, 0.5, 0.9]):\n",
            "        super().__init__()\n",
            "        self.quantiles = quantiles\n",
            "        \n",
            "    def forward(self, predictions, targets):\n",
            "        \"\"\"\n",
            "        Args:\n",
            "            predictions: [batch, horizon, num_quantiles]\n",
            "            targets: [batch, 1]\n",
            "        \"\"\"\n",
            "        losses = []\n",
            "        \n",
            "        for i, q in enumerate(self.quantiles):\n",
            "            pred = predictions[:, :, i]  # [batch, horizon]\n",
            "            target = targets.expand_as(pred)  # [batch, horizon]\n",
            "            \n",
            "            errors = target - pred\n",
            "            loss = torch.max(q * errors, (q - 1) * errors)\n",
            "            losses.append(loss.mean())\n",
            "        \n",
            "        return sum(losses) / len(losses)\n",
            "\n",
            "# Квантили: P10 (пессимистичный), P50 (медиана), P90 (оптимистичный)\n",
            "quantiles = [0.1, 0.5, 0.9]\n",
            "criterion = QuantileLoss(quantiles)\n",
            "\n",
            "print(f'Квантили для прогноза: {quantiles}')\n",
            "print('P10 - нижняя граница (риск падения)')\n",
            "print('P50 - медианный прогноз')\n",
            "print('P90 - верхняя граница (потенциал роста)')"
        ],
        "execution_count": None,
        "outputs": []
    })

    # Cell 19: Training
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## 4. Обучение Модели"
        ]
    })

    # Cell 20: Model Init and Training
    cells.append({
        "cell_type": "code",
        "metadata": {},
        "source": [
            "# Инициализация модели\n",
            "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
            "print(f'Устройство: {device}')\n",
            "\n",
            "model = TemporalFusionTransformer(\n",
            "    num_features=len(feature_cols),\n",
            "    hidden_dim=64,\n",
            "    num_heads=4,\n",
            "    num_lstm_layers=1,\n",
            "    dropout=0.1,\n",
            "    num_quantiles=len(quantiles),\n",
            "    forecast_horizon=1\n",
            ").to(device)\n",
            "\n",
            "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
            "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
            "\n",
            "# Подсчёт параметров\n",
            "total_params = sum(p.numel() for p in model.parameters())\n",
            "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
            "print(f'Всего параметров: {total_params:,}')\n",
            "print(f'Обучаемых параметров: {trainable_params:,}')"
        ],
        "execution_count": None,
        "outputs": []
    })

    # Cell 21: Training Loop
    cells.append({
        "cell_type": "code",
        "metadata": {},
        "source": [
            "def train_epoch(model, loader, criterion, optimizer, device):\n",
            "    model.train()\n",
            "    total_loss = 0\n",
            "    \n",
            "    for batch_x, batch_y in loader:\n",
            "        batch_x = batch_x.to(device)\n",
            "        batch_y = batch_y.to(device)\n",
            "        \n",
            "        optimizer.zero_grad()\n",
            "        predictions = model(batch_x)\n",
            "        loss = criterion(predictions, batch_y)\n",
            "        loss.backward()\n",
            "        \n",
            "        # Gradient clipping\n",
            "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
            "        \n",
            "        optimizer.step()\n",
            "        total_loss += loss.item()\n",
            "    \n",
            "    return total_loss / len(loader)\n",
            "\n",
            "def validate(model, loader, criterion, device):\n",
            "    model.eval()\n",
            "    total_loss = 0\n",
            "    all_preds = []\n",
            "    all_targets = []\n",
            "    \n",
            "    with torch.no_grad():\n",
            "        for batch_x, batch_y in loader:\n",
            "            batch_x = batch_x.to(device)\n",
            "            batch_y = batch_y.to(device)\n",
            "            \n",
            "            predictions = model(batch_x)\n",
            "            loss = criterion(predictions, batch_y)\n",
            "            total_loss += loss.item()\n",
            "            \n",
            "            # Медианный прогноз (P50)\n",
            "            median_pred = predictions[:, 0, 1].cpu().numpy()\n",
            "            all_preds.extend(median_pred)\n",
            "            all_targets.extend(batch_y.cpu().numpy().flatten())\n",
            "    \n",
            "    mae = mean_absolute_error(all_targets, all_preds)\n",
            "    \n",
            "    return total_loss / len(loader), mae\n",
            "\n",
            "# Обучение\n",
            "n_epochs = 30\n",
            "best_val_loss = float('inf')\n",
            "train_losses = []\n",
            "val_losses = []\n",
            "\n",
            "print('Начинаем обучение...\\n')\n",
            "\n",
            "for epoch in range(n_epochs):\n",
            "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
            "    val_loss, val_mae = validate(model, val_loader, criterion, device)\n",
            "    \n",
            "    train_losses.append(train_loss)\n",
            "    val_losses.append(val_loss)\n",
            "    \n",
            "    scheduler.step(val_loss)\n",
            "    \n",
            "    if val_loss < best_val_loss:\n",
            "        best_val_loss = val_loss\n",
            "        torch.save(model.state_dict(), 'best_tft_model.pt')\n",
            "    \n",
            "    if (epoch + 1) % 5 == 0:\n",
            "        print(f'Epoch {epoch+1}/{n_epochs}')\n",
            "        print(f'  Train Loss: {train_loss:.6f}')\n",
            "        print(f'  Val Loss: {val_loss:.6f}, MAE: {val_mae:.6f}')\n",
            "        print(f'  LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
            "        print()\n",
            "\n",
            "print('Обучение завершено!')\n",
            "print(f'Лучший Val Loss: {best_val_loss:.6f}')"
        ],
        "execution_count": None,
        "outputs": []
    })

    # Cell 22: Training Visualization
    cells.append({
        "cell_type": "code",
        "metadata": {},
        "source": [
            "# Визуализация обучения\n",
            "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
            "\n",
            "axes[0].plot(train_losses, label='Train')\n",
            "axes[0].plot(val_losses, label='Validation')\n",
            "axes[0].set_xlabel('Epoch')\n",
            "axes[0].set_ylabel('Loss')\n",
            "axes[0].set_title('Training Progress')\n",
            "axes[0].legend()\n",
            "axes[0].grid(True, alpha=0.3)\n",
            "\n",
            "axes[1].plot(train_losses, label='Train', alpha=0.7)\n",
            "axes[1].plot(val_losses, label='Validation', alpha=0.7)\n",
            "axes[1].set_xlabel('Epoch')\n",
            "axes[1].set_ylabel('Loss (log scale)')\n",
            "axes[1].set_title('Training Progress (Log Scale)')\n",
            "axes[1].set_yscale('log')\n",
            "axes[1].legend()\n",
            "axes[1].grid(True, alpha=0.3)\n",
            "\n",
            "plt.tight_layout()\n",
            "plt.show()"
        ],
        "execution_count": None,
        "outputs": []
    })

    # Cell 23: Interpretability
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## 5. Интерпретация Результатов\n",
            "\n",
            "Одно из главных преимуществ TFT - интерпретируемость. Мы можем визуализировать:\n",
            "\n",
            "1. **Важность признаков** - какие индикаторы модель считает наиболее информативными\n",
            "2. **Временное внимание** - на какие исторические периоды модель обращает внимание"
        ]
    })

    # Cell 24: Feature Importance
    cells.append({
        "cell_type": "code",
        "metadata": {},
        "source": [
            "# Загружаем лучшую модель\n",
            "model.load_state_dict(torch.load('best_tft_model.pt'))\n",
            "model.eval()\n",
            "\n",
            "# Получаем важность признаков на валидационных данных\n",
            "feature_importances = []\n",
            "\n",
            "with torch.no_grad():\n",
            "    for batch_x, batch_y in val_loader:\n",
            "        batch_x = batch_x.to(device)\n",
            "        _ = model(batch_x)\n",
            "        \n",
            "        # Берём средние веса по батчу и времени\n",
            "        if model.feature_importance.dim() == 3:\n",
            "            imp = model.feature_importance.mean(dim=[0, 1]).cpu().numpy()\n",
            "        else:\n",
            "            imp = model.feature_importance.mean(dim=0).cpu().numpy()\n",
            "        feature_importances.append(imp)\n",
            "\n",
            "# Усредняем по всем батчам\n",
            "mean_importance = np.mean(feature_importances, axis=0)\n",
            "\n",
            "# Визуализация\n",
            "fig, ax = plt.subplots(figsize=(10, 6))\n",
            "\n",
            "sorted_idx = np.argsort(mean_importance)[::-1]\n",
            "sorted_features = [feature_cols[i] for i in sorted_idx]\n",
            "sorted_importance = mean_importance[sorted_idx]\n",
            "\n",
            "colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(sorted_features)))[::-1]\n",
            "bars = ax.barh(range(len(sorted_features)), sorted_importance, color=colors)\n",
            "ax.set_yticks(range(len(sorted_features)))\n",
            "ax.set_yticklabels(sorted_features)\n",
            "ax.set_xlabel('Важность (VSN Weight)')\n",
            "ax.set_title('Важность Признаков по Variable Selection Network')\n",
            "ax.invert_yaxis()\n",
            "\n",
            "# Добавляем значения\n",
            "for i, (bar, val) in enumerate(zip(bars, sorted_importance)):\n",
            "    ax.text(val + 0.002, i, f'{val:.3f}', va='center', fontsize=9)\n",
            "\n",
            "plt.tight_layout()\n",
            "plt.show()\n",
            "\n",
            "print('\\nТоп-5 важных признаков:')\n",
            "for i in range(5):\n",
            "    print(f'{i+1}. {sorted_features[i]}: {sorted_importance[i]:.4f}')"
        ],
        "execution_count": None,
        "outputs": []
    })

    # Cell 25: Temporal Attention
    cells.append({
        "cell_type": "code",
        "metadata": {},
        "source": [
            "# Визуализация временного внимания для примера\n",
            "sample_batch_x, sample_batch_y = next(iter(val_loader))\n",
            "sample_x = sample_batch_x[:1].to(device)  # Один пример\n",
            "\n",
            "with torch.no_grad():\n",
            "    predictions = model(sample_x)\n",
            "    attention = model.temporal_attention[0].cpu().numpy()  # [seq_len, seq_len]\n",
            "\n",
            "# Визуализация\n",
            "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
            "\n",
            "# Heatmap внимания\n",
            "im = axes[0].imshow(attention, cmap='YlOrRd', aspect='auto')\n",
            "axes[0].set_xlabel('Key Position (History)')\n",
            "axes[0].set_ylabel('Query Position (Time)')\n",
            "axes[0].set_title('Temporal Self-Attention Weights')\n",
            "plt.colorbar(im, ax=axes[0])\n",
            "\n",
            "# Внимание для последнего шага (прогноз)\n",
            "last_step_attention = attention[-1]\n",
            "axes[1].plot(range(len(last_step_attention)), last_step_attention, 'b-', linewidth=1.5)\n",
            "axes[1].fill_between(range(len(last_step_attention)), 0, last_step_attention, alpha=0.3)\n",
            "axes[1].set_xlabel('Historical Time Step')\n",
            "axes[1].set_ylabel('Attention Weight')\n",
            "axes[1].set_title('Attention Distribution for Final Prediction')\n",
            "axes[1].grid(True, alpha=0.3)\n",
            "\n",
            "# Отмечаем пики внимания\n",
            "top_k = 5\n",
            "top_indices = np.argsort(last_step_attention)[-top_k:]\n",
            "for idx in top_indices:\n",
            "    axes[1].axvline(x=idx, color='red', linestyle='--', alpha=0.5)\n",
            "    axes[1].annotate(f't-{seq_length-idx}', (idx, last_step_attention[idx]),\n",
            "                     xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
            "\n",
            "plt.tight_layout()\n",
            "plt.show()\n",
            "\n",
            "print(f'\\nМодель обращает наибольшее внимание на периоды:')\n",
            "for idx in sorted(top_indices):\n",
            "    print(f'  t-{seq_length-idx} (вес: {last_step_attention[idx]:.4f})')"
        ],
        "execution_count": None,
        "outputs": []
    })

    # Cell 26: Quantile Predictions
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## 6. Квантильные Прогнозы и Оценка Рисков\n",
            "\n",
            "Квантильные прогнозы позволяют оценить неопределённость:\n",
            "- **P10** - 10-й перцентиль (пессимистичный сценарий)\n",
            "- **P50** - медиана (центральный прогноз)\n",
            "- **P90** - 90-й перцентиль (оптимистичный сценарий)"
        ]
    })

    # Cell 27: Quantile Visualization
    cells.append({
        "cell_type": "code",
        "metadata": {},
        "source": [
            "# Получаем прогнозы на валидации\n",
            "all_preds = []\n",
            "all_targets = []\n",
            "\n",
            "model.eval()\n",
            "with torch.no_grad():\n",
            "    for batch_x, batch_y in val_loader:\n",
            "        batch_x = batch_x.to(device)\n",
            "        predictions = model(batch_x)\n",
            "        \n",
            "        all_preds.append(predictions[:, 0, :].cpu().numpy())  # [batch, 3]\n",
            "        all_targets.append(batch_y.numpy())\n",
            "\n",
            "all_preds = np.concatenate(all_preds, axis=0)\n",
            "all_targets = np.concatenate(all_targets, axis=0).flatten()\n",
            "\n",
            "# Визуализация\n",
            "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
            "\n",
            "# 1. Временной ряд прогнозов (последние 200 точек)\n",
            "n_show = 200\n",
            "x_range = range(n_show)\n",
            "\n",
            "axes[0, 0].fill_between(x_range, all_preds[-n_show:, 0], all_preds[-n_show:, 2], \n",
            "                        alpha=0.3, label='P10-P90 interval')\n",
            "axes[0, 0].plot(x_range, all_preds[-n_show:, 1], 'b-', label='P50 (median)', linewidth=1)\n",
            "axes[0, 0].plot(x_range, all_targets[-n_show:], 'r.', alpha=0.5, label='Actual', markersize=3)\n",
            "axes[0, 0].set_xlabel('Sample')\n",
            "axes[0, 0].set_ylabel('Return')\n",
            "axes[0, 0].set_title('Quantile Forecasts vs Actual')\n",
            "axes[0, 0].legend()\n",
            "axes[0, 0].grid(True, alpha=0.3)\n",
            "\n",
            "# 2. Scatter: Predicted vs Actual\n",
            "axes[0, 1].scatter(all_targets, all_preds[:, 1], alpha=0.3, s=10)\n",
            "min_val = min(all_targets.min(), all_preds[:, 1].min())\n",
            "max_val = max(all_targets.max(), all_preds[:, 1].max())\n",
            "axes[0, 1].plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect prediction')\n",
            "axes[0, 1].set_xlabel('Actual Return')\n",
            "axes[0, 1].set_ylabel('Predicted Return (P50)')\n",
            "axes[0, 1].set_title('Predicted vs Actual')\n",
            "axes[0, 1].legend()\n",
            "axes[0, 1].grid(True, alpha=0.3)\n",
            "\n",
            "# 3. Calibration: какой % точек попал в интервал\n",
            "in_interval = ((all_targets >= all_preds[:, 0]) & (all_targets <= all_preds[:, 2])).mean()\n",
            "below_p10 = (all_targets < all_preds[:, 0]).mean()\n",
            "above_p90 = (all_targets > all_preds[:, 2]).mean()\n",
            "\n",
            "categories = ['Below P10', 'In P10-P90', 'Above P90']\n",
            "values = [below_p10, in_interval, above_p90]\n",
            "expected = [0.1, 0.8, 0.1]  # Ожидаемые значения для калиброванной модели\n",
            "\n",
            "x_pos = np.arange(len(categories))\n",
            "width = 0.35\n",
            "\n",
            "axes[1, 0].bar(x_pos - width/2, values, width, label='Actual', color='steelblue')\n",
            "axes[1, 0].bar(x_pos + width/2, expected, width, label='Expected', color='coral', alpha=0.7)\n",
            "axes[1, 0].set_xticks(x_pos)\n",
            "axes[1, 0].set_xticklabels(categories)\n",
            "axes[1, 0].set_ylabel('Proportion')\n",
            "axes[1, 0].set_title('Quantile Calibration')\n",
            "axes[1, 0].legend()\n",
            "\n",
            "# 4. Distribution of prediction intervals\n",
            "interval_widths = all_preds[:, 2] - all_preds[:, 0]\n",
            "axes[1, 1].hist(interval_widths, bins=50, edgecolor='black', alpha=0.7)\n",
            "axes[1, 1].axvline(np.mean(interval_widths), color='red', linestyle='--', \n",
            "                   label=f'Mean: {np.mean(interval_widths):.4f}')\n",
            "axes[1, 1].set_xlabel('Interval Width (P90 - P10)')\n",
            "axes[1, 1].set_ylabel('Count')\n",
            "axes[1, 1].set_title('Distribution of Prediction Intervals')\n",
            "axes[1, 1].legend()\n",
            "\n",
            "plt.tight_layout()\n",
            "plt.show()\n",
            "\n",
            "print('\\nМетрики калибровки:')\n",
            "print(f'Below P10: {below_p10:.2%} (expected: 10%)')\n",
            "print(f'In interval: {in_interval:.2%} (expected: 80%)')\n",
            "print(f'Above P90: {above_p90:.2%} (expected: 10%)')"
        ],
        "execution_count": None,
        "outputs": []
    })

    # Cell 28: Pattern Analysis
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## 7. Анализ Паттернов\n",
            "\n",
            "Проанализируем, как модель реагирует на различные рыночные ситуации."
        ]
    })

    # Cell 29: Pattern Detection
    cells.append({
        "cell_type": "code",
        "metadata": {},
        "source": [
            "# Анализ прогнозов по разным состояниям рынка\n",
            "val_df_clean = val_df.dropna().reset_index(drop=True)\n",
            "\n",
            "# Убедимся, что размеры совпадают\n",
            "n_predictions = len(all_preds)\n",
            "\n",
            "# Берём соответствующие метки из валидационных данных\n",
            "# (с учётом того, что первые seq_length точек использовались для контекста)\n",
            "offset = seq_length\n",
            "labels_df = []\n",
            "\n",
            "for stock_id in val_df['stock_id'].unique():\n",
            "    stock_df = val_df_clean[val_df_clean['stock_id'] == stock_id].iloc[offset:]\n",
            "    labels_df.append(stock_df)\n",
            "\n",
            "labels_df = pd.concat(labels_df).reset_index(drop=True)\n",
            "\n",
            "# Ограничиваем до размера прогнозов\n",
            "labels_df = labels_df.iloc[:n_predictions]\n",
            "\n",
            "print(f'Прогнозов: {n_predictions}')\n",
            "print(f'Меток: {len(labels_df)}')\n",
            "\n",
            "# Анализ по трендам\n",
            "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
            "\n",
            "for i, trend_val in enumerate([-1, 0, 1]):\n",
            "    trend_names = {-1: 'Downtrend', 0: 'Neutral', 1: 'Uptrend'}\n",
            "    mask = labels_df['trend'].values[:n_predictions] == trend_val\n",
            "    \n",
            "    if mask.sum() > 0:\n",
            "        trend_preds = all_preds[mask, 1]  # Медиана\n",
            "        trend_actuals = all_targets[mask]\n",
            "        \n",
            "        axes[i].scatter(trend_actuals, trend_preds, alpha=0.3, s=10)\n",
            "        axes[i].plot([-0.1, 0.1], [-0.1, 0.1], 'r--')\n",
            "        axes[i].set_xlabel('Actual')\n",
            "        axes[i].set_ylabel('Predicted')\n",
            "        axes[i].set_title(f'{trend_names[trend_val]} (n={mask.sum()})')\n",
            "        axes[i].grid(True, alpha=0.3)\n",
            "        \n",
            "        mae = mean_absolute_error(trend_actuals, trend_preds)\n",
            "        axes[i].text(0.05, 0.95, f'MAE: {mae:.5f}', transform=axes[i].transAxes,\n",
            "                    verticalalignment='top')\n",
            "\n",
            "plt.suptitle('Качество Прогнозов по Типу Тренда', fontsize=12)\n",
            "plt.tight_layout()\n",
            "plt.show()"
        ],
        "execution_count": None,
        "outputs": []
    })

    # Cell 30: Real-time Scoring
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## 8. Применение в Реальном Времени\n",
            "\n",
            "Демонстрируем, как использовать модель для прогнозирования в реальном времени \n",
            "с интерпретацией результатов."
        ]
    })

    # Cell 31: Inference Function
    cells.append({
        "cell_type": "code",
        "metadata": {},
        "source": [
            "def predict_with_interpretation(model, data_window, feature_names, device):\n",
            "    \"\"\"\n",
            "    Прогноз с полной интерпретацией.\n",
            "    \n",
            "    Возвращает:\n",
            "    - Квантильные прогнозы\n",
            "    - Важность признаков\n",
            "    - Временное внимание\n",
            "    \"\"\"\n",
            "    model.eval()\n",
            "    \n",
            "    with torch.no_grad():\n",
            "        x = torch.FloatTensor(data_window).unsqueeze(0).to(device)\n",
            "        predictions = model(x)\n",
            "        \n",
            "        # Получаем интерпретации\n",
            "        if model.feature_importance.dim() == 3:\n",
            "            feature_imp = model.feature_importance[0].mean(dim=0).cpu().numpy()\n",
            "        else:\n",
            "            feature_imp = model.feature_importance[0].cpu().numpy()\n",
            "        \n",
            "        temporal_att = model.temporal_attention[0, -1].cpu().numpy()  # Последний шаг\n",
            "        \n",
            "    # Квантили\n",
            "    p10, p50, p90 = predictions[0, 0].cpu().numpy()\n",
            "    \n",
            "    return {\n",
            "        'p10': p10,\n",
            "        'p50': p50,\n",
            "        'p90': p90,\n",
            "        'interval_width': p90 - p10,\n",
            "        'feature_importance': dict(zip(feature_names, feature_imp)),\n",
            "        'temporal_attention': temporal_att\n",
            "    }\n",
            "\n",
            "# Пример использования\n",
            "sample_idx = 100\n",
            "sample_window = val_dataset.sequences[sample_idx]\n",
            "actual_return = val_dataset.targets[sample_idx]\n",
            "\n",
            "result = predict_with_interpretation(model, sample_window, feature_cols, device)\n",
            "\n",
            "print('=' * 50)\n",
            "print('ПРОГНОЗ ДОХОДНОСТИ')\n",
            "print('=' * 50)\n",
            "print(f'\\nКвантильные прогнозы:')\n",
            "print(f'  P10 (пессимистичный): {result[\"p10\"]*100:+.3f}%')\n",
            "print(f'  P50 (медиана):        {result[\"p50\"]*100:+.3f}%')\n",
            "print(f'  P90 (оптимистичный):  {result[\"p90\"]*100:+.3f}%')\n",
            "print(f'  Ширина интервала:     {result[\"interval_width\"]*100:.3f}%')\n",
            "print(f'\\nФактическая доходность: {actual_return*100:+.3f}%')\n",
            "\n",
            "print(f'\\nТоп-5 важных признаков:')\n",
            "sorted_features = sorted(result['feature_importance'].items(), key=lambda x: x[1], reverse=True)\n",
            "for feat, imp in sorted_features[:5]:\n",
            "    print(f'  {feat}: {imp:.3f}')\n",
            "\n",
            "# Сигнал для трейдинга\n",
            "print(f'\\nТорговый сигнал:')\n",
            "if result['p10'] > 0:\n",
            "    print('  STRONG BUY - даже пессимистичный сценарий положительный')\n",
            "elif result['p50'] > 0 and result['p10'] < 0:\n",
            "    print('  BUY - медиана положительная, но есть риск')\n",
            "elif result['p50'] < 0 and result['p90'] > 0:\n",
            "    print('  HOLD - неопределённость высока')\n",
            "elif result['p90'] < 0:\n",
            "    print('  SELL - даже оптимистичный сценарий отрицательный')\n",
            "else:\n",
            "    print('  NEUTRAL')"
        ],
        "execution_count": None,
        "outputs": []
    })

    # Cell 32: Summary
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## Заключение\n",
            "\n",
            "### Что мы реализовали:\n",
            "\n",
            "1. **Temporal Fusion Transformer** для прогнозирования финансовых временных рядов\n",
            "2. **Variable Selection Network** для автоматического отбора важных индикаторов\n",
            "3. **Quantile Forecasting** для оценки неопределённости и рисков\n",
            "4. **Interpretable Attention** для понимания, на что смотрит модель\n",
            "\n",
            "### Преимущества TFT для финансов:\n",
            "\n",
            "- **Интерпретируемость**: понимаем, какие факторы влияют на прогноз\n",
            "- **Оценка рисков**: квантильные прогнозы дают границы неопределённости\n",
            "- **Гибкость**: работает с разными типами признаков (статические, временные)\n",
            "- **Устойчивость**: attention механизм помогает находить важные паттерны в шуме\n",
            "\n",
            "### Рекомендации по применению:\n",
            "\n",
            "1. Используйте больше данных (годы истории)\n",
            "2. Добавьте внешние факторы (макроэкономика, новости)\n",
            "3. Калибруйте квантили на out-of-sample данных\n",
            "4. Регулярно переобучайте модель (концепт дрейфа)\n",
            "5. Комбинируйте с традиционным техническим анализом"
        ]
    })

    # Create notebook structure
    notebook = {
        "nbformat": 4,
        "nbformat_minor": 4,
        "metadata": {
            "kernelspec": {
                "display_name": "Python 3",
                "language": "python",
                "name": "python3"
            },
            "language_info": {
                "name": "python",
                "version": "3.8.0"
            }
        },
        "cells": cells
    }

    return notebook

if __name__ == "__main__":
    notebook = create_notebook()
    output_path = "/home/user/test/notebooks/phase4_transformers/bonus_financial_tft.ipynb"

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(notebook, f, ensure_ascii=False, indent=1)

    print(f"Notebook created: {output_path}")
    print(f"Total cells: {len(notebook['cells'])}")

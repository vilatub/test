{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Multi-Layer Perceptron (MLP) –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "**Phase 2: Deep Learning Basics - Step 1**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ –¶–µ–ª–∏ –Ω–æ—É—Ç–±—É–∫–∞\n",
    "\n",
    "1. **–ü–æ–Ω—è—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É MLP** –∏ backpropagation\n",
    "2. **–†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å MLP –Ω–∞ PyTorch** –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "3. **–°—Ä–∞–≤–Ω–∏—Ç—å —Å –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏** (XGBoost –∏–∑ Phase 1)\n",
    "4. **–ò–∑—É—á–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏:** dropout, batch normalization, –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã\n",
    "5. **–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã:** learning rate, batch size, epochs, architecture\n",
    "\n",
    "---\n",
    "\n",
    "## üíº –ë–∏–∑–Ω–µ—Å-–∑–∞–¥–∞—á–∞: Titanic Survival Prediction\n",
    "\n",
    "**–ö–æ–Ω—Ç–µ–∫—Å—Ç:** –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤—ã–∂–∏–≤–∞–µ–º–æ—Å—Ç–∏ –ø–∞—Å—Å–∞–∂–∏—Ä–æ–≤ –¢–∏—Ç–∞–Ω–∏–∫–∞.\n",
    "\n",
    "**–ü–æ—á–µ–º—É MLP –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö?**\n",
    "- üìä **–ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:** MLP –º–æ–∂–µ—Ç –∑–∞—Ö–≤–∞—Ç–∏—Ç—å —Å–ª–æ–∂–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã\n",
    "- üîó **Feature interactions:** –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —á–µ—Ä–µ–∑ —Å–∫—Ä—ã—Ç—ã–µ —Å–ª–æ–∏\n",
    "- üß™ **–ê–Ω—Å–∞–º–±–ª–∏:** MLP + Gradient Boosting —á–∞—Å—Ç–æ –ª—É—á—à–µ\n",
    "\n",
    "**–ß–µ—Å—Ç–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ:**\n",
    "- ‚ùå **MLP –æ–±—ã—á–Ω–æ –•–£–ñ–ï XGBoost** –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "- ‚úÖ **–ù–æ:** –ø–æ–ª–µ–∑–µ–Ω –≤ –∞–Ω—Å–∞–º–±–ª—è—Ö –∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è Deep Learning\n",
    "- ‚úÖ **–¶–µ–ª—å:** –ü–æ–Ω—è—Ç—å –æ—Å–Ω–æ–≤—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω—ã –¥–ª—è CNN, RNN, Transformers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö –ß–∞—Å—Ç—å 1: –¢–µ–æ—Ä–∏—è Multi-Layer Perceptron\n",
    "\n",
    "### 1.1 –ß—Ç–æ —Ç–∞–∫–æ–µ MLP?\n",
    "\n",
    "**Multi-Layer Perceptron (MLP)** ‚Äî —ç—Ç–æ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–∞—è (fully-connected) –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å.\n",
    "\n",
    "#### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞\n",
    "\n",
    "**–°–ª–æ–∏:**\n",
    "1. **Input Layer:** –í—Ö–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, Age, Sex, Fare –¥–ª—è –¢–∏—Ç–∞–Ω–∏–∫–∞)\n",
    "2. **Hidden Layers:** –û–¥–∏–Ω –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ–µ–≤\n",
    "3. **Output Layer:** –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π (1 –Ω–µ–π—Ä–æ–Ω –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏)\n",
    "\n",
    "**–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏:**\n",
    "\n",
    "–î–ª—è —Å–ª–æ—è $l$:\n",
    "\n",
    "$$z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}$$\n",
    "\n",
    "$$a^{(l)} = \\sigma(z^{(l)})$$\n",
    "\n",
    "–≥–¥–µ:\n",
    "- $W^{(l)}$ ‚Äî –º–∞—Ç—Ä–∏—Ü–∞ –≤–µ—Å–æ–≤ —Å–ª–æ—è $l$\n",
    "- $b^{(l)}$ ‚Äî –≤–µ–∫—Ç–æ—Ä bias\n",
    "- $a^{(l-1)}$ ‚Äî –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–ª–æ—è\n",
    "- $\\sigma$ ‚Äî —Ñ—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏\n",
    "\n",
    "**–ü–æ–ª–Ω–∞—è forward pass:**\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a^{(0)} &= x \\quad \\text{(input)} \\\\\n",
    "z^{(1)} &= W^{(1)} a^{(0)} + b^{(1)} \\\\\n",
    "a^{(1)} &= \\sigma(z^{(1)}) \\\\\n",
    "z^{(2)} &= W^{(2)} a^{(1)} + b^{(2)} \\\\\n",
    "a^{(2)} &= \\sigma(z^{(2)}) \\quad \\text{(output)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 –§—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏\n",
    "\n",
    "**–ó–∞—á–µ–º –Ω—É–∂–Ω—ã?** –ë–µ–∑ –Ω–µ–ª–∏–Ω–µ–π–Ω–æ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏, –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω–∞—è —Å–µ—Ç—å = –ª–∏–Ω–µ–π–Ω–∞—è –º–æ–¥–µ–ª—å!\n",
    "\n",
    "#### 1.2.1 Sigmoid\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "**–°–≤–æ–π—Å—Ç–≤–∞:**\n",
    "- –í—ã—Ö–æ–¥: $(0, 1)$\n",
    "- ‚úÖ –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫–∞–∫ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å\n",
    "- ‚ùå **Vanishing gradient** –¥–ª—è –±–æ–ª—å—à–∏—Ö $|x|$\n",
    "- ‚ùå **Not zero-centered** (–≤—Å–µ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã)\n",
    "\n",
    "**–ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è:**\n",
    "\n",
    "$$\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.2.2 Tanh\n",
    "\n",
    "$$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "\n",
    "**–°–≤–æ–π—Å—Ç–≤–∞:**\n",
    "- –í—ã—Ö–æ–¥: $(-1, 1)$\n",
    "- ‚úÖ **Zero-centered**\n",
    "- ‚ùå **Vanishing gradient** –¥–ª—è –±–æ–ª—å—à–∏—Ö $|x|$\n",
    "\n",
    "**–ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è:**\n",
    "\n",
    "$$\\tanh'(x) = 1 - \\tanh^2(x)$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.2.3 ReLU (Rectified Linear Unit) ‚≠ê\n",
    "\n",
    "$$\\text{ReLU}(x) = \\max(0, x)$$\n",
    "\n",
    "**–°–≤–æ–π—Å—Ç–≤–∞:**\n",
    "- ‚úÖ **–ü—Ä–æ—Å—Ç–æ—Ç–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π**\n",
    "- ‚úÖ **–ù–µ—Ç vanishing gradient** –¥–ª—è $x > 0$\n",
    "- ‚úÖ **Sparse activations** (–º–Ω–æ–≥–∏–µ –Ω–µ–π—Ä–æ–Ω—ã = 0)\n",
    "- ‚ùå **Dying ReLU** –ø—Ä–æ–±–ª–µ–º–∞ (–Ω–µ–π—Ä–æ–Ω—ã \"—É–º–∏—Ä–∞—é—Ç\" –ø—Ä–∏ $x \\leq 0$)\n",
    "\n",
    "**–ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è:**\n",
    "\n",
    "$$\\text{ReLU}'(x) = \\begin{cases} 1, & \\text{if } x > 0 \\\\ 0, & \\text{if } x \\leq 0 \\end{cases}$$\n",
    "\n",
    "**–ü–æ—á–µ–º—É ReLU —Å—Ç–∞–ª —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–º?**\n",
    "- Faster convergence (–¥–æ 6x –±—ã—Å—Ç—Ä–µ–µ, —á–µ–º sigmoid/tanh)\n",
    "- Simplicity\n",
    "- –ë–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω–æ\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.2.4 Leaky ReLU\n",
    "\n",
    "$$\\text{LeakyReLU}(x) = \\max(0.01x, x)$$\n",
    "\n",
    "**–†–µ—à–∞–µ—Ç dying ReLU:**\n",
    "- –ú–∞–ª—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç (0.01) –¥–ª—è $x < 0$ –≤–º–µ—Å—Ç–æ 0\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.2.5 ELU (Exponential Linear Unit)\n",
    "\n",
    "$$\\text{ELU}(x) = \\begin{cases} x, & \\text{if } x > 0 \\\\ \\alpha(e^x - 1), & \\text{if } x \\leq 0 \\end{cases}$$\n",
    "\n",
    "**–°–≤–æ–π—Å—Ç–≤–∞:**\n",
    "- ‚úÖ Smooth –≤–µ–∑–¥–µ\n",
    "- ‚úÖ Mean activations –±–ª–∏–∂–µ –∫ –Ω—É–ª—é\n",
    "- ‚ùå –ú–µ–¥–ª–µ–Ω–Ω–µ–µ ReLU (—ç–∫—Å–ø–æ–Ω–µ–Ω—Ç–∞)\n",
    "\n",
    "---\n",
    "\n",
    "#### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–π\n",
    "\n",
    "| –ê–∫—Ç–∏–≤–∞—Ü–∏—è | Vanishing Gradient | Dying Units | –°–∫–æ—Ä–æ—Å—Ç—å | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |\n",
    "|-----------|-------------------|-------------|----------|-------------|\n",
    "| **Sigmoid** | ‚ùå –î–∞ | ‚úÖ –ù–µ—Ç | Slow | –¢–æ–ª—å–∫–æ output –¥–ª—è binary |\n",
    "| **Tanh** | ‚ùå –î–∞ | ‚úÖ –ù–µ—Ç | Slow | RNN (–∏–Ω–æ–≥–¥–∞) |\n",
    "| **ReLU** | ‚úÖ –ù–µ—Ç | ‚ùå –î–∞ | ‚ö° Fast | ‚≠ê –°—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è hidden |\n",
    "| **Leaky ReLU** | ‚úÖ –ù–µ—Ç | ‚úÖ –ù–µ—Ç | ‚ö° Fast | –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ ReLU |\n",
    "| **ELU** | ‚úÖ –ù–µ—Ç | ‚úÖ –ù–µ—Ç | Slower | –ö–æ–≥–¥–∞ –Ω—É–∂–Ω–∞ –≥–ª–∞–¥–∫–æ—Å—Ç—å |\n",
    "\n",
    "**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:**\n",
    "1. **Default:** ReLU –¥–ª—è –≤—Å–µ—Ö —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ–µ–≤\n",
    "2. **–ï—Å–ª–∏ dying ReLU –ø—Ä–æ–±–ª–µ–º–∞:** Leaky ReLU –∏–ª–∏ ELU\n",
    "3. **Output layer (binary):** Sigmoid\n",
    "4. **Output layer (multiclass):** Softmax\n",
    "5. **Output layer (regression):** Linear (no activation)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Backpropagation –∏ Gradient Descent\n",
    "\n",
    "#### Loss Function\n",
    "\n",
    "**Binary Cross-Entropy (–¥–ª—è classification):**\n",
    "\n",
    "$$L = -\\frac{1}{N} \\sum_{i=1}^N \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]$$\n",
    "\n",
    "–≥–¥–µ:\n",
    "- $y_i$ ‚Äî –∏—Å—Ç–∏–Ω–Ω–∞—è –º–µ—Ç–∫–∞ (0 –∏–ª–∏ 1)\n",
    "- $\\hat{y}_i$ ‚Äî –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å\n",
    "\n",
    "**Mean Squared Error (–¥–ª—è regression):**\n",
    "\n",
    "$$L = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Backpropagation Algorithm\n",
    "\n",
    "**–¶–µ–ª—å:** –í—ã—á–∏—Å–ª–∏—Ç—å $\\frac{\\partial L}{\\partial W^{(l)}}$ –¥–ª—è –≤—Å–µ—Ö —Å–ª–æ–µ–≤.\n",
    "\n",
    "**Chain rule:**\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W^{(l)}} = \\frac{\\partial L}{\\partial a^{(L)}} \\cdot \\frac{\\partial a^{(L)}}{\\partial z^{(L)}} \\cdot \\ldots \\cdot \\frac{\\partial z^{(l)}}{\\partial W^{(l)}}$$\n",
    "\n",
    "**–ê–ª–≥–æ—Ä–∏—Ç–º:**\n",
    "\n",
    "1. **Forward pass:** –í—ã—á–∏—Å–ª–∏—Ç—å $a^{(l)}$ –¥–ª—è –≤—Å–µ—Ö —Å–ª–æ–µ–≤\n",
    "2. **Compute loss:** $L = \\text{loss}(a^{(L)}, y)$\n",
    "3. **Backward pass:** \n",
    "   - –í—ã—á–∏—Å–ª–∏—Ç—å $\\delta^{(L)} = \\frac{\\partial L}{\\partial z^{(L)}}$\n",
    "   - –î–ª—è $l = L-1, L-2, \\ldots, 1$:\n",
    "     $$\\delta^{(l)} = (W^{(l+1)})^T \\delta^{(l+1)} \\odot \\sigma'(z^{(l)})$$\n",
    "   - –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤–µ—Å–æ–≤:\n",
    "     $$\\frac{\\partial L}{\\partial W^{(l)}} = \\delta^{(l)} (a^{(l-1)})^T$$\n",
    "\n",
    "4. **Update weights:**\n",
    "   $$W^{(l)} \\leftarrow W^{(l)} - \\eta \\frac{\\partial L}{\\partial W^{(l)}}$$\n",
    "\n",
    "–≥–¥–µ $\\eta$ ‚Äî learning rate.\n",
    "\n",
    "---\n",
    "\n",
    "#### Gradient Descent –≤–∞—Ä–∏–∞–Ω—Ç—ã\n",
    "\n",
    "**1. Batch Gradient Descent:**\n",
    "- –ò—Å–ø–æ–ª—å–∑—É–µ–º **–≤—Å–µ** –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞\n",
    "- ‚úÖ Stable convergence\n",
    "- ‚ùå Slow –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "**2. Stochastic Gradient Descent (SGD):**\n",
    "- –ò—Å–ø–æ–ª—å–∑—É–µ–º **–æ–¥–∏–Ω** –ø—Ä–∏–º–µ—Ä –∑–∞ —Ä–∞–∑\n",
    "- ‚úÖ Fast updates\n",
    "- ‚ùå Noisy gradient, –º–æ–∂–µ—Ç –Ω–µ —Å–æ–π—Ç–∏—Å—å\n",
    "\n",
    "**3. Mini-Batch Gradient Descent:** ‚≠ê\n",
    "- –ò—Å–ø–æ–ª—å–∑—É–µ–º **batch** –ø—Ä–∏–º–µ—Ä–æ–≤ (–æ–±—ã—á–Ω–æ 32, 64, 128, 256)\n",
    "- ‚úÖ –ë–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
    "- ‚úÖ –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã\n",
    "\n",
    "#### 1.4.1 SGD (Stochastic Gradient Descent)\n",
    "\n",
    "$$W_{t+1} = W_t - \\eta \\nabla L(W_t)$$\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º—ã:**\n",
    "- ‚ùå –û–¥–∏–Ω–∞–∫–æ–≤—ã–π learning rate –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "- ‚ùå –ú–æ–∂–µ—Ç –∑–∞—Å—Ç—Ä—è—Ç—å –≤ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–∞—Ö\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.4.2 SGD with Momentum\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v_t &= \\beta v_{t-1} + \\nabla L(W_t) \\\\\n",
    "W_{t+1} &= W_t - \\eta v_t\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "–≥–¥–µ $\\beta$ (–æ–±—ã—á–Ω–æ 0.9) ‚Äî momentum coefficient.\n",
    "\n",
    "**–≠—Ñ—Ñ–µ–∫—Ç:**\n",
    "- ‚úÖ –£—Å–∫–æ—Ä–µ–Ω–∏–µ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏\n",
    "- ‚úÖ –ü–æ–¥–∞–≤–ª–µ–Ω–∏–µ –∫–æ–ª–µ–±–∞–Ω–∏–π\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.4.3 RMSprop (Root Mean Square Propagation)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E[g^2]_t &= \\beta E[g^2]_{t-1} + (1-\\beta) (\\nabla L)^2 \\\\\n",
    "W_{t+1} &= W_t - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} \\nabla L\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**–≠—Ñ—Ñ–µ–∫—Ç:**\n",
    "- ‚úÖ –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π learning rate –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞\n",
    "- ‚úÖ –ú–∞–ª—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å –±–æ–ª—å—à–∏–º–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.4.4 Adam (Adaptive Moment Estimation) ‚≠ê\n",
    "\n",
    "**–ö–æ–º–±–∏–Ω–∞—Ü–∏—è Momentum + RMSprop:**\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "m_t &= \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla L \\quad \\text{(1st moment, momentum)} \\\\\n",
    "v_t &= \\beta_2 v_{t-1} + (1 - \\beta_2) (\\nabla L)^2 \\quad \\text{(2nd moment, RMSprop)} \\\\\n",
    "\\hat{m}_t &= \\frac{m_t}{1 - \\beta_1^t} \\quad \\text{(bias correction)} \\\\\n",
    "\\hat{v}_t &= \\frac{v_t}{1 - \\beta_2^t} \\\\\n",
    "W_{t+1} &= W_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Default –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:**\n",
    "- $\\beta_1 = 0.9$\n",
    "- $\\beta_2 = 0.999$\n",
    "- $\\epsilon = 10^{-8}$\n",
    "\n",
    "**–ü–æ—á–µ–º—É Adam –ª—É—á—à–∏–π?**\n",
    "- ‚úÖ Adaptive learning rates\n",
    "- ‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç out-of-the-box –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∑–∞–¥–∞—á\n",
    "- ‚úÖ –ú–µ–Ω—å—à–µ tuning, —á–µ–º SGD\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.4.5 AdamW\n",
    "\n",
    "Adam + **decoupled weight decay:**\n",
    "\n",
    "$$W_{t+1} = W_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t - \\eta \\lambda W_t$$\n",
    "\n",
    "–≥–¥–µ $\\lambda$ ‚Äî weight decay coefficient.\n",
    "\n",
    "**–£–ª—É—á—à–µ–Ω–∏–µ:** –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (–≤ Adam –±—ã–ª–∞ –æ—à–∏–±–∫–∞).\n",
    "\n",
    "---\n",
    "\n",
    "#### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤\n",
    "\n",
    "| –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä | –°–∫–æ—Ä–æ—Å—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ | Tuning —Å–ª–æ–∂–Ω–æ—Å—Ç—å | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |\n",
    "|-------------|-------------------|------------------|-------------|\n",
    "| **SGD** | Slow | High (LR, momentum) | –ö–æ–≥–¥–∞ –Ω—É–∂–Ω–∞ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å |\n",
    "| **SGD + Momentum** | Medium | Medium | Computer Vision (–∏–Ω–æ–≥–¥–∞) |\n",
    "| **RMSprop** | Fast | Low | RNN (–∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏) |\n",
    "| **Adam** | ‚ö° Fast | ‚≠ê Very Low | **Default –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∑–∞–¥–∞—á** |\n",
    "| **AdamW** | ‚ö° Fast | ‚≠ê Very Low | Transformers, modern DL |\n",
    "\n",
    "**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:**\n",
    "1. **Start with Adam** (lr=0.001)\n",
    "2. –ï—Å–ª–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ ‚Üí AdamW (weight_decay=0.01)\n",
    "3. –î–ª—è CV –∏–Ω–æ–≥–¥–∞ SGD+Momentum –ª—É—á—à–µ (–Ω–æ –Ω—É–∂–µ–Ω tuning)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –∏ Batch Normalization\n",
    "\n",
    "#### 1.5.1 Dropout\n",
    "\n",
    "**–ò–¥–µ—è:** –°–ª—É—á–∞–π–Ω–æ \"–≤—ã–∫–ª—é—á–∞—Ç—å\" –Ω–µ–π—Ä–æ–Ω—ã –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è.\n",
    "\n",
    "**–ê–ª–≥–æ—Ä–∏—Ç–º (training):**\n",
    "```python\n",
    "for each neuron:\n",
    "    with probability p:\n",
    "        set activation to 0\n",
    "    else:\n",
    "        keep activation / (1-p)  # scaling\n",
    "```\n",
    "\n",
    "**–ü—Ä–∏ inference:** All neurons active (no dropout).\n",
    "\n",
    "**–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏:**\n",
    "\n",
    "$$a_i^{\\text{train}} = \\frac{1}{1-p} \\cdot a_i \\cdot r_i$$\n",
    "\n",
    "–≥–¥–µ $r_i \\sim \\text{Bernoulli}(1-p)$.\n",
    "\n",
    "**–ü–æ—á–µ–º—É —Ä–∞–±–æ—Ç–∞–µ—Ç?**\n",
    "- ‚úÖ **Ensemble —ç—Ñ—Ñ–µ–∫—Ç:** –û–±—É—á–∞–µ–º –º–Ω–æ–≥–æ \"–ø–æ–¥—Å–µ—Ç–µ–π\"\n",
    "- ‚úÖ **–ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ co-adaptation:** –ù–µ–π—Ä–æ–Ω—ã –Ω–µ –º–æ–≥—É—Ç –ø–æ–ª–∞–≥–∞—Ç—å—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥—Ä—É–≥–∏–µ\n",
    "- ‚úÖ **Regularization:** –£–º–µ–Ω—å—à–∞–µ—Ç overfitting\n",
    "\n",
    "**–¢–∏–ø–∏—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:**\n",
    "- $p = 0.2$ –¥–ª—è –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è\n",
    "- $p = 0.5$ –¥–ª—è —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ–µ–≤\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.5.2 L1/L2 Regularization\n",
    "\n",
    "**L2 (Weight Decay):**\n",
    "\n",
    "$$L_{\\text{total}} = L_{\\text{data}} + \\frac{\\lambda}{2} \\sum_i w_i^2$$\n",
    "\n",
    "**–≠—Ñ—Ñ–µ–∫—Ç:** –ú–∞–ª—ã–µ –≤–µ—Å–∞ ‚Üí –ø—Ä–æ—â–µ –º–æ–¥–µ–ª—å ‚Üí –º–µ–Ω—å—à–µ overfitting.\n",
    "\n",
    "**L1 (Lasso):**\n",
    "\n",
    "$$L_{\\text{total}} = L_{\\text{data}} + \\lambda \\sum_i |w_i|$$\n",
    "\n",
    "**–≠—Ñ—Ñ–µ–∫—Ç:** Sparse weights (–º–Ω–æ–≥–∏–µ = 0) ‚Üí feature selection.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.5.3 Batch Normalization\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º–∞:** Internal covariate shift ‚Äî —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–π –º–µ–Ω—è–µ—Ç—Å—è –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è.\n",
    "\n",
    "**–†–µ—à–µ–Ω–∏–µ:** –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ mini-batch.\n",
    "\n",
    "**–ê–ª–≥–æ—Ä–∏—Ç–º:**\n",
    "\n",
    "–î–ª—è batch $B = \\{x_1, \\ldots, x_m\\}$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu_B &= \\frac{1}{m} \\sum_{i=1}^m x_i \\quad \\text{(mean)} \\\\\n",
    "\\sigma_B^2 &= \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu_B)^2 \\quad \\text{(variance)} \\\\\n",
    "\\hat{x}_i &= \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\quad \\text{(normalize)} \\\\\n",
    "y_i &= \\gamma \\hat{x}_i + \\beta \\quad \\text{(scale and shift)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "–≥–¥–µ $\\gamma, \\beta$ ‚Äî learnable parameters.\n",
    "\n",
    "**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**\n",
    "- ‚úÖ **Faster training** (–º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª—å—à–∏–π learning rate)\n",
    "- ‚úÖ **Regularization —ç—Ñ—Ñ–µ–∫—Ç** (–º–µ–Ω—å—à–µ –Ω—É–∂–µ–Ω dropout)\n",
    "- ‚úÖ **–ú–µ–Ω—å—à–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏**\n",
    "\n",
    "**–ì–¥–µ –≤—Å—Ç–∞–≤–ª—è—Ç—å?**\n",
    "\n",
    "```\n",
    "Linear ‚Üí BatchNorm ‚Üí ReLU\n",
    "```\n",
    "\n",
    "(–∏–ª–∏ –∏–Ω–æ–≥–¥–∞: Linear ‚Üí ReLU ‚Üí BatchNorm)\n",
    "\n",
    "---\n",
    "\n",
    "#### 1.5.4 Early Stopping\n",
    "\n",
    "**–ò–¥–µ—è:** –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ, –∫–æ–≥–¥–∞ validation loss –ø–µ—Ä–µ—Å—Ç–∞–µ—Ç —É–ª—É—á—à–∞—Ç—å—Å—è.\n",
    "\n",
    "**–ê–ª–≥–æ—Ä–∏—Ç–º:**\n",
    "```python\n",
    "best_val_loss = infinity\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    train(...)\n",
    "    val_loss = validate(...)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        save_model()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        \n",
    "    if patience_counter >= patience:\n",
    "        break  # Stop training\n",
    "```\n",
    "\n",
    "**–¢–∏–ø–∏—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:**\n",
    "- `patience = 5-20` epochs\n",
    "\n",
    "---\n",
    "\n",
    "## –¢–µ–æ—Ä–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –ø—Ä–∞–∫—Ç–∏–∫–µ üöÄ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä –ß–∞—Å—Ç—å 2: –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "\n",
    "### 2.1 –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û—Å–Ω–æ–≤–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Baseline –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "# Seed\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "\n",
    "print('‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¢–∏—Ç–∞–Ω–∏–∫–∞\n",
    "import os\n",
    "\n",
    "data_path = '../../data/titanic_train.csv'\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    print('‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω! –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø—É—Ç—å...')\n",
    "    # –ü–æ–ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–æ–π –ø—É—Ç—å\n",
    "    data_path = '../titanic/titanic_train.csv'\n",
    "    if not os.path.exists(data_path):\n",
    "        print('‚ùå –î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Å Kaggle.')\n",
    "else:\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f'‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {df.shape[0]:,} —Å—Ç—Ä–æ–∫, {df.shape[1]} —Å—Ç–æ–ª–±—Ü–æ–≤')\n",
    "    print(f'Target: Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–µ—Ä–≤—ã–π –≤–∑–≥–ª—è–¥\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ø—Ä–æ—Å—Ç–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏)\n",
    "# –í—ã–±–µ—Ä–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "\n",
    "# –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏\n",
    "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
    "df['Fare'].fillna(df['Fare'].median(), inplace=True)\n",
    "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "df['Sex'] = (df['Sex'] == 'male').astype(int)\n",
    "df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n",
    "\n",
    "# One-hot –¥–ª—è Embarked\n",
    "df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)\n",
    "\n",
    "# –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "features = ['Pclass', 'Sex', 'Age', 'Fare', 'FamilySize', 'IsAlone'] + \\\n",
    "           [col for col in df.columns if 'Embarked_' in col]\n",
    "\n",
    "X = df[features].values\n",
    "y = df['Survived'].values\n",
    "\n",
    "print(f'Features: {features}')\n",
    "print(f'X shape: {X.shape}')\n",
    "print(f'y shape: {y.shape}')\n",
    "print(f'Survival rate: {y.mean():.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Val/Test split\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.2, random_state=RANDOM_STATE, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f'Train: {X_train.shape[0]:,} samples')\n",
    "print(f'Val: {X_val.shape[0]:,} samples')\n",
    "print(f'Test: {X_test.shape[0]:,} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ (–∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f'Scaled X_train mean: {X_train_scaled.mean():.4f}')\n",
    "print(f'Scaled X_train std: {X_train_scaled.std():.4f}')\n",
    "print('‚úÖ –î–∞–Ω–Ω—ã–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω—ã (mean=0, std=1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "\n",
    "X_val_tensor = torch.FloatTensor(X_val_scaled)\n",
    "y_val_tensor = torch.FloatTensor(y_val)\n",
    "\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "y_test_tensor = torch.FloatTensor(y_test)\n",
    "\n",
    "print(f'X_train_tensor shape: {X_train_tensor.shape}')\n",
    "print(f'y_train_tensor shape: {y_train_tensor.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞–µ–º DataLoaders –¥–ª—è mini-batch training\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f'Batch size: {batch_size}')\n",
    "print(f'Number of batches (train): {len(train_loader)}')\n",
    "print(f'Number of batches (val): {len(val_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ MLP –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–æ—Å—Ç–∞—è MLP –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1=64, hidden_dim2=32, dropout_rate=0.3):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim1)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim2)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_dim2, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Layer 1: Linear ‚Üí BatchNorm ‚Üí ReLU ‚Üí Dropout\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Layer 2: Linear ‚Üí BatchNorm ‚Üí ReLU ‚Üí Dropout\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Output: Linear ‚Üí Sigmoid\n",
    "        x = self.fc3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "model = SimpleMLP(input_dim=input_dim)\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)\n",
    "print(f'\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
    "print(f'Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss –∏ optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(f'Loss function: {criterion}')\n",
    "print(f'Optimizer: {optimizer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training —Ñ—É–Ω–∫—Ü–∏—è\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for X_batch, y_batch in loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device).unsqueeze(1)  # (batch_size, 1)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # –ú–µ—Ç—Ä–∏–∫–∏\n",
    "        total_loss += loss.item() * X_batch.size(0)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Validation —Ñ—É–Ω–∫—Ü–∏—è\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device).unsqueeze(1)\n",
    "            \n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            total_loss += loss.item() * X_batch.size(0)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "            \n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy, all_preds, all_labels\n",
    "\n",
    "print('‚úÖ Training functions –≥–æ—Ç–æ–≤—ã')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 100\n",
    "patience = 10\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "print('–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...')\n",
    "print('='*60)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, _, _ = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Print –∫–∞–∂–¥—ã–µ 10 —ç–ø–æ—Ö\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "        print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "        print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "        print(f'  Best Val Loss: {best_val_loss:.4f}, Patience: {patience_counter}/{patience}')\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= patience:\n",
    "        print(f'\\nEarly stopping –Ω–∞ epoch {epoch+1}')\n",
    "        break\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "print('='*60)\n",
    "print(f'‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!')\n",
    "print(f'Best validation loss: {best_val_loss:.4f}')\n",
    "print(f'Total epochs: {epoch+1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "ax1.plot(train_losses, label='Train Loss', alpha=0.8)\n",
    "ax1.plot(val_losses, label='Val Loss', alpha=0.8)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss (BCE)')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax2.plot(train_accs, label='Train Accuracy', alpha=0.8)\n",
    "ax2.plot(val_accs, label='Val Accuracy', alpha=0.8)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Final train accuracy: {train_accs[-1]:.4f}')\n",
    "print(f'Final val accuracy: {val_accs[-1]:.4f}')\n",
    "print(f'Overfitting: {(train_accs[-1] - val_accs[-1]):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 –û—Ü–µ–Ω–∫–∞ –Ω–∞ test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û—Ü–µ–Ω–∫–∞ –Ω–∞ test\n",
    "test_loss, test_acc, test_preds, test_labels = validate_epoch(\n",
    "    model, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "# –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ numpy\n",
    "test_preds_np = np.array(test_preds).flatten()\n",
    "test_labels_np = np.array(test_labels).flatten()\n",
    "test_preds_binary = (test_preds_np > 0.5).astype(int)\n",
    "\n",
    "# –ú–µ—Ç—Ä–∏–∫–∏\n",
    "test_precision = precision_score(test_labels_np, test_preds_binary)\n",
    "test_recall = recall_score(test_labels_np, test_preds_binary)\n",
    "test_f1 = f1_score(test_labels_np, test_preds_binary)\n",
    "test_roc_auc = roc_auc_score(test_labels_np, test_preds_np)\n",
    "\n",
    "print('üìä MLP Test Results:')\n",
    "print('='*50)\n",
    "print(f'  Test Loss: {test_loss:.4f}')\n",
    "print(f'  Test Accuracy: {test_acc:.4f}')\n",
    "print(f'  Precision: {test_precision:.4f}')\n",
    "print(f'  Recall: {test_recall:.4f}')\n",
    "print(f'  F1-score: {test_f1:.4f}')\n",
    "print(f'  ROC-AUC: {test_roc_auc:.4f}')\n",
    "print('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(test_labels_np, test_preds_binary)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('MLP Confusion Matrix (Test Set)')\n",
    "plt.show()\n",
    "\n",
    "print(f'True Negatives: {cm[0,0]}')\n",
    "print(f'False Positives: {cm[0,1]}')\n",
    "print(f'False Negatives: {cm[1,0]}')\n",
    "print(f'True Positives: {cm[1,1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å XGBoost (Phase 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
    "print('–û–±—É—á–∞–µ–º XGBoost –¥–ª—è —á–µ—Å—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è...')\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=4,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è XGBoost\n",
    "xgb_preds = xgb_model.predict(X_test_scaled)\n",
    "xgb_proba = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# –ú–µ—Ç—Ä–∏–∫–∏ XGBoost\n",
    "xgb_acc = accuracy_score(y_test, xgb_preds)\n",
    "xgb_precision = precision_score(y_test, xgb_preds)\n",
    "xgb_recall = recall_score(y_test, xgb_preds)\n",
    "xgb_f1 = f1_score(y_test, xgb_preds)\n",
    "xgb_roc_auc = roc_auc_score(y_test, xgb_proba)\n",
    "\n",
    "print('‚úÖ XGBoost –æ–±—É—á–µ–Ω')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['MLP (PyTorch)', 'XGBoost'],\n",
    "    'Accuracy': [test_acc, xgb_acc],\n",
    "    'Precision': [test_precision, xgb_precision],\n",
    "    'Recall': [test_recall, xgb_recall],\n",
    "    'F1-score': [test_f1, xgb_f1],\n",
    "    'ROC-AUC': [test_roc_auc, xgb_roc_auc]\n",
    "})\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('üèÜ –°–†–ê–í–ù–ï–ù–ò–ï: MLP vs XGBoost')\n",
    "print('='*70)\n",
    "print(comparison.to_string(index=False))\n",
    "print('='*70)\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "metrics = ['Accuracy', 'F1-score', 'ROC-AUC']\n",
    "for i, metric in enumerate(metrics):\n",
    "    axes[i].bar(['MLP', 'XGBoost'], comparison[metric], alpha=0.7, edgecolor='black')\n",
    "    axes[i].set_ylabel(metric)\n",
    "    axes[i].set_title(f'{metric} Comparison')\n",
    "    axes[i].set_ylim([0.7, 0.9])\n",
    "    axes[i].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "    for j, v in enumerate(comparison[metric]):\n",
    "        axes[i].text(j, v + 0.01, f'{v:.4f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ –í—ã–≤–æ–¥—ã\n",
    "\n",
    "### –ß—Ç–æ –º—ã –∏–∑—É—á–∏–ª–∏:\n",
    "\n",
    "1. **MLP –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:**\n",
    "   - –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏ (Linear)\n",
    "   - –§—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (ReLU, Sigmoid)\n",
    "   - Batch Normalization\n",
    "   - Dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏\n",
    "\n",
    "2. **–û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π:**\n",
    "   - Backpropagation –∏ gradient descent\n",
    "   - Adam optimizer\n",
    "   - Binary Cross-Entropy loss\n",
    "   - Early stopping\n",
    "   - Mini-batch training\n",
    "\n",
    "3. **PyTorch –æ—Å–Ω–æ–≤—ã:**\n",
    "   - `nn.Module` –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–∏\n",
    "   - `DataLoader` –¥–ª—è batch processing\n",
    "   - Training loop (forward ‚Üí loss ‚Üí backward ‚Üí update)\n",
    "   - GPU support\n",
    "\n",
    "### MLP vs XGBoost –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:\n",
    "\n",
    "**–¢–∏–ø–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:**\n",
    "- MLP: Accuracy ~78-82%, ROC-AUC ~0.82-0.85\n",
    "- XGBoost: Accuracy ~80-84%, ROC-AUC ~0.84-0.87\n",
    "\n",
    "**–í—ã–≤–æ–¥:** XGBoost –æ–±—ã—á–Ω–æ **–ª—É—á—à–µ** –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö!\n",
    "\n",
    "### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å MLP?\n",
    "\n",
    "‚úÖ **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ MLP –∫–æ–≥–¥–∞:**\n",
    "- –ú–Ω–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö (>100k –ø—Ä–∏–º–µ—Ä–æ–≤)\n",
    "- –°–ª–æ–∂–Ω—ã–µ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏\n",
    "- –ù—É–∂–µ–Ω ensemble —Å Gradient Boosting\n",
    "- Transfer learning (–ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–º–µ–∂–Ω–æ–π –∑–∞–¥–∞—á–µ)\n",
    "- –ò–∑—É—á–µ–Ω–∏–µ Deep Learning –æ—Å–Ω–æ–≤\n",
    "\n",
    "‚ùå **–ù–ï –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ MLP –∫–æ–≥–¥–∞:**\n",
    "- –ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö (<10k)\n",
    "- –ù—É–∂–Ω–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å\n",
    "- –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã (XGBoost –±—ã—Å—Ç—Ä–µ–µ)\n",
    "- Production —Å–∏—Å—Ç–µ–º–∞ (XGBoost –ø—Ä–æ—â–µ deploy)\n",
    "\n",
    "### –ö–ª—é—á–µ–≤—ã–µ —É—Ä–æ–∫–∏:\n",
    "\n",
    "1. **Scaling –∫—Ä–∏—Ç–∏—á–µ–Ω!** –ù–µ–π—Ä–æ—Å–µ—Ç–∏ —Ç—Ä–µ–±—É—é—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ (mean=0, std=1)\n",
    "2. **Batch Normalization** —É—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ\n",
    "3. **Dropout** –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç overfitting\n",
    "4. **Early stopping** —ç–∫–æ–Ω–æ–º–∏—Ç –≤—Ä–µ–º—è –∏ —É–ª—É—á—à–∞–µ—Ç –æ–±–æ–±—â–µ–Ω–∏–µ\n",
    "5. **Learning rate** - –∫—Ä–∏—Ç–∏—á–Ω—ã–π –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä\n",
    "6. **Adam optimizer** —Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ out-of-the-box\n",
    "\n",
    "### –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:\n",
    "\n",
    "1. **–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã:**\n",
    "   - –†–∞–∑–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã (–≥–ª—É–±–∂–µ, —à–∏—Ä–µ)\n",
    "   - –î—Ä—É–≥–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (ELU, LeakyReLU)\n",
    "   - Learning rate scheduling\n",
    "   - –†–∞–∑–Ω—ã–µ optimizers (SGD, AdamW)\n",
    "\n",
    "2. **1D-CNN:** Convolutional layers –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "3. **Autoencoders:** Unsupervised learning, anomaly detection\n",
    "4. **Transfer Learning:** –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥—Ä—É–≥–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Phase 2, Step 1 –∑–∞–≤–µ—Ä—à–µ–Ω!\n",
    "\n",
    "–í—ã –æ—Å–≤–æ–∏–ª–∏ –æ—Å–Ω–æ–≤—ã Deep Learning —Å MLP. –¢–µ–ø–µ—Ä—å –≥–æ—Ç–æ–≤—ã –∫ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º! üöÄ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
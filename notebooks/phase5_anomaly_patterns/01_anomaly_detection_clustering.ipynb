{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç Anomaly Detection, Clustering & Pattern Mining\n",
    "\n",
    "**Phase 5: Unsupervised Learning –¥–ª—è Real-World Problems**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ –ü—Ä–æ–±–ª–µ–º—ã Real World\n",
    "\n",
    "### –î–æ —Å–∏—Ö –ø–æ—Ä –º—ã —Ä–µ—à–∞–ª–∏ supervised tasks:\n",
    "\n",
    "- ‚úÖ **Classification**: –µ—Å—Ç—å labels (survived/not survived, >50K/<=50K)\n",
    "- ‚úÖ **Regression**: –µ—Å—Ç—å target (electricity consumption, price)\n",
    "- ‚úÖ **Time Series Forecasting**: –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º future values\n",
    "\n",
    "**–ù–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –º–∏—Ä–µ:**\n",
    "\n",
    "- ‚ùì **Fraud Detection**: 99.9% transactions –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ, 0.1% - fraud (–∫—Ä–∞–π–Ω–∏–π –¥–∏—Å–±–∞–ª–∞–Ω—Å!)\n",
    "- ‚ùì **Customer Segmentation**: –Ω–µ—Ç –∑–∞—Ä–∞–Ω–µ–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã—Ö –≥—Ä—É–ø–ø\n",
    "- ‚ùì **Network Intrusions**: –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Å–æ–±—ã—Ç–∏–π –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ, –∞–Ω–æ–º–∞–ª–∏–∏ —Ä–µ–¥–∫–∏\n",
    "- ‚ùì **Equipment Failures**: failure events —Ä–µ–¥–∫–∏, –Ω–æ –∫—Ä–∏—Ç–∏—á–Ω—ã\n",
    "- ‚ùì **New Attack Types**: –Ω–µ –≤–∏–¥–µ–ª–∏ —Ä–∞–Ω—å—à–µ, –Ω–µ—Ç labels\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º—ã:**\n",
    "1. **No labels** –∏–ª–∏ –æ—á–µ–Ω—å –º–∞–ª–æ labeled data\n",
    "2. **Class imbalance**: anomalies —Å–æ—Å—Ç–∞–≤–ª—è—é—Ç 0.001% - 1%\n",
    "3. **Novel patterns**: –Ω–æ–≤—ã–µ —Ç–∏–ø—ã –∞–Ω–æ–º–∞–ª–∏–π, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ –±—ã–ª–æ –≤ training\n",
    "4. **Interpretability**: –Ω—É–∂–Ω–æ –æ–±—ä—è—Å–Ω–∏—Ç—å, –ü–û–ß–ï–ú–£ —á—Ç–æ-—Ç–æ –∞–Ω–æ–º–∞–ª–∏—è\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Enter Unsupervised Learning\n",
    "\n",
    "### 1. Clustering (–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è)\n",
    "\n",
    "**–ó–∞–¥–∞—á–∞:** –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –±–µ–∑ labels\n",
    "\n",
    "**Use Cases:**\n",
    "- üõí **Customer Segmentation**: –≥—Ä—É–ø–ø—ã –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å –ø–æ—Ö–æ–∂–∏–º –ø–æ–≤–µ–¥–µ–Ω–∏–µ–º\n",
    "- üè• **Patient Stratification**: –≥—Ä—É–ø–ø—ã –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤ –¥–ª—è personalized treatment\n",
    "- üìÑ **Document Clustering**: —Ç–µ–º—ã –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤\n",
    "- üéµ **Music Recommendation**: –ø–æ—Ö–æ–∂–∏–µ –ø–µ—Å–Ω–∏/–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª–∏\n",
    "\n",
    "**–ú–µ—Ç–æ–¥—ã:**\n",
    "- **K-Means**: partition-based, fast, assumes spherical clusters\n",
    "- **DBSCAN**: density-based, finds arbitrary shapes, handles outliers\n",
    "- **Hierarchical**: creates dendrogram, no need to specify K\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Anomaly Detection (–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π)\n",
    "\n",
    "**–ó–∞–¥–∞—á–∞:** –ù–∞–π—Ç–∏ —Ä–µ–¥–∫–∏–µ, –Ω–µ–æ–±—ã—á–Ω—ã–µ observations\n",
    "\n",
    "**Use Cases:**\n",
    "- üí≥ **Fraud Detection**: –Ω–µ–æ–±—ã—á–Ω—ã–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏\n",
    "- üè≠ **Predictive Maintenance**: –∞–Ω–æ–º–∞–ª—å–Ω—ã–µ sensor readings ‚Üí failure prediction\n",
    "- üîí **Cybersecurity**: intrusion detection, DDoS attacks\n",
    "- üè• **Healthcare**: rare diseases, abnormal vitals\n",
    "- üìä **Finance**: market manipulation, insider trading\n",
    "\n",
    "**–ú–µ—Ç–æ–¥—ã:**\n",
    "- **Isolation Forest**: isolate anomalies —á–µ—Ä–µ–∑ random partitioning\n",
    "- **LOF (Local Outlier Factor)**: density-based, –ª–æ–∫–∞–ª—å–Ω—ã–µ outliers\n",
    "- **One-Class SVM**: learn boundary of \"normal\" data\n",
    "- **Autoencoders**: reconstruction error –¥–ª—è –∞–Ω–æ–º–∞–ª–∏–π\n",
    "- **Statistical**: Z-score, IQR, Mahalanobis distance\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Pattern Mining\n",
    "\n",
    "**–ó–∞–¥–∞—á–∞:** –ù–∞–π—Ç–∏ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏\n",
    "\n",
    "**Use Cases:**\n",
    "- üõí **Market Basket Analysis**: \"–ª—é–¥–∏, –∫—É–ø–∏–≤—à–∏–µ X, —Ç–∞–∫–∂–µ –ø–æ–∫—É–ø–∞—é—Ç Y\"\n",
    "- üìä **Feature Engineering**: automatic feature interactions\n",
    "- üîó **Recommendation**: association rules\n",
    "\n",
    "**–ú–µ—Ç–æ–¥—ã:**\n",
    "- **Apriori**: frequent itemsets\n",
    "- **FP-Growth**: faster alternative\n",
    "\n",
    "---\n",
    "\n",
    "## üìä –ß—Ç–æ –º—ã —Ä–µ–∞–ª–∏–∑—É–µ–º\n",
    "\n",
    "### Dataset: Credit Card Transactions (Synthetic)\n",
    "\n",
    "**–ü–æ—á–µ–º—É credit card fraud?**\n",
    "- ‚úÖ Real-world problem (–º–∏–ª–ª–∏–∞—Ä–¥—ã —É–±—ã—Ç–∫–æ–≤ –µ–∂–µ–≥–æ–¥–Ω–æ)\n",
    "- ‚úÖ Extreme class imbalance (~0.1% fraud)\n",
    "- ‚úÖ Unlabeled data –≤ production (–Ω–æ–≤—ã–µ fraud patterns)\n",
    "- ‚úÖ –ù—É–∂–Ω–∞ interpretability (–æ–±—ä—è—Å–Ω–∏—Ç—å –∫–ª–∏–µ–Ω—Ç—É)\n",
    "\n",
    "**–°–æ–∑–¥–∞–¥–∏–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ:**\n",
    "- ~50,000 transactions\n",
    "- ~0.2% fraud (realistic ratio)\n",
    "- Features: amount, time, merchant category, location, etc.\n",
    "- PCA-transformed features (–∫–∞–∫ –≤ real Kaggle dataset)\n",
    "\n",
    "**–ó–∞–¥–∞—á–∏:**\n",
    "1. **Clustering**: –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ transactions (normal spending patterns)\n",
    "2. **Anomaly Detection**: –ù–∞–π—Ç–∏ fraud –±–µ–∑ labels\n",
    "3. **Comparison**: Multiple methods (Isolation Forest, LOF, etc.)\n",
    "4. **Visualization**: t-SNE/UMAP –¥–ª—è interpretability\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª –ß–∞—Å—Ç—å 1: Setup –∏ Dataset\n",
    "\n",
    "### 1.1 –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ë–∞–∑–æ–≤—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn - Clustering\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "# Sklearn - Anomaly Detection\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "# Sklearn - Preprocessing & Metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, average_precision_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Scipy\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# PyTorch (–¥–ª—è Autoencoders)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"\\n‚úÖ –í—Å–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 –°–æ–∑–¥–∞–Ω–∏–µ Synthetic Credit Card Dataset\n",
    "\n",
    "**Realistic fraud detection scenario:**\n",
    "- 50,000 transactions\n",
    "- ~0.2% fraud rate (100 fraud –∏–∑ 50,000)\n",
    "- PCA-transformed features (V1-V10)\n",
    "- Amount, Time features\n",
    "- Fraud patterns: unusual amounts, unusual times, specific feature combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fraud_dataset(n_samples=50000, fraud_rate=0.002):\n",
    "    \"\"\"\n",
    "    Create synthetic credit card fraud dataset\n",
    "    Similar to Kaggle Credit Card Fraud dataset structure\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    n_fraud = int(n_samples * fraud_rate)\n",
    "    n_normal = n_samples - n_fraud\n",
    "    \n",
    "    print(f\"Creating dataset: {n_samples:,} transactions\")\n",
    "    print(f\"  Normal: {n_normal:,} ({n_normal/n_samples*100:.2f}%)\")\n",
    "    print(f\"  Fraud:  {n_fraud:,} ({n_fraud/n_samples*100:.2f}%)\")\n",
    "    \n",
    "    # Normal transactions\n",
    "    normal_data = np.random.randn(n_normal, 10) * 2  # 10 PCA features\n",
    "    normal_time = np.random.uniform(0, 172800, n_normal)  # 48 hours in seconds\n",
    "    normal_amount = np.random.lognormal(3, 1.5, n_normal)  # log-normal distribution\n",
    "    normal_amount = np.clip(normal_amount, 1, 1000)\n",
    "    \n",
    "    # Fraud transactions (different patterns)\n",
    "    fraud_data = np.random.randn(n_fraud, 10) * 3 + 5  # shifted mean, higher variance\n",
    "    # Some fraud features more extreme\n",
    "    fraud_data[:, 0] += np.random.uniform(3, 8, n_fraud)  # V1 extreme\n",
    "    fraud_data[:, 3] += np.random.uniform(-8, -3, n_fraud)  # V4 extreme\n",
    "    \n",
    "    fraud_time = np.random.uniform(0, 172800, n_fraud)\n",
    "    # Fraud tends to have unusual amounts\n",
    "    fraud_amount = np.concatenate([\n",
    "        np.random.uniform(0.5, 5, n_fraud//2),  # very small\n",
    "        np.random.uniform(500, 5000, n_fraud//2)  # very large\n",
    "    ])\n",
    "    np.random.shuffle(fraud_amount)\n",
    "    \n",
    "    # Combine\n",
    "    X = np.vstack([normal_data, fraud_data])\n",
    "    time = np.concatenate([normal_time, fraud_time])\n",
    "    amount = np.concatenate([normal_amount, fraud_amount])\n",
    "    y = np.concatenate([np.zeros(n_normal), np.ones(n_fraud)])\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(X, columns=[f'V{i+1}' for i in range(10)])\n",
    "    df['Time'] = time\n",
    "    df['Amount'] = amount\n",
    "    df['Class'] = y.astype(int)\n",
    "    \n",
    "    # Shuffle\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset created: {len(df):,} rows, {len(df.columns)} columns\")\n",
    "    return df\n",
    "\n",
    "# Create dataset\n",
    "df = create_fraud_dataset(n_samples=50000, fraud_rate=0.002)\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Dataset Statistics:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total transactions: {len(df):,}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['Class'].value_counts())\n",
    "print(f\"\\nFraud rate: {df['Class'].mean()*100:.3f}%\")\n",
    "print(\"\\nAmount statistics:\")\n",
    "print(df.groupby('Class')['Amount'].describe())\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# 1. Class distribution\n",
    "df['Class'].value_counts().plot(kind='bar', ax=axes[0, 0], color=['steelblue', 'red'])\n",
    "axes[0, 0].set_title('Class Distribution (Extreme Imbalance!)', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Class (0=Normal, 1=Fraud)')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].set_xticklabels(['Normal', 'Fraud'], rotation=0)\n",
    "\n",
    "# 2. Amount distribution\n",
    "axes[0, 1].hist(df[df['Class']==0]['Amount'], bins=50, alpha=0.6, label='Normal', color='steelblue')\n",
    "axes[0, 1].hist(df[df['Class']==1]['Amount'], bins=50, alpha=0.6, label='Fraud', color='red')\n",
    "axes[0, 1].set_title('Amount Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Amount')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].set_yscale('log')\n",
    "\n",
    "# 3. Time distribution\n",
    "axes[1, 0].hist(df[df['Class']==0]['Time'], bins=50, alpha=0.6, label='Normal', color='steelblue')\n",
    "axes[1, 0].hist(df[df['Class']==1]['Time'], bins=50, alpha=0.6, label='Fraud', color='red')\n",
    "axes[1, 0].set_title('Time Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Time (seconds)')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4. V1 vs V4 (PCA features)\n",
    "normal = df[df['Class']==0].sample(1000, random_state=42)\n",
    "fraud = df[df['Class']==1]\n",
    "axes[1, 1].scatter(normal['V1'], normal['V4'], alpha=0.3, s=10, label='Normal', color='steelblue')\n",
    "axes[1, 1].scatter(fraud['V1'], fraud['V4'], alpha=0.7, s=30, label='Fraud', color='red', edgecolors='black')\n",
    "axes[1, 1].set_title('V1 vs V4 (sample)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('V1')\n",
    "axes[1, 1].set_ylabel('V4')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Key Observations:\")\n",
    "print(\"  - Extreme class imbalance (~0.2% fraud) - realistic!\")\n",
    "print(\"  - Fraud has different amount distribution (very small or very large)\")\n",
    "print(\"  - Fraud clearly separable in some feature dimensions (V1, V4)\")\n",
    "print(\"  - Perfect use case for anomaly detection!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç –ß–∞—Å—Ç—å 2: Clustering Methods\n",
    "\n",
    "### 2.1 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for clustering\n",
    "# Use subset for computational efficiency\n",
    "sample_size = 10000\n",
    "df_sample = df.sample(sample_size, random_state=42).copy()\n",
    "\n",
    "feature_cols = [f'V{i+1}' for i in range(10)] + ['Time', 'Amount']\n",
    "X = df_sample[feature_cols].values\n",
    "y_true = df_sample['Class'].values\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"Sample for clustering: {X_scaled.shape}\")\n",
    "print(f\"Features: {feature_cols}\")\n",
    "print(f\"True fraud in sample: {y_true.sum()} ({y_true.mean()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means with different k values\n",
    "k_values = [2, 3, 5, 8]\n",
    "kmeans_results = {}\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # Metrics\n",
    "    silhouette = silhouette_score(X_scaled, labels)\n",
    "    calinski = calinski_harabasz_score(X_scaled, labels)\n",
    "    davies = davies_bouldin_score(X_scaled, labels)\n",
    "    \n",
    "    kmeans_results[k] = {\n",
    "        'labels': labels,\n",
    "        'silhouette': silhouette,\n",
    "        'calinski': calinski,\n",
    "        'davies': davies,\n",
    "        'centers': kmeans.cluster_centers_\n",
    "    }\n",
    "    \n",
    "    print(f\"K={k}: Silhouette={silhouette:.3f}, Calinski-Harabasz={calinski:.1f}, Davies-Bouldin={davies:.3f}\")\n",
    "\n",
    "print(\"\\nüìä Best K by Silhouette Score (higher is better):\")\n",
    "best_k = max(kmeans_results.keys(), key=lambda k: kmeans_results[k]['silhouette'])\n",
    "print(f\"  K={best_k} with Silhouette={kmeans_results[best_k]['silhouette']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize K-Means results (K=3)\n",
    "k = 3\n",
    "labels_km = kmeans_results[k]['labels']\n",
    "\n",
    "# PCA –¥–ª—è visualization\n",
    "pca_vis = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca_vis.fit_transform(X_scaled)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# K-Means clusters\n",
    "for cluster_id in range(k):\n",
    "    mask = labels_km == cluster_id\n",
    "    axes[0].scatter(X_pca[mask, 0], X_pca[mask, 1], \n",
    "                   alpha=0.5, s=20, label=f'Cluster {cluster_id}')\n",
    "axes[0].set_title(f'K-Means Clustering (K={k})', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('PC1')\n",
    "axes[0].set_ylabel('PC2')\n",
    "axes[0].legend()\n",
    "\n",
    "# True labels (fraud vs normal)\n",
    "axes[1].scatter(X_pca[y_true==0, 0], X_pca[y_true==0, 1], \n",
    "               alpha=0.3, s=10, label='Normal', color='steelblue')\n",
    "axes[1].scatter(X_pca[y_true==1, 0], X_pca[y_true==1, 1], \n",
    "               alpha=0.8, s=50, label='Fraud', color='red', edgecolors='black')\n",
    "axes[1].set_title('True Labels (Normal vs Fraud)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('PC1')\n",
    "axes[1].set_ylabel('PC2')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Fraud distribution across clusters\n",
    "print(\"\\nFraud distribution across K-Means clusters:\")\n",
    "for cluster_id in range(k):\n",
    "    mask = labels_km == cluster_id\n",
    "    n_total = mask.sum()\n",
    "    n_fraud = y_true[mask].sum()\n",
    "    fraud_rate = n_fraud / n_total if n_total > 0 else 0\n",
    "    print(f\"  Cluster {cluster_id}: {n_fraud}/{n_total} fraud ({fraud_rate*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üö® –ß–∞—Å—Ç—å 3: Anomaly Detection Methods\n",
    "\n",
    "### 3.1 Isolation Forest\n",
    "\n",
    "**–ü—Ä–∏–Ω—Ü–∏–ø:** Anomalies –ª–µ–≥—á–µ –∏–∑–æ–ª–∏—Ä–æ–≤–∞—Ç—å (–º–µ–Ω—å—à–µ splits –Ω—É–∂–Ω–æ)\n",
    "\n",
    "**–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç:**\n",
    "1. –°–ª—É—á–∞–π–Ω–æ –≤—ã–±–∏—Ä–∞–µ—Ç feature\n",
    "2. –°–ª—É—á–∞–π–Ω–æ –≤—ã–±–∏—Ä–∞–µ—Ç split value\n",
    "3. –ê–Ω–æ–º–∞–ª–∏–∏ –±—ã—Å—Ç—Ä–æ –∏–∑–æ–ª–∏—Ä—É—é—Ç—Å—è (–∫–æ—Ä–æ—Ç–∫–∏–π path)\n",
    "4. Normal points —Ç—Ä–µ–±—É—é—Ç –±–æ–ª—å—à–µ splits\n",
    "\n",
    "**Anomaly Score:** –°—Ä–µ–¥–Ω–µ–µ path length (–º–µ–Ω—å—à–µ = –±–æ–ª–µ–µ –∞–Ω–æ–º–∞–ª—å–Ω–æ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation Forest\n",
    "# contamination = expected fraud rate\n",
    "iso_forest = IsolationForest(\n",
    "    contamination=0.002,  # expected fraud rate\n",
    "    random_state=42,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "# Fit –Ω–∞ –≤—Å–µ–º –¥–∞—Ç–∞—Å–µ—Ç–µ (unsupervised!)\n",
    "X_full = df[feature_cols].values\n",
    "X_full_scaled = scaler.fit_transform(X_full)\n",
    "\n",
    "iso_predictions = iso_forest.fit_predict(X_full_scaled)\n",
    "iso_scores = iso_forest.score_samples(X_full_scaled)  # anomaly scores\n",
    "\n",
    "# Convert: -1 (anomaly) ‚Üí 1, 1 (normal) ‚Üí 0\n",
    "iso_predictions_binary = (iso_predictions == -1).astype(int)\n",
    "\n",
    "print(f\"Isolation Forest Results:\")\n",
    "print(f\"  Predicted anomalies: {iso_predictions_binary.sum():,} ({iso_predictions_binary.mean()*100:.3f}%)\")\n",
    "print(f\"  True fraud: {df['Class'].sum():,} ({df['Class'].mean()*100:.3f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Isolation Forest\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "y_true_full = df['Class'].values\n",
    "\n",
    "iso_precision = precision_score(y_true_full, iso_predictions_binary)\n",
    "iso_recall = recall_score(y_true_full, iso_predictions_binary)\n",
    "iso_f1 = f1_score(y_true_full, iso_predictions_binary)\n",
    "# Use anomaly scores for AUC (need to negate because lower score = more anomalous)\n",
    "iso_auc = roc_auc_score(y_true_full, -iso_scores)\n",
    "\n",
    "print(\"\\nIsolation Forest Performance:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"  Precision: {iso_precision:.4f}\")\n",
    "print(f\"  Recall:    {iso_recall:.4f}\")\n",
    "print(f\"  F1 Score:  {iso_f1:.4f}\")\n",
    "print(f\"  ROC AUC:   {iso_auc:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true_full, iso_predictions_binary)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "           xticklabels=['Normal', 'Fraud'],\n",
    "           yticklabels=['Normal', 'Fraud'])\n",
    "plt.title('Isolation Forest Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True')\n",
    "plt.xlabel('Predicted')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Interpretation:\")\n",
    "print(f\"  True Negatives (TN): {cm[0,0]:,} - correctly identified normal\")\n",
    "print(f\"  False Positives (FP): {cm[0,1]:,} - normal flagged as fraud\")\n",
    "print(f\"  False Negatives (FN): {cm[1,0]:,} - missed fraud\")\n",
    "print(f\"  True Positives (TP): {cm[1,1]:,} - correctly caught fraud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Local Outlier Factor (LOF)\n",
    "\n",
    "**–ü—Ä–∏–Ω—Ü–∏–ø:** Density-based anomaly detection\n",
    "\n",
    "**–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç:**\n",
    "1. –î–ª—è –∫–∞–∂–¥–æ–π —Ç–æ—á–∫–∏ –≤—ã—á–∏—Å–ª—è–µ—Ç –ª–æ–∫–∞–ª—å–Ω—É—é –ø–ª–æ—Ç–Ω–æ—Å—Ç—å (density)\n",
    "2. –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Å –ø–ª–æ—Ç–Ω–æ—Å—Ç—å—é —Å–æ—Å–µ–¥–µ–π\n",
    "3. LOF > 1: —Ç–æ—á–∫–∞ –≤ –º–µ–Ω–µ–µ –ø–ª–æ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏ ‚Üí outlier\n",
    "4. LOF ‚âà 1: –ø–æ—Ö–æ–∂–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å —Å —Å–æ—Å–µ–¥—è–º–∏ ‚Üí normal\n",
    "\n",
    "**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ:** –ù–∞—Ö–æ–¥–∏—Ç –ª–æ–∫–∞–ª—å–Ω—ã–µ outliers (–¥–∞–∂–µ –µ—Å–ª–∏ —Ä—è–¥–æ–º —Å –ø–ª–æ—Ç–Ω—ã–º –∫–ª–∞—Å—Ç–µ—Ä–æ–º)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOF (–Ω–∞ sample –∏–∑-–∑–∞ computational cost)\n",
    "sample_size_lof = 10000\n",
    "sample_indices = np.random.choice(len(X_full_scaled), sample_size_lof, replace=False)\n",
    "X_lof = X_full_scaled[sample_indices]\n",
    "y_lof = y_true_full[sample_indices]\n",
    "\n",
    "lof = LocalOutlierFactor(\n",
    "    n_neighbors=20,\n",
    "    contamination=0.002\n",
    ")\n",
    "\n",
    "lof_predictions = lof.fit_predict(X_lof)\n",
    "lof_scores = lof.negative_outlier_factor_  # higher = more normal\n",
    "\n",
    "# Convert: -1 ‚Üí 1, 1 ‚Üí 0\n",
    "lof_predictions_binary = (lof_predictions == -1).astype(int)\n",
    "\n",
    "print(f\"LOF Results (on {sample_size_lof:,} sample):\")\n",
    "print(f\"  Predicted anomalies: {lof_predictions_binary.sum():,}\")\n",
    "print(f\"  True fraud in sample: {y_lof.sum():,}\")\n",
    "\n",
    "# Evaluate\n",
    "lof_precision = precision_score(y_lof, lof_predictions_binary)\n",
    "lof_recall = recall_score(y_lof, lof_predictions_binary)\n",
    "lof_f1 = f1_score(y_lof, lof_predictions_binary)\n",
    "lof_auc = roc_auc_score(y_lof, -lof_scores)  # negate for AUC\n",
    "\n",
    "print(\"\\nLOF Performance:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"  Precision: {lof_precision:.4f}\")\n",
    "print(f\"  Recall:    {lof_recall:.4f}\")\n",
    "print(f\"  F1 Score:  {lof_f1:.4f}\")\n",
    "print(f\"  ROC AUC:   {lof_auc:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 One-Class SVM\n",
    "\n",
    "**–ü—Ä–∏–Ω—Ü–∏–ø:** Learn boundary of \"normal\" data\n",
    "\n",
    "**–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç:**\n",
    "1. Fit hypersphere –≤ feature space –≤–æ–∫—Ä—É–≥ normal data\n",
    "2. Points outside hypersphere ‚Üí anomalies\n",
    "3. nu parameter: expected fraction of anomalies\n",
    "\n",
    "**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ:** –†–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ –≤ high-dimensional spaces (kernel trick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Class SVM (–Ω–∞ sample)\n",
    "ocsvm = OneClassSVM(\n",
    "    kernel='rbf',\n",
    "    gamma='auto',\n",
    "    nu=0.002  # expected fraction of anomalies\n",
    ")\n",
    "\n",
    "ocsvm_predictions = ocsvm.fit_predict(X_lof)\n",
    "ocsvm_scores = ocsvm.score_samples(X_lof)\n",
    "\n",
    "# Convert: -1 ‚Üí 1, 1 ‚Üí 0\n",
    "ocsvm_predictions_binary = (ocsvm_predictions == -1).astype(int)\n",
    "\n",
    "print(f\"One-Class SVM Results:\")\n",
    "print(f\"  Predicted anomalies: {ocsvm_predictions_binary.sum():,}\")\n",
    "\n",
    "# Evaluate\n",
    "ocsvm_precision = precision_score(y_lof, ocsvm_predictions_binary)\n",
    "ocsvm_recall = recall_score(y_lof, ocsvm_predictions_binary)\n",
    "ocsvm_f1 = f1_score(y_lof, ocsvm_predictions_binary)\n",
    "ocsvm_auc = roc_auc_score(y_lof, -ocsvm_scores)\n",
    "\n",
    "print(\"\\nOne-Class SVM Performance:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"  Precision: {ocsvm_precision:.4f}\")\n",
    "print(f\"  Recall:    {ocsvm_recall:.4f}\")\n",
    "print(f\"  F1 Score:  {ocsvm_f1:.4f}\")\n",
    "print(f\"  ROC AUC:   {ocsvm_auc:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Method Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all methods\n",
    "results = pd.DataFrame({\n",
    "    'Method': ['Isolation Forest', 'LOF', 'One-Class SVM'],\n",
    "    'Precision': [iso_precision, lof_precision, ocsvm_precision],\n",
    "    'Recall': [iso_recall, lof_recall, ocsvm_recall],\n",
    "    'F1 Score': [iso_f1, lof_f1, ocsvm_f1],\n",
    "    'ROC AUC': [iso_auc, lof_auc, ocsvm_auc]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANOMALY DETECTION METHODS COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(results.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Precision/Recall/F1\n",
    "metrics = ['Precision', 'Recall', 'F1 Score']\n",
    "x = np.arange(len(results))\n",
    "width = 0.25\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    axes[0].bar(x + i*width, results[metric], width, label=metric)\n",
    "\n",
    "axes[0].set_xlabel('Method')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_title('Precision, Recall, F1 Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x + width)\n",
    "axes[0].set_xticklabels(results['Method'], rotation=15)\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# ROC AUC\n",
    "axes[1].bar(results['Method'], results['ROC AUC'], color=['steelblue', 'orange', 'green'])\n",
    "axes[1].set_ylabel('ROC AUC')\n",
    "axes[1].set_title('ROC AUC Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticklabels(results['Method'], rotation=15)\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "for i, v in enumerate(results['ROC AUC']):\n",
    "    axes[1].text(i, v + 0.01, f\"{v:.3f}\", ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Key Findings:\")\n",
    "best_auc_idx = results['ROC AUC'].idxmax()\n",
    "best_f1_idx = results['F1 Score'].idxmax()\n",
    "print(f\"  Best ROC AUC: {results.loc[best_auc_idx, 'Method']} ({results.loc[best_auc_idx, 'ROC AUC']:.3f})\")\n",
    "print(f\"  Best F1: {results.loc[best_f1_idx, 'Method']} ({results.loc[best_f1_idx, 'F1 Score']:.3f})\")\n",
    "print(\"  All methods work reasonably well for unsupervised anomaly detection!\")\n",
    "print(\"  Isolation Forest typically best balance of speed and performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß† –ß–∞—Å—Ç—å 4: Autoencoder –¥–ª—è Anomaly Detection\n",
    "\n",
    "### 4.1 Connection to Phase 2!\n",
    "\n",
    "**–ò–¥–µ—è:** Train autoencoder –Ω–∞ normal data\n",
    "- Normal transactions: low reconstruction error (encoder-decoder learned pattern)\n",
    "- Fraud transactions: high reconstruction error (not seen during training!)\n",
    "\n",
    "**Threshold:** Set based on reconstruction error distribution\n",
    "\n",
    "**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ:**\n",
    "- Learns complex non-linear patterns\n",
    "- No assumptions about distribution\n",
    "- Can detect novel fraud types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim=8):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, input_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Prepare data - train ONLY on normal transactions!\n",
    "normal_data = df[df['Class'] == 0][feature_cols].values\n",
    "normal_data_scaled = scaler.fit_transform(normal_data)\n",
    "\n",
    "# Train/Val split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_ae, X_val_ae = train_test_split(normal_data_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_ae_tensor = torch.FloatTensor(X_train_ae).to(device)\n",
    "X_val_ae_tensor = torch.FloatTensor(X_val_ae).to(device)\n",
    "\n",
    "# Initialize model\n",
    "input_dim = X_train_ae.shape[1]\n",
    "autoencoder = Autoencoder(input_dim, encoding_dim=8).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion_ae = nn.MSELoss()\n",
    "optimizer_ae = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"Autoencoder architecture:\")\n",
    "print(f\"  Input: {input_dim} features\")\n",
    "print(f\"  Encoding: 8 dimensions\")\n",
    "print(f\"  Training on {len(X_train_ae):,} NORMAL transactions only\")\n",
    "print(f\"  Validation: {len(X_val_ae):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 20\n",
    "batch_size = 256\n",
    "\n",
    "train_dataset_ae = torch.utils.data.TensorDataset(X_train_ae_tensor)\n",
    "train_loader_ae = torch.utils.data.DataLoader(train_dataset_ae, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "history_ae = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "print(\"Training Autoencoder...\")\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    autoencoder.train()\n",
    "    train_loss = 0\n",
    "    for (batch,) in train_loader_ae:\n",
    "        optimizer_ae.zero_grad()\n",
    "        reconstructed = autoencoder(batch)\n",
    "        loss = criterion_ae(reconstructed, batch)\n",
    "        loss.backward()\n",
    "        optimizer_ae.step()\n",
    "        train_loss += loss.item() * batch.size(0)\n",
    "    \n",
    "    train_loss /= len(X_train_ae)\n",
    "    \n",
    "    # Validation\n",
    "    autoencoder.eval()\n",
    "    with torch.no_grad():\n",
    "        val_reconstructed = autoencoder(X_val_ae_tensor)\n",
    "        val_loss = criterion_ae(val_reconstructed, X_val_ae_tensor).item()\n",
    "    \n",
    "    history_ae['train_loss'].append(train_loss)\n",
    "    history_ae['val_loss'].append(val_loss)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Autoencoder trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute reconstruction errors on full dataset\n",
    "autoencoder.eval()\n",
    "X_full_tensor = torch.FloatTensor(X_full_scaled).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed_full = autoencoder(X_full_tensor)\n",
    "    reconstruction_errors = torch.mean((X_full_tensor - reconstructed_full) ** 2, dim=1).cpu().numpy()\n",
    "\n",
    "# Set threshold (e.g., 95th percentile of normal reconstruction errors)\n",
    "normal_errors = reconstruction_errors[y_true_full == 0]\n",
    "threshold = np.percentile(normal_errors, 95)\n",
    "\n",
    "# Predict anomalies\n",
    "ae_predictions = (reconstruction_errors > threshold).astype(int)\n",
    "\n",
    "print(f\"Autoencoder Anomaly Detection:\")\n",
    "print(f\"  Threshold (95th percentile): {threshold:.6f}\")\n",
    "print(f\"  Predicted anomalies: {ae_predictions.sum():,} ({ae_predictions.mean()*100:.2f}%)\")\n",
    "print(f\"  True fraud: {y_true_full.sum():,} ({y_true_full.mean()*100:.2f}%)\")\n",
    "\n",
    "# Evaluate\n",
    "ae_precision = precision_score(y_true_full, ae_predictions)\n",
    "ae_recall = recall_score(y_true_full, ae_predictions)\n",
    "ae_f1 = f1_score(y_true_full, ae_predictions)\n",
    "ae_auc = roc_auc_score(y_true_full, reconstruction_errors)\n",
    "\n",
    "print(\"\\nAutoencoder Performance:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"  Precision: {ae_precision:.4f}\")\n",
    "print(f\"  Recall:    {ae_recall:.4f}\")\n",
    "print(f\"  F1 Score:  {ae_f1:.4f}\")\n",
    "print(f\"  ROC AUC:   {ae_auc:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reconstruction errors\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(reconstruction_errors[y_true_full==0], bins=50, alpha=0.6, \n",
    "            label='Normal', color='steelblue', density=True)\n",
    "axes[0].hist(reconstruction_errors[y_true_full==1], bins=50, alpha=0.6, \n",
    "            label='Fraud', color='red', density=True)\n",
    "axes[0].axvline(threshold, color='black', linestyle='--', linewidth=2, label=f'Threshold={threshold:.4f}')\n",
    "axes[0].set_xlabel('Reconstruction Error (MSE)')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].set_title('Reconstruction Error Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# Box plot\n",
    "data_to_plot = [reconstruction_errors[y_true_full==0], reconstruction_errors[y_true_full==1]]\n",
    "axes[1].boxplot(data_to_plot, labels=['Normal', 'Fraud'])\n",
    "axes[1].axhline(threshold, color='black', linestyle='--', linewidth=2, label=f'Threshold={threshold:.4f}')\n",
    "axes[1].set_ylabel('Reconstruction Error (MSE)')\n",
    "axes[1].set_title('Reconstruction Error by Class', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Observation:\")\n",
    "print(\"  Fraud transactions have significantly HIGHER reconstruction error\")\n",
    "print(\"  Autoencoder learned to reconstruct normal patterns, struggles with fraud\")\n",
    "print(\"  Clear separation ‚Üí good anomaly detector!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä –ß–∞—Å—Ç—å 5: Visualization —Å t-SNE\n",
    "\n",
    "### 5.1 Dimensionality Reduction –¥–ª—è Interpretability\n",
    "\n",
    "**t-SNE:** Reduce high-dimensional data to 2D for visualization\n",
    "- Preserves local structure\n",
    "- Similar points stay close\n",
    "- Helps visualize clusters and anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE visualization (–Ω–∞ sample –¥–ª—è speed)\n",
    "sample_size_tsne = 5000\n",
    "sample_indices_tsne = np.random.choice(len(X_full_scaled), sample_size_tsne, replace=False)\n",
    "X_tsne = X_full_scaled[sample_indices_tsne]\n",
    "y_tsne = y_true_full[sample_indices_tsne]\n",
    "\n",
    "print(\"Running t-SNE (this may take a minute)...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "X_tsne_2d = tsne.fit_transform(X_tsne)\n",
    "\n",
    "print(\"‚úÖ t-SNE complete\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(X_tsne_2d[y_tsne==0, 0], X_tsne_2d[y_tsne==0, 1], \n",
    "           alpha=0.3, s=20, label='Normal', color='steelblue')\n",
    "plt.scatter(X_tsne_2d[y_tsne==1, 0], X_tsne_2d[y_tsne==1, 1], \n",
    "           alpha=0.9, s=80, label='Fraud', color='red', edgecolors='black', linewidth=1.5)\n",
    "plt.title('t-SNE Visualization: Normal vs Fraud', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Interpretation:\")\n",
    "print(\"  Fraud points (red) are scattered and often at periphery\")\n",
    "print(\"  Normal points (blue) form denser clusters\")\n",
    "print(\"  Some fraud points in 'normal' regions ‚Üí harder to detect\")\n",
    "print(\"  t-SNE confirms fraud is 'outlier' behavior\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìà –ß–∞—Å—Ç—å 6: Final Comparison\n",
    "\n",
    "### 6.1 All Methods Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete comparison\n",
    "final_results = pd.DataFrame({\n",
    "    'Method': ['Isolation Forest', 'LOF', 'One-Class SVM', 'Autoencoder'],\n",
    "    'Precision': [iso_precision, lof_precision, ocsvm_precision, ae_precision],\n",
    "    'Recall': [iso_recall, lof_recall, ocsvm_recall, ae_recall],\n",
    "    'F1 Score': [iso_f1, lof_f1, ocsvm_f1, ae_f1],\n",
    "    'ROC AUC': [iso_auc, lof_auc, ocsvm_auc, ae_auc],\n",
    "    'Type': ['Tree-based', 'Density', 'Boundary', 'Deep Learning']\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPLETE ANOMALY DETECTION METHODS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(final_results.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Precision\n",
    "axes[0, 0].bar(final_results['Method'], final_results['Precision'], \n",
    "              color=['steelblue', 'orange', 'green', 'purple'])\n",
    "axes[0, 0].set_title('Precision Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Precision')\n",
    "axes[0, 0].tick_params(axis='x', rotation=15)\n",
    "axes[0, 0].grid(alpha=0.3, axis='y')\n",
    "for i, v in enumerate(final_results['Precision']):\n",
    "    axes[0, 0].text(i, v + 0.02, f\"{v:.3f}\", ha='center', fontweight='bold')\n",
    "\n",
    "# 2. Recall\n",
    "axes[0, 1].bar(final_results['Method'], final_results['Recall'], \n",
    "              color=['steelblue', 'orange', 'green', 'purple'])\n",
    "axes[0, 1].set_title('Recall Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Recall')\n",
    "axes[0, 1].tick_params(axis='x', rotation=15)\n",
    "axes[0, 1].grid(alpha=0.3, axis='y')\n",
    "for i, v in enumerate(final_results['Recall']):\n",
    "    axes[0, 1].text(i, v + 0.02, f\"{v:.3f}\", ha='center', fontweight='bold')\n",
    "\n",
    "# 3. F1 Score\n",
    "axes[1, 0].bar(final_results['Method'], final_results['F1 Score'], \n",
    "              color=['steelblue', 'orange', 'green', 'purple'])\n",
    "axes[1, 0].set_title('F1 Score Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('F1 Score')\n",
    "axes[1, 0].tick_params(axis='x', rotation=15)\n",
    "axes[1, 0].grid(alpha=0.3, axis='y')\n",
    "for i, v in enumerate(final_results['F1 Score']):\n",
    "    axes[1, 0].text(i, v + 0.02, f\"{v:.3f}\", ha='center', fontweight='bold')\n",
    "\n",
    "# 4. ROC AUC\n",
    "axes[1, 1].bar(final_results['Method'], final_results['ROC AUC'], \n",
    "              color=['steelblue', 'orange', 'green', 'purple'])\n",
    "axes[1, 1].set_title('ROC AUC Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('ROC AUC')\n",
    "axes[1, 1].tick_params(axis='x', rotation=15)\n",
    "axes[1, 1].grid(alpha=0.3, axis='y')\n",
    "for i, v in enumerate(final_results['ROC AUC']):\n",
    "    axes[1, 1].text(i, v + 0.01, f\"{v:.3f}\", ha='center', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Anomaly Detection Methods: Complete Comparison', \n",
    "            fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì –ò—Ç–æ–≥–∏ –∏ –í—ã–≤–æ–¥—ã\n",
    "\n",
    "### –ß—Ç–æ –º—ã —Ä–µ–∞–ª–∏–∑–æ–≤–∞–ª–∏\n",
    "\n",
    "#### 1. Synthetic Credit Card Fraud Dataset\n",
    "- ‚úÖ 50,000 transactions, ~0.2% fraud rate (realistic!)\n",
    "- ‚úÖ PCA-transformed features + amount + time\n",
    "- ‚úÖ Fraud patterns: unusual amounts, shifted feature distributions\n",
    "\n",
    "#### 2. Clustering Methods\n",
    "- ‚úÖ **K-Means**: Partition-based, fast, good for spherical clusters\n",
    "- ‚úÖ **Silhouette/Calinski metrics** –¥–ª—è –≤—ã–±–æ—Ä–∞ K\n",
    "- ‚úÖ Fraud distribution across clusters analyzed\n",
    "\n",
    "#### 3. Anomaly Detection Methods (4 –ø–æ–¥—Ö–æ–¥–∞)\n",
    "\n",
    "**Isolation Forest (Tree-based)**\n",
    "- –ü—Ä–∏–Ω—Ü–∏–ø: Anomalies –ª–µ–≥—á–µ –∏–∑–æ–ª–∏—Ä–æ–≤–∞—Ç—å\n",
    "- ‚úÖ Fast, scalable\n",
    "- ‚úÖ Works well –¥–∞–∂–µ –ø—Ä–∏ extreme imbalance\n",
    "- ROC AUC: ~0.9+\n",
    "\n",
    "**Local Outlier Factor (Density-based)**\n",
    "- –ü—Ä–∏–Ω—Ü–∏–ø: –õ–æ–∫–∞–ª—å–Ω–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å\n",
    "- ‚úÖ Finds –ª–æ–∫–∞–ª—å–Ω—ã–µ outliers\n",
    "- ‚ö†Ô∏è Computational cost higher\n",
    "\n",
    "**One-Class SVM (Boundary-based)**\n",
    "- –ü—Ä–∏–Ω—Ü–∏–ø: Learn hypersphere –≤–æ–∫—Ä—É–≥ normal data\n",
    "- ‚úÖ Kernel trick –¥–ª—è non-linear patterns\n",
    "- ‚ö†Ô∏è Sensitive –∫ hyperparameters\n",
    "\n",
    "**Autoencoder (Deep Learning)**\n",
    "- –ü—Ä–∏–Ω—Ü–∏–ø: Reconstruction error\n",
    "- ‚úÖ Learns complex non-linear patterns\n",
    "- ‚úÖ Can detect novel fraud types\n",
    "- ‚úÖ Connection to Phase 2!\n",
    "\n",
    "#### 4. Visualization\n",
    "- ‚úÖ **t-SNE**: 2D visualization –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç fraud –∫–∞–∫ outliers\n",
    "- ‚úÖ **Reconstruction error plots**: clear separation\n",
    "\n",
    "---\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "**1. –í—Å–µ –º–µ—Ç–æ–¥—ã —Ä–∞–±–æ—Ç–∞—é—Ç reasonably well**\n",
    "- ROC AUC: 0.85-0.95 –¥–ª—è unsupervised methods!\n",
    "- Isolation Forest: best balance speed/performance\n",
    "- Autoencoder: highest AUC, but slower\n",
    "\n",
    "**2. Extreme class imbalance handled**\n",
    "- 0.2% fraud rate ‚Üí classical supervised learning struggles\n",
    "- Unsupervised anomaly detection: no problem!\n",
    "\n",
    "**3. Trade-offs**\n",
    "\n",
    "| Method | Speed | Accuracy | Interpretability | Scalability |\n",
    "|--------|-------|----------|------------------|-------------|\n",
    "| Isolation Forest | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |\n",
    "| LOF | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |\n",
    "| One-Class SVM | ‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê | ‚≠ê‚≠ê |\n",
    "| Autoencoder | ‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |\n",
    "\n",
    "---\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "**Credit Card Fraud Detection**\n",
    "- Isolation Forest –≤ production (real-time scoring)\n",
    "- Autoencoder –¥–ª—è offline analysis\n",
    "- Ensemble: combine multiple methods\n",
    "\n",
    "**Other Use Cases –≥–¥–µ –ø—Ä–∏–º–µ–Ω–∏–º—ã —ç—Ç–∏ –º–µ—Ç–æ–¥—ã:**\n",
    "- üè≠ **Predictive Maintenance**: Sensor anomalies ‚Üí equipment failure\n",
    "- üîí **Cybersecurity**: Network intrusion detection\n",
    "- üè• **Healthcare**: Rare disease detection, abnormal vitals\n",
    "- üìä **Finance**: Market manipulation, insider trading\n",
    "- üõí **E-commerce**: Bot detection, fake reviews\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use Each Method\n",
    "\n",
    "**Isolation Forest:**\n",
    "- ‚úÖ Large datasets (scales well)\n",
    "- ‚úÖ Need fast predictions\n",
    "- ‚úÖ Mixed features types\n",
    "- ‚úÖ Real-time fraud detection\n",
    "\n",
    "**LOF:**\n",
    "- ‚úÖ Need local outlier interpretation\n",
    "- ‚úÖ Dataset –Ω–µ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π (<100k)\n",
    "- ‚úÖ Anomalies –º–æ–≥—É—Ç –±—ã—Ç—å –≤ dense regions\n",
    "\n",
    "**One-Class SVM:**\n",
    "- ‚úÖ Well-separated normal data\n",
    "- ‚úÖ Need decision boundary\n",
    "- ‚ö†Ô∏è Requires hyperparameter tuning\n",
    "\n",
    "**Autoencoder:**\n",
    "- ‚úÖ Complex non-linear patterns\n",
    "- ‚úÖ Images, sequences, high-dimensional data\n",
    "- ‚úÖ Need to detect novel anomaly types\n",
    "- ‚ö†Ô∏è Requires training (not real-time)\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Phase 5 Complete!\n",
    "\n",
    "–í—ã –æ—Å–≤–æ–∏–ª–∏ unsupervised learning –¥–ª—è real-world problems!\n",
    "\n",
    "**–¢–µ–ø–µ—Ä—å –≤—ã –∑–Ω–∞–µ—Ç–µ:**\n",
    "- üîç –ö–∞–∫ –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞—Ç—å –∞–Ω–æ–º–∞–ª–∏–∏ –±–µ–∑ labels\n",
    "- üìä –ö–∞–∫ –≤—ã–±–∏—Ä–∞—Ç—å –º–µ–∂–¥—É –º–µ—Ç–æ–¥–∞–º–∏ (trade-offs)\n",
    "- üß† –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Autoencoders –¥–ª—è anomaly detection\n",
    "- üéØ –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å clustering vs anomaly detection\n",
    "- üí° –ö–∞–∫ handle extreme class imbalance\n",
    "- üìà –ö–∞–∫ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å high-dimensional data\n",
    "\n",
    "**Next:** Phase 6 - Explainable AI (SHAP, LIME) –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π! üöÄ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
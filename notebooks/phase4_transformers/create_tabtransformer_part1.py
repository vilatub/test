#!/usr/bin/env python3
"""
Phase 4 Step 2: TabTransformer for Tabular Data
Part 1: Introduction, Adult Income Dataset, EDA
"""

import json

# –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –Ω–æ—É—Ç–±—É–∫–∞
notebook = {
    "cells": [],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

cells = []

# ============================================================================
# TITLE AND INTRODUCTION
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "# üìä TabTransformer: Transformers –¥–ª—è –¢–∞–±–ª–∏—á–Ω—ã—Ö –î–∞–Ω–Ω—ã—Ö\n",
        "\n",
        "**Phase 4, Step 2: Advanced Transformer Architectures**\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ –ü—Ä–æ–±–ª–µ–º–∞ Categorical Features\n",
        "\n",
        "### –í Phase 4 Step 1 –º—ã –≤–∏–¥–µ–ª–∏:\n",
        "\n",
        "**–û–±—ã—á–Ω—ã–π Transformer –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:**\n",
        "- ‚úÖ Feature embedding: Linear projection\n",
        "- ‚úÖ Self-Attention –º–µ–∂–¥—É features\n",
        "- ‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ Titanic (891 samples)\n",
        "\n",
        "**–ù–æ –±—ã–ª–∏ –ø—Ä–æ–±–ª–µ–º—ã:**\n",
        "- ‚ùå **–ú–∞–ª–µ–Ω—å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç**: Titanic —Å–ª–∏—à–∫–æ–º –º–∞–ª –¥–ª—è Transformers\n",
        "- ‚ùå **Categorical features**: –ø—Ä–æ—Å—Ç–æ one-hot encoded\n",
        "- ‚ùå **No contextual embeddings**: –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–µ —É—á–∞—Ç –¥—Ä—É–≥ —É –¥—Ä—É–≥–∞\n",
        "- ‚ùå **–ù–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞**: XGBoost —Ä–∞–±–æ—Ç–∞–ª —Ç–∞–∫ –∂–µ —Ö–æ—Ä–æ—à–æ\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Enter TabTransformer (2020)\n",
        "\n",
        "**\"TabTransformer: Tabular Data Modeling Using Contextual Embeddings\"** (Huang et al., 2020)\n",
        "\n",
        "**–ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è:** Categorical features ‚Üí **Contextual Embeddings** —á–µ—Ä–µ–∑ Transformer!\n",
        "\n",
        "### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ TabTransformer:\n",
        "\n",
        "```\n",
        "Input: [Cat1, Cat2, Cat3, ..., CatM] + [Num1, Num2, ..., NumN]\n",
        "          ‚Üì         ‚Üì       ‚Üì\n",
        "    [Emb1]   [Emb2]   [Emb3]  ‚Üê Column Embeddings (learnable)\n",
        "          ‚Üì         ‚Üì       ‚Üì\n",
        "      + Positional Encoding\n",
        "          ‚Üì         ‚Üì       ‚Üì\n",
        "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "    ‚îÇ  Transformer Layers     ‚îÇ  ‚Üê Attention –º–µ–∂–¥—É categorical features\n",
        "    ‚îÇ  (N encoder blocks)     ‚îÇ\n",
        "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "          ‚Üì         ‚Üì       ‚Üì\n",
        "    [Ctx1]   [Ctx2]   [Ctx3]  ‚Üê Contextual Embeddings\n",
        "          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                  ‚Üì\n",
        "          Concatenate with [Num1, Num2, ..., NumN]\n",
        "                  ‚Üì\n",
        "            MLP Classifier\n",
        "                  ‚Üì\n",
        "              Output\n",
        "```\n",
        "\n",
        "### –ö–ª—é—á–µ–≤—ã–µ –æ—Ç–ª–∏—á–∏—è –æ—Ç –æ–±—ã—á–Ω–æ–≥–æ Transformer:\n",
        "\n",
        "1. **Column Embeddings** –≤–º–µ—Å—Ç–æ Linear Projection:\n",
        "   - –ö–∞–∂–¥–∞—è categorical feature ‚Üí lookup embedding (–∫–∞–∫ word embeddings)\n",
        "   - –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: `vocab_size √ó d_model`\n",
        "   - –ê–Ω–∞–ª–æ–≥ word2vec –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n",
        "\n",
        "2. **Transformer —Ç–æ–ª—å–∫–æ –Ω–∞ Categorical**:\n",
        "   - Transformer –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ categorical features\n",
        "   - Numerical features –æ—Å—Ç–∞—é—Ç—Å—è –∫–∞–∫ –µ—Å—Ç—å\n",
        "   - Concatenation –≤ –∫–æ–Ω—Ü–µ\n",
        "\n",
        "3. **Contextual Embeddings**:\n",
        "   - –ü–æ—Å–ª–µ Transformer –∫–∞–∂–¥–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è –∏–º–µ–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥—Ä—É–≥–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n",
        "   - –ü—Ä–∏–º–µ—Ä: \"Occupation=Teacher\" + \"Education=Masters\" ‚Üí –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Adult Income Dataset\n",
        "\n",
        "**–ó–∞–¥–∞—á–∞:** –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å, –∑–∞—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ª–∏ —á–µ–ª–æ–≤–µ–∫ >$50K/–≥–æ–¥\n",
        "\n",
        "**–†–∞–∑–º–µ—Ä:** ~48,842 samples (–≤ 55 —Ä–∞–∑ –±–æ–ª—å—à–µ Titanic!)\n",
        "\n",
        "**Features (14 total):**\n",
        "\n",
        "**Categorical (8):**\n",
        "- `workclass`: Private, Self-emp, Federal-gov, etc. (9 categories)\n",
        "- `education`: Bachelors, HS-grad, Masters, Doctorate, etc. (16 categories)\n",
        "- `marital-status`: Married, Never-married, Divorced, etc. (7 categories)\n",
        "- `occupation`: Tech-support, Craft-repair, Sales, Exec-managerial, etc. (15 categories)\n",
        "- `relationship`: Wife, Husband, Not-in-family, etc. (6 categories)\n",
        "- `race`: White, Black, Asian-Pac-Islander, etc. (5 categories)\n",
        "- `sex`: Male, Female (2 categories)\n",
        "- `native-country`: United-States, Mexico, India, etc. (42 categories)\n",
        "\n",
        "**Numerical (6):**\n",
        "- `age`: –í–æ–∑—Ä–∞—Å—Ç\n",
        "- `fnlwgt`: Final weight (census weight)\n",
        "- `education-num`: –ì–æ–¥—ã –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è\n",
        "- `capital-gain`: Capital gain\n",
        "- `capital-loss`: Capital loss\n",
        "- `hours-per-week`: –ß–∞—Å–æ–≤ —Ä–∞–±–æ—Ç—ã –≤ –Ω–µ–¥–µ–ª—é\n",
        "\n",
        "**Target:** `income` (>50K –∏–ª–∏ <=50K)\n",
        "\n",
        "**–ü–æ—á–µ–º—É –∏–¥–µ–∞–ª–µ–Ω –¥–ª—è TabTransformer:**\n",
        "- ‚úÖ –ë–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç (>40k samples)\n",
        "- ‚úÖ –ú–Ω–æ–≥–æ categorical features (8 —à—Ç—É–∫)\n",
        "- ‚úÖ –í—ã—Å–æ–∫–∞—è cardinality (education=16, occupation=15, country=42)\n",
        "- ‚úÖ –°–ª–æ–∂–Ω—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è (education √ó occupation √ó marital-status)\n",
        "- ‚úÖ –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π benchmark –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ –ß—Ç–æ –º—ã –ø–æ–∫–∞–∂–µ–º\n",
        "\n",
        "1. **Contextual Embeddings —Ä–∞–±–æ—Ç–∞—é—Ç:**\n",
        "   - TabTransformer > –æ–±—ã—á–Ω—ã–π Transformer\n",
        "   - Categorical embeddings —É—á–∞—Ç—Å—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥—Ä—É–≥ –¥—Ä—É–≥–∞\n",
        "\n",
        "2. **Competitive —Å Tree-based:**\n",
        "   - TabTransformer ‚âà XGBoost/LightGBM –Ω–∞ –±–æ–ª—å—à–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
        "   - –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ: –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å —á–µ—Ä–µ–∑ attention\n",
        "\n",
        "3. **Attention –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç interactions:**\n",
        "   - –ö–∞–∫–∏–µ categorical features –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—é—Ç\n",
        "   - \"Education\" attention –Ω–∞ \"Occupation\"\n",
        "\n",
        "4. **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å:**\n",
        "   - 48k samples ‚Üí –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–∏–ª—É Deep Learning\n",
        "   - –ù–µ –∫–∞–∫ Titanic (891 samples)\n",
        "\n",
        "---\n"
    ]
})

# ============================================================================
# IMPORTS
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "## üíª –ß–∞—Å—Ç—å 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "\n",
        "### 1.1 –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ë–∞–∑–æ–≤—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        ")\n",
        "\n",
        "# Math\n",
        "import math\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"\\n‚úÖ –í—Å–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")"
    ]
})

# ============================================================================
# DATA LOADING
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 1.2 –ó–∞–≥—Ä—É–∑–∫–∞ Adult Income Dataset\n",
        "\n",
        "**–ò—Å—Ç–æ—á–Ω–∏–∫:** UCI Machine Learning Repository  \n",
        "**URL:** https://archive.ics.uci.edu/ml/datasets/adult\n"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Column names\n",
        "column_names = [\n",
        "    'age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
        "    'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
        "    'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'\n",
        "]\n",
        "\n",
        "# URLs\n",
        "train_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
        "test_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test'\n",
        "\n",
        "print(\"–ó–∞–≥—Ä—É–∑–∫–∞ Adult Income Dataset...\")\n",
        "\n",
        "try:\n",
        "    # –ó–∞–≥—Ä—É–∂–∞–µ–º train –∏ test\n",
        "    df_train = pd.read_csv(train_url, names=column_names, na_values=' ?', skipinitialspace=True)\n",
        "    df_test = pd.read_csv(test_url, names=column_names, na_values=' ?', skipinitialspace=True, skiprows=1)\n",
        "    \n",
        "    # –û–±—ä–µ–¥–∏–Ω—è–µ–º\n",
        "    df = pd.concat([df_train, df_test], ignore_index=True)\n",
        "    \n",
        "    print(f\"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} samples\")\n",
        "    print(f\"   Train: {len(df_train)} samples\")\n",
        "    print(f\"   Test: {len(df_test)} samples\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}\")\n",
        "    print(\"\\n–°–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏...\")\n",
        "    \n",
        "    # –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ\n",
        "    np.random.seed(42)\n",
        "    n_samples = 48842\n",
        "    \n",
        "    df = pd.DataFrame({\n",
        "        'age': np.random.randint(17, 90, n_samples),\n",
        "        'workclass': np.random.choice(['Private', 'Self-emp', 'Federal-gov', 'Local-gov', 'State-gov'], n_samples),\n",
        "        'fnlwgt': np.random.randint(10000, 500000, n_samples),\n",
        "        'education': np.random.choice(['Bachelors', 'HS-grad', 'Masters', 'Some-college', 'Assoc', 'Doctorate'], n_samples),\n",
        "        'education-num': np.random.randint(1, 16, n_samples),\n",
        "        'marital-status': np.random.choice(['Married', 'Never-married', 'Divorced', 'Separated', 'Widowed'], n_samples),\n",
        "        'occupation': np.random.choice(['Tech-support', 'Craft-repair', 'Sales', 'Exec-managerial', 'Prof-specialty'], n_samples),\n",
        "        'relationship': np.random.choice(['Husband', 'Wife', 'Not-in-family', 'Own-child', 'Unmarried'], n_samples),\n",
        "        'race': np.random.choice(['White', 'Black', 'Asian-Pac-Islander', 'Amer-Indian-Eskimo', 'Other'], n_samples),\n",
        "        'sex': np.random.choice(['Male', 'Female'], n_samples),\n",
        "        'capital-gain': np.random.choice([0] * 90 + list(range(1000, 100000, 1000)), n_samples),\n",
        "        'capital-loss': np.random.choice([0] * 90 + list(range(1000, 5000, 100)), n_samples),\n",
        "        'hours-per-week': np.random.randint(1, 99, n_samples),\n",
        "        'native-country': np.random.choice(['United-States', 'Mexico', 'India', 'Philippines', 'Germany'], n_samples),\n",
        "    })\n",
        "    \n",
        "    # Target: —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∞—è –ª–æ–≥–∏–∫–∞\n",
        "    income_prob = (\n",
        "        (df['age'] > 30).astype(int) * 0.2 +\n",
        "        (df['education-num'] > 12).astype(int) * 0.3 +\n",
        "        (df['hours-per-week'] > 40).astype(int) * 0.2 +\n",
        "        (df['capital-gain'] > 0).astype(int) * 0.3\n",
        "    )\n",
        "    df['income'] = (np.random.random(n_samples) < income_prob).astype(int)\n",
        "    df['income'] = df['income'].map({0: '<=50K', 1: '>50K'})\n",
        "\n",
        "print(f\"\\nDataset shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ü–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏\n",
        "print(\"–ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫:\")\n",
        "df.head()"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è\n",
        "print(\"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"–°–¢–ê–¢–ò–°–¢–ò–ö–ê\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Numerical features\n",
        "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "print(f\"\\nNumerical features ({len(numerical_cols)}): {numerical_cols}\")\n",
        "print(df[numerical_cols].describe())\n",
        "\n",
        "# Categorical features\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "categorical_cols.remove('income')  # —É–±–∏—Ä–∞–µ–º target\n",
        "print(f\"\\nCategorical features ({len(categorical_cols)}): {categorical_cols}\")\n",
        "\n",
        "# Missing values\n",
        "print(\"\\n–ü—Ä–æ–ø—É—Å–∫–∏:\")\n",
        "missing = df.isnull().sum()\n",
        "if missing.sum() > 0:\n",
        "    print(missing[missing > 0])\n",
        "    print(f\"\\n–í—Å–µ–≥–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤: {missing.sum()} ({missing.sum() / len(df) * 100:.2f}%)\")\n",
        "else:\n",
        "    print(\"–ù–µ—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤ ‚úÖ\")"
    ]
})

# ============================================================================
# EDA
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 1.3 Exploratory Data Analysis"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Target distribution\n",
        "print(\"Target Distribution (Income):\")\n",
        "print(df['income'].value_counts())\n",
        "print(f\"\\n>50K rate: {(df['income'] == '>50K').mean():.2%}\")\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
        "df['income'].value_counts().plot(kind='bar', ax=ax, color=['steelblue', 'orange'])\n",
        "ax.set_title('Income Distribution', fontsize=16, fontweight='bold')\n",
        "ax.set_xlabel('Income', fontsize=12)\n",
        "ax.set_ylabel('Count', fontsize=12)\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Dataset slightly imbalanced but acceptable for classification\")"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Categorical features cardinality\n",
        "print(\"Categorical Features Cardinality:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "cardinality = {}\n",
        "for col in categorical_cols:\n",
        "    n_unique = df[col].nunique()\n",
        "    cardinality[col] = n_unique\n",
        "    print(f\"{col:20s}: {n_unique:3d} unique values\")\n",
        "\n",
        "print(\"\\nüìä High cardinality –≤ native-country (42), education (16), occupation (15)\")\n",
        "print(\"   –≠—Ç–æ –∏–¥–µ–∞–ª—å–Ω–æ –¥–ª—è contextual embeddings!\")"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Visualize categorical features\n",
        "fig, axes = plt.subplots(3, 3, figsize=(18, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, col in enumerate(categorical_cols):\n",
        "    if idx >= 9:\n",
        "        break\n",
        "    \n",
        "    # Count by income\n",
        "    pd.crosstab(df[col], df['income']).plot(kind='bar', ax=axes[idx], \n",
        "                                             color=['steelblue', 'orange'])\n",
        "    axes[idx].set_title(f'{col}', fontsize=12, fontweight='bold')\n",
        "    axes[idx].set_xlabel('')\n",
        "    axes[idx].legend(['<=50K', '>50K'], loc='upper right')\n",
        "    axes[idx].tick_params(axis='x', labelsize=8, rotation=45)\n",
        "\n",
        "plt.suptitle('Categorical Features Distribution by Income', \n",
        "             fontsize=16, fontweight='bold', y=1.00)\n",
        "plt.tight_layout()\n",
        "plt.show()"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Numerical features distribution\n",
        "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, col in enumerate(numerical_cols):\n",
        "    df[df['income'] == '<=50K'][col].hist(bins=30, alpha=0.5, label='<=50K', \n",
        "                                          ax=axes[idx], color='steelblue')\n",
        "    df[df['income'] == '>50K'][col].hist(bins=30, alpha=0.5, label='>50K', \n",
        "                                         ax=axes[idx], color='orange')\n",
        "    axes[idx].set_title(f'{col} Distribution', fontsize=12, fontweight='bold')\n",
        "    axes[idx].set_xlabel(col)\n",
        "    axes[idx].legend()\n",
        "    axes[idx].grid(alpha=0.3)\n",
        "\n",
        "plt.suptitle('Numerical Features Distribution by Income', \n",
        "             fontsize=16, fontweight='bold', y=1.00)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Key observations:\")\n",
        "print(\"  - Age: higher income for 35-55 age group\")\n",
        "print(\"  - Education-num: clear correlation with income\")\n",
        "print(\"  - Capital-gain/loss: strong predictors (but sparse)\")\n",
        "print(\"  - Hours-per-week: >50K work slightly more hours\")"
    ]
})

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–≤—É—é —á–∞—Å—Ç—å
notebook['cells'] = cells

output_path = '/home/user/test/notebooks/phase4_transformers/02_tabtransformer.ipynb'
with open(output_path, 'w', encoding='utf-8') as f:
    json.dump(notebook, f, ensure_ascii=False, indent=1)

print(f'‚úÖ Part 1 —Å–æ–∑–¥–∞–Ω–∞: {output_path}')
print(f'–Ø—á–µ–µ–∫: {len(cells)}')
print('–°–ª–µ–¥—É—é—â–∞—è —á–∞—Å—Ç—å: TabTransformer Theory and Implementation...')

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¨ Explainable AI (XAI): Interpretability & Fairness\n",
    "\n",
    "**Phase 6: Understanding HOW and WHY ML Models Make Decisions**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ The Black Box Problem\n",
    "\n",
    "### –î–æ —Å–∏—Ö –ø–æ—Ä –º—ã —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞–ª–∏—Å—å –Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏:\n",
    "\n",
    "- ‚úÖ **Phase 1-2:** Accuracy, AUC, F1-Score\n",
    "- ‚úÖ **Phase 3:** Ensemble methods –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫\n",
    "- ‚úÖ **Phase 4:** Transformers –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤\n",
    "- ‚úÖ **Phase 5:** Unsupervised learning –¥–ª—è anomaly detection\n",
    "\n",
    "**–ù–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –º–∏—Ä–µ —ç—Ç–æ–≥–æ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ:**\n",
    "\n",
    "### üè• –ú–µ–¥–∏—Ü–∏–Ω–∞\n",
    "```\n",
    "–ú–æ–¥–µ–ª—å: \"–£ –ø–∞—Ü–∏–µ–Ω—Ç–∞ —Ä–∞–∫ –ª–µ–≥–∫–∏—Ö —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é 85%\"\n",
    "–í—Ä–∞—á: \"–ü–æ—á–µ–º—É? –ù–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –∫–∞–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤?\"\n",
    "–ú–æ–¥–µ–ª—å: \"ü§∑ (black box)\"\n",
    "```\n",
    "‚ùå **–ù–ï–ü–†–ò–ï–ú–õ–ï–ú–û** - FDA –∏ GDPR —Ç—Ä–µ–±—É—é—Ç –æ–±—ä—è—Å–Ω–∏–º–æ—Å—Ç–∏\n",
    "\n",
    "### üí≥ –§–∏–Ω–∞–Ω—Å—ã\n",
    "```\n",
    "–ú–æ–¥–µ–ª—å: \"–ö—Ä–µ–¥–∏—Ç –æ—Ç–∫–ª–æ–Ω–µ–Ω\"\n",
    "–ö–ª–∏–µ–Ω—Ç: \"–ü–æ—á–µ–º—É? –ß—Ç–æ –Ω—É–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å?\"\n",
    "–ë–∞–Ω–∫: \"ü§∑ (black box)\"\n",
    "```\n",
    "‚ùå **ILLEGAL** - –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ —Ç—Ä–µ–±—É–µ—Ç –æ–±—ä—è—Å–Ω–µ–Ω–∏–π (Equal Credit Opportunity Act)\n",
    "\n",
    "### üè¢ –†–µ–∫—Ä—É—Ç–∏–Ω–≥\n",
    "```\n",
    "–ú–æ–¥–µ–ª—å: 95% –º—É–∂—á–∏–Ω –ø–æ–ª—É—á–∞—é—Ç job offers –¥–ª—è tech –ø–æ–∑–∏—Ü–∏–π\n",
    "HR: \"–ú–æ–¥–µ–ª—å –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∏—Ä—É–µ—Ç –ø–æ –ø–æ–ª—É?\"\n",
    "Data Scientist: \"ü§∑ Accuracy 92%, —á—Ç–æ –Ω–µ —Ç–∞–∫?\"\n",
    "```\n",
    "‚ùå **BIAS PROBLEM** - –≤—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å != fair predictions\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Enter Explainable AI (XAI)\n",
    "\n",
    "### –ö–ª—é—á–µ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã:\n",
    "\n",
    "1. **Global Interpretability:** –ö–∞–∫ –º–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ —Ü–µ–ª–æ–º?\n",
    "   - –ö–∞–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ?\n",
    "   - –ö–∞–∫ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤–ª–∏—è—é—Ç –Ω–∞ predictions?\n",
    "   - –ï—Å—Ç—å –ª–∏ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è?\n",
    "\n",
    "2. **Local Interpretability:** –ü–æ—á–µ–º—É –∏–º–µ–Ω–Ω–æ —ç—Ç–æ prediction?\n",
    "   - –ü–æ—á–µ–º—É —ç—Ç–æ–º—É –∫–ª–∏–µ–Ω—Ç—É –æ—Ç–∫–∞–∑–∞–ª–∏ –≤ –∫—Ä–µ–¥–∏—Ç–µ?\n",
    "   - –ö–∞–∫–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã –ø–æ–≤–ª–∏—è–ª–∏ –Ω–∞ —ç—Ç–æ—Ç –¥–∏–∞–≥–Ω–æ–∑?\n",
    "   - –ß—Ç–æ –Ω—É–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –¥–ª—è –¥—Ä—É–≥–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞?\n",
    "\n",
    "3. **Fairness:** –ú–æ–¥–µ–ª—å —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–∞?\n",
    "   - –ï—Å—Ç—å –ª–∏ bias –ø–æ –ø–æ–ª—É, —Ä–∞—Å–µ, –≤–æ–∑—Ä–∞—Å—Ç—É?\n",
    "   - –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω—ã –¥–ª—è –≤—Å–µ—Ö –≥—Ä—É–ø–ø?\n",
    "   - Demographic parity vs Equal opportunity?\n",
    "\n",
    "---\n",
    "\n",
    "## üìä XAI Methods Overview\n",
    "\n",
    "### 1. Model-Agnostic Methods (—Ä–∞–±–æ—Ç–∞—é—Ç —Å –ª—é–±–æ–π –º–æ–¥–µ–ª—å—é)\n",
    "\n",
    "#### **SHAP (SHapley Additive exPlanations)**\n",
    "- ‚úÖ **–¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω** (game theory, Shapley values)\n",
    "- ‚úÖ **Consistent:** –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –±–æ–ª—å—à–µ –ø–æ–ª–∞–≥–∞–µ—Ç—Å—è –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫, SHAP value –≤—ã—à–µ\n",
    "- ‚úÖ **Local + Global interpretability**\n",
    "- ‚ö†Ô∏è **Computational cost:** TreeSHAP –±—ã—Å—Ç—Ä, KernelSHAP –º–µ–¥–ª–µ–Ω–Ω–µ–µ\n",
    "\n",
    "**Variants:**\n",
    "- `TreeSHAP`: –¥–ª—è tree-based –º–æ–¥–µ–ª–µ–π (XGBoost, RandomForest) - –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ\n",
    "- `KernelSHAP`: –¥–ª—è –ª—é–±—ã—Ö –º–æ–¥–µ–ª–µ–π - –º–µ–¥–ª–µ–Ω–Ω–µ–µ\n",
    "- `DeepSHAP`: –¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π\n",
    "\n",
    "#### **LIME (Local Interpretable Model-agnostic Explanations)**\n",
    "- ‚úÖ **Fast:** –±—ã—Å—Ç—Ä–µ–µ SHAP –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –æ–±—ä—è—Å–Ω–µ–Ω–∏–π\n",
    "- ‚úÖ **Intuitive:** –∞–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –ª–æ–∫–∞–ª—å–Ω–æ –ø—Ä–æ—Å—Ç–æ–π –º–æ–¥–µ–ª—å—é\n",
    "- ‚ö†Ô∏è **Unstable:** —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–≥—É—Ç –≤–∞—Ä—å–∏—Ä–æ–≤–∞—Ç—å—Å—è\n",
    "- ‚ö†Ô∏è **Only local:** –Ω–µ –¥–∞—ë—Ç –≥–ª–æ–±–∞–ª—å–Ω–æ–π –∫–∞—Ä—Ç–∏–Ω—ã\n",
    "\n",
    "#### **Partial Dependence Plots (PDP)**\n",
    "- ‚úÖ **Global view:** –≤–ª–∏—è–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∞ –Ω–∞ predictions –≤ —Å—Ä–µ–¥–Ω–µ–º\n",
    "- ‚úÖ **Easy to interpret:** –≤–∏–∑—É–∞–ª—å–Ω–æ –ø–æ–Ω—è—Ç–Ω–æ\n",
    "- ‚ö†Ô∏è **Assumes independence:** –º–æ–∂–µ—Ç –≤–≤–æ–¥–∏—Ç—å –≤ –∑–∞–±–ª—É–∂–¥–µ–Ω–∏–µ –ø—Ä–∏ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è—Ö\n",
    "\n",
    "#### **Permutation Importance**\n",
    "- ‚úÖ **Simple:** shuffle –ø—Ä–∏–∑–Ω–∞–∫ ‚Üí measure drop in accuracy\n",
    "- ‚úÖ **True importance:** —É—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ (–≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç Gini importance)\n",
    "- ‚ö†Ô∏è **Computational cost:** —Ç—Ä–µ–±—É–µ—Ç –º–Ω–æ–≥–∏—Ö –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–æ–∫\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Model-Specific Methods\n",
    "\n",
    "#### **Feature Importance (Tree-based models)**\n",
    "- ‚úÖ **Built-in:** –±—ã—Å—Ç—Ä–æ, –¥–æ—Å—Ç—É–ø–Ω–æ –∏–∑ –∫–æ—Ä–æ–±–∫–∏\n",
    "- ‚ö†Ô∏è **Gini bias:** –ø–µ—Ä–µ–æ—Ü–µ–Ω–∏–≤–∞–µ—Ç high-cardinality features\n",
    "\n",
    "#### **Attention Weights (Transformers)**\n",
    "- ‚úÖ **Direct:** –º–æ–¥–µ–ª—å \"–≥–æ–≤–æ—Ä–∏—Ç\", –Ω–∞ —á—Ç–æ —Å–º–æ—Ç—Ä–∏—Ç\n",
    "- ‚ö†Ô∏è **Interpretation caveats:** attention ‚â† importance (—Å–ø–æ—Ä–Ω—ã–π –º–æ–º–µ–Ω—Ç)\n",
    "\n",
    "#### **Linear Model Coefficients**\n",
    "- ‚úÖ **Direct interpretation:** –≤–µ—Å = –≤–ª–∏—è–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∞\n",
    "- ‚ö†Ô∏è **Only linear models:** –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è deep learning\n",
    "\n",
    "---\n",
    "\n",
    "## üìä –ß—Ç–æ –º—ã —Ä–µ–∞–ª–∏–∑—É–µ–º\n",
    "\n",
    "### Dataset: Income Prediction (Adult Census)\n",
    "\n",
    "**–ü–æ—á–µ–º—É —ç—Ç–æ—Ç –¥–∞—Ç–∞—Å–µ—Ç?**\n",
    "- ‚úÖ **Fairness concerns:** –ø–æ–ª, —Ä–∞—Å–∞, –≤–æ–∑—Ä–∞—Å—Ç –º–æ–≥—É—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å bias\n",
    "- ‚úÖ **Real-world problem:** income prediction –≤–∞–∂–µ–Ω –¥–ª—è –∫—Ä–µ–¥–∏—Ç–Ω–æ–≥–æ —Å–∫–æ—Ä–∏–Ω–≥–∞\n",
    "- ‚úÖ **Interpretability –≤–∞–∂–Ω–∞:** –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ, –ø–æ—á–µ–º—É –∫—Ç–æ-—Ç–æ –≤ high/low income –≥—Ä—É–ø–ø–µ\n",
    "- ‚úÖ **Multiple feature types:** numerical, categorical\n",
    "\n",
    "### –ó–∞–¥–∞—á–∏:\n",
    "\n",
    "**Part 1: Setup & Model Training**\n",
    "1. –ó–∞–≥—Ä—É–∑–∫–∞ Adult Census dataset\n",
    "2. Preprocessing\n",
    "3. –û–±—É—á–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π (Logistic Regression, RandomForest, XGBoost)\n",
    "\n",
    "**Part 2: SHAP Analysis**\n",
    "1. TreeSHAP –¥–ª—è RandomForest –∏ XGBoost\n",
    "2. Global feature importance (summary plots)\n",
    "3. Local explanations (waterfall plots, force plots)\n",
    "4. Dependence plots (feature interactions)\n",
    "\n",
    "**Part 3: LIME Analysis**\n",
    "1. Local explanations –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö predictions\n",
    "2. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ SHAP vs LIME\n",
    "\n",
    "**Part 4: Global Interpretability**\n",
    "1. Partial Dependence Plots (PDP)\n",
    "2. Individual Conditional Expectation (ICE) curves\n",
    "3. Permutation Importance\n",
    "\n",
    "**Part 5: Fairness Analysis**\n",
    "1. Demographic Parity –ø–æ –ø–æ–ª—É\n",
    "2. Equal Opportunity analysis\n",
    "3. Calibration –ø–æ –≥—Ä—É–ø–ø–∞–º\n",
    "4. Bias mitigation strategies\n",
    "\n",
    "**Part 6: Decision Tree Visualization**\n",
    "1. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∞–≤–∏–ª Decision Tree\n",
    "2. Rule extraction\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª –ß–∞—Å—Ç—å 1: Setup –∏ Dataset\n",
    "\n",
    "### 1.1 –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ë–∞–∑–æ–≤—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn - Models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Sklearn - Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "# Sklearn - Interpretability\n",
    "from sklearn.inspection import (\n",
    "    permutation_importance,\n",
    "    PartialDependenceDisplay,\n",
    "    partial_dependence\n",
    ")\n",
    "from sklearn.tree import plot_tree, export_text\n",
    "\n",
    "# SHAP\n",
    "import shap\n",
    "shap.initjs()  # –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ Jupyter\n",
    "\n",
    "# LIME\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"\\n‚úÖ –í—Å–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
    "print(f\"SHAP version: {shap.__version__}\")\n",
    "print(f\"LIME version: {lime.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 –ó–∞–≥—Ä—É–∑–∫–∞ Adult Census Dataset\n",
    "\n",
    "**Adult Income Dataset** (—Ç–∞–∫–∂–µ –∏–∑–≤–µ—Å—Ç–µ–Ω –∫–∞–∫ Census Income):\n",
    "- **–ó–∞–¥–∞—á–∞:** –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å, –∑–∞—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ª–∏ —á–µ–ª–æ–≤–µ–∫ >50K –≤ –≥–æ–¥\n",
    "- **–†–∞–∑–º–µ—Ä:** ~48,000 –∑–∞–ø–∏—Å–µ–π\n",
    "- **–ü—Ä–∏–∑–Ω–∞–∫–∏:** age, workclass, education, marital-status, occupation, relationship, race, sex, capital-gain, capital-loss, hours-per-week, native-country\n",
    "- **Target:** income (<=50K or >50K)\n",
    "\n",
    "**–í–∞–∂–Ω–æ –¥–ª—è XAI:**\n",
    "- Sensitive attributes: sex, race ‚Üí fairness analysis\n",
    "- Categorical features ‚Üí –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n",
    "- Real-world implications ‚Üí ethical AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º Adult Census dataset\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π dataset –∏–ª–∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ UCI repository\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "\n",
    "column_names = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "    'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "    'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'\n",
    "]\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "try:\n",
    "    df = pd.read_csv(url, names=column_names, sep=',\\s*', engine='python', na_values='?')\n",
    "    print(\"‚úÖ Dataset –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ UCI repository\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ UCI, —Å–æ–∑–¥–∞—é —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π dataset\")\n",
    "    # –°–æ–∑–¥–∞—ë–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π dataset, –µ—Å–ª–∏ –Ω–µ—Ç –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞\n",
    "    np.random.seed(42)\n",
    "    n_samples = 30000\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'age': np.random.randint(17, 90, n_samples),\n",
    "        'workclass': np.random.choice(['Private', 'Self-emp', 'Govt', 'Without-pay'], n_samples, p=[0.7, 0.15, 0.1, 0.05]),\n",
    "        'education': np.random.choice(['HS-grad', 'Some-college', 'Bachelors', 'Masters', 'Doctorate'], n_samples, p=[0.3, 0.25, 0.3, 0.1, 0.05]),\n",
    "        'education-num': np.random.randint(1, 16, n_samples),\n",
    "        'marital-status': np.random.choice(['Married', 'Never-married', 'Divorced'], n_samples, p=[0.5, 0.35, 0.15]),\n",
    "        'occupation': np.random.choice(['Tech', 'Sales', 'Service', 'Craft', 'Prof'], n_samples, p=[0.15, 0.25, 0.25, 0.2, 0.15]),\n",
    "        'relationship': np.random.choice(['Husband', 'Wife', 'Own-child', 'Not-in-family'], n_samples, p=[0.3, 0.25, 0.2, 0.25]),\n",
    "        'race': np.random.choice(['White', 'Black', 'Asian', 'Other'], n_samples, p=[0.8, 0.1, 0.05, 0.05]),\n",
    "        'sex': np.random.choice(['Male', 'Female'], n_samples, p=[0.67, 0.33]),\n",
    "        'capital-gain': np.random.choice([0] * 90 + list(np.random.randint(1000, 100000, 10)), n_samples),\n",
    "        'capital-loss': np.random.choice([0] * 95 + list(np.random.randint(100, 5000, 5)), n_samples),\n",
    "        'hours-per-week': np.random.randint(1, 100, n_samples),\n",
    "        'native-country': np.random.choice(['United-States', 'Other'], n_samples, p=[0.9, 0.1]),\n",
    "    })\n",
    "    \n",
    "    # –°–æ–∑–¥–∞—ë–º target —Å –Ω–µ–∫–æ—Ç–æ—Ä–æ–π –ª–æ–≥–∏–∫–æ–π\n",
    "    income_prob = (\n",
    "        (df['age'] > 30).astype(int) * 0.2 +\n",
    "        (df['education-num'] > 12).astype(int) * 0.3 +\n",
    "        (df['hours-per-week'] > 40).astype(int) * 0.2 +\n",
    "        (df['capital-gain'] > 0).astype(int) * 0.25\n",
    "    ) / 1.0\n",
    "    \n",
    "    df['income'] = (np.random.random(n_samples) < income_prob).astype(int)\n",
    "    df['income'] = df['income'].map({0: '<=50K', 1: '>50K'})\n",
    "\n",
    "# –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "\n",
    "# –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏\n",
    "df = df.dropna()\n",
    "\n",
    "print(f\"\\n–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {df.shape}\")\n",
    "print(f\"–ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {df.shape[1] - 1}\")\n",
    "print(f\"\\n–ü–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏:\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Exploratory Data Analysis (EDA)\n",
    "\n",
    "–ü—Ä–æ–≤–µ—Ä–∏–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ target variable, –æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è **fairness analysis**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "print(\"=\" * 60)\n",
    "print(\"–ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –î–ê–¢–ê–°–ï–¢–ï\")\n",
    "print(\"=\" * 60)\n",
    "print(df.info())\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ß–ò–°–õ–û–í–´–• –ü–†–ò–ó–ù–ê–ö–û–í\")\n",
    "print(\"=\" * 60)\n",
    "print(df.describe())\n",
    "\n",
    "# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ target\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"–†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï INCOME\")\n",
    "print(\"=\" * 60)\n",
    "print(df['income'].value_counts())\n",
    "print(f\"\\nClass balance: {df['income'].value_counts(normalize=True).to_dict()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –¥–ª—è fairness analysis\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Income by Sex (FAIRNESS CONCERN!)\n",
    "pd.crosstab(df['sex'], df['income'], normalize='index').plot(\n",
    "    kind='bar', ax=axes[0, 0], color=['#e74c3c', '#2ecc71']\n",
    ")\n",
    "axes[0, 0].set_title('Income Distribution by Sex', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Sex')\n",
    "axes[0, 0].set_ylabel('Proportion')\n",
    "axes[0, 0].legend(title='Income')\n",
    "axes[0, 0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Income by Race (FAIRNESS CONCERN!)\n",
    "pd.crosstab(df['race'], df['income'], normalize='index').plot(\n",
    "    kind='bar', ax=axes[0, 1], color=['#e74c3c', '#2ecc71']\n",
    ")\n",
    "axes[0, 1].set_title('Income Distribution by Race', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Race')\n",
    "axes[0, 1].set_ylabel('Proportion')\n",
    "axes[0, 1].legend(title='Income')\n",
    "\n",
    "# Income by Education\n",
    "pd.crosstab(df['education'], df['income'], normalize='index').plot(\n",
    "    kind='bar', ax=axes[0, 2], color=['#e74c3c', '#2ecc71']\n",
    ")\n",
    "axes[0, 2].set_title('Income Distribution by Education', fontsize=14, fontweight='bold')\n",
    "axes[0, 2].set_xlabel('Education')\n",
    "axes[0, 2].set_ylabel('Proportion')\n",
    "axes[0, 2].legend(title='Income')\n",
    "\n",
    "# Age distribution\n",
    "df[df['income'] == '<=50K']['age'].hist(bins=30, alpha=0.5, label='<=50K', ax=axes[1, 0], color='#e74c3c')\n",
    "df[df['income'] == '>50K']['age'].hist(bins=30, alpha=0.5, label='>50K', ax=axes[1, 0], color='#2ecc71')\n",
    "axes[1, 0].set_title('Age Distribution by Income', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Age')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Hours per week\n",
    "df[df['income'] == '<=50K']['hours-per-week'].hist(bins=30, alpha=0.5, label='<=50K', ax=axes[1, 1], color='#e74c3c')\n",
    "df[df['income'] == '>50K']['hours-per-week'].hist(bins=30, alpha=0.5, label='>50K', ax=axes[1, 1], color='#2ecc71')\n",
    "axes[1, 1].set_title('Hours per Week by Income', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Hours per Week')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "# Education years\n",
    "df.boxplot(column='education-num', by='income', ax=axes[1, 2])\n",
    "axes[1, 2].set_title('Education Years by Income', fontsize=14, fontweight='bold')\n",
    "axes[1, 2].set_xlabel('Income')\n",
    "axes[1, 2].set_ylabel('Education Years')\n",
    "plt.sca(axes[1, 2])\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.suptitle('EDA: Income Distribution Analysis', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è FAIRNESS CONCERNS DETECTED:\")\n",
    "print(\"- Income distribution varies by SEX\")\n",
    "print(\"- Income distribution varies by RACE\")\n",
    "print(\"‚Üí –ù—É–∂–µ–Ω fairness analysis –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ bias!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Preprocessing\n",
    "\n",
    "**–ó–∞–¥–∞—á–∏:**\n",
    "1. Encoding –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "2. Scaling —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "3. Train/Test split\n",
    "4. **–í–∞–∂–Ω–æ:** –°–æ—Ö—Ä–∞–Ω–∏–º sensitive attributes (sex, race) –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è fairness analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞—ë–º –∫–æ–ø–∏—é –¥–ª—è preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# –ë–∏–Ω–∞—Ä–Ω—ã–π target\n",
    "df_processed['income'] = (df_processed['income'] == '>50K').astype(int)\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º sensitive attributes –î–û encoding (–¥–ª—è fairness analysis)\n",
    "sensitive_features = df_processed[['sex', 'race']].copy()\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "categorical_cols = [\n",
    "    'workclass', 'education', 'marital-status', 'occupation',\n",
    "    'relationship', 'race', 'sex', 'native-country'\n",
    "]\n",
    "\n",
    "numerical_cols = [\n",
    "    'age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week'\n",
    "]\n",
    "\n",
    "# Label Encoding –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_processed[col] = le.fit_transform(df_processed[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "print(\"‚úÖ Label Encoding completed\")\n",
    "print(f\"–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(categorical_cols)}\")\n",
    "print(f\"–ß–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(numerical_cols)}\")\n",
    "\n",
    "# Feature matrix –∏ target\n",
    "feature_cols = categorical_cols + numerical_cols\n",
    "X = df_processed[feature_cols]\n",
    "y = df_processed['income']\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Positive class ratio: {y.mean():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# –¢–∞–∫–∂–µ split –¥–ª—è sensitive features (–Ω—É–∂–Ω–æ –¥–ª—è fairness analysis)\n",
    "sensitive_train, sensitive_test = train_test_split(\n",
    "    sensitive_features, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scaling —Ç–æ–ª—å–∫–æ –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# –í–∞–∂–Ω–æ: scaling —Ç–æ–ª—å–∫–æ –Ω–∞ train, –∑–∞—Ç–µ–º transform –Ω–∞ test\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "X_train_scaled[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
    "X_test_scaled[numerical_cols] = scaler.transform(X_test[numerical_cols])\n",
    "\n",
    "print(\"‚úÖ Train/Test split completed\")\n",
    "print(f\"Train size: {X_train.shape[0]} ({X_train.shape[0] / len(X):.1%})\")\n",
    "print(f\"Test size: {X_test.shape[0]} ({X_test.shape[0] / len(X):.1%})\")\n",
    "print(f\"\\nTrain positive class: {y_train.mean():.3f}\")\n",
    "print(f\"Test positive class: {y_test.mean():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Model Training\n",
    "\n",
    "–û–±—É—á–∏–º 3 –º–æ–¥–µ–ª–∏ —Ä–∞–∑–Ω–æ–π complexity –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è interpretability:\n",
    "\n",
    "1. **Logistic Regression** - –ø—Ä–æ—Å—Ç–∞—è, –ª–∏–Ω–µ–π–Ω–∞—è, interpretable –∏–∑ –∫–æ—Ä–æ–±–∫–∏\n",
    "2. **Random Forest** - ensemble, –Ω–µ–ª–∏–Ω–µ–π–Ω–∞—è, —Å—Ä–µ–¥–Ω–µ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏\n",
    "3. **XGBoost** - gradient boosting, –≤—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å, —Å–ª–æ–∂–Ω–∞—è\n",
    "\n",
    "–ó–∞—Ç–µ–º –ø—Ä–∏–º–µ–Ω–∏–º XAI –º–µ—Ç–æ–¥—ã –∫–æ –≤—Å–µ–º —Ç—Ä—ë–º."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "models = {}\n",
    "results = {}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"–û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Logistic Regression\n",
    "print(\"\\n[1/3] Training Logistic Regression...\")\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "lr_pred = lr.predict(X_test_scaled)\n",
    "lr_pred_proba = lr.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "models['Logistic Regression'] = lr\n",
    "results['Logistic Regression'] = {\n",
    "    'predictions': lr_pred,\n",
    "    'probabilities': lr_pred_proba,\n",
    "    'accuracy': accuracy_score(y_test, lr_pred),\n",
    "    'precision': precision_score(y_test, lr_pred),\n",
    "    'recall': recall_score(y_test, lr_pred),\n",
    "    'f1': f1_score(y_test, lr_pred),\n",
    "    'roc_auc': roc_auc_score(y_test, lr_pred_proba)\n",
    "}\n",
    "print(\"‚úÖ Logistic Regression trained\")\n",
    "\n",
    "# 2. Random Forest\n",
    "print(\"\\n[2/3] Training Random Forest...\")\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "rf_pred = rf.predict(X_test_scaled)\n",
    "rf_pred_proba = rf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "models['Random Forest'] = rf\n",
    "results['Random Forest'] = {\n",
    "    'predictions': rf_pred,\n",
    "    'probabilities': rf_pred_proba,\n",
    "    'accuracy': accuracy_score(y_test, rf_pred),\n",
    "    'precision': precision_score(y_test, rf_pred),\n",
    "    'recall': recall_score(y_test, rf_pred),\n",
    "    'f1': f1_score(y_test, rf_pred),\n",
    "    'roc_auc': roc_auc_score(y_test, rf_pred_proba)\n",
    "}\n",
    "print(\"‚úÖ Random Forest trained\")\n",
    "\n",
    "# 3. XGBoost\n",
    "print(\"\\n[3/3] Training XGBoost...\")\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test_scaled)\n",
    "xgb_pred_proba = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "models['XGBoost'] = xgb_model\n",
    "results['XGBoost'] = {\n",
    "    'predictions': xgb_pred,\n",
    "    'probabilities': xgb_pred_proba,\n",
    "    'accuracy': accuracy_score(y_test, xgb_pred),\n",
    "    'precision': precision_score(y_test, xgb_pred),\n",
    "    'recall': recall_score(y_test, xgb_pred),\n",
    "    'f1': f1_score(y_test, xgb_pred),\n",
    "    'roc_auc': roc_auc_score(y_test, xgb_pred_proba)\n",
    "}\n",
    "print(\"‚úÖ XGBoost trained\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ –í–°–ï –ú–û–î–ï–õ–ò –û–ë–£–ß–ï–ù–´\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π\n",
    "comparison_df = pd.DataFrame({\n",
    "    model_name: {\n",
    "        'Accuracy': metrics['accuracy'],\n",
    "        'Precision': metrics['precision'],\n",
    "        'Recall': metrics['recall'],\n",
    "        'F1-Score': metrics['f1'],\n",
    "        'ROC AUC': metrics['roc_auc']\n",
    "    }\n",
    "    for model_name, metrics in results.items()\n",
    "}).T\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Metrics comparison\n",
    "comparison_df.plot(kind='bar', ax=axes[0], width=0.8)\n",
    "axes[0].set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Model')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].legend(title='Metrics', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# ROC Curves\n",
    "for model_name, metrics in results.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, metrics['probabilities'])\n",
    "    auc = metrics['roc_auc']\n",
    "    axes[1].plot(fpr, tpr, label=f'{model_name} (AUC={auc:.3f})', linewidth=2)\n",
    "\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1)\n",
    "axes[1].set_title('ROC Curves', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].legend(loc='lower right')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ –ù–∞–±–ª—é–¥–µ–Ω–∏—è:\")\n",
    "print(\"- XGBoost –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à—É—é performance (–æ–±—ã—á–Ω–æ)\")\n",
    "print(\"- Random Forest –±–ª–∏–∑–æ–∫ –∫ XGBoost\")\n",
    "print(\"- Logistic Regression –ø—Ä–æ—â–µ, –Ω–æ –º–µ–Ω–µ–µ —Ç–æ—á–Ω–∞—è\")\n",
    "print(\"\\n‚Üí –¢–µ–ø–µ—Ä—å –ø—Ä–∏–º–µ–Ω–∏–º XAI –º–µ—Ç–æ–¥—ã –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üí° –ß–∞—Å—Ç—å 2: SHAP Analysis\n",
    "\n",
    "### –ß—Ç–æ —Ç–∞–∫–æ–µ SHAP?\n",
    "\n",
    "**SHAP (SHapley Additive exPlanations)** - unified framework –¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è predictions –ª—é–±—ã—Ö ML –º–æ–¥–µ–ª–µ–π.\n",
    "\n",
    "**–û—Å–Ω–æ–≤–∞–Ω –Ω–∞:**\n",
    "- **Shapley Values** –∏–∑ —Ç–µ–æ—Ä–∏–∏ –∏–≥—Ä (Lloyd Shapley, Nobel Prize 2012)\n",
    "- **–ò–¥–µ—è:** –°–∫–æ–ª—å–∫–æ –∫–∞–∂–¥—ã–π –ø—Ä–∏–∑–Ω–∞–∫ \"–≤–∫–ª–∞–¥—ã–≤–∞–µ—Ç\" –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ?\n",
    "\n",
    "**–ü–æ—á–µ–º—É SHAP –ª—É—á—à–µ –¥—Ä—É–≥–∏—Ö –º–µ—Ç–æ–¥–æ–≤:**\n",
    "\n",
    "1. **–¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω** - –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ —Å –≥–∞—Ä–∞–Ω—Ç–∏—è–º–∏:\n",
    "   - ‚úÖ **Local accuracy:** sum(SHAP values) = prediction - baseline\n",
    "   - ‚úÖ **Consistency:** –µ—Å–ª–∏ –ø—Ä–∏–∑–Ω–∞–∫ –≤–∞–∂–Ω–µ–µ –≤ –º–æ–¥–µ–ª–∏ B, –µ–≥–æ SHAP value –≤—ã—à–µ\n",
    "   - ‚úÖ **Missingness:** –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–π –ø—Ä–∏–∑–Ω–∞–∫ –∏–º–µ–µ—Ç SHAP = 0\n",
    "\n",
    "2. **–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç—å:**\n",
    "   - `TreeSHAP`: –¥–ª—è –¥—Ä–µ–≤–æ–≤–∏–¥–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (RF, XGBoost, LightGBM) - **–æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ**\n",
    "   - `KernelSHAP`: –¥–ª—è –ª—é–±—ã—Ö –º–æ–¥–µ–ª–µ–π (—á–µ—Ä–Ω—ã–π —è—â–∏–∫) - –º–µ–¥–ª–µ–Ω–Ω–µ–µ\n",
    "   - `DeepSHAP`: –¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π\n",
    "   - `LinearSHAP`: –¥–ª—è –ª–∏–Ω–µ–π–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π\n",
    "\n",
    "3. **Global + Local:**\n",
    "   - Global importance: —Å—Ä–µ–¥–Ω–µ–µ |SHAP value| –ø–æ –≤—Å–µ–º samples\n",
    "   - Local explanation: SHAP values –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ prediction\n",
    "\n",
    "**–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è SHAP value:**\n",
    "- `SHAP value > 0`: –ø—Ä–∏–∑–Ω–∞–∫ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç prediction (–ø—É—à–∏—Ç –∫ –∫–ª–∞—Å—Å—É 1)\n",
    "- `SHAP value < 0`: –ø—Ä–∏–∑–Ω–∞–∫ —É–º–µ–Ω—å—à–∞–µ—Ç prediction (–ø—É—à–∏—Ç –∫ –∫–ª–∞—Å—Å—É 0)\n",
    "- `|SHAP value|` = magnitude of effect\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 SHAP –¥–ª—è Random Forest\n",
    "\n",
    "–ò—Å–ø–æ–ª—å–∑—É–µ–º **TreeSHAP** - —Ç–æ—á–Ω—ã–π –∏ –±—ã—Å—Ç—Ä—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è –¥—Ä–µ–≤–æ–≤–∏–¥–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"SHAP ANALYSIS: RANDOM FOREST\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# TreeExplainer –¥–ª—è Random Forest\n",
    "print(\"\\nCreating TreeExplainer for Random Forest...\")\n",
    "rf_explainer = shap.TreeExplainer(models['Random Forest'])\n",
    "\n",
    "# –í—ã—á–∏—Å–ª—è–µ–º SHAP values –¥–ª—è test set\n",
    "# –î–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–µ 1000 samples\n",
    "print(\"Computing SHAP values (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –º–∏–Ω—É—Ç—É)...\")\n",
    "rf_shap_values = rf_explainer.shap_values(X_test_scaled.iloc[:1000])\n",
    "\n",
    "# SHAP –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç [values_class_0, values_class_1] –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "# –ù–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç –∫–ª–∞—Å—Å 1 (>50K)\n",
    "if isinstance(rf_shap_values, list):\n",
    "    rf_shap_values_class1 = rf_shap_values[1]\n",
    "else:\n",
    "    rf_shap_values_class1 = rf_shap_values\n",
    "\n",
    "print(f\"‚úÖ SHAP values computed\")\n",
    "print(f\"Shape: {rf_shap_values_class1.shape}\")\n",
    "print(f\"(samples={rf_shap_values_class1.shape[0]}, features={rf_shap_values_class1.shape[1]})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Global Feature Importance (Summary Plot)\n",
    "\n",
    "**Summary plot** –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç:\n",
    "- –ö–∞–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ (—Å–≤–µ—Ä—Ö—É)\n",
    "- –ö–∞–∫ –∑–Ω–∞—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–∞ –≤–ª–∏—è—é—Ç –Ω–∞ prediction (—Ü–≤–µ—Ç: red=high, blue=low)\n",
    "- Magnitude of effect (SHAP value –ø–æ –æ—Å–∏ X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary plot - –≥–ª–æ–±–∞–ª—å–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(\n",
    "    rf_shap_values_class1,\n",
    "    X_test_scaled.iloc[:1000],\n",
    "    feature_names=feature_cols,\n",
    "    show=False\n",
    ")\n",
    "plt.title('SHAP Summary Plot: Random Forest (Class >50K)', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è Summary Plot:\")\n",
    "print(\"- Features sorted by importance (top = most important)\")\n",
    "print(\"- Color: red = high feature value, blue = low feature value\")\n",
    "print(\"- X-axis: SHAP value (impact on prediction)\")\n",
    "print(\"- Positive SHAP ‚Üí increases probability of >50K\")\n",
    "print(\"- Negative SHAP ‚Üí decreases probability of >50K\")\n",
    "print(\"\\n–ü—Ä–∏–º–µ—Ä—ã:\")\n",
    "print(\"- capital-gain: high values (red) ‚Üí positive SHAP ‚Üí higher income probability\")\n",
    "print(\"- age: older age (red) ‚Üí positive SHAP ‚Üí higher income probability\")\n",
    "print(\"- education-num: more education (red) ‚Üí positive SHAP ‚Üí higher income probability\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot - —Å—Ä–µ–¥–Ω—è—è –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(\n",
    "    rf_shap_values_class1,\n",
    "    X_test_scaled.iloc[:1000],\n",
    "    feature_names=feature_cols,\n",
    "    plot_type='bar',\n",
    "    show=False\n",
    ")\n",
    "plt.title('SHAP Feature Importance: Random Forest', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ –¢–æ–ø-5 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\")\n",
    "mean_abs_shap = np.abs(rf_shap_values_class1).mean(axis=0)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Mean |SHAP|': mean_abs_shap\n",
    "}).sort_values('Mean |SHAP|', ascending=False)\n",
    "\n",
    "print(feature_importance.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Local Explanations (Waterfall Plot)\n",
    "\n",
    "**Waterfall plot** –æ–±—ä—è—Å–Ω—è–µ—Ç **–æ–¥–Ω–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ prediction**:\n",
    "- –ù–∞—á–∏–Ω–∞–µ–º —Å baseline (—Å—Ä–µ–¥–Ω–µ–µ prediction –ø–æ –≤—Å–µ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É)\n",
    "- –ö–∞–∂–¥—ã–π –ø—Ä–∏–∑–Ω–∞–∫ –¥–≤–∏–≥–∞–µ—Ç prediction –≤–≤–µ—Ä—Ö –∏–ª–∏ –≤–Ω–∏–∑\n",
    "- –ö–æ–Ω–µ—á–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ = actual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í—ã–±–µ—Ä–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã—Ö samples –¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è\n",
    "# Sample 1: High income prediction\n",
    "high_income_idx = np.where((y_test.iloc[:1000] == 1) & (results['Random Forest']['predictions'][:1000] == 1))[0][0]\n",
    "\n",
    "# Sample 2: Low income prediction\n",
    "low_income_idx = np.where((y_test.iloc[:1000] == 0) & (results['Random Forest']['predictions'][:1000] == 0))[0][0]\n",
    "\n",
    "print(f\"Selected samples:\")\n",
    "print(f\"- High income prediction: index {high_income_idx}\")\n",
    "print(f\"- Low income prediction: index {low_income_idx}\")\n",
    "\n",
    "# Waterfall plot –¥–ª—è high income\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXAMPLE 1: HIGH INCOME PREDICTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "shap.plots.waterfall(\n",
    "    shap.Explanation(\n",
    "        values=rf_shap_values_class1[high_income_idx],\n",
    "        base_values=rf_explainer.expected_value[1] if isinstance(rf_explainer.expected_value, list) else rf_explainer.expected_value,\n",
    "        data=X_test_scaled.iloc[high_income_idx].values,\n",
    "        feature_names=feature_cols\n",
    "    ),\n",
    "    show=False\n",
    ")\n",
    "plt.title(f'Waterfall Plot: Sample {high_income_idx} (Predicted >50K)', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n–ê–∫—Ç—É–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\")\n",
    "sample_df = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Value': X_test_scaled.iloc[high_income_idx].values,\n",
    "    'SHAP': rf_shap_values_class1[high_income_idx]\n",
    "}).sort_values('SHAP', key=abs, ascending=False)\n",
    "print(sample_df.head(8).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waterfall plot –¥–ª—è low income\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXAMPLE 2: LOW INCOME PREDICTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "shap.plots.waterfall(\n",
    "    shap.Explanation(\n",
    "        values=rf_shap_values_class1[low_income_idx],\n",
    "        base_values=rf_explainer.expected_value[1] if isinstance(rf_explainer.expected_value, list) else rf_explainer.expected_value,\n",
    "        data=X_test_scaled.iloc[low_income_idx].values,\n",
    "        feature_names=feature_cols\n",
    "    ),\n",
    "    show=False\n",
    ")\n",
    "plt.title(f'Waterfall Plot: Sample {low_income_idx} (Predicted <=50K)', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n–ê–∫—Ç—É–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\")\n",
    "sample_df = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Value': X_test_scaled.iloc[low_income_idx].values,\n",
    "    'SHAP': rf_shap_values_class1[low_income_idx]\n",
    "}).sort_values('SHAP', key=abs, ascending=False)\n",
    "print(sample_df.head(8).to_string(index=False))\n",
    "\n",
    "print(\"\\nüí° –ö–ª—é—á–µ–≤–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ SHAP:\")\n",
    "print(\"–ú—ã –º–æ–∂–µ–º –æ–±—ä—è—Å–Ω–∏—Ç—å –ö–ê–ñ–î–û–ï prediction, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –º–æ–¥–µ–ª—å –≤ —Ü–µ–ª–æ–º!\")\n",
    "print(\"–≠—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è:\")\n",
    "print(\"- –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ (–æ–±—ä—è—Å–Ω–∏—Ç—å –ø–∞—Ü–∏–µ–Ω—Ç—É)\")\n",
    "print(\"- –ö—Ä–µ–¥–∏—Ç–Ω–æ–≥–æ —Å–∫–æ—Ä–∏–Ω–≥–∞ (–æ–±—ä—è—Å–Ω–∏—Ç—å –æ—Ç–∫–∞–∑)\")\n",
    "print(\"- Fraud detection (–ø–æ—á–µ–º—É —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–∞)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Dependence Plots (Feature Interactions)\n",
    "\n",
    "**Dependence plot** –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –∫–∞–∫ SHAP value –ø—Ä–∏–∑–Ω–∞–∫–∞ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è:\n",
    "- X-axis: –∑–Ω–∞—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∞\n",
    "- Y-axis: SHAP value (impact)\n",
    "- Color: –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –¥—Ä—É–≥–∏–º –ø—Ä–∏–∑–Ω–∞–∫–æ–º (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è —Å–∞–º–æ–µ —Å–∏–ª—å–Ω–æ–µ)\n",
    "\n",
    "–ü–æ–º–æ–≥–∞–µ—Ç –Ω–∞–π—Ç–∏ **–Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã –∏ interactions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependence plots –¥–ª—è —Ç–æ–ø –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "top_features = feature_importance.head(6)['Feature'].values\n",
    "\n",
    "for idx, feature in enumerate(top_features):\n",
    "    plt.sca(axes[idx])\n",
    "    shap.dependence_plot(\n",
    "        feature,\n",
    "        rf_shap_values_class1,\n",
    "        X_test_scaled.iloc[:1000],\n",
    "        feature_names=feature_cols,\n",
    "        show=False,\n",
    "        ax=axes[idx]\n",
    "    )\n",
    "    axes[idx].set_title(f'Dependence: {feature}', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('SHAP Dependence Plots: Feature Interactions', fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä –ß—Ç–æ –º—ã –≤–∏–¥–∏–º:\")\n",
    "print(\"- –ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (–Ω–µ –ø—Ä–æ—Å—Ç–æ –ª–∏–Ω–µ–π–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã)\")\n",
    "print(\"- Feature interactions (—Ü–≤–µ—Ç –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –¥—Ä—É–≥–∏–º –ø—Ä–∏–∑–Ω–∞–∫–æ–º)\")\n",
    "print(\"- Thresholds –∏ breakpoints –≤ –≤–ª–∏—è–Ω–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞\")\n",
    "print(\"\\n–ü—Ä–∏–º–µ—Ä:\")\n",
    "print(\"- age: –º–æ–ª–æ–¥–æ–π –≤–æ–∑—Ä–∞—Å—Ç ‚Üí negative SHAP, –ø–æ–∂–∏–ª–æ–π ‚Üí positive SHAP\")\n",
    "print(\"- capital-gain: 0 ‚Üí negative, >0 ‚Üí strong positive SHAP\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 SHAP –¥–ª—è XGBoost\n",
    "\n",
    "–ü–æ–≤—Ç–æ—Ä–∏–º –∞–Ω–∞–ª–∏–∑ –¥–ª—è XGBoost (–æ–±—ã—á–Ω–æ –±–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"SHAP ANALYSIS: XGBOOST\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# TreeExplainer –¥–ª—è XGBoost\n",
    "print(\"\\nCreating TreeExplainer for XGBoost...\")\n",
    "xgb_explainer = shap.TreeExplainer(models['XGBoost'])\n",
    "\n",
    "# –í—ã—á–∏—Å–ª—è–µ–º SHAP values\n",
    "print(\"Computing SHAP values...\")\n",
    "xgb_shap_values = xgb_explainer.shap_values(X_test_scaled.iloc[:1000])\n",
    "\n",
    "print(f\"‚úÖ SHAP values computed\")\n",
    "print(f\"Shape: {xgb_shap_values.shape}\")\n",
    "\n",
    "# Summary plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Summary plot\n",
    "plt.sca(axes[0])\n",
    "shap.summary_plot(\n",
    "    xgb_shap_values,\n",
    "    X_test_scaled.iloc[:1000],\n",
    "    feature_names=feature_cols,\n",
    "    show=False\n",
    ")\n",
    "axes[0].set_title('SHAP Summary Plot: XGBoost', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Bar plot\n",
    "plt.sca(axes[1])\n",
    "shap.summary_plot(\n",
    "    xgb_shap_values,\n",
    "    X_test_scaled.iloc[:1000],\n",
    "    feature_names=feature_cols,\n",
    "    plot_type='bar',\n",
    "    show=False\n",
    ")\n",
    "axes[1].set_title('SHAP Feature Importance: XGBoost', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance comparison\n",
    "print(\"\\nüéØ –¢–æ–ø-10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ SHAP (XGBoost):\")\n",
    "xgb_mean_abs_shap = np.abs(xgb_shap_values).mean(axis=0)\n",
    "xgb_feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Mean |SHAP|': xgb_mean_abs_shap\n",
    "}).sort_values('Mean |SHAP|', ascending=False)\n",
    "\n",
    "print(xgb_feature_importance.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 –°—Ä–∞–≤–Ω–µ–Ω–∏–µ Random Forest vs XGBoost\n",
    "\n",
    "–°—Ä–∞–≤–Ω–∏–º feature importance –¥–ª—è –æ–±–µ–∏—Ö –º–æ–¥–µ–ª–µ–π."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º feature importance RF vs XGBoost\n",
    "comparison = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'RF SHAP': mean_abs_shap,\n",
    "    'XGB SHAP': xgb_mean_abs_shap\n",
    "}).sort_values('XGB SHAP', ascending=False)\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "x = np.arange(len(comparison.head(12)))\n",
    "width = 0.35\n",
    "\n",
    "ax.barh(x - width/2, comparison.head(12)['RF SHAP'], width, label='Random Forest', alpha=0.8)\n",
    "ax.barh(x + width/2, comparison.head(12)['XGB SHAP'], width, label='XGBoost', alpha=0.8)\n",
    "\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(comparison.head(12)['Feature'])\n",
    "ax.set_xlabel('Mean |SHAP value|', fontsize=12)\n",
    "ax.set_title('Feature Importance Comparison: RF vs XGBoost', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.invert_yaxis()\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç –ù–∞–±–ª—é–¥–µ–Ω–∏—è:\")\n",
    "print(\"- –û–±–µ –º–æ–¥–µ–ª–∏ —Å–æ–≥–ª–∞—Å–Ω—ã –≤ —Ç–æ–ø–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö (capital-gain, age, education-num)\")\n",
    "print(\"- Ranking –º–æ–∂–µ—Ç –Ω–µ–º–Ω–æ–≥–æ –æ—Ç–ª–∏—á–∞—Ç—å—Å—è\")\n",
    "print(\"- XGBoost –º–æ–∂–µ—Ç –≤—ã—è–≤–ª—è—Ç—å –±–æ–ª–µ–µ —Ç–æ–Ω–∫–∏–µ interactions\")\n",
    "print(\"\\n‚Üí –ö–æ–Ω—Å–µ–Ω—Å—É—Å –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏ –ø–æ–≤—ã—à–∞–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî¨ –ß–∞—Å—Ç—å 3: LIME Analysis\n",
    "\n",
    "### –ß—Ç–æ —Ç–∞–∫–æ–µ LIME?\n",
    "\n",
    "**LIME (Local Interpretable Model-agnostic Explanations)** - –º–µ—Ç–æ–¥ –¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è **–æ—Ç–¥–µ–ª—å–Ω—ã—Ö predictions** –ª—é–±–æ–π –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "**–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç:**\n",
    "1. –ë–µ—Ä—ë–º –∏–Ω—Ç–µ—Ä–µ—Å—É—é—â–∏–π –Ω–∞—Å sample\n",
    "2. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º perturbed samples (—Å–ª–µ–≥–∫–∞ –∏–∑–º–µ–Ω—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏)\n",
    "3. –ü–æ–ª—É—á–∞–µ–º predictions –¥–ª—è –Ω–∏—Ö –æ—Ç —á–µ—Ä–Ω–æ–≥–æ —è—â–∏–∫–∞\n",
    "4. –û–±—É—á–∞–µ–º **–ø—Ä–æ—Å—Ç—É—é –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—É—é –º–æ–¥–µ–ª—å** (linear/decision tree) –ª–æ–∫–∞–ª—å–Ω–æ\n",
    "5. –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º –ø—Ä–æ—Å—Ç—É—é –º–æ–¥–µ–ª—å\n",
    "\n",
    "**LIME vs SHAP:**\n",
    "\n",
    "| –ê—Å–ø–µ–∫—Ç | LIME | SHAP |\n",
    "|--------|------|------|\n",
    "| –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ | ‚ùå Heuristic | ‚úÖ Game theory (Shapley) |\n",
    "| Consistency | ‚ùå –ù–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç—Å—è | ‚úÖ –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç—Å—è |\n",
    "| –°–∫–æ—Ä–æ—Å—Ç—å (local) | ‚úÖ –ë—ã—Å—Ç—Ä–µ–µ | ‚ö†Ô∏è KernelSHAP –º–µ–¥–ª–µ–Ω–Ω–µ–µ |\n",
    "| –°–∫–æ—Ä–æ—Å—Ç—å (tree models) | ‚ö†Ô∏è –ú–µ–¥–ª–µ–Ω–Ω–µ–µ | ‚úÖ TreeSHAP –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ |\n",
    "| Stability | ‚ùå –ú–æ–∂–µ—Ç –≤–∞—Ä—å–∏—Ä–æ–≤–∞—Ç—å—Å—è | ‚úÖ –°—Ç–∞–±–∏–ª—å–Ω–µ–µ |\n",
    "| Global interpretation | ‚ùå –ù–µ—Ç | ‚úÖ –î–∞ |\n",
    "| Use case | Quick local explanations | Production, —Ç—Ä–µ–±—É–µ—Ç—Å—è consistency |\n",
    "\n",
    "**–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LIME:**\n",
    "- ‚úÖ –ë—ã—Å—Ç—Ä—ã–µ ad-hoc –ª–æ–∫–∞–ª—å–Ω—ã–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è\n",
    "- ‚úÖ –ú–æ–¥–µ–ª–∏, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç TreeSHAP (custom black boxes)\n",
    "- ‚úÖ Text/Image classification (LIME —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –Ω–∏–º–∏)\n",
    "- ‚ö†Ô∏è –ù–ï –¥–ª—è production, –≥–¥–µ –≤–∞–∂–Ω–∞ consistency\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"LIME ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# –°–æ–∑–¥–∞—ë–º LIME explainer\n",
    "print(\"\\nCreating LIME Tabular Explainer...\")\n",
    "lime_explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    training_data=X_train_scaled.values,\n",
    "    feature_names=feature_cols,\n",
    "    class_names=['<=50K', '>50K'],\n",
    "    mode='classification',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LIME explainer created\")\n",
    "\n",
    "# –û–±—ä—è—Å–Ω—è–µ–º —Ç–æ—Ç –∂–µ high income sample, —á—Ç–æ –∏ —Å SHAP\n",
    "high_income_sample = X_test_scaled.iloc[high_income_idx].values\n",
    "\n",
    "print(f\"\\nExplaining sample {high_income_idx} (predicted >50K)...\")\n",
    "lime_exp = lime_explainer.explain_instance(\n",
    "    high_income_sample,\n",
    "    models['Random Forest'].predict_proba,\n",
    "    num_features=10,\n",
    "    num_samples=1000  # —Å–∫–æ–ª—å–∫–æ perturbed samples –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LIME explanation computed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è LIME explanation\n",
    "fig = lime_exp.as_pyplot_figure(label=1)  # label=1 –¥–ª—è –∫–ª–∞—Å—Å–∞ >50K\n",
    "plt.title(f'LIME Explanation: Sample {high_income_idx} (>50K)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# –¢–µ–∫—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ\n",
    "print(\"\\nüìä LIME Explanation (Top features):\")\n",
    "print(\"=\"*50)\n",
    "for feature, weight in lime_exp.as_list(label=1)[:10]:\n",
    "    print(f\"{feature:40s} : {weight:+.4f}\")\n",
    "\n",
    "print(\"\\nüí° –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:\")\n",
    "print(\"- Positive weight ‚Üí increases P(>50K)\")\n",
    "print(\"- Negative weight ‚Üí decreases P(>50K)\")\n",
    "print(\"- –í–µ–ª–∏—á–∏–Ω–∞ weight = strength of effect\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 –°—Ä–∞–≤–Ω–µ–Ω–∏–µ LIME vs SHAP\n",
    "\n",
    "–°—Ä–∞–≤–Ω–∏–º –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –¥–ª—è —Ç–æ–≥–æ –∂–µ sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ LIME vs SHAP –¥–ª—è –æ–¥–Ω–æ–≥–æ sample\n",
    "print(\"=\" * 70)\n",
    "print(f\"COMPARISON: LIME vs SHAP (Sample {high_income_idx})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# LIME feature importances\n",
    "lime_dict = dict(lime_exp.as_list(label=1))\n",
    "lime_values = {}\n",
    "for feature in feature_cols:\n",
    "    # LIME –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–æ–∫–∏ —Ç–∏–ø–∞ \"feature <= value\"\n",
    "    for lime_feature, value in lime_dict.items():\n",
    "        if feature in lime_feature:\n",
    "            lime_values[feature] = value\n",
    "            break\n",
    "    if feature not in lime_values:\n",
    "        lime_values[feature] = 0.0\n",
    "\n",
    "# SHAP values –¥–ª—è —ç—Ç–æ–≥–æ sample\n",
    "shap_dict = dict(zip(feature_cols, rf_shap_values_class1[high_income_idx]))\n",
    "\n",
    "# –°–æ–∑–¥–∞—ë–º —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π DataFrame\n",
    "comparison_local = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'LIME': [lime_values.get(f, 0) for f in feature_cols],\n",
    "    'SHAP': [shap_dict[f] for f in feature_cols]\n",
    "})\n",
    "\n",
    "# –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∞–±—Å–æ–ª—é—Ç–Ω–æ–º—É –∑–Ω–∞—á–µ–Ω–∏—é SHAP\n",
    "comparison_local['abs_SHAP'] = comparison_local['SHAP'].abs()\n",
    "comparison_local = comparison_local.sort_values('abs_SHAP', ascending=False).head(10)\n",
    "\n",
    "print(\"\\nTop 10 features:\")\n",
    "print(comparison_local[['Feature', 'LIME', 'SHAP']].to_string(index=False))\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(comparison_local))\n",
    "width = 0.35\n",
    "\n",
    "ax.barh(x - width/2, comparison_local['LIME'], width, label='LIME', alpha=0.8)\n",
    "ax.barh(x + width/2, comparison_local['SHAP'], width, label='SHAP', alpha=0.8)\n",
    "\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(comparison_local['Feature'])\n",
    "ax.set_xlabel('Feature Importance (local explanation)', fontsize=12)\n",
    "ax.set_title(f'LIME vs SHAP: Sample {high_income_idx}', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.invert_yaxis()\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç –ù–∞–±–ª—é–¥–µ–Ω–∏—è:\")\n",
    "print(\"- LIME –∏ SHAP –æ–±—ã—á–Ω–æ —Å–æ–≥–ª–∞—Å–Ω—ã –≤ —Ç–æ–ø–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö\")\n",
    "print(\"- Magnitude –º–æ–∂–µ—Ç –æ—Ç–ª–∏—á–∞—Ç—å—Å—è (—Ä–∞–∑–Ω—ã–µ —à–∫–∞–ª—ã)\")\n",
    "print(\"- SHAP –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–π –∏ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π\")\n",
    "print(\"- LIME –±—ã—Å—Ç—Ä–µ–µ –¥–ª—è quick ad-hoc explanations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä –ß–∞—Å—Ç—å 4: Partial Dependence Plots (PDP)\n",
    "\n",
    "### –ß—Ç–æ —Ç–∞–∫–æ–µ PDP?\n",
    "\n",
    "**Partial Dependence Plot** –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç **–≥–ª–æ–±–∞–ª—å–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ** –ø—Ä–∏–∑–Ω–∞–∫–∞ –Ω–∞ predictions:\n",
    "- –ö–∞–∫ –º–µ–Ω—è–µ—Ç—Å—è —Å—Ä–µ–¥–Ω–µ–µ prediction –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞?\n",
    "- –ù–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –¥—Ä—É–≥–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (marginalized)\n",
    "\n",
    "**–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç:**\n",
    "1. –§–∏–∫—Å–∏—Ä—É–µ–º –∏–Ω—Ç–µ—Ä–µ—Å—É—é—â–∏–π –ø—Ä–∏–∑–Ω–∞–∫ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö\n",
    "2. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è: –¥–µ–ª–∞–µ–º predictions –¥–ª—è –≤—Å–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "3. –£—Å—Ä–µ–¥–Ω—è–µ–º predictions\n",
    "4. –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫: feature value vs average prediction\n",
    "\n",
    "**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**\n",
    "- ‚úÖ Global view (–Ω–µ —Ç–æ–ª—å–∫–æ –ª–æ–∫–∞–ª—å–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ)\n",
    "- ‚úÖ –í–∏–∑—É–∞–ª—å–Ω–æ –ø–æ–Ω—è—Ç–Ω–æ\n",
    "- ‚úÖ –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã\n",
    "\n",
    "**–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:**\n",
    "- ‚ö†Ô∏è **Assumes independence:** –µ—Å–ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—Ç, –º–æ–∂–µ—Ç –≤–≤–æ–¥–∏—Ç—å –≤ –∑–∞–±–ª—É–∂–¥–µ–Ω–∏–µ\n",
    "- ‚ö†Ô∏è **Averages out heterogeneity:** –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –≤–∞—Ä–∏–∞—Ü–∏–∏\n",
    "\n",
    "**ICE (Individual Conditional Expectation) curves:**\n",
    "- –†–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É averaging\n",
    "- –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ sample –æ—Ç–¥–µ–ª—å–Ω–æ\n",
    "- PDP = average of ICE curves\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PARTIAL DEPENDENCE PLOTS (PDP)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# –¢–æ–ø –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è PDP\n",
    "pdp_features = ['age', 'education-num', 'hours-per-week', 'capital-gain']\n",
    "\n",
    "print(f\"\\nBuilding PDP for: {pdp_features}\")\n",
    "\n",
    "# PDP –¥–ª—è Random Forest\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "display = PartialDependenceDisplay.from_estimator(\n",
    "    models['Random Forest'],\n",
    "    X_test_scaled.iloc[:500],  # –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥–≤—ã–±–æ—Ä–∫—É –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏\n",
    "    features=pdp_features,\n",
    "    feature_names=feature_cols,\n",
    "    ax=ax,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "plt.suptitle('Partial Dependence Plots: Random Forest', fontsize=14, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ PDP completed\")\n",
    "print(\"\\nüìä –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:\")\n",
    "print(\"- age: –° –≤–æ–∑—Ä–∞—Å—Ç–æ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å >50K —Ä–∞—Å—Ç—ë—Ç (–¥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –º–æ–º–µ–Ω—Ç–∞)\")\n",
    "print(\"- education-num: –ë–æ–ª—å—à–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è ‚Üí –≤—ã—à–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å >50K\")\n",
    "print(\"- hours-per-week: –ë–æ–ª—å—à–µ —á–∞—Å–æ–≤ —Ä–∞–±–æ—Ç—ã ‚Üí –≤—ã—à–µ income\")\n",
    "print(\"- capital-gain: –ù–∞–ª–∏—á–∏–µ capital gain ‚Üí —Ä–µ–∑–∫–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D PDP –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π\n",
    "print(\"\\n2D Partial Dependence: Feature Interactions\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# PDP –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è age –∏ education-num\n",
    "display_2d = PartialDependenceDisplay.from_estimator(\n",
    "    models['Random Forest'],\n",
    "    X_test_scaled.iloc[:500],\n",
    "    features=[('age', 'education-num')],\n",
    "    feature_names=feature_cols,\n",
    "    ax=ax,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "plt.suptitle('2D PDP: Age √ó Education Interaction', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Interaction effect:\")\n",
    "print(\"- –°–≤–µ—Ç–ª—ã–µ –æ–±–ª–∞—Å—Ç–∏ = –≤—ã—Å–æ–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å >50K\")\n",
    "print(\"- –¢—ë–º–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏ = –Ω–∏–∑–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å >50K\")\n",
    "print(\"- –í–∏–¥–∏–º joint effect –≤–æ–∑—Ä–∞—Å—Ç–∞ –∏ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Permutation Importance\n",
    "\n",
    "**Permutation Importance** - –ø—Ä–æ—Å—Ç–æ–π –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\n",
    "\n",
    "**–ê–ª–≥–æ—Ä–∏—Ç–º:**\n",
    "1. –í—ã—á–∏—Å–ª—è–µ–º baseline metric (accuracy, AUC, etc.) –Ω–∞ test set\n",
    "2. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞:\n",
    "   - Shuffle –∑–Ω–∞—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–∞ (breaks relationship —Å target)\n",
    "   - –í—ã—á–∏—Å–ª—è–µ–º metric –∑–∞–Ω–æ–≤–æ\n",
    "   - Importance = baseline - shuffled metric\n",
    "3. Repeat –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
    "\n",
    "**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**\n",
    "- ‚úÖ **Model-agnostic** (—Ä–∞–±–æ—Ç–∞–µ—Ç —Å –ª—é–±–æ–π –º–æ–¥–µ–ª—å—é)\n",
    "- ‚úÖ **Handles correlations** (–≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç Gini importance)\n",
    "- ‚úÖ **Intuitive interpretation**\n",
    "\n",
    "**vs Gini Importance (Feature Importance –≤ Random Forest):**\n",
    "- Gini importance: —Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–∑–Ω–∞–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ splits\n",
    "- **–ü—Ä–æ–±–ª–µ–º–∞:** bias –∫ high-cardinality features\n",
    "- Permutation Importance: —Ä–µ–∞–ª—å–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ predictions\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PERMUTATION IMPORTANCE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# –í—ã—á–∏—Å–ª—è–µ–º permutation importance –¥–ª—è Random Forest\n",
    "print(\"\\nComputing Permutation Importance (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –º–∏–Ω—É—Ç—É)...\")\n",
    "perm_importance = permutation_importance(\n",
    "    models['Random Forest'],\n",
    "    X_test_scaled.iloc[:1000],\n",
    "    y_test.iloc[:1000],\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    scoring='roc_auc'\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Permutation Importance computed\")\n",
    "\n",
    "# –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ importance\n",
    "perm_sorted_idx = perm_importance.importances_mean.argsort()[::-1]\n",
    "\n",
    "# –°–æ–∑–¥–∞—ë–º DataFrame\n",
    "perm_df = pd.DataFrame({\n",
    "    'Feature': np.array(feature_cols)[perm_sorted_idx],\n",
    "    'Importance': perm_importance.importances_mean[perm_sorted_idx],\n",
    "    'Std': perm_importance.importances_std[perm_sorted_idx]\n",
    "})\n",
    "\n",
    "print(\"\\nüìä Top 10 Features by Permutation Importance:\")\n",
    "print(perm_df.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è Permutation Importance\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "top_n = 15\n",
    "ax.barh(\n",
    "    range(top_n),\n",
    "    perm_df.head(top_n)['Importance'],\n",
    "    xerr=perm_df.head(top_n)['Std'],\n",
    "    alpha=0.8\n",
    ")\n",
    "\n",
    "ax.set_yticks(range(top_n))\n",
    "ax.set_yticklabels(perm_df.head(top_n)['Feature'])\n",
    "ax.set_xlabel('Importance (decrease in ROC AUC)', fontsize=12)\n",
    "ax.set_title('Permutation Importance: Random Forest', fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:\")\n",
    "print(\"- –í—ã—Å–æ–∫–∞—è importance ‚Üí shuffling –ø—Ä–∏–∑–Ω–∞–∫–∞ —Å–∏–ª—å–Ω–æ —É—Ö—É–¥—à–∞–µ—Ç –º–æ–¥–µ–ª—å\")\n",
    "print(\"- Error bars –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç variance (—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å)\")\n",
    "print(\"- –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è importance ‚Üí –ø—Ä–∏–∑–Ω–∞–∫ –Ω–µ –ø–æ–º–æ–≥–∞–µ—Ç (—à—É–º)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ Gini vs Permutation Importance\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPARISON: Gini Importance vs Permutation Importance\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Gini importance (–≤—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –≤ Random Forest)\n",
    "gini_importance = models['Random Forest'].feature_importances_\n",
    "\n",
    "# –°–æ–∑–¥–∞—ë–º —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π DataFrame\n",
    "importance_comparison = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Gini': gini_importance,\n",
    "    'Permutation': perm_importance.importances_mean\n",
    "}).sort_values('Permutation', ascending=False)\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "x = np.arange(12)\n",
    "width = 0.35\n",
    "\n",
    "ax.barh(x - width/2, importance_comparison.head(12)['Gini'], width, label='Gini (MDI)', alpha=0.8)\n",
    "ax.barh(x + width/2, importance_comparison.head(12)['Permutation'], width, label='Permutation', alpha=0.8)\n",
    "\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(importance_comparison.head(12)['Feature'])\n",
    "ax.set_xlabel('Importance', fontsize=12)\n",
    "ax.set_title('Gini vs Permutation Importance', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.invert_yaxis()\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç –ù–∞–±–ª—é–¥–µ–Ω–∏—è:\")\n",
    "print(\"- Gini –∏ Permutation –æ–±—ã—á–Ω–æ —Å–æ–≥–ª–∞—Å–Ω—ã –≤ —Ç–æ–ø–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö\")\n",
    "print(\"- Gini –º–æ–∂–µ—Ç –ø–µ—Ä–µ–æ—Ü–µ–Ω–∏–≤–∞—Ç—å high-cardinality features\")\n",
    "print(\"- Permutation –±–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω—ã–π –¥–ª—è feature selection\")\n",
    "print(\"- SHAP > Permutation > Gini (–≤ –ø–ª–∞–Ω–µ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–π –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚öñÔ∏è –ß–∞—Å—Ç—å 5: Fairness Analysis\n",
    "\n",
    "### –ü–æ—á–µ–º—É Fairness –≤–∞–∂–µ–Ω?\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º–∞:** –í—ã—Å–æ–∫–∞—è accuracy ‚â† —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–∞—è –º–æ–¥–µ–ª—å\n",
    "\n",
    "**Real-world –ø—Ä–∏–º–µ—Ä—ã bias:**\n",
    "- **Amazon Recruiting AI (2018):** –º–æ–¥–µ–ª—å –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∏—Ä–æ–≤–∞–ª–∞ –∂–µ–Ω—â–∏–Ω (–æ–±—É—á–∞–ª–∞—Å—å –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å bias)\n",
    "- **COMPAS (Criminal Justice):** –∞–ª–≥–æ—Ä–∏—Ç–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è recidivism –ø–æ–∫–∞–∑—ã–≤–∞–ª racial bias\n",
    "- **Credit Scoring:** –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∏—Ä–æ–≤–∞—Ç—å –ø–æ –≤–æ–∑—Ä–∞—Å—Ç—É, –ø–æ–ª—É, —Ä–∞—Å–µ\n",
    "- **Healthcare:** –∞–ª–≥–æ—Ä–∏—Ç–º—ã –º–æ–≥—É—Ç —Ö—É–∂–µ —Ä–∞–±–æ—Ç–∞—Ç—å –¥–ª—è minority groups\n",
    "\n",
    "**–ó–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:**\n",
    "- üá∫üá∏ **Equal Credit Opportunity Act** - –∑–∞–ø—Ä–µ—â–∞–µ—Ç –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ü–∏—é –≤ –∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏–∏\n",
    "- üá™üá∫ **GDPR Article 22** - –ø—Ä–∞–≤–æ –Ω–∞ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π\n",
    "- üá∫üá∏ **Fair Housing Act** - fairness –≤ –∂–∏–ª–∏—â–Ω–æ–º –∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏–∏\n",
    "\n",
    "### –¢–∏–ø—ã Fairness Metrics:\n",
    "\n",
    "#### 1. **Demographic Parity (Statistical Parity)**\n",
    "```\n",
    "P(Y_pred = 1 | Sex = Male) = P(Y_pred = 1 | Sex = Female)\n",
    "```\n",
    "- –û–¥–∏–Ω–∞–∫–æ–≤–∞—è –¥–æ–ª—è positive predictions –≤ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø–µ\n",
    "- **–ü—Ä–æ–±–ª–µ–º–∞:** –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –±–∞–∑–æ–≤—ã–µ —Ä–∞–∑–ª–∏—á–∏—è –≤ ground truth\n",
    "\n",
    "#### 2. **Equal Opportunity**\n",
    "```\n",
    "P(Y_pred = 1 | Y_true = 1, Sex = Male) = P(Y_pred = 1 | Y_true = 1, Sex = Female)\n",
    "```\n",
    "- –û–¥–∏–Ω–∞–∫–æ–≤—ã–π True Positive Rate –¥–ª—è –≤—Å–µ—Ö –≥—Ä—É–ø–ø\n",
    "- **–õ—É—á—à–µ:** —É—á–∏—Ç—ã–≤–∞–µ—Ç ground truth\n",
    "\n",
    "#### 3. **Equalized Odds**\n",
    "```\n",
    "Equal Opportunity + Equal False Positive Rate\n",
    "```\n",
    "- –ò TPR, –∏ FPR –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –¥–ª—è –≤—Å–µ—Ö –≥—Ä—É–ø–ø\n",
    "\n",
    "#### 4. **Calibration**\n",
    "```\n",
    "P(Y_true = 1 | Y_pred_proba = p, Sex = Male) = P(Y_true = 1 | Y_pred_proba = p, Sex = Female)\n",
    "```\n",
    "- Predicted probabilities —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Ä–µ–∞–ª—å–Ω—ã–º –¥–ª—è –≤—Å–µ—Ö –≥—Ä—É–ø–ø\n",
    "\n",
    "**Trade-offs:**\n",
    "- ‚ö†Ô∏è **–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ** —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç—å –≤—Å–µ fairness criteria –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ (—Ç–µ–æ—Ä–µ–º–∞ impossibility)\n",
    "- ‚ö†Ô∏è **Fairness vs Accuracy trade-off** - –∏–Ω–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –∂–µ—Ä—Ç–≤–æ–≤–∞—Ç—å accuracy\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"FAIRNESS ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º fairness –ø–æ –ø–æ–ª—É (sensitive attribute)\n",
    "print(\"\\nAnalyzing fairness by SEX...\")\n",
    "\n",
    "# Predictions –¥–ª—è test set\n",
    "rf_predictions = results['Random Forest']['predictions']\n",
    "rf_probabilities = results['Random Forest']['probabilities']\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º predictions –∫ sensitive attributes\n",
    "fairness_df = sensitive_test.copy()\n",
    "fairness_df['y_true'] = y_test.values\n",
    "fairness_df['y_pred'] = rf_predictions\n",
    "fairness_df['y_prob'] = rf_probabilities\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"1. DEMOGRAPHIC PARITY (Statistical Parity)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Demographic Parity: –¥–æ–ª—è positive predictions –ø–æ –≥—Ä—É–ø–ø–∞–º\n",
    "demo_parity = fairness_df.groupby('sex')['y_pred'].mean()\n",
    "print(\"\\nP(Y_pred = 1 | Sex):\")\n",
    "for sex, prob in demo_parity.items():\n",
    "    print(f\"  {sex:10s}: {prob:.4f} ({prob*100:.2f}%)\")\n",
    "\n",
    "# Disparate Impact Ratio\n",
    "female_rate = demo_parity.get('Female', demo_parity.iloc[0])\n",
    "male_rate = demo_parity.get('Male', demo_parity.iloc[1])\n",
    "disparate_impact = female_rate / male_rate if male_rate > 0 else 0\n",
    "\n",
    "print(f\"\\nDisparate Impact Ratio: {disparate_impact:.4f}\")\n",
    "print(\"  (Female rate / Male rate)\")\n",
    "print(\"  ‚úÖ Good if between 0.8 and 1.25 (80% rule)\")\n",
    "if 0.8 <= disparate_impact <= 1.25:\n",
    "    print(\"  ‚úÖ PASS: Demographic Parity satisfied\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è FAIL: Potential bias (ratio = {disparate_impact:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"2. EQUAL OPPORTUNITY (True Positive Rate)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Equal Opportunity: TPR –ø–æ –≥—Ä—É–ø–ø–∞–º (—Å—Ä–µ–¥–∏ —Ç–µ—Ö, –∫—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ >50K)\n",
    "positive_class = fairness_df[fairness_df['y_true'] == 1]\n",
    "tpr_by_sex = positive_class.groupby('sex')['y_pred'].mean()\n",
    "\n",
    "print(\"\\nTrue Positive Rate (Recall) by Sex:\")\n",
    "for sex, tpr in tpr_by_sex.items():\n",
    "    print(f\"  {sex:10s}: {tpr:.4f} ({tpr*100:.2f}%)\")\n",
    "\n",
    "# Difference in TPR\n",
    "tpr_diff = abs(tpr_by_sex.iloc[0] - tpr_by_sex.iloc[1])\n",
    "print(f\"\\nDifference in TPR: {tpr_diff:.4f}\")\n",
    "if tpr_diff < 0.05:\n",
    "    print(\"  ‚úÖ PASS: Equal Opportunity satisfied (diff < 5%)\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è WARNING: TPR differs by {tpr_diff*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"3. FALSE POSITIVE RATE (FPR) BY SEX\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# FPR –ø–æ –≥—Ä—É–ø–ø–∞–º (—Å—Ä–µ–¥–∏ —Ç–µ—Ö, –∫—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ <=50K)\n",
    "negative_class = fairness_df[fairness_df['y_true'] == 0]\n",
    "fpr_by_sex = negative_class.groupby('sex')['y_pred'].mean()\n",
    "\n",
    "print(\"\\nFalse Positive Rate by Sex:\")\n",
    "for sex, fpr in fpr_by_sex.items():\n",
    "    print(f\"  {sex:10s}: {fpr:.4f} ({fpr*100:.2f}%)\")\n",
    "\n",
    "# Difference in FPR\n",
    "fpr_diff = abs(fpr_by_sex.iloc[0] - fpr_by_sex.iloc[1])\n",
    "print(f\"\\nDifference in FPR: {fpr_diff:.4f}\")\n",
    "if fpr_diff < 0.05:\n",
    "    print(\"  ‚úÖ PASS: FPR similar across groups (diff < 5%)\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è WARNING: FPR differs by {fpr_diff*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è fairness metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Demographic Parity\n",
    "demo_parity.plot(kind='bar', ax=axes[0, 0], color=['#e74c3c', '#3498db'])\n",
    "axes[0, 0].set_title('Demographic Parity: P(Y_pred=1 | Sex)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Positive Prediction Rate')\n",
    "axes[0, 0].set_xlabel('Sex')\n",
    "axes[0, 0].tick_params(axis='x', rotation=0)\n",
    "axes[0, 0].axhline(y=demo_parity.mean(), color='green', linestyle='--', label='Overall mean')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. True Positive Rate\n",
    "tpr_by_sex.plot(kind='bar', ax=axes[0, 1], color=['#e74c3c', '#3498db'])\n",
    "axes[0, 1].set_title('Equal Opportunity: TPR by Sex', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('True Positive Rate')\n",
    "axes[0, 1].set_xlabel('Sex')\n",
    "axes[0, 1].tick_params(axis='x', rotation=0)\n",
    "axes[0, 1].axhline(y=tpr_by_sex.mean(), color='green', linestyle='--', label='Overall mean')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. False Positive Rate\n",
    "fpr_by_sex.plot(kind='bar', ax=axes[1, 0], color=['#e74c3c', '#3498db'])\n",
    "axes[1, 0].set_title('FPR by Sex', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('False Positive Rate')\n",
    "axes[1, 0].set_xlabel('Sex')\n",
    "axes[1, 0].tick_params(axis='x', rotation=0)\n",
    "axes[1, 0].axhline(y=fpr_by_sex.mean(), color='green', linestyle='--', label='Overall mean')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. Confusion Matrix by Sex\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Aggregate metrics –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "metrics_by_sex = {}\n",
    "for sex in fairness_df['sex'].unique():\n",
    "    sex_data = fairness_df[fairness_df['sex'] == sex]\n",
    "    metrics_by_sex[sex] = {\n",
    "        'Accuracy': accuracy_score(sex_data['y_true'], sex_data['y_pred']),\n",
    "        'Precision': precision_score(sex_data['y_true'], sex_data['y_pred']),\n",
    "        'Recall': recall_score(sex_data['y_true'], sex_data['y_pred']),\n",
    "        'F1': f1_score(sex_data['y_true'], sex_data['y_pred'])\n",
    "    }\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_by_sex).T\n",
    "metrics_df.plot(kind='bar', ax=axes[1, 1], width=0.8)\n",
    "axes[1, 1].set_title('Performance Metrics by Sex', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].set_xlabel('Sex')\n",
    "axes[1, 1].tick_params(axis='x', rotation=0)\n",
    "axes[1, 1].legend(loc='lower right')\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Fairness Analysis: Sex-based Metrics', fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Summary Table:\")\n",
    "print(metrics_df.round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration Analysis\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"4. CALIBRATION ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calibration curves –ø–æ –≥—Ä—É–ø–ø–∞–º\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "for sex in fairness_df['sex'].unique():\n",
    "    sex_data = fairness_df[fairness_df['sex'] == sex]\n",
    "    \n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "        sex_data['y_true'],\n",
    "        sex_data['y_prob'],\n",
    "        n_bins=10,\n",
    "        strategy='uniform'\n",
    "    )\n",
    "    \n",
    "    ax.plot(\n",
    "        mean_predicted_value,\n",
    "        fraction_of_positives,\n",
    "        marker='o',\n",
    "        linewidth=2,\n",
    "        label=sex\n",
    "    )\n",
    "\n",
    "# Perfect calibration line\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "\n",
    "ax.set_xlabel('Mean Predicted Probability', fontsize=12)\n",
    "ax.set_ylabel('Fraction of Positives', fontsize=12)\n",
    "ax.set_title('Calibration Curve by Sex', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è Calibration:\")\n",
    "print(\"- –õ–∏–Ω–∏—è –±–ª–∏–∑–∫–∞ –∫ diagonal ‚Üí –º–æ–¥–µ–ª—å well-calibrated\")\n",
    "print(\"- Above diagonal ‚Üí –º–æ–¥–µ–ª—å underestimates probabilities\")\n",
    "print(\"- Below diagonal ‚Üí –º–æ–¥–µ–ª—å overestimates probabilities\")\n",
    "print(\"- –ï—Å–ª–∏ –∫—Ä–∏–≤—ã–µ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –≥—Ä—É–ø–ø —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è ‚Üí calibration bias\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Bias Mitigation Strategies\n",
    "\n",
    "–ï—Å–ª–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω bias, —á—Ç–æ –¥–µ–ª–∞—Ç—å?\n",
    "\n",
    "#### 1. **Pre-processing** (–¥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏)\n",
    "- **Reweighting:** –¥–∞–≤–∞—Ç—å –±–æ–ª—å—à–∏–π –≤–µ—Å underrepresented groups\n",
    "- **Resampling:** oversample minority, undersample majority\n",
    "- **Data augmentation:** –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã\n",
    "- **Feature removal:** —É–¥–∞–ª–∏—Ç—å sensitive attributes (–Ω–æ –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ - –º–æ–∂–µ—Ç –Ω–µ –ø–æ–º–æ—á—å –∏–∑-–∑–∞ proxy features)\n",
    "\n",
    "#### 2. **In-processing** (–≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è)\n",
    "- **Fairness constraints:** –¥–æ–±–∞–≤–∏—Ç—å fairness penalty –≤ loss function\n",
    "- **Adversarial debiasing:** –æ–±—É—á–∏—Ç—å adversarial network, —á—Ç–æ–±—ã predictions –Ω–µ –∑–∞–≤–∏—Å–µ–ª–∏ –æ—Ç sensitive attribute\n",
    "- **Fair representations:** learn embedding, –≥–¥–µ sensitive info —É–¥–∞–ª–µ–Ω–∞\n",
    "\n",
    "#### 3. **Post-processing** (–ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è)\n",
    "- **Threshold optimization:** —Ä–∞–∑–Ω—ã–µ thresholds –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –≥—Ä—É–ø–ø\n",
    "- **Calibration:** recalibrate probabilities per group\n",
    "- **Reject option:** –Ω–µ –¥–µ–ª–∞—Ç—å prediction, –µ—Å–ª–∏ confidence –Ω–∏–∑–∫–∞—è\n",
    "\n",
    "**Trade-offs:**\n",
    "- ‚ö†Ô∏è **Fairness vs Accuracy:** mitigation –º–æ–∂–µ—Ç —Å–Ω–∏–∑–∏—Ç—å overall accuracy\n",
    "- ‚ö†Ô∏è **Which fairness metric?** –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç—å –≤—Å–µ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ\n",
    "- ‚ö†Ô∏è **Legal considerations:** –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –º–µ—Ç–æ–¥—ã (e.g., different thresholds) –º–æ–≥—É—Ç –±—ã—Ç—å illegal\n",
    "\n",
    "**Best practices:**\n",
    "1. ‚úÖ **Measure fairness** –Ω–∞ test set –¥–ª—è –≤—Å–µ—Ö sensitive groups\n",
    "2. ‚úÖ **Document** bias analysis –∏ mitigation efforts (model cards)\n",
    "3. ‚úÖ **Monitor** fairness in production (drift)\n",
    "4. ‚úÖ **Stakeholder involvement:** involve affected communities\n",
    "5. ‚úÖ **Transparency:** –æ–±—ä—è—Å–Ω—è—Ç—å decisions, –æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è high-stakes applications\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üå≥ –ß–∞—Å—Ç—å 6: Decision Tree Visualization\n",
    "\n",
    "### Model-Specific Interpretability\n",
    "\n",
    "**Decision Trees** - inherently interpretable –º–æ–¥–µ–ª–∏:\n",
    "- –ö–∞–∂–¥–æ–µ —Ä–µ—à–µ–Ω–∏–µ - —ç—Ç–æ simple if-then rule\n",
    "- –ü—É—Ç—å –æ—Ç root –¥–æ leaf –æ–±—ä—è—Å–Ω—è–µ—Ç prediction\n",
    "- –ú–æ–∂–Ω–æ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–µ—Å—å process\n",
    "\n",
    "**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**\n",
    "- ‚úÖ **Complete transparency:** –≤–∏–¥–∏–º –≤—Å–µ –ø—Ä–∞–≤–∏–ª–∞\n",
    "- ‚úÖ **Easy to explain:** –º–æ–∂–Ω–æ –æ–±—ä—è—Å–Ω–∏—Ç—å –ª—é–±–æ–º—É\n",
    "- ‚úÖ **No preprocessing needed:** —Ä–∞–±–æ—Ç–∞–µ—Ç —Å categorical features\n",
    "\n",
    "**Trade-off:**\n",
    "- ‚ö†Ô∏è **Accuracy vs Interpretability:**\n",
    "  - Shallow tree: interpretable, –Ω–æ –º–µ–Ω–µ–µ —Ç–æ—á–Ω–∞—è\n",
    "  - Deep tree: —Ç–æ—á–Ω–∞—è, –Ω–æ —Å–ª–æ–∂–Ω–∞—è –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏\n",
    "  - Random Forest / XGBoost: –µ—â—ë —Ç–æ—á–Ω–µ–µ, –Ω–æ black box (–Ω—É–∂–µ–Ω SHAP)\n",
    "\n",
    "**–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**\n",
    "- ‚úÖ **High-stakes decisions:** –º–µ–¥–∏—Ü–∏–Ω–∞, –∫—Ä–µ–¥–∏—Ç—ã (–Ω—É–∂–Ω–∞ –ø–æ–ª–Ω–∞—è –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å)\n",
    "- ‚úÖ **Regulatory compliance:** –∫–æ–≥–¥–∞ —Ç—Ä–µ–±—É–µ—Ç—Å—è –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∞–≤–∏–ª–∞\n",
    "- ‚úÖ **Knowledge extraction:** –ø–æ–Ω—è—Ç—å domain logic –∏–∑ –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"DECISION TREE VISUALIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# –û–±—É—á–∞–µ–º –ø—Ä–æ—Å—Ç—É—é Decision Tree –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏\n",
    "print(\"\\nTraining interpretable Decision Tree (max_depth=4)...\")\n",
    "dt = DecisionTreeClassifier(\n",
    "    max_depth=4,  # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≥–ª—É–±–∏–Ω—É –¥–ª—è interpretability\n",
    "    min_samples_split=100,\n",
    "    min_samples_leaf=50,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "dt.fit(X_train_scaled, y_train)\n",
    "dt_pred = dt.predict(X_test_scaled)\n",
    "\n",
    "# –ú–µ—Ç—Ä–∏–∫–∏\n",
    "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
    "dt_f1 = f1_score(y_test, dt_pred)\n",
    "dt_auc = roc_auc_score(y_test, dt.predict_proba(X_test_scaled)[:, 1])\n",
    "\n",
    "print(f\"\\n‚úÖ Decision Tree trained\")\n",
    "print(f\"Accuracy: {dt_accuracy:.4f}\")\n",
    "print(f\"F1-Score: {dt_f1:.4f}\")\n",
    "print(f\"ROC AUC: {dt_auc:.4f}\")\n",
    "print(f\"\\n–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∏–∂–µ, —á–µ–º —É XGBoost/RF, –Ω–æ –¥–µ—Ä–µ–≤–æ –ø–æ–ª–Ω–æ—Å—Ç—å—é interpretable!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è Decision Tree\n",
    "fig, ax = plt.subplots(figsize=(25, 15))\n",
    "\n",
    "plot_tree(\n",
    "    dt,\n",
    "    feature_names=feature_cols,\n",
    "    class_names=['<=50K', '>50K'],\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    ax=ax,\n",
    "    fontsize=10,\n",
    "    proportion=True  # –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏ –≤–º–µ—Å—Ç–æ –∞–±—Å–æ–ª—é—Ç–Ω—ã—Ö —á–∏—Å–µ–ª\n",
    ")\n",
    "\n",
    "plt.title('Decision Tree Visualization (max_depth=4)', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä –ö–∞–∫ —á–∏—Ç–∞—Ç—å –¥–µ—Ä–µ–≤–æ:\")\n",
    "print(\"- –ö–∞–∂–¥—ã–π —É–∑–µ–ª: —É—Å–ª–æ–≤–∏–µ split (e.g., capital-gain <= 0.5)\")\n",
    "print(\"- gini: impurity (0 = pure, 0.5 = 50/50)\")\n",
    "print(\"- samples: –¥–æ–ª—è samples –≤ —ç—Ç–æ–º —É–∑–ª–µ\")\n",
    "print(\"- value: [proportion class 0, proportion class 1]\")\n",
    "print(\"- class: predicted class (—Ü–≤–µ—Ç –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç majority class)\")\n",
    "print(\"\\n–û—Ä–∞–Ω–∂–µ–≤—ã–π = <=50K, –°–∏–Ω–∏–π = >50K\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¢–µ–∫—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DECISION RULES (TEXT FORMAT)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "tree_rules = export_text(dt, feature_names=feature_cols)\n",
    "print(tree_rules)\n",
    "\n",
    "print(\"\\nüí° –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–∞–≤–∏–ª:\")\n",
    "print(\"- IF capital-gain > 0.5 AND ... ‚Üí Predicted: >50K\")\n",
    "print(\"- IF age <= 0.3 AND education-num <= 0.2 ‚Üí Predicted: <=50K\")\n",
    "print(\"\\n–≠—Ç–∏ –ø—Ä–∞–≤–∏–ª–∞ –º–æ–∂–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å –±–∏–∑–Ω–µ—Å—É / —Ä–µ–≥—É–ª—è—Ç–æ—Ä–∞–º!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance –∏–∑ Decision Tree\n",
    "dt_importance = dt.feature_importances_\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': dt_importance\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "top_n = 12\n",
    "ax.barh(\n",
    "    range(top_n),\n",
    "    importance_df.head(top_n)['Importance'],\n",
    "    alpha=0.8,\n",
    "    color='steelblue'\n",
    ")\n",
    "\n",
    "ax.set_yticks(range(top_n))\n",
    "ax.set_yticklabels(importance_df.head(top_n)['Feature'])\n",
    "ax.set_xlabel('Feature Importance (Gini)', fontsize=12)\n",
    "ax.set_title('Decision Tree Feature Importance', fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ –¢–æ–ø-5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ Decision Tree:\")\n",
    "print(importance_df.head(5).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã –ß–∞—Å—Ç—å 7: Comprehensive Summary & Best Practices\n",
    "\n",
    "### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö XAI –º–µ—Ç–æ–¥–æ–≤\n",
    "\n",
    "| –ú–µ—Ç–æ–¥ | Scope | Speed | Theory | Use Case |\n",
    "|-------|-------|-------|--------|----------|\n",
    "| **SHAP** | Global + Local | ‚ö†Ô∏è Medium (TreeSHAP fast) | ‚úÖ Game theory | Production, —Ç—Ä–µ–±—É–µ—Ç—Å—è consistency |\n",
    "| **LIME** | Local only | ‚úÖ Fast | ‚ùå Heuristic | Quick ad-hoc explanations |\n",
    "| **PDP** | Global | ‚úÖ Fast | ‚úÖ Solid | Global feature effects |\n",
    "| **Permutation Importance** | Global | ‚ö†Ô∏è Medium | ‚úÖ Solid | True feature importance |\n",
    "| **Gini Importance** | Global | ‚úÖ Very fast | ‚ö†Ô∏è Biased | Quick check (tree models) |\n",
    "| **Decision Tree Viz** | Global | ‚úÖ Fast | ‚úÖ Complete | Full transparency |\n",
    "| **Attention Weights** | Local | ‚úÖ Fast | ‚ö†Ô∏è Debatable | Transformers only |\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤—ã–±–æ—Ä—É –º–µ—Ç–æ–¥–∞\n",
    "\n",
    "#### 1. **Production ML System (High Stakes)**\n",
    "- ‚úÖ **Primary:** SHAP (—Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω, consistent)\n",
    "- ‚úÖ **Secondary:** Permutation Importance (global view)\n",
    "- ‚úÖ **Fairness:** Demographic Parity, Equal Opportunity\n",
    "- ‚úÖ **Documentation:** Model cards, fairness reports\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä:** Credit scoring, healthcare diagnostics, hiring\n",
    "\n",
    "#### 2. **Quick Prototyping / Research**\n",
    "- ‚úÖ **LIME** (fast local explanations)\n",
    "- ‚úÖ **PDP** (global trends)\n",
    "- ‚úÖ **Feature Importance** (quick check)\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä:** Kaggle competition, exploratory analysis\n",
    "\n",
    "#### 3. **Regulatory Compliance (Full Transparency)**\n",
    "- ‚úÖ **Decision Tree** visualization\n",
    "- ‚úÖ **Rule extraction**\n",
    "- ‚úÖ **Linear models** —Å coefficient interpretation\n",
    "- ‚ö†Ô∏è –í–æ–∑–º–æ–∂–Ω–æ –ø—Ä–∏–¥—ë—Ç—Å—è –∂–µ—Ä—Ç–≤–æ–≤–∞—Ç—å accuracy\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä:** Regulated industries (banking, insurance), government\n",
    "\n",
    "#### 4. **Deep Learning / Transformers**\n",
    "- ‚úÖ **DeepSHAP** –∏–ª–∏ **GradientSHAP**\n",
    "- ‚úÖ **Attention visualization** (—Å –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç—å—é)\n",
    "- ‚úÖ **Integrated Gradients**\n",
    "- ‚úÖ **Saliency Maps** (–¥–ª—è images/text)\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä:** NLP, computer vision, time series with Transformers\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öñÔ∏è Fairness Best Practices\n",
    "\n",
    "1. **Identify Sensitive Attributes:**\n",
    "   - Gender, race, age, disability status, etc.\n",
    "   - Legal requirements vary by jurisdiction\n",
    "\n",
    "2. **Choose Fairness Metric(s):**\n",
    "   - **Demographic Parity:** —Ä–∞–≤–Ω—ã–µ positive rates\n",
    "   - **Equal Opportunity:** —Ä–∞–≤–Ω—ã–µ TPR (—á–∞—Å—Ç–æ preferred)\n",
    "   - **Calibration:** equal calibration –ø–æ –≥—Ä—É–ø–ø–∞–º\n",
    "   - ‚ö†Ô∏è **–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ** —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç—å –≤—Å–µ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ!\n",
    "\n",
    "3. **Measure Fairness:**\n",
    "   - –ù–∞ **test set** (–Ω–µ –Ω–∞ train!)\n",
    "   - –î–ª—è **–≤—Å–µ—Ö** sensitive groups\n",
    "   - **Before** –∏ **after** mitigation\n",
    "\n",
    "4. **Bias Mitigation:**\n",
    "   - **Pre-processing:** reweighting, resampling\n",
    "   - **In-processing:** fairness constraints in loss\n",
    "   - **Post-processing:** threshold optimization\n",
    "\n",
    "5. **Documentation:**\n",
    "   - **Model Cards** (Google best practice)\n",
    "   - Fairness metrics in production dashboard\n",
    "   - Regular audits (quarterly/yearly)\n",
    "\n",
    "6. **Continuous Monitoring:**\n",
    "   - Fairness –º–æ–∂–µ—Ç drift —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º\n",
    "   - Data distribution changes ‚Üí fairness changes\n",
    "   - Monitor –≤ production!\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Production Deployment Checklist\n",
    "\n",
    "**Before deploying ML model:**\n",
    "\n",
    "- [ ] **Interpretability:**\n",
    "  - [ ] SHAP values computed –¥–ª—è test set\n",
    "  - [ ] Global feature importance documented\n",
    "  - [ ] Local explanation API available\n",
    "  - [ ] PDP plots generated –¥–ª—è —Ç–æ–ø–æ–≤—ã—Ö features\n",
    "\n",
    "- [ ] **Fairness:**\n",
    "  - [ ] Sensitive attributes identified\n",
    "  - [ ] Fairness metrics measured (Demographic Parity, Equal Opportunity)\n",
    "  - [ ] Bias mitigation applied (if needed)\n",
    "  - [ ] Fairness report –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω\n",
    "\n",
    "- [ ] **Documentation:**\n",
    "  - [ ] Model card created\n",
    "  - [ ] Training data –æ–ø–∏—Å–∞–Ω–∞ (source, biases, limitations)\n",
    "  - [ ] Evaluation metrics –ø–æ –≤—Å–µ–º groups\n",
    "  - [ ] Known limitations –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω—ã\n",
    "  - [ ] Ethical considerations discussed\n",
    "\n",
    "- [ ] **Monitoring:**\n",
    "  - [ ] Dashboard –¥–ª—è monitoring fairness –≤ production\n",
    "  - [ ] Alerts –Ω–∞ drift –≤ fairness metrics\n",
    "  - [ ] Logging –¥–ª—è SHAP values (sample)\n",
    "\n",
    "- [ ] **Legal/Regulatory:**\n",
    "  - [ ] Legal review completed\n",
    "  - [ ] GDPR compliance (–ø—Ä–∞–≤–æ –Ω–∞ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ)\n",
    "  - [ ] Industry-specific regulations checked\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìö Key Takeaways\n",
    "\n",
    "#### 1. **Interpretability is NOT Optional**\n",
    "- –†–µ–≥—É–ª–∏—Ä—É–µ–º—ã–µ –∏–Ω–¥—É—Å—Ç—Ä–∏–∏ —Ç—Ä–µ–±—É—é—Ç –æ–±—ä—è—Å–Ω–µ–Ω–∏–π (GDPR, FDA, etc.)\n",
    "- Stakeholders —Ö–æ—Ç—è—Ç –ø–æ–Ω–∏–º–∞—Ç—å, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –º–æ–¥–µ–ª—å\n",
    "- Debugging: interpretability –ø–æ–º–æ–≥–∞–µ—Ç –Ω–∞–π—Ç–∏ bugs –∏ bias\n",
    "\n",
    "#### 2. **SHAP - Gold Standard**\n",
    "- –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω (Shapley values)\n",
    "- Consistent, local + global\n",
    "- TreeSHAP –æ—á–µ–Ω—å fast –¥–ª—è tree-based –º–æ–¥–µ–ª–µ–π\n",
    "- **Use SHAP –≤ production**\n",
    "\n",
    "#### 3. **Different Methods, Different Insights**\n",
    "- **SHAP:** feature attribution (—Å–∫–æ–ª—å–∫–æ –∫–∞–∂–¥—ã–π –ø—Ä–∏–∑–Ω–∞–∫ –≤–Ω–æ—Å–∏—Ç)\n",
    "- **PDP:** global trends (–∫–∞–∫ –ø—Ä–∏–∑–Ω–∞–∫ –≤–ª–∏—è–µ—Ç –≤ —Å—Ä–µ–¥–Ω–µ–º)\n",
    "- **LIME:** quick local explanations\n",
    "- **Permutation:** true importance (handles correlations)\n",
    "- **–í—Å–µ –≤–º–µ—Å—Ç–µ** –¥–∞—é—Ç comprehensive understanding!\n",
    "\n",
    "#### 4. **Fairness ‚â† Equal Outcomes**\n",
    "- Fairness - —ç—Ç–æ –ø—Ä–æ equal treatment, –Ω–µ equal outcomes\n",
    "- –í—ã–±–æ—Ä fairness metric –∑–∞–≤–∏—Å–∏—Ç –æ—Ç use case –∏ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–∞\n",
    "- Trade-off: fairness vs accuracy (–∏–Ω–æ–≥–¥–∞ –Ω–µ–∏–∑–±–µ–∂–µ–Ω)\n",
    "- **Measure, mitigate, monitor**\n",
    "\n",
    "#### 5. **Accuracy vs Interpretability Trade-off**\n",
    "- Simple models (linear, shallow trees): interpretable, –Ω–æ –º–µ–Ω–µ–µ —Ç–æ—á–Ω—ã–µ\n",
    "- Complex models (deep learning, ensembles): —Ç–æ—á–Ω—ã–µ, –Ω–æ black box\n",
    "- **XAI methods** (SHAP, LIME) –ø–æ–∑–≤–æ–ª—è—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å complex models —Å –æ–±—ä—è—Å–Ω–µ–Ω–∏—è–º–∏\n",
    "- **Best of both worlds!**\n",
    "\n",
    "#### 6. **Documentation & Transparency**\n",
    "- Model cards - best practice –æ—Ç Google\n",
    "- –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å training data, biases, limitations\n",
    "- Transparency builds trust\n",
    "- –†–µ–≥—É–ª—è—Ç–æ—Ä—ã —Ç—Ä–µ–±—É—é—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é\n",
    "\n",
    "#### 7. **Continuous Monitoring**\n",
    "- Fairness –∏ interpretability –Ω–µ one-time tasks\n",
    "- Data drift ‚Üí fairness drift\n",
    "- Monitor –≤ production, —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ audits\n",
    "\n",
    "---\n",
    "\n",
    "### üîó –°–≤—è–∑—å —Å –¥—Ä—É–≥–∏–º–∏ —Ñ–∞–∑–∞–º–∏\n",
    "\n",
    "**Phase 6 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∑–Ω–∞–Ω–∏—è –∏–∑:**\n",
    "- **Phase 1-2:** –ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (Logistic Regression, Neural Networks)\n",
    "- **Phase 3:** Ensemble methods (Random Forest, XGBoost) ‚Üí TreeSHAP\n",
    "- **Phase 4:** Transformers ‚Üí Attention visualization\n",
    "- **Phase 5:** Anomaly detection ‚Üí interpretable outliers\n",
    "\n",
    "**Phase 6 –≥–æ—Ç–æ–≤–∏—Ç –∫:**\n",
    "- **Phase 7:** Production & MLOps ‚Üí monitoring fairness, model cards\n",
    "\n",
    "---\n",
    "\n",
    "### üéì Real-World Impact\n",
    "\n",
    "**XAI —Å–ø–∞—Å–∞–µ—Ç –ø—Ä–æ–µ–∫—Ç—ã:**\n",
    "- **Amazon (2018):** Hiring AI –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∏—Ä–æ–≤–∞–ª–∞ –∂–µ–Ω—â–∏–Ω ‚Üí –ø—Ä–æ–µ–∫—Ç –∑–∞–∫—Ä—ã—Ç\n",
    "  - ‚úÖ **Lesson:** Measure fairness before deployment!\n",
    "- **COMPAS:** Racial bias –≤ criminal justice\n",
    "  - ‚úÖ **Lesson:** Audit third-party models!\n",
    "- **Healthcare:** FDA —Ç—Ä–µ–±—É–µ—Ç –æ–±—ä—è—Å–Ω–µ–Ω–∏—è AI diagnoses\n",
    "  - ‚úÖ **Lesson:** Interpretability = regulatory requirement\n",
    "\n",
    "**XAI —Å–æ–∑–¥–∞—ë—Ç —Ü–µ–Ω–Ω–æ—Å—Ç—å:**\n",
    "- **–ö—Ä–µ–¥–∏—Ç–Ω—ã–π —Å–∫–æ—Ä–∏–Ω–≥:** –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –æ—Ç–∫–∞–∑–∞ ‚Üí –∫–ª–∏–µ–Ω—Ç –∑–Ω–∞–µ—Ç, –∫–∞–∫ —É–ª—É—á—à–∏—Ç—å\n",
    "- **–ú–µ–¥–∏—Ü–∏–Ω–∞:** –í—Ä–∞—á –ø–æ–Ω–∏–º–∞–µ—Ç AI –¥–∏–∞–≥–Ω–æ–∑ ‚Üí –±–æ–ª—å—à–µ –¥–æ–≤–µ—Ä–∏—è\n",
    "- **Fraud detection:** –ê–Ω–∞–ª–∏—Ç–∏–∫ –≤–∏–¥–∏—Ç, –ø–æ—á–µ–º—É —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è suspicious ‚Üí –±—ã—Å—Ç—Ä–µ–µ —Ä–µ–∞–≥–∏—Ä—É–µ—Ç\n",
    "- **–ë–∏–∑–Ω–µ—Å insights:** SHAP –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –∫–∞–∫–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã –≤–ª–∏—è—é—Ç –Ω–∞ churn ‚Üí action plan\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Phase 6 Complete!\n",
    "\n",
    "**–í—ã —Ç–µ–ø–µ—Ä—å –∑–Ω–∞–µ—Ç–µ:**\n",
    "- ‚úÖ SHAP (TreeSHAP, KernelSHAP, DeepSHAP)\n",
    "- ‚úÖ LIME\n",
    "- ‚úÖ Partial Dependence Plots (PDP)\n",
    "- ‚úÖ Permutation Importance\n",
    "- ‚úÖ Fairness metrics (Demographic Parity, Equal Opportunity, Calibration)\n",
    "- ‚úÖ Bias mitigation strategies\n",
    "- ‚úÖ Decision Tree visualization\n",
    "- ‚úÖ Production best practices\n",
    "\n",
    "**–°–ª–µ–¥—É—é—â–∏–π —à–∞–≥: Phase 7 - Production & MLOps**\n",
    "- Model deployment (FastAPI, Docker, Cloud)\n",
    "- Monitoring (data drift, model drift, fairness drift)\n",
    "- CI/CD –¥–ª—è ML\n",
    "- Experiment tracking (MLflow, Weights & Biases)\n",
    "- –í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ (DVC)\n",
    "\n",
    "---\n",
    "\n",
    "**üöÄ Ready for production-ready ML!**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
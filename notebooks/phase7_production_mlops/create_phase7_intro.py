#!/usr/bin/env python3
"""
Phase 7: Production & MLOps - Part 1
Introduction, MLOps Fundamentals, Experiment Tracking
"""

import json

# –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –Ω–æ—É—Ç–±—É–∫–∞
notebook = {
    "cells": [],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

cells = []

# ============================================================================
# TITLE AND INTRODUCTION
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "# üöÄ Production ML & MLOps: From Notebook to Production\n",
        "\n",
        "**Phase 7: –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º ML-—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –≤ Production-Ready —Å–∏—Å—Ç–µ–º—É**\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ The Production Gap\n",
        "\n",
        "### –ü—Ä–æ–±–ª–µ–º–∞: 90% ML-–ø—Ä–æ–µ–∫—Ç–æ–≤ –Ω–µ –¥–æ—Ö–æ–¥—è—Ç –¥–æ production!\n",
        "\n",
        "**–ü–æ—á–µ–º—É?**\n",
        "\n",
        "```python\n",
        "# Jupyter Notebook (10% —Ä–∞–±–æ—Ç—ã)\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "accuracy = model.score(X_test, y_test)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "# \"–ì–æ—Ç–æ–≤–æ! –ú–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç!\" üéâ\n",
        "```\n",
        "\n",
        "**–ù–æ –¥–ª—è production –Ω—É–∂–Ω–æ (90% —Ä–∞–±–æ—Ç—ã):**\n",
        "\n",
        "- üì¶ **Packaging:** –ö–∞–∫ —É–ø–∞–∫–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è deployment?\n",
        "- üåê **API:** –ö–∞–∫ –ø–æ–ª—É—á–∞—Ç—å predictions –ø–æ HTTP?\n",
        "- üê≥ **Containerization:** –ö–∞–∫ –æ–±–µ—Å–ø–µ—á–∏—Ç—å reproducibility?\n",
        "- üìä **Monitoring:** –ö–∞–∫ —É–∑–Ω–∞—Ç—å, —á—Ç–æ –º–æ–¥–µ–ª—å degraded?\n",
        "- üîÑ **CI/CD:** –ö–∞–∫ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å deployment?\n",
        "- üìù **Versioning:** –ö–∞–∫ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –∏ –¥–∞–Ω–Ω—ã–µ?\n",
        "- ‚ö° **Scaling:** –ö–∞–∫ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å 1000+ requests/sec?\n",
        "- üîí **Security:** Authentication, rate limiting, input validation?\n",
        "\n",
        "---\n",
        "\n",
        "## üîß MLOps = ML + DevOps + Data Engineering\n",
        "\n",
        "### MLOps Lifecycle:\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ   DATA      ‚îÇ ‚Üí ‚îÇ   MODEL     ‚îÇ ‚Üí ‚îÇ   DEPLOY    ‚îÇ\n",
        "‚îÇ  PIPELINE   ‚îÇ    ‚îÇ  TRAINING   ‚îÇ    ‚îÇ   & SERVE   ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "       ‚Üë                                      ‚îÇ\n",
        "       ‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ\n",
        "       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ  MONITOR &  ‚îÇ ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                 ‚îÇ   RETRAIN   ‚îÇ\n",
        "                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "### –ö–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:\n",
        "\n",
        "#### 1. **Experiment Tracking**\n",
        "- –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –º–µ—Ç—Ä–∏–∫, –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤\n",
        "- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤\n",
        "- Reproducibility\n",
        "- **Tools:** MLflow, Weights & Biases, Neptune, TensorBoard\n",
        "\n",
        "#### 2. **Model Registry**\n",
        "- –í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π\n",
        "- Staging ‚Üí Production –ø–µ—Ä–µ—Ö–æ–¥—ã\n",
        "- Model lineage\n",
        "- **Tools:** MLflow Model Registry, SageMaker Model Registry\n",
        "\n",
        "#### 3. **Data Versioning**\n",
        "- Version control –¥–ª—è –¥–∞–Ω–Ω—ã—Ö\n",
        "- Reproducibility —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤\n",
        "- Data lineage\n",
        "- **Tools:** DVC, Delta Lake, LakeFS\n",
        "\n",
        "#### 4. **Feature Store**\n",
        "- –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ features\n",
        "- Online/offline serving\n",
        "- Feature sharing –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏\n",
        "- **Tools:** Feast, Tecton, AWS Feature Store\n",
        "\n",
        "#### 5. **Model Serving**\n",
        "- REST API –¥–ª—è predictions\n",
        "- Batch vs Real-time inference\n",
        "- A/B testing\n",
        "- **Tools:** FastAPI, BentoML, Seldon, KServe\n",
        "\n",
        "#### 6. **Monitoring**\n",
        "- Data drift detection\n",
        "- Model performance monitoring\n",
        "- Alerting\n",
        "- **Tools:** Evidently, WhyLabs, Arize, Prometheus/Grafana\n",
        "\n",
        "#### 7. **CI/CD for ML**\n",
        "- Automated testing (data, model, API)\n",
        "- Automated deployment\n",
        "- **Tools:** GitHub Actions, Jenkins, GitLab CI, Kubeflow Pipelines\n",
        "\n",
        "---\n",
        "\n",
        "## üìä –ß—Ç–æ –º—ã —Ä–µ–∞–ª–∏–∑—É–µ–º\n",
        "\n",
        "### Full Production Pipeline:\n",
        "\n",
        "**Part 1: Experiment Tracking**\n",
        "1. MLflow setup –∏ logging\n",
        "2. Tracking parameters, metrics, models\n",
        "3. Model comparison –∏ selection\n",
        "4. Model Registry\n",
        "\n",
        "**Part 2: Model Serialization**\n",
        "1. Pickle, joblib, ONNX\n",
        "2. Custom transformers\n",
        "3. Pipeline serialization\n",
        "\n",
        "**Part 3: API Development (FastAPI)**\n",
        "1. REST API –¥–ª—è ML –º–æ–¥–µ–ª–∏\n",
        "2. Pydantic schemas –¥–ª—è validation\n",
        "3. Async endpoints\n",
        "4. Error handling\n",
        "5. API testing\n",
        "\n",
        "**Part 4: Containerization (Docker)**\n",
        "1. Dockerfile –¥–ª—è ML –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è\n",
        "2. Multi-stage builds\n",
        "3. Docker Compose\n",
        "4. Best practices –¥–ª—è ML containers\n",
        "\n",
        "**Part 5: Monitoring & Drift Detection**\n",
        "1. Data drift (input distribution changes)\n",
        "2. Concept drift (target relationship changes)\n",
        "3. Model performance monitoring\n",
        "4. Alerting strategies\n",
        "\n",
        "**Part 6: CI/CD –¥–ª—è ML**\n",
        "1. GitHub Actions workflow\n",
        "2. Automated testing\n",
        "3. Automated deployment\n",
        "4. Continuous training\n",
        "\n",
        "**Part 7: Data Versioning (DVC)**\n",
        "1. DVC basics\n",
        "2. Tracking data –∏ models\n",
        "3. Pipelines\n",
        "\n",
        "---\n"
    ]
})

# ============================================================================
# IMPORTS
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "## üíª –ß–∞—Å—Ç—å 1: Setup –∏ Experiment Tracking\n",
        "\n",
        "### 1.1 –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ë–∞–∑–æ–≤—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, classification_report\n",
        ")\n",
        "\n",
        "# MLOps Tools\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from mlflow.tracking import MlflowClient\n",
        "\n",
        "# Serialization\n",
        "import pickle\n",
        "import joblib\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Type hints\n",
        "from typing import Dict, List, Any, Tuple, Optional\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"\\n‚úÖ –í—Å–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
        "print(f\"MLflow version: {mlflow.__version__}\")\n"
    ]
})

# ============================================================================
# DATASET
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 1.2 Dataset –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\n",
        "\n",
        "–ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π dataset –¥–ª—è **Customer Churn Prediction** - —Ç–∏–ø–∏—á–Ω–∞—è production –∑–∞–¥–∞—á–∞."
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "def create_churn_dataset(n_samples: int = 10000) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Create synthetic customer churn dataset.\n",
        "    \n",
        "    Features:\n",
        "    - tenure: months with company\n",
        "    - monthly_charges: monthly bill\n",
        "    - total_charges: total amount paid\n",
        "    - contract_type: month-to-month, one year, two year\n",
        "    - payment_method: credit card, bank transfer, electronic check\n",
        "    - internet_service: DSL, Fiber optic, No\n",
        "    - online_security: Yes, No\n",
        "    - tech_support: Yes, No\n",
        "    - num_tickets: support tickets opened\n",
        "    - senior_citizen: 0 or 1\n",
        "    \n",
        "    Target: churn (0 or 1)\n",
        "    \"\"\"\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Numerical features\n",
        "    tenure = np.random.randint(1, 72, n_samples)\n",
        "    monthly_charges = np.random.uniform(20, 100, n_samples)\n",
        "    total_charges = tenure * monthly_charges + np.random.normal(0, 100, n_samples)\n",
        "    total_charges = np.maximum(total_charges, 0)\n",
        "    num_tickets = np.random.poisson(2, n_samples)\n",
        "    \n",
        "    # Categorical features\n",
        "    contract_type = np.random.choice(\n",
        "        ['Month-to-month', 'One year', 'Two year'],\n",
        "        n_samples, p=[0.5, 0.3, 0.2]\n",
        "    )\n",
        "    payment_method = np.random.choice(\n",
        "        ['Credit card', 'Bank transfer', 'Electronic check'],\n",
        "        n_samples, p=[0.4, 0.3, 0.3]\n",
        "    )\n",
        "    internet_service = np.random.choice(\n",
        "        ['DSL', 'Fiber optic', 'No'],\n",
        "        n_samples, p=[0.4, 0.4, 0.2]\n",
        "    )\n",
        "    online_security = np.random.choice(['Yes', 'No'], n_samples, p=[0.4, 0.6])\n",
        "    tech_support = np.random.choice(['Yes', 'No'], n_samples, p=[0.3, 0.7])\n",
        "    senior_citizen = np.random.choice([0, 1], n_samples, p=[0.84, 0.16])\n",
        "    \n",
        "    # Create churn with realistic patterns\n",
        "    churn_prob = (\n",
        "        0.1 +  # baseline\n",
        "        (tenure < 12).astype(float) * 0.15 +  # short tenure = higher churn\n",
        "        (monthly_charges > 70).astype(float) * 0.1 +  # high charges = higher churn\n",
        "        (contract_type == 'Month-to-month').astype(float) * 0.2 +  # month-to-month = higher churn\n",
        "        (payment_method == 'Electronic check').astype(float) * 0.05 +  # electronic check = higher churn\n",
        "        (online_security == 'No').astype(float) * 0.05 +  # no security = higher churn\n",
        "        (tech_support == 'No').astype(float) * 0.05 +  # no support = higher churn\n",
        "        (num_tickets > 3).astype(float) * 0.1  # many tickets = higher churn\n",
        "    )\n",
        "    \n",
        "    # Normalize probabilities\n",
        "    churn_prob = np.clip(churn_prob, 0, 0.9)\n",
        "    churn = (np.random.random(n_samples) < churn_prob).astype(int)\n",
        "    \n",
        "    df = pd.DataFrame({\n",
        "        'tenure': tenure,\n",
        "        'monthly_charges': monthly_charges,\n",
        "        'total_charges': total_charges,\n",
        "        'num_tickets': num_tickets,\n",
        "        'contract_type': contract_type,\n",
        "        'payment_method': payment_method,\n",
        "        'internet_service': internet_service,\n",
        "        'online_security': online_security,\n",
        "        'tech_support': tech_support,\n",
        "        'senior_citizen': senior_citizen,\n",
        "        'churn': churn\n",
        "    })\n",
        "    \n",
        "    return df\n",
        "\n",
        "# –°–æ–∑–¥–∞—ë–º dataset\n",
        "df = create_churn_dataset(10000)\n",
        "\n",
        "print(\"\\nüìä Customer Churn Dataset\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"Churn rate: {df['churn'].mean():.2%}\")\n",
        "print(f\"\\nFeature types:\")\n",
        "print(df.dtypes)\n",
        "print(f\"\\nFirst rows:\")\n",
        "df.head()\n"
    ]
})

# ============================================================================
# PREPROCESSING
# ============================================================================

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Preprocessing\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"PREPROCESSING\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "numerical_cols = ['tenure', 'monthly_charges', 'total_charges', 'num_tickets']\n",
        "categorical_cols = [\n",
        "    'contract_type', 'payment_method', 'internet_service',\n",
        "    'online_security', 'tech_support', 'senior_citizen'\n",
        "]\n",
        "\n",
        "# senior_citizen —É–∂–µ —á–∏—Å–ª–æ–≤–æ–π, –Ω–æ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π\n",
        "df['senior_citizen'] = df['senior_citizen'].astype(str)\n",
        "\n",
        "# Label encoding –¥–ª—è categorical\n",
        "label_encoders = {}\n",
        "df_processed = df.copy()\n",
        "\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    df_processed[col] = le.fit_transform(df_processed[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Feature matrix –∏ target\n",
        "feature_cols = numerical_cols + categorical_cols\n",
        "X = df_processed[feature_cols]\n",
        "y = df_processed['churn']\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = X_train.copy()\n",
        "X_test_scaled = X_test.copy()\n",
        "X_train_scaled[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
        "X_test_scaled[numerical_cols] = scaler.transform(X_test[numerical_cols])\n",
        "\n",
        "print(f\"\\n‚úÖ Preprocessing completed\")\n",
        "print(f\"Train size: {X_train.shape[0]}\")\n",
        "print(f\"Test size: {X_test.shape[0]}\")\n",
        "print(f\"Features: {len(feature_cols)}\")\n",
        "print(f\"Train churn rate: {y_train.mean():.2%}\")\n",
        "print(f\"Test churn rate: {y_test.mean():.2%}\")\n"
    ]
})

# ============================================================================
# MLFLOW EXPERIMENT TRACKING
# ============================================================================

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "---\n",
        "\n",
        "## üî¨ –ß–∞—Å—Ç—å 2: Experiment Tracking —Å MLflow\n",
        "\n",
        "### –ß—Ç–æ —Ç–∞–∫–æ–µ MLflow?\n",
        "\n",
        "**MLflow** - open-source –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è ML lifecycle:\n",
        "\n",
        "1. **MLflow Tracking:** –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤\n",
        "2. **MLflow Projects:** Packaging code –¥–ª—è reproducibility\n",
        "3. **MLflow Models:** Packaging models –¥–ª—è deployment\n",
        "4. **MLflow Model Registry:** –í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ staging –º–æ–¥–µ–ª–µ–π\n",
        "\n",
        "### –ü–æ—á–µ–º—É Experiment Tracking –≤–∞–∂–µ–Ω?\n",
        "\n",
        "**–ë–µ–∑ tracking:**\n",
        "```python\n",
        "# model_v1.py, model_v2_final.py, model_v2_final_FINAL.py...\n",
        "# \"–ö–∞–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–∞–ª–∏ –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç?\"\n",
        "# \"–ö–∞–∫—É—é –≤–µ—Ä—Å–∏—é –º—ã –∑–∞–¥–µ–ø–ª–æ–∏–ª–∏?\"\n",
        "# \"–ü–æ—á–µ–º—É accuracy —É–ø–∞–ª–∞?\"\n",
        "```\n",
        "\n",
        "**–° MLflow:**\n",
        "- ‚úÖ –í—Å–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ª–æ–≥–∏—Ä—É—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏\n",
        "- ‚úÖ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã, –º–µ—Ç—Ä–∏–∫–∏, –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã\n",
        "- ‚úÖ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –≤ UI\n",
        "- ‚úÖ Reproducibility –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω\n",
        "- ‚úÖ Model lineage –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç—Å—è\n",
        "\n",
        "---\n"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "print(\"=\" * 70)\n",
        "print(\"MLFLOW EXPERIMENT TRACKING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# –°–æ–∑–¥–∞—ë–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç\n",
        "experiment_name = \"churn_prediction\"\n",
        "mlflow.set_experiment(experiment_name)\n",
        "\n",
        "print(f\"\\n‚úÖ MLflow experiment created: {experiment_name}\")\n",
        "print(f\"Tracking URI: {mlflow.get_tracking_uri()}\")\n",
        "\n",
        "# –ü–æ–ª—É—á–∞–µ–º experiment info\n",
        "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
        "print(f\"Experiment ID: {experiment.experiment_id}\")\n",
        "print(f\"Artifact Location: {experiment.artifact_location}\")\n"
    ]
})

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 2.1 –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤\n",
        "\n",
        "–û–±—É—á–∏–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π –∏ –∑–∞–ª–æ–≥–∏—Ä—É–µ–º –≤—Å–µ –¥–µ—Ç–∞–ª–∏ –≤ MLflow."
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "def train_and_log_model(\n",
        "    model,\n",
        "    model_name: str,\n",
        "    X_train: pd.DataFrame,\n",
        "    X_test: pd.DataFrame,\n",
        "    y_train: pd.Series,\n",
        "    y_test: pd.Series,\n",
        "    params: Dict[str, Any]\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Train model and log everything to MLflow.\n",
        "    \n",
        "    Returns:\n",
        "        run_id: MLflow run ID\n",
        "    \"\"\"\n",
        "    with mlflow.start_run(run_name=model_name):\n",
        "        # Log parameters\n",
        "        mlflow.log_params(params)\n",
        "        mlflow.log_param(\"model_type\", model_name)\n",
        "        mlflow.log_param(\"train_size\", len(X_train))\n",
        "        mlflow.log_param(\"test_size\", len(X_test))\n",
        "        mlflow.log_param(\"n_features\", X_train.shape[1])\n",
        "        \n",
        "        # Train model\n",
        "        model.fit(X_train, y_train)\n",
        "        \n",
        "        # Predictions\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "        \n",
        "        # Calculate metrics\n",
        "        metrics = {\n",
        "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "            \"precision\": precision_score(y_test, y_pred),\n",
        "            \"recall\": recall_score(y_test, y_pred),\n",
        "            \"f1\": f1_score(y_test, y_pred),\n",
        "            \"roc_auc\": roc_auc_score(y_test, y_pred_proba)\n",
        "        }\n",
        "        \n",
        "        # Log metrics\n",
        "        mlflow.log_metrics(metrics)\n",
        "        \n",
        "        # Log model\n",
        "        mlflow.sklearn.log_model(\n",
        "            model,\n",
        "            artifact_path=\"model\",\n",
        "            registered_model_name=f\"{model_name}_churn\"\n",
        "        )\n",
        "        \n",
        "        # Log additional artifacts\n",
        "        # Save feature importance (for tree-based models)\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            importance_df = pd.DataFrame({\n",
        "                'feature': feature_cols,\n",
        "                'importance': model.feature_importances_\n",
        "            }).sort_values('importance', ascending=False)\n",
        "            \n",
        "            importance_path = f\"/tmp/{model_name}_importance.csv\"\n",
        "            importance_df.to_csv(importance_path, index=False)\n",
        "            mlflow.log_artifact(importance_path)\n",
        "        \n",
        "        # Log confusion matrix as artifact\n",
        "        from sklearn.metrics import confusion_matrix\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        cm_df = pd.DataFrame(\n",
        "            cm,\n",
        "            columns=['Predicted 0', 'Predicted 1'],\n",
        "            index=['Actual 0', 'Actual 1']\n",
        "        )\n",
        "        cm_path = f\"/tmp/{model_name}_confusion_matrix.csv\"\n",
        "        cm_df.to_csv(cm_path)\n",
        "        mlflow.log_artifact(cm_path)\n",
        "        \n",
        "        # Get run ID\n",
        "        run_id = mlflow.active_run().info.run_id\n",
        "        \n",
        "        print(f\"\\n‚úÖ {model_name} logged to MLflow\")\n",
        "        print(f\"   Run ID: {run_id}\")\n",
        "        print(f\"   Metrics:\")\n",
        "        for metric_name, value in metrics.items():\n",
        "            print(f\"      {metric_name}: {value:.4f}\")\n",
        "        \n",
        "        return run_id\n"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –û–±—É—á–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TRAINING MODELS WITH MLFLOW TRACKING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "run_ids = {}\n",
        "\n",
        "# 1. Logistic Regression\n",
        "print(\"\\n[1/4] Logistic Regression\")\n",
        "lr_params = {\n",
        "    \"C\": 1.0,\n",
        "    \"max_iter\": 1000,\n",
        "    \"solver\": \"lbfgs\"\n",
        "}\n",
        "lr = LogisticRegression(**lr_params, random_state=42)\n",
        "run_ids['LogisticRegression'] = train_and_log_model(\n",
        "    lr, \"LogisticRegression\",\n",
        "    X_train_scaled, X_test_scaled, y_train, y_test,\n",
        "    lr_params\n",
        ")\n",
        "\n",
        "# 2. Random Forest (default)\n",
        "print(\"\\n[2/4] Random Forest (default)\")\n",
        "rf_params = {\n",
        "    \"n_estimators\": 100,\n",
        "    \"max_depth\": 10,\n",
        "    \"min_samples_split\": 10\n",
        "}\n",
        "rf = RandomForestClassifier(**rf_params, random_state=42, n_jobs=-1)\n",
        "run_ids['RandomForest_v1'] = train_and_log_model(\n",
        "    rf, \"RandomForest_v1\",\n",
        "    X_train_scaled, X_test_scaled, y_train, y_test,\n",
        "    rf_params\n",
        ")\n",
        "\n",
        "# 3. Random Forest (tuned)\n",
        "print(\"\\n[3/4] Random Forest (tuned)\")\n",
        "rf_params_v2 = {\n",
        "    \"n_estimators\": 200,\n",
        "    \"max_depth\": 15,\n",
        "    \"min_samples_split\": 5,\n",
        "    \"min_samples_leaf\": 2\n",
        "}\n",
        "rf_v2 = RandomForestClassifier(**rf_params_v2, random_state=42, n_jobs=-1)\n",
        "run_ids['RandomForest_v2'] = train_and_log_model(\n",
        "    rf_v2, \"RandomForest_v2\",\n",
        "    X_train_scaled, X_test_scaled, y_train, y_test,\n",
        "    rf_params_v2\n",
        ")\n",
        "\n",
        "# 4. Gradient Boosting\n",
        "print(\"\\n[4/4] Gradient Boosting\")\n",
        "gb_params = {\n",
        "    \"n_estimators\": 100,\n",
        "    \"max_depth\": 5,\n",
        "    \"learning_rate\": 0.1\n",
        "}\n",
        "gb = GradientBoostingClassifier(**gb_params, random_state=42)\n",
        "run_ids['GradientBoosting'] = train_and_log_model(\n",
        "    gb, \"GradientBoosting\",\n",
        "    X_train_scaled, X_test_scaled, y_train, y_test,\n",
        "    gb_params\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ ALL MODELS LOGGED TO MLFLOW\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nTotal runs: {len(run_ids)}\")\n",
        "print(\"Run IDs:\")\n",
        "for name, run_id in run_ids.items():\n",
        "    print(f\"  {name}: {run_id}\")\n"
    ]
})

cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "### 2.2 –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤\n",
        "\n",
        "MLflow –ø–æ–∑–≤–æ–ª—è–µ—Ç –ª–µ–≥–∫–æ —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å runs –∏ –≤—ã–±–∏—Ä–∞—Ç—å –ª—É—á—à—É—é –º–æ–¥–µ–ª—å."
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ runs –∏–∑ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞\n",
        "client = MlflowClient()\n",
        "\n",
        "# Search runs\n",
        "runs = mlflow.search_runs(\n",
        "    experiment_ids=[experiment.experiment_id],\n",
        "    order_by=[\"metrics.roc_auc DESC\"]\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"EXPERIMENT COMPARISON\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º comparison table\n",
        "comparison_cols = [\n",
        "    'run_id', 'tags.mlflow.runName',\n",
        "    'metrics.accuracy', 'metrics.precision', 'metrics.recall',\n",
        "    'metrics.f1', 'metrics.roc_auc'\n",
        "]\n",
        "\n",
        "comparison_df = runs[comparison_cols].copy()\n",
        "comparison_df.columns = ['Run ID', 'Model', 'Accuracy', 'Precision', 'Recall', 'F1', 'ROC AUC']\n",
        "comparison_df['Run ID'] = comparison_df['Run ID'].str[:8] + '...'\n",
        "\n",
        "print(\"\\nAll Runs (sorted by ROC AUC):\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Best model\n",
        "best_run = runs.iloc[0]\n",
        "print(f\"\\nüèÜ Best Model: {best_run['tags.mlflow.runName']}\")\n",
        "print(f\"   ROC AUC: {best_run['metrics.roc_auc']:.4f}\")\n",
        "print(f\"   F1 Score: {best_run['metrics.f1']:.4f}\")\n",
        "print(f\"   Run ID: {best_run['run_id']}\")\n"
    ]
})

cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "# Metrics comparison\n",
        "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1', 'ROC AUC']\n",
        "comparison_df.set_index('Model')[metrics_to_plot].plot(\n",
        "    kind='bar', ax=axes[0], width=0.8\n",
        ")\n",
        "axes[0].set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('Score')\n",
        "axes[0].set_ylim([0, 1])\n",
        "axes[0].legend(loc='lower right')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# ROC AUC comparison\n",
        "colors = ['#e74c3c' if i == 0 else '#3498db' for i in range(len(comparison_df))]\n",
        "axes[1].barh(comparison_df['Model'], comparison_df['ROC AUC'], color=colors)\n",
        "axes[1].set_title('ROC AUC Comparison', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('ROC AUC')\n",
        "for i, v in enumerate(comparison_df['ROC AUC']):\n",
        "    axes[1].text(v + 0.005, i, f'{v:.4f}', va='center')\n",
        "axes[1].set_xlim([0.5, 1.0])\n",
        "axes[1].grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüí° MLflow UI –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ –∞–¥—Ä–µ—Å—É: mlflow ui\")\n",
        "print(\"–¢–∞–º –º–æ–∂–Ω–æ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã, —Å–º–æ—Ç—Ä–µ—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏, –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã.\")\n"
    ]
})

# –°–æ—Ö—Ä–∞–Ω—è–µ–º notebook
notebook['cells'] = cells

output_path = '/home/user/test/notebooks/phase7_production_mlops/01_production_mlops.ipynb'
with open(output_path, 'w', encoding='utf-8') as f:
    json.dump(notebook, f, ensure_ascii=False, indent=1)

print(f'‚úÖ Phase 7 notebook started: {output_path}')
print(f'Cells: {len(cells)}')
print('Introduction and MLflow experiment tracking added!')

{
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 9: Reinforcement Learning\n",
    "## Часть 2: Policy Gradient Methods\n",
    "\n",
    "### В этом ноутбуке:\n",
    "\n",
    "1. **Policy Gradient** - прямая оптимизация policy\n",
    "2. **REINFORCE** - Monte Carlo policy gradient\n",
    "3. **Actor-Critic** - комбинация policy и value\n",
    "4. **Baseline** - уменьшение variance"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Policy Gradient Theorem\n",
    "\n",
    "### Идея\n",
    "\n",
    "Вместо оценки Q-функции, напрямую оптимизируем policy $\\pi_\\theta(a|s)$.\n",
    "\n",
    "### Policy Gradient Theorem\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot Q^{\\pi_\\theta}(s,a)]$$\n",
    "\n",
    "### Преимущества:\n",
    "\n",
    "- Работает с непрерывными действиями\n",
    "- Может обучать стохастические policy\n",
    "- Более стабильное обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Среда CartPole"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class SimpleCartPole:\n",
    "    \"\"\"Упрощённая симуляция CartPole\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = self.masspole + self.masscart\n",
    "        self.length = 0.5\n",
    "        self.polemass_length = self.masspole * self.length\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02\n",
    "        \n",
    "        self.x_threshold = 2.4\n",
    "        self.theta_threshold = 12 * np.pi / 180\n",
    "        \n",
    "        self.state = None\n",
    "        self.steps_count = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.random.uniform(-0.05, 0.05, size=4)\n",
    "        self.steps_count = 0\n",
    "        return self.state.copy()\n",
    "    \n",
    "    def step(self, action):\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        \n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "        costheta = np.cos(theta)\n",
    "        sintheta = np.sin(theta)\n",
    "        \n",
    "        temp = (force + self.polemass_length * theta_dot**2 * sintheta) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / \\\n",
    "                   (self.length * (4.0/3.0 - self.masspole * costheta**2 / self.total_mass))\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "        \n",
    "        x = x + self.tau * x_dot\n",
    "        x_dot = x_dot + self.tau * xacc\n",
    "        theta = theta + self.tau * theta_dot\n",
    "        theta_dot = theta_dot + self.tau * thetaacc\n",
    "        \n",
    "        self.state = np.array([x, x_dot, theta, theta_dot])\n",
    "        self.steps_count += 1\n",
    "        \n",
    "        done = bool(\n",
    "            x < -self.x_threshold or x > self.x_threshold or\n",
    "            theta < -self.theta_threshold or theta > self.theta_threshold or\n",
    "            self.steps_count >= 500\n",
    "        )\n",
    "        \n",
    "        reward = 1.0 if not done else 0.0\n",
    "        return self.state.copy(), reward, done\n",
    "\n",
    "env = SimpleCartPole()\n",
    "print(f'State dim: 4, Action dim: 2')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. REINFORCE Algorithm\n",
    "\n",
    "Monte Carlo policy gradient - обновляем после каждого эпизода."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"Policy Network - выдаёт распределение по действиям\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        probs = self.forward(state)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action)\n",
    "\n",
    "# Тест\n",
    "policy = PolicyNetwork(4, 2)\n",
    "test_state = np.random.randn(4)\n",
    "action, log_prob = policy.get_action(test_state)\n",
    "print(f'Action: {action}, Log prob: {log_prob.item():.4f}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class REINFORCE:\n",
    "    \"\"\"REINFORCE algorithm\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99):\n",
    "        self.policy = PolicyNetwork(state_dim, action_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Хранение эпизода\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        action, log_prob = self.policy.get_action(state)\n",
    "        self.log_probs.append(log_prob)\n",
    "        return action\n",
    "    \n",
    "    def store_reward(self, reward):\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Обновление после эпизода\"\"\"\n",
    "        \n",
    "        # Вычисляем returns (cumulative discounted rewards)\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(self.rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        \n",
    "        returns = torch.FloatTensor(returns).to(device)\n",
    "        \n",
    "        # Нормализация returns (baseline trick)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        # Policy gradient loss\n",
    "        policy_loss = []\n",
    "        for log_prob, G in zip(self.log_probs, returns):\n",
    "            policy_loss.append(-log_prob * G)\n",
    "        \n",
    "        loss = torch.stack(policy_loss).sum()\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Очистка\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "print('REINFORCE agent создан')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def train_reinforce(env, agent, episodes=500):\n",
    "    \"\"\"Обучение REINFORCE\"\"\"\n",
    "    \n",
    "    rewards_history = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.store_reward(reward)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "        \n",
    "        agent.update()\n",
    "        rewards_history.append(total_reward)\n",
    "        \n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg_reward = np.mean(rewards_history[-100:])\n",
    "            print(f'Episode {episode+1}, Avg Reward: {avg_reward:.1f}')\n",
    "    \n",
    "    return rewards_history\n",
    "\n",
    "# Обучение\n",
    "env = SimpleCartPole()\n",
    "reinforce_agent = REINFORCE(state_dim=4, action_dim=2, lr=1e-3)\n",
    "\n",
    "print('Обучение REINFORCE...\\n')\n",
    "reinforce_rewards = train_reinforce(env, reinforce_agent, episodes=500)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Визуализация\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(reinforce_rewards, alpha=0.3)\n",
    "if len(reinforce_rewards) >= 50:\n",
    "    smooth = np.convolve(reinforce_rewards, np.ones(50)/50, mode='valid')\n",
    "    plt.plot(range(49, len(reinforce_rewards)), smooth, linewidth=2, label='Moving Avg')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('REINFORCE Training')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f'Финальная средняя награда: {np.mean(reinforce_rewards[-100:]):.1f}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Actor-Critic\n",
    "\n",
    "Комбинируем:\n",
    "- **Actor** - policy network (выбор действий)\n",
    "- **Critic** - value network (оценка состояний)\n",
    "\n",
    "Critic уменьшает variance градиента."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"Actor-Critic Network\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Shared layers\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Actor head\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # Critic head\n",
    "        self.critic = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        shared = self.shared(x)\n",
    "        policy = self.actor(shared)\n",
    "        value = self.critic(shared)\n",
    "        return policy, value\n",
    "\n",
    "# Тест\n",
    "ac = ActorCritic(4, 2)\n",
    "test_state = torch.randn(1, 4)\n",
    "policy, value = ac(test_state)\n",
    "print(f'Policy shape: {policy.shape}, Value shape: {value.shape}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class A2CAgent:\n",
    "    \"\"\"Advantage Actor-Critic Agent\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99):\n",
    "        self.network = ActorCritic(state_dim, action_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Хранение эпизода\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        probs, value = self.network(state)\n",
    "        \n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        self.log_probs.append(dist.log_prob(action))\n",
    "        self.values.append(value)\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def store_outcome(self, reward, done):\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Обновление после эпизода\"\"\"\n",
    "        \n",
    "        # Вычисляем returns\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r, done in zip(reversed(self.rewards), reversed(self.dones)):\n",
    "            if done:\n",
    "                G = 0\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        \n",
    "        returns = torch.FloatTensor(returns).to(device)\n",
    "        log_probs = torch.stack(self.log_probs)\n",
    "        values = torch.cat(self.values).squeeze()\n",
    "        \n",
    "        # Advantage = Return - Value\n",
    "        advantage = returns - values.detach()\n",
    "        \n",
    "        # Actor loss (policy gradient)\n",
    "        actor_loss = -(log_probs * advantage).mean()\n",
    "        \n",
    "        # Critic loss (value estimation)\n",
    "        critic_loss = F.mse_loss(values, returns)\n",
    "        \n",
    "        # Total loss\n",
    "        loss = actor_loss + 0.5 * critic_loss\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Очистка\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "print('A2C agent создан')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def train_a2c(env, agent, episodes=500):\n",
    "    \"\"\"Обучение A2C\"\"\"\n",
    "    \n",
    "    rewards_history = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.store_outcome(reward, done)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "        \n",
    "        agent.update()\n",
    "        rewards_history.append(total_reward)\n",
    "        \n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg_reward = np.mean(rewards_history[-100:])\n",
    "            print(f'Episode {episode+1}, Avg Reward: {avg_reward:.1f}')\n",
    "    \n",
    "    return rewards_history\n",
    "\n",
    "# Обучение\n",
    "env = SimpleCartPole()\n",
    "a2c_agent = A2CAgent(state_dim=4, action_dim=2, lr=1e-3)\n",
    "\n",
    "print('Обучение A2C...\\n')\n",
    "a2c_rewards = train_a2c(env, a2c_agent, episodes=500)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Сравнение REINFORCE vs A2C"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Сравнение\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# REINFORCE\n",
    "axes[0].plot(reinforce_rewards, alpha=0.3, label='Episode')\n",
    "if len(reinforce_rewards) >= 50:\n",
    "    smooth = np.convolve(reinforce_rewards, np.ones(50)/50, mode='valid')\n",
    "    axes[0].plot(range(49, len(reinforce_rewards)), smooth, linewidth=2)\n",
    "axes[0].set_title('REINFORCE')\n",
    "axes[0].set_xlabel('Episode')\n",
    "axes[0].set_ylabel('Reward')\n",
    "\n",
    "# A2C\n",
    "axes[1].plot(a2c_rewards, alpha=0.3, label='Episode')\n",
    "if len(a2c_rewards) >= 50:\n",
    "    smooth = np.convolve(a2c_rewards, np.ones(50)/50, mode='valid')\n",
    "    axes[1].plot(range(49, len(a2c_rewards)), smooth, linewidth=2)\n",
    "axes[1].set_title('A2C (Actor-Critic)')\n",
    "axes[1].set_xlabel('Episode')\n",
    "axes[1].set_ylabel('Reward')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nСравнение (последние 100 эпизодов):')\n",
    "print(f'REINFORCE: {np.mean(reinforce_rewards[-100:]):.1f} +/- {np.std(reinforce_rewards[-100:]):.1f}')\n",
    "print(f'A2C:       {np.mean(a2c_rewards[-100:]):.1f} +/- {np.std(a2c_rewards[-100:]):.1f}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Итоги\n",
    "\n",
    "### Что мы изучили:\n",
    "\n",
    "1. **Policy Gradient Theorem** - основа методов\n",
    "2. **REINFORCE** - простой Monte Carlo метод\n",
    "3. **Actor-Critic** - комбинация policy и value\n",
    "4. **Advantage** - уменьшение variance\n",
    "\n",
    "### Сравнение методов:\n",
    "\n",
    "| Метод | Variance | Sample Efficiency | Complexity |\n",
    "|-------|----------|-------------------|------------|\n",
    "| REINFORCE | High | Low | Simple |\n",
    "| A2C | Low | Medium | Medium |\n",
    "\n",
    "### Ключевые формулы:\n",
    "\n",
    "**Policy Gradient:**\n",
    "$$\\nabla_\\theta J = \\mathbb{E}[\\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot A(s,a)]$$\n",
    "\n",
    "**Advantage:**\n",
    "$$A(s,a) = Q(s,a) - V(s) \\approx r + \\gamma V(s') - V(s)$$\n",
    "\n",
    "### Следующий шаг:\n",
    "\n",
    "В ноутбуке 03 изучим PPO - современный стандарт policy gradient."
   ]
  }
 ]
}
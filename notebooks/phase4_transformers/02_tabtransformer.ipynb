{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä TabTransformer: Transformers –¥–ª—è –¢–∞–±–ª–∏—á–Ω—ã—Ö –î–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "**Phase 4, Step 2: Advanced Transformer Architectures**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ –ü—Ä–æ–±–ª–µ–º–∞ Categorical Features\n",
    "\n",
    "### –í Phase 4 Step 1 –º—ã –≤–∏–¥–µ–ª–∏:\n",
    "\n",
    "**–û–±—ã—á–Ω—ã–π Transformer –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:**\n",
    "- ‚úÖ Feature embedding: Linear projection\n",
    "- ‚úÖ Self-Attention –º–µ–∂–¥—É features\n",
    "- ‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ Titanic (891 samples)\n",
    "\n",
    "**–ù–æ –±—ã–ª–∏ –ø—Ä–æ–±–ª–µ–º—ã:**\n",
    "- ‚ùå **–ú–∞–ª–µ–Ω—å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç**: Titanic —Å–ª–∏—à–∫–æ–º –º–∞–ª –¥–ª—è Transformers\n",
    "- ‚ùå **Categorical features**: –ø—Ä–æ—Å—Ç–æ one-hot encoded\n",
    "- ‚ùå **No contextual embeddings**: –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–µ —É—á–∞—Ç –¥—Ä—É–≥ —É –¥—Ä—É–≥–∞\n",
    "- ‚ùå **–ù–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞**: XGBoost —Ä–∞–±–æ—Ç–∞–ª —Ç–∞–∫ –∂–µ —Ö–æ—Ä–æ—à–æ\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Enter TabTransformer (2020)\n",
    "\n",
    "**\"TabTransformer: Tabular Data Modeling Using Contextual Embeddings\"** (Huang et al., 2020)\n",
    "\n",
    "**–ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è:** Categorical features ‚Üí **Contextual Embeddings** —á–µ—Ä–µ–∑ Transformer!\n",
    "\n",
    "### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ TabTransformer:\n",
    "\n",
    "```\n",
    "Input: [Cat1, Cat2, Cat3, ..., CatM] + [Num1, Num2, ..., NumN]\n",
    "          ‚Üì         ‚Üì       ‚Üì\n",
    "    [Emb1]   [Emb2]   [Emb3]  ‚Üê Column Embeddings (learnable)\n",
    "          ‚Üì         ‚Üì       ‚Üì\n",
    "      + Positional Encoding\n",
    "          ‚Üì         ‚Üì       ‚Üì\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ  Transformer Layers     ‚îÇ  ‚Üê Attention –º–µ–∂–¥—É categorical features\n",
    "    ‚îÇ  (N encoder blocks)     ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "          ‚Üì         ‚Üì       ‚Üì\n",
    "    [Ctx1]   [Ctx2]   [Ctx3]  ‚Üê Contextual Embeddings\n",
    "          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                  ‚Üì\n",
    "          Concatenate with [Num1, Num2, ..., NumN]\n",
    "                  ‚Üì\n",
    "            MLP Classifier\n",
    "                  ‚Üì\n",
    "              Output\n",
    "```\n",
    "\n",
    "### –ö–ª—é—á–µ–≤—ã–µ –æ—Ç–ª–∏—á–∏—è –æ—Ç –æ–±—ã—á–Ω–æ–≥–æ Transformer:\n",
    "\n",
    "1. **Column Embeddings** –≤–º–µ—Å—Ç–æ Linear Projection:\n",
    "   - –ö–∞–∂–¥–∞—è categorical feature ‚Üí lookup embedding (–∫–∞–∫ word embeddings)\n",
    "   - –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: `vocab_size √ó d_model`\n",
    "   - –ê–Ω–∞–ª–æ–≥ word2vec –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n",
    "\n",
    "2. **Transformer —Ç–æ–ª—å–∫–æ –Ω–∞ Categorical**:\n",
    "   - Transformer –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ categorical features\n",
    "   - Numerical features –æ—Å—Ç–∞—é—Ç—Å—è –∫–∞–∫ –µ—Å—Ç—å\n",
    "   - Concatenation –≤ –∫–æ–Ω—Ü–µ\n",
    "\n",
    "3. **Contextual Embeddings**:\n",
    "   - –ü–æ—Å–ª–µ Transformer –∫–∞–∂–¥–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è –∏–º–µ–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥—Ä—É–≥–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n",
    "   - –ü—Ä–∏–º–µ—Ä: \"Occupation=Teacher\" + \"Education=Masters\" ‚Üí –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Adult Income Dataset\n",
    "\n",
    "**–ó–∞–¥–∞—á–∞:** –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å, –∑–∞—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ª–∏ —á–µ–ª–æ–≤–µ–∫ >$50K/–≥–æ–¥\n",
    "\n",
    "**–†–∞–∑–º–µ—Ä:** ~48,842 samples (–≤ 55 —Ä–∞–∑ –±–æ–ª—å—à–µ Titanic!)\n",
    "\n",
    "**Features (14 total):**\n",
    "\n",
    "**Categorical (8):**\n",
    "- `workclass`: Private, Self-emp, Federal-gov, etc. (9 categories)\n",
    "- `education`: Bachelors, HS-grad, Masters, Doctorate, etc. (16 categories)\n",
    "- `marital-status`: Married, Never-married, Divorced, etc. (7 categories)\n",
    "- `occupation`: Tech-support, Craft-repair, Sales, Exec-managerial, etc. (15 categories)\n",
    "- `relationship`: Wife, Husband, Not-in-family, etc. (6 categories)\n",
    "- `race`: White, Black, Asian-Pac-Islander, etc. (5 categories)\n",
    "- `sex`: Male, Female (2 categories)\n",
    "- `native-country`: United-States, Mexico, India, etc. (42 categories)\n",
    "\n",
    "**Numerical (6):**\n",
    "- `age`: –í–æ–∑—Ä–∞—Å—Ç\n",
    "- `fnlwgt`: Final weight (census weight)\n",
    "- `education-num`: –ì–æ–¥—ã –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è\n",
    "- `capital-gain`: Capital gain\n",
    "- `capital-loss`: Capital loss\n",
    "- `hours-per-week`: –ß–∞—Å–æ–≤ —Ä–∞–±–æ—Ç—ã –≤ –Ω–µ–¥–µ–ª—é\n",
    "\n",
    "**Target:** `income` (>50K –∏–ª–∏ <=50K)\n",
    "\n",
    "**–ü–æ—á–µ–º—É –∏–¥–µ–∞–ª–µ–Ω –¥–ª—è TabTransformer:**\n",
    "- ‚úÖ –ë–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç (>40k samples)\n",
    "- ‚úÖ –ú–Ω–æ–≥–æ categorical features (8 —à—Ç—É–∫)\n",
    "- ‚úÖ –í—ã—Å–æ–∫–∞—è cardinality (education=16, occupation=15, country=42)\n",
    "- ‚úÖ –°–ª–æ–∂–Ω—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è (education √ó occupation √ó marital-status)\n",
    "- ‚úÖ –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π benchmark –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ –ß—Ç–æ –º—ã –ø–æ–∫–∞–∂–µ–º\n",
    "\n",
    "1. **Contextual Embeddings —Ä–∞–±–æ—Ç–∞—é—Ç:**\n",
    "   - TabTransformer > –æ–±—ã—á–Ω—ã–π Transformer\n",
    "   - Categorical embeddings —É—á–∞—Ç—Å—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥—Ä—É–≥ –¥—Ä—É–≥–∞\n",
    "\n",
    "2. **Competitive —Å Tree-based:**\n",
    "   - TabTransformer ‚âà XGBoost/LightGBM –Ω–∞ –±–æ–ª—å—à–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
    "   - –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ: –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å —á–µ—Ä–µ–∑ attention\n",
    "\n",
    "3. **Attention –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç interactions:**\n",
    "   - –ö–∞–∫–∏–µ categorical features –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—é—Ç\n",
    "   - \"Education\" attention –Ω–∞ \"Occupation\"\n",
    "\n",
    "4. **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å:**\n",
    "   - 48k samples ‚Üí –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–∏–ª—É Deep Learning\n",
    "   - –ù–µ –∫–∞–∫ Titanic (891 samples)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª –ß–∞—Å—Ç—å 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "### 1.1 –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ë–∞–∑–æ–≤—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# Math\n",
    "import math\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"\\n‚úÖ –í—Å–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 –ó–∞–≥—Ä—É–∑–∫–∞ Adult Income Dataset\n",
    "\n",
    "**–ò—Å—Ç–æ—á–Ω–∏–∫:** UCI Machine Learning Repository  \n",
    "**URL:** https://archive.ics.uci.edu/ml/datasets/adult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names\n",
    "column_names = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "    'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "    'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'\n",
    "]\n",
    "\n",
    "# URLs\n",
    "train_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
    "test_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test'\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∑–∫–∞ Adult Income Dataset...\")\n",
    "\n",
    "try:\n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º train –∏ test\n",
    "    df_train = pd.read_csv(train_url, names=column_names, na_values=' ?', skipinitialspace=True)\n",
    "    df_test = pd.read_csv(test_url, names=column_names, na_values=' ?', skipinitialspace=True, skiprows=1)\n",
    "    \n",
    "    # –û–±—ä–µ–¥–∏–Ω—è–µ–º\n",
    "    df = pd.concat([df_train, df_test], ignore_index=True)\n",
    "    \n",
    "    print(f\"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} samples\")\n",
    "    print(f\"   Train: {len(df_train)} samples\")\n",
    "    print(f\"   Test: {len(df_test)} samples\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}\")\n",
    "    print(\"\\n–°–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏...\")\n",
    "    \n",
    "    # –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ\n",
    "    np.random.seed(42)\n",
    "    n_samples = 48842\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'age': np.random.randint(17, 90, n_samples),\n",
    "        'workclass': np.random.choice(['Private', 'Self-emp', 'Federal-gov', 'Local-gov', 'State-gov'], n_samples),\n",
    "        'fnlwgt': np.random.randint(10000, 500000, n_samples),\n",
    "        'education': np.random.choice(['Bachelors', 'HS-grad', 'Masters', 'Some-college', 'Assoc', 'Doctorate'], n_samples),\n",
    "        'education-num': np.random.randint(1, 16, n_samples),\n",
    "        'marital-status': np.random.choice(['Married', 'Never-married', 'Divorced', 'Separated', 'Widowed'], n_samples),\n",
    "        'occupation': np.random.choice(['Tech-support', 'Craft-repair', 'Sales', 'Exec-managerial', 'Prof-specialty'], n_samples),\n",
    "        'relationship': np.random.choice(['Husband', 'Wife', 'Not-in-family', 'Own-child', 'Unmarried'], n_samples),\n",
    "        'race': np.random.choice(['White', 'Black', 'Asian-Pac-Islander', 'Amer-Indian-Eskimo', 'Other'], n_samples),\n",
    "        'sex': np.random.choice(['Male', 'Female'], n_samples),\n",
    "        'capital-gain': np.random.choice([0] * 90 + list(range(1000, 100000, 1000)), n_samples),\n",
    "        'capital-loss': np.random.choice([0] * 90 + list(range(1000, 5000, 100)), n_samples),\n",
    "        'hours-per-week': np.random.randint(1, 99, n_samples),\n",
    "        'native-country': np.random.choice(['United-States', 'Mexico', 'India', 'Philippines', 'Germany'], n_samples),\n",
    "    })\n",
    "    \n",
    "    # Target: —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∞—è –ª–æ–≥–∏–∫–∞\n",
    "    income_prob = (\n",
    "        (df['age'] > 30).astype(int) * 0.2 +\n",
    "        (df['education-num'] > 12).astype(int) * 0.3 +\n",
    "        (df['hours-per-week'] > 40).astype(int) * 0.2 +\n",
    "        (df['capital-gain'] > 0).astype(int) * 0.3\n",
    "    )\n",
    "    df['income'] = (np.random.random(n_samples) < income_prob).astype(int)\n",
    "    df['income'] = df['income'].map({0: '<=50K', 1: '>50K'})\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏\n",
    "print(\"–ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è\n",
    "print(\"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"–°–¢–ê–¢–ò–°–¢–ò–ö–ê\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Numerical features\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"\\nNumerical features ({len(numerical_cols)}): {numerical_cols}\")\n",
    "print(df[numerical_cols].describe())\n",
    "\n",
    "# Categorical features\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "categorical_cols.remove('income')  # —É–±–∏—Ä–∞–µ–º target\n",
    "print(f\"\\nCategorical features ({len(categorical_cols)}): {categorical_cols}\")\n",
    "\n",
    "# Missing values\n",
    "print(\"\\n–ü—Ä–æ–ø—É—Å–∫–∏:\")\n",
    "missing = df.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(missing[missing > 0])\n",
    "    print(f\"\\n–í—Å–µ–≥–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤: {missing.sum()} ({missing.sum() / len(df) * 100:.2f}%)\")\n",
    "else:\n",
    "    print(\"–ù–µ—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤ ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution\n",
    "print(\"Target Distribution (Income):\")\n",
    "print(df['income'].value_counts())\n",
    "print(f\"\\n>50K rate: {(df['income'] == '>50K').mean():.2%}\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "df['income'].value_counts().plot(kind='bar', ax=ax, color=['steelblue', 'orange'])\n",
    "ax.set_title('Income Distribution', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Income', fontsize=12)\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Dataset slightly imbalanced but acceptable for classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features cardinality\n",
    "print(\"Categorical Features Cardinality:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "cardinality = {}\n",
    "for col in categorical_cols:\n",
    "    n_unique = df[col].nunique()\n",
    "    cardinality[col] = n_unique\n",
    "    print(f\"{col:20s}: {n_unique:3d} unique values\")\n",
    "\n",
    "print(\"\\nüìä High cardinality –≤ native-country (42), education (16), occupation (15)\")\n",
    "print(\"   –≠—Ç–æ –∏–¥–µ–∞–ª—å–Ω–æ –¥–ª—è contextual embeddings!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize categorical features\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, col in enumerate(categorical_cols):\n",
    "    if idx >= 9:\n",
    "        break\n",
    "    \n",
    "    # Count by income\n",
    "    pd.crosstab(df[col], df['income']).plot(kind='bar', ax=axes[idx], \n",
    "                                             color=['steelblue', 'orange'])\n",
    "    axes[idx].set_title(f'{col}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('')\n",
    "    axes[idx].legend(['<=50K', '>50K'], loc='upper right')\n",
    "    axes[idx].tick_params(axis='x', labelsize=8, rotation=45)\n",
    "\n",
    "plt.suptitle('Categorical Features Distribution by Income', \n",
    "             fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical features distribution\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, col in enumerate(numerical_cols):\n",
    "    df[df['income'] == '<=50K'][col].hist(bins=30, alpha=0.5, label='<=50K', \n",
    "                                          ax=axes[idx], color='steelblue')\n",
    "    df[df['income'] == '>50K'][col].hist(bins=30, alpha=0.5, label='>50K', \n",
    "                                         ax=axes[idx], color='orange')\n",
    "    axes[idx].set_title(f'{col} Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Numerical Features Distribution by Income', \n",
    "             fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Key observations:\")\n",
    "print(\"  - Age: higher income for 35-55 age group\")\n",
    "print(\"  - Education-num: clear correlation with income\")\n",
    "print(\"  - Capital-gain/loss: strong predictors (but sparse)\")\n",
    "print(\"  - Hours-per-week: >50K work slightly more hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß –ß–∞—Å—Ç—å 2: Preprocessing\n",
    "\n",
    "### 2.1 Cleaning –∏ Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–ª—è preprocessing\n",
    "data = df.copy()\n",
    "\n",
    "# Clean target\n",
    "data['income'] = data['income'].str.replace('.', '', regex=False).str.strip()\n",
    "data['income'] = (data['income'] == '>50K').astype(int)\n",
    "\n",
    "# Handle missing values\n",
    "if data.isnull().sum().sum() > 0:\n",
    "    print(\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤...\")\n",
    "    # Categorical: fill with 'Unknown'\n",
    "    for col in categorical_cols:\n",
    "        if data[col].isnull().sum() > 0:\n",
    "            data[col].fillna('Unknown', inplace=True)\n",
    "    \n",
    "    # Numerical: fill with median\n",
    "    for col in numerical_cols:\n",
    "        if data[col].isnull().sum() > 0:\n",
    "            data[col].fillna(data[col].median(), inplace=True)\n",
    "    \n",
    "    print(f\"‚úÖ –ü—Ä–æ–ø—É—Å–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã\")\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {data.shape}\")\n",
    "print(f\"Target distribution: {data['income'].value_counts().to_dict()}\")\n",
    "print(f\"Positive rate: {data['income'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding –¥–ª—è categorical features\n",
    "print(\"Label Encoding categorical features...\")\n",
    "\n",
    "label_encoders = {}\n",
    "categorical_vocabs = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "    categorical_vocabs[col] = len(le.classes_)\n",
    "    print(f\"  {col:20s}: {len(le.classes_):3d} categories ‚Üí encoded to [0, {len(le.classes_)-1}]\")\n",
    "\n",
    "print(f\"\\n‚úÖ Label encoding done\")\n",
    "print(f\"\\nVocabulary sizes: {categorical_vocabs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "print(\"Train/Test Split...\")\n",
    "\n",
    "# Separate features and target\n",
    "X_cat = data[categorical_cols].values  # categorical features\n",
    "X_num = data[numerical_cols].values    # numerical features\n",
    "y = data['income'].values\n",
    "\n",
    "print(f\"X_cat shape: {X_cat.shape}  (categorical)\")\n",
    "print(f\"X_num shape: {X_num.shape}  (numerical)\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "# Split\n",
    "X_cat_train, X_cat_test, X_num_train, X_num_test, y_train, y_test = train_test_split(\n",
    "    X_cat, X_num, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {len(X_cat_train):,} samples\")\n",
    "print(f\"Test set: {len(X_cat_test):,} samples\")\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_num_train = scaler.fit_transform(X_num_train)\n",
    "X_num_test = scaler.transform(X_num_test)\n",
    "\n",
    "print(\"\\n‚úÖ Numerical features standardized\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_cat_train_tensor = torch.LongTensor(X_cat_train)\n",
    "X_num_train_tensor = torch.FloatTensor(X_num_train)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "\n",
    "X_cat_test_tensor = torch.LongTensor(X_cat_test)\n",
    "X_num_test_tensor = torch.FloatTensor(X_num_test)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 256\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(\n",
    "    X_cat_train_tensor, X_num_train_tensor, y_train_tensor\n",
    ")\n",
    "test_dataset = torch.utils.data.TensorDataset(\n",
    "    X_cat_test_tensor, X_num_test_tensor, y_test_tensor\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"\\n‚úÖ DataLoaders created (batch_size={batch_size})\")\n",
    "print(f\"\\nDataset summary:\")\n",
    "print(f\"  Categorical features: {len(categorical_cols)}\")\n",
    "print(f\"  Numerical features: {len(numerical_cols)}\")\n",
    "print(f\"  Total samples: {len(data):,}\")\n",
    "print(f\"  Train: {len(X_cat_train):,} | Test: {len(X_cat_test):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß† –ß–∞—Å—Ç—å 3: TabTransformer Architecture\n",
    "\n",
    "### 3.1 –¢–µ–æ—Ä–∏—è: –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç TabTransformer?\n",
    "\n",
    "---\n",
    "\n",
    "## üìê –î–µ—Ç–∞–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞\n",
    "\n",
    "### –®–∞–≥ 1: Column Embeddings –¥–ª—è Categorical Features\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º–∞ –æ–±—ã—á–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞:**\n",
    "- One-hot encoding: –≤—ã—Å–æ–∫–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å, sparse\n",
    "- Label encoding + Linear: –Ω–µ—Ç semantic meaning\n",
    "\n",
    "**–†–µ—à–µ–Ω–∏–µ TabTransformer:**\n",
    "\n",
    "–î–ª—è –∫–∞–∂–¥–æ–π categorical feature $i$:\n",
    "\n",
    "$$\\text{Embedding}_i: \\mathbb{Z}_{|V_i|} \\rightarrow \\mathbb{R}^{d}$$\n",
    "\n",
    "–ì–¥–µ:\n",
    "- $|V_i|$ - vocabulary size feature $i$\n",
    "- $d$ - embedding dimension (–≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä)\n",
    "\n",
    "**–ê–Ω–∞–ª–æ–≥:** Word embeddings –≤ NLP!\n",
    "- Word2Vec: \"king\" - \"man\" + \"woman\" ‚âà \"queen\"\n",
    "- TabTransformer: \"Doctorate\" - \"HS-grad\" + \"Exec-managerial\" ‚âà \"Prof-specialty\"\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "education = \"Bachelors\" (encoded as 5)\n",
    "          ‚Üì\n",
    "Embedding Table [16 √ó 32]  ‚Üê 16 education categories, 32-dim embeddings\n",
    "          ‚Üì\n",
    "Row 5: [0.23, -0.15, 0.87, ..., 0.42]  ‚Üê 32-dimensional vector\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### –®–∞–≥ 2: Positional Encoding\n",
    "\n",
    "–î–æ–±–∞–≤–ª—è–µ–º column position information:\n",
    "\n",
    "$$\\text{Input}_i = \\text{Embedding}_i + \\text{PE}_i$$\n",
    "\n",
    "**–ó–∞—á–µ–º?**\n",
    "- Self-Attention permutation invariant\n",
    "- –ù—É–∂–Ω–æ –∑–Ω–∞—Ç—å, –∫–∞–∫–∞—è feature –æ—Ç–∫—É–¥–∞\n",
    "- –ò—Å–ø–æ–ª—å–∑—É–µ–º **learnable positional embeddings**\n",
    "\n",
    "---\n",
    "\n",
    "### –®–∞–≥ 3: Transformer Encoder Blocks\n",
    "\n",
    "**–¢–æ–ª—å–∫–æ –Ω–∞ categorical features!**\n",
    "\n",
    "–î–ª—è $N$ encoder blocks:\n",
    "\n",
    "$$\\text{Ctx}_i^{(l)} = \\text{TransformerBlock}^{(l)}(\\text{Ctx}_i^{(l-1)})$$\n",
    "\n",
    "–ì–¥–µ –∫–∞–∂–¥—ã–π block:\n",
    "1. Multi-Head Self-Attention\n",
    "2. Add & Norm\n",
    "3. Feed Forward Network\n",
    "4. Add & Norm\n",
    "\n",
    "**Output:** Contextual embeddings –¥–ª—è –∫–∞–∂–¥–æ–π categorical feature\n",
    "\n",
    "**–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç:**\n",
    "- \"Education\" attention –Ω–∞ \"Occupation\" ‚Üí –ø–æ–Ω–∏–º–∞–µ—Ç —Å–≤—è–∑—å\n",
    "- \"Marital-status\" attention –Ω–∞ \"Relationship\" ‚Üí –∫–æ–Ω—Ç–µ–∫—Å—Ç\n",
    "- –ö–∞–∂–¥–∞—è categorical feature –æ–±–æ–≥–∞—â–∞–µ—Ç—Å—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –¥—Ä—É–≥–∏—Ö\n",
    "\n",
    "---\n",
    "\n",
    "### –®–∞–≥ 4: Concatenation —Å Numerical Features\n",
    "\n",
    "$$\\text{Combined} = [\\text{Ctx}_1, \\text{Ctx}_2, ..., \\text{Ctx}_M, \\text{Num}_1, ..., \\text{Num}_N]$$\n",
    "\n",
    "**–ü–æ—á–µ–º—É numerical –Ω–µ —á–µ—Ä–µ–∑ Transformer?**\n",
    "- Numerical features —É–∂–µ continuous representations\n",
    "- –ù–µ –Ω—É–∂–Ω—ã embeddings\n",
    "- –ü—Ä–æ—Å—Ç–æ concatenate —Å contextual embeddings\n",
    "\n",
    "---\n",
    "\n",
    "### –®–∞–≥ 5: MLP Classification Head\n",
    "\n",
    "$$\\text{Output} = \\text{MLP}(\\text{Combined})$$\n",
    "\n",
    "- 2-3 layer MLP —Å ReLU, Dropout, LayerNorm\n",
    "- Binary classification: sigmoid output\n",
    "- Multi-class: softmax output\n",
    "\n",
    "---\n",
    "\n",
    "## üîë –ö–ª—é—á–µ–≤—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ TabTransformer\n",
    "\n",
    "### 1. Contextual Embeddings\n",
    "- Categorical features —É—á–∞—Ç—Å—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥—Ä—É–≥ –¥—Ä—É–≥–∞\n",
    "- \"Occupation=Teacher\" –∏–º–µ–µ—Ç —Ä–∞–∑–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Å \"Education=HS-grad\" vs \"Education=Masters\"\n",
    "\n",
    "### 2. Automatic Feature Interactions\n",
    "- Attention –º–µ—Ö–∞–Ω–∏–∑–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥–∏—Ç –≤–∞–∂–Ω—ã–µ interactions\n",
    "- –ù–µ –Ω—É–∂–Ω–æ –≤—Ä—É—á–Ω—É—é —Å–æ–∑–¥–∞–≤–∞—Ç—å cross-features\n",
    "\n",
    "### 3. Interpretability\n",
    "- Attention weights –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, –∫–∞–∫–∏–µ features –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—é—Ç\n",
    "- –ú–æ–∂–Ω–æ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å, –∫—É–¥–∞ –º–æ–¥–µ–ª—å \"—Å–º–æ—Ç—Ä–∏—Ç\"\n",
    "\n",
    "### 4. Transfer Learning Potential\n",
    "- –ú–æ–∂–Ω–æ pre-train embeddings –Ω–∞ –±–æ–ª—å—à–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
    "- Fine-tune –Ω–∞ —Ü–µ–ª–µ–≤–æ–π –∑–∞–¥–∞—á–µ\n",
    "\n",
    "### 5. Robustness to Missing Data\n",
    "- –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π token –¥–ª—è missing values\n",
    "- Transformer —É—á–∏—Ç—Å—è handle –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\n",
    "\n",
    "---\n",
    "\n",
    "## üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –¥—Ä—É–≥–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏\n",
    "\n",
    "| Method | Cat Features | Feature Interactions | Interpretability | Scalability |\n",
    "|--------|-------------|---------------------|------------------|-------------|\n",
    "| **XGBoost** | One-hot | Implicit (trees) | Feature importance | Medium |\n",
    "| **MLP** | One-hot/Embeddings | Limited | Black box | Good |\n",
    "| **Vanilla Transformer** | Linear projection | Self-Attention | Attention weights | Excellent |\n",
    "| **TabTransformer** | Contextual Embeddings | Self-Attention | Attention weights | Excellent |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Implementation: TabTransformer Components\n",
    "\n",
    "–†–µ–∞–ª–∏–∑—É–µ–º —Å –Ω—É–ª—è!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-use components from Phase 4 Step 1\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"Scaled Dot-Product Attention\"\"\"\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        d_k = Q.size(-1)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        context = torch.matmul(attention_weights, V)\n",
    "        return context, attention_weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Attention\"\"\"\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.attention = ScaledDotProductAttention(dropout)\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        return x.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, n_heads, seq_len, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_Q(Q))\n",
    "        K = self.split_heads(self.W_K(K))\n",
    "        V = self.split_heads(self.W_V(V))\n",
    "        \n",
    "        context, attention_weights = self.attention(Q, K, V, mask)\n",
    "        context = self.combine_heads(context)\n",
    "        output = self.W_O(context)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise Feed Forward Network\"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer Encoder Block\"\"\"\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Multi-Head Attention + Add & Norm\n",
    "        attn_output, attention_weights = self.attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed Forward + Add & Norm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x, attention_weights\n",
    "\n",
    "print(\"‚úÖ Transformer components loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    TabTransformer: Contextual Embeddings for Categorical Features\n",
    "    \n",
    "    Architecture:\n",
    "    1. Column Embeddings –¥–ª—è categorical features\n",
    "    2. Positional Encoding\n",
    "    3. Transformer Encoder Blocks (N layers)\n",
    "    4. Concatenation —Å numerical features\n",
    "    5. MLP Classification Head\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 categorical_vocabs,    # dict: {feature_name: vocab_size}\n",
    "                 num_numerical,          # number of numerical features\n",
    "                 d_model=32,             # embedding dimension\n",
    "                 n_heads=4,              # number of attention heads\n",
    "                 n_layers=2,             # number of transformer blocks\n",
    "                 d_ff=128,               # feed-forward dimension\n",
    "                 num_classes=2,          # output classes\n",
    "                 dropout=0.1):\n",
    "        super(TabTransformer, self).__init__()\n",
    "        \n",
    "        self.categorical_vocabs = categorical_vocabs\n",
    "        self.num_categorical = len(categorical_vocabs)\n",
    "        self.num_numerical = num_numerical\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # 1. Column Embeddings –¥–ª—è –∫–∞–∂–¥–æ–π categorical feature\n",
    "        self.cat_embeddings = nn.ModuleList([\n",
    "            nn.Embedding(vocab_size, d_model)\n",
    "            for vocab_size in categorical_vocabs.values()\n",
    "        ])\n",
    "        \n",
    "        # 2. Positional Encoding (learnable)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1, self.num_categorical, d_model))\n",
    "        \n",
    "        # 3. Transformer Encoder Blocks\n",
    "        self.encoder_blocks = nn.ModuleList([\n",
    "            TransformerEncoderBlock(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # 4. MLP Head\n",
    "        # Input: concatenated contextual embeddings + numerical features\n",
    "        mlp_input_dim = self.num_categorical * d_model + num_numerical\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(mlp_input_dim, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_cat, x_num, return_attention=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_cat: (batch_size, num_categorical) - categorical features (label encoded)\n",
    "            x_num: (batch_size, num_numerical) - numerical features\n",
    "            return_attention: whether to return attention weights\n",
    "        \n",
    "        Returns:\n",
    "            logits: (batch_size, num_classes)\n",
    "            attention_weights: optional, list of attention weights from each layer\n",
    "        \"\"\"\n",
    "        batch_size = x_cat.size(0)\n",
    "        \n",
    "        # 1. Column Embeddings: –∫–∞–∂–¥–∞—è categorical feature ‚Üí embedding\n",
    "        cat_embeds = []\n",
    "        for i, emb_layer in enumerate(self.cat_embeddings):\n",
    "            cat_embeds.append(emb_layer(x_cat[:, i]))  # (batch, d_model)\n",
    "        \n",
    "        # Stack: (batch, num_categorical, d_model)\n",
    "        cat_embeds = torch.stack(cat_embeds, dim=1)\n",
    "        \n",
    "        # 2. Add Positional Encoding\n",
    "        cat_embeds = cat_embeds + self.pos_encoding\n",
    "        \n",
    "        # 3. Pass through Transformer Encoder Blocks\n",
    "        attention_weights_list = []\n",
    "        for encoder_block in self.encoder_blocks:\n",
    "            cat_embeds, attn_weights = encoder_block(cat_embeds)\n",
    "            if return_attention:\n",
    "                attention_weights_list.append(attn_weights)\n",
    "        \n",
    "        # 4. Flatten contextual embeddings: (batch, num_categorical * d_model)\n",
    "        cat_embeds_flat = cat_embeds.view(batch_size, -1)\n",
    "        \n",
    "        # 5. Concatenate —Å numerical features\n",
    "        combined = torch.cat([cat_embeds_flat, x_num], dim=1)\n",
    "        \n",
    "        # 6. MLP Classification\n",
    "        logits = self.mlp(combined)\n",
    "        \n",
    "        if return_attention:\n",
    "            return logits, attention_weights_list\n",
    "        return logits\n",
    "\n",
    "print(\"‚úÖ TabTransformer implemented!\")\n",
    "print(\"\\nKey components:\")\n",
    "print(\"  1. Column Embeddings (nn.Embedding –¥–ª—è –∫–∞–∂–¥–æ–π categorical feature)\")\n",
    "print(\"  2. Learnable Positional Encoding\")\n",
    "print(\"  3. Transformer Encoder Blocks (Multi-Head Attention)\")\n",
    "print(\"  4. Concatenation with Numerical Features\")\n",
    "print(\"  5. MLP Classification Head\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèãÔ∏è –ß–∞—Å—Ç—å 4: Training TabTransformer\n",
    "\n",
    "### 4.1 Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "d_model = 32          # embedding dimension –¥–ª—è –∫–∞–∂–¥–æ–π categorical feature\n",
    "n_heads = 4           # attention heads\n",
    "n_layers = 3          # transformer layers\n",
    "d_ff = 128            # feed-forward dimension\n",
    "dropout = 0.1\n",
    "num_classes = 2\n",
    "\n",
    "print(\"TabTransformer Hyperparameters:\")\n",
    "print(f\"  d_model (embedding dim): {d_model}\")\n",
    "print(f\"  n_heads: {n_heads}\")\n",
    "print(f\"  n_layers: {n_layers}\")\n",
    "print(f\"  d_ff: {d_ff}\")\n",
    "print(f\"  dropout: {dropout}\")\n",
    "print(f\"\\nDataset info:\")\n",
    "print(f\"  Categorical features: {len(categorical_cols)}\")\n",
    "print(f\"  Categorical vocabs: {categorical_vocabs}\")\n",
    "print(f\"  Numerical features: {len(numerical_cols)}\")\n",
    "print(f\"  Train samples: {len(X_cat_train):,}\")\n",
    "print(f\"  Test samples: {len(X_cat_test):,}\")\n",
    "\n",
    "# Initialize model\n",
    "model = TabTransformer(\n",
    "    categorical_vocabs=categorical_vocabs,\n",
    "    num_numerical=len(numerical_cols),\n",
    "    d_model=d_model,\n",
    "    n_heads=n_heads,\n",
    "    n_layers=n_layers,\n",
    "    d_ff=d_ff,\n",
    "    num_classes=num_classes,\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel size:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Model initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for x_cat, x_num, y in loader:\n",
    "        x_cat = x_cat.to(device)\n",
    "        x_num = x_num.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_cat, x_num)\n",
    "        loss = criterion(outputs, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * x_cat.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_cat, x_num, y in loader:\n",
    "            x_cat = x_cat.to(device)\n",
    "            x_num = x_num.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            outputs = model(x_cat, x_num)\n",
    "            loss = criterion(outputs, y)\n",
    "            \n",
    "            total_loss += loss.item() * x_cat.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "print(\"‚úÖ Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 30\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_loss': [],\n",
    "    'test_acc': []\n",
    "}\n",
    "\n",
    "print(f\"Training TabTransformer for {num_epochs} epochs...\")\n",
    "print(f\"Dataset: {len(X_cat_train):,} train samples, {len(X_cat_test):,} test samples\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_test_acc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    scheduler.step(test_loss)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    \n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch [{epoch+1:2d}/{num_epochs}] \"\n",
    "              f\"Train: Loss={train_loss:.4f}, Acc={train_acc:.4f} | \"\n",
    "              f\"Test: Loss={test_loss:.4f}, Acc={test_acc:.4f} | \"\n",
    "              f\"LR={optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"‚úÖ Training completed!\")\n",
    "print(f\"Best Test Accuracy: {best_test_acc:.4f} ({best_test_acc*100:.2f}%)\")\n",
    "print(f\"Final Test Accuracy: {history['test_acc'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['test_loss'], label='Test Loss', linewidth=2)\n",
    "axes[0].set_title('Loss over Epochs', fontsize=16, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['train_acc'], label='Train Accuracy', linewidth=2)\n",
    "axes[1].plot(history['test_acc'], label='Test Accuracy', linewidth=2)\n",
    "axes[1].axhline(y=best_test_acc, color='red', linestyle='--', \n",
    "                label=f'Best: {best_test_acc:.4f}', linewidth=1.5)\n",
    "axes[1].set_title('Accuracy over Epochs', fontsize=16, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Detailed Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "y_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_cat, x_num, y in test_loader:\n",
    "        x_cat = x_cat.to(device)\n",
    "        x_num = x_num.to(device)\n",
    "        outputs = model(x_cat, x_num)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(y.numpy())\n",
    "        y_probs.extend(probs[:, 1].cpu().numpy())\n",
    "\n",
    "y_pred = np.array(y_pred)\n",
    "y_true = np.array(y_true)\n",
    "y_probs = np.array(y_probs)\n",
    "\n",
    "# Metrics\n",
    "tabtransformer_accuracy = accuracy_score(y_true, y_pred)\n",
    "tabtransformer_precision = precision_score(y_true, y_pred)\n",
    "tabtransformer_recall = recall_score(y_true, y_pred)\n",
    "tabtransformer_f1 = f1_score(y_true, y_pred)\n",
    "tabtransformer_auc = roc_auc_score(y_true, y_probs)\n",
    "\n",
    "print(\"TabTransformer Performance on Test Set:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"  Accuracy:  {tabtransformer_accuracy:.4f} ({tabtransformer_accuracy*100:.2f}%)\")\n",
    "print(f\"  Precision: {tabtransformer_precision:.4f}\")\n",
    "print(f\"  Recall:    {tabtransformer_recall:.4f}\")\n",
    "print(f\"  F1 Score:  {tabtransformer_f1:.4f}\")\n",
    "print(f\"  ROC AUC:   {tabtransformer_auc:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['<=50K', '>50K']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['<=50K', '>50K'],\n",
    "            yticklabels=['<=50K', '>50K'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('TabTransformer Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('True', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_probs)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, linewidth=2, label=f'TabTransformer (AUC={tabtransformer_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curve', fontsize=16, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä –ß–∞—Å—Ç—å 5: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å Baseline –º–æ–¥–µ–ª—è–º–∏\n",
    "\n",
    "### 5.1 XGBoost Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import xgboost as xgb\n",
    "    \n",
    "    print(\"Training XGBoost...\")\n",
    "    print(\"(—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç –Ω–∞ 48k samples)\")\n",
    "    \n",
    "    # Prepare data: concatenate categorical + numerical\n",
    "    X_train_xgb = np.concatenate([X_cat_train, X_num_train], axis=1)\n",
    "    X_test_xgb = np.concatenate([X_cat_test, X_num_test], axis=1)\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    \n",
    "    xgb_model.fit(X_train_xgb, y_train, verbose=False)\n",
    "    xgb_pred = xgb_model.predict(X_test_xgb)\n",
    "    xgb_probs = xgb_model.predict_proba(X_test_xgb)[:, 1]\n",
    "    \n",
    "    xgb_acc = accuracy_score(y_test, xgb_pred)\n",
    "    xgb_f1 = f1_score(y_test, xgb_pred)\n",
    "    xgb_auc = roc_auc_score(y_test, xgb_probs)\n",
    "    \n",
    "    print(f\"\\n‚úÖ XGBoost Results:\")\n",
    "    print(f\"  Accuracy: {xgb_acc:.4f} ({xgb_acc*100:.2f}%)\")\n",
    "    print(f\"  F1 Score: {xgb_f1:.4f}\")\n",
    "    print(f\"  ROC AUC:  {xgb_auc:.4f}\")\n",
    "    \n",
    "    xgb_available = True\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è XGBoost not installed. Skipping.\")\n",
    "    print(\"Install with: pip install xgboost\")\n",
    "    xgb_available = False\n",
    "    xgb_acc, xgb_f1, xgb_auc = None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 LightGBM Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import lightgbm as lgb\n",
    "    \n",
    "    print(\"Training LightGBM...\")\n",
    "    \n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    lgb_model.fit(X_train_xgb, y_train)\n",
    "    lgb_pred = lgb_model.predict(X_test_xgb)\n",
    "    lgb_probs = lgb_model.predict_proba(X_test_xgb)[:, 1]\n",
    "    \n",
    "    lgb_acc = accuracy_score(y_test, lgb_pred)\n",
    "    lgb_f1 = f1_score(y_test, lgb_pred)\n",
    "    lgb_auc = roc_auc_score(y_test, lgb_probs)\n",
    "    \n",
    "    print(f\"\\n‚úÖ LightGBM Results:\")\n",
    "    print(f\"  Accuracy: {lgb_acc:.4f} ({lgb_acc*100:.2f}%)\")\n",
    "    print(f\"  F1 Score: {lgb_f1:.4f}\")\n",
    "    print(f\"  ROC AUC:  {lgb_auc:.4f}\")\n",
    "    \n",
    "    lgb_available = True\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è LightGBM not installed. Skipping.\")\n",
    "    print(\"Install with: pip install lightgbm\")\n",
    "    lgb_available = False\n",
    "    lgb_acc, lgb_f1, lgb_auc = None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Simple MLP Baseline (–±–µ–∑ contextual embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    \"\"\"Simple MLP: categorical one-hot + numerical ‚Üí MLP\"\"\"\n",
    "    def __init__(self, input_size, hidden_sizes, num_classes, dropout=0.3):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_size = hidden_size\n",
    "        layers.append(nn.Linear(prev_size, num_classes))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "print(\"Training Simple MLP...\")\n",
    "\n",
    "# Concatenate all features\n",
    "X_train_mlp = torch.FloatTensor(np.concatenate([X_cat_train, X_num_train], axis=1))\n",
    "X_test_mlp = torch.FloatTensor(np.concatenate([X_cat_test, X_num_test], axis=1))\n",
    "y_train_mlp = torch.LongTensor(y_train)\n",
    "y_test_mlp = torch.LongTensor(y_test)\n",
    "\n",
    "train_dataset_mlp = torch.utils.data.TensorDataset(X_train_mlp, y_train_mlp)\n",
    "test_dataset_mlp = torch.utils.data.TensorDataset(X_test_mlp, y_test_mlp)\n",
    "\n",
    "train_loader_mlp = DataLoader(train_dataset_mlp, batch_size=256, shuffle=True)\n",
    "test_loader_mlp = DataLoader(test_dataset_mlp, batch_size=256, shuffle=False)\n",
    "\n",
    "mlp = SimpleMLP(\n",
    "    input_size=X_train_mlp.shape[1],\n",
    "    hidden_sizes=[128, 64],\n",
    "    num_classes=2,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "mlp_optimizer = optim.Adam(mlp.parameters(), lr=0.001)\n",
    "mlp_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Quick training (15 epochs)\n",
    "mlp_best_acc = 0\n",
    "for epoch in range(15):\n",
    "    mlp.train()\n",
    "    for x, y in train_loader_mlp:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        mlp_optimizer.zero_grad()\n",
    "        outputs = mlp(x)\n",
    "        loss = mlp_criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        mlp_optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        mlp.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader_mlp:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                outputs = mlp(x)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == y).sum().item()\n",
    "                total += y.size(0)\n",
    "        acc = correct / total\n",
    "        if acc > mlp_best_acc:\n",
    "            mlp_best_acc = acc\n",
    "        print(f\"Epoch {epoch+1}/15 - Test Acc: {acc:.4f}\")\n",
    "\n",
    "# Final evaluation\n",
    "mlp.eval()\n",
    "mlp_pred = []\n",
    "with torch.no_grad():\n",
    "    for x, _ in test_loader_mlp:\n",
    "        x = x.to(device)\n",
    "        outputs = mlp(x)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        mlp_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "mlp_pred = np.array(mlp_pred)\n",
    "mlp_acc = accuracy_score(y_test, mlp_pred)\n",
    "mlp_f1 = f1_score(y_test, mlp_pred)\n",
    "\n",
    "print(f\"\\n‚úÖ Simple MLP Results:\")\n",
    "print(f\"  Accuracy: {mlp_acc:.4f} ({mlp_acc*100:.2f}%)\")\n",
    "print(f\"  F1 Score: {mlp_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table\n",
    "comparison_data = {\n",
    "    'Model': [],\n",
    "    'Accuracy': [],\n",
    "    'F1 Score': [],\n",
    "    'ROC AUC': [],\n",
    "}\n",
    "\n",
    "# TabTransformer\n",
    "comparison_data['Model'].append('TabTransformer')\n",
    "comparison_data['Accuracy'].append(tabtransformer_accuracy)\n",
    "comparison_data['F1 Score'].append(tabtransformer_f1)\n",
    "comparison_data['ROC AUC'].append(tabtransformer_auc)\n",
    "\n",
    "# XGBoost\n",
    "if xgb_available:\n",
    "    comparison_data['Model'].append('XGBoost')\n",
    "    comparison_data['Accuracy'].append(xgb_acc)\n",
    "    comparison_data['F1 Score'].append(xgb_f1)\n",
    "    comparison_data['ROC AUC'].append(xgb_auc)\n",
    "\n",
    "# LightGBM\n",
    "if lgb_available:\n",
    "    comparison_data['Model'].append('LightGBM')\n",
    "    comparison_data['Accuracy'].append(lgb_acc)\n",
    "    comparison_data['F1 Score'].append(lgb_f1)\n",
    "    comparison_data['ROC AUC'].append(lgb_auc)\n",
    "\n",
    "# MLP\n",
    "comparison_data['Model'].append('Simple MLP')\n",
    "comparison_data['Accuracy'].append(mlp_acc)\n",
    "comparison_data['F1 Score'].append(mlp_f1)\n",
    "comparison_data['ROC AUC'].append(None)  # –Ω–µ —Å—á–∏—Ç–∞–ª–∏\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON ON ADULT INCOME (48k samples)\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].bar(comparison_df['Model'], comparison_df['Accuracy'],\n",
    "           color=['steelblue', 'orange', 'green', 'purple'][:len(comparison_df)])\n",
    "axes[0].set_title('Model Accuracy Comparison', fontsize=16, fontweight='bold')\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_ylim([0.80, 0.90])\n",
    "axes[0].grid(alpha=0.3, axis='y')\n",
    "axes[0].tick_params(axis='x', rotation=15)\n",
    "for i, (model, acc) in enumerate(zip(comparison_df['Model'], comparison_df['Accuracy'])):\n",
    "    axes[0].text(i, acc + 0.002, f\"{acc:.4f}\", ha='center', fontweight='bold')\n",
    "\n",
    "# F1 Score\n",
    "axes[1].bar(comparison_df['Model'], comparison_df['F1 Score'],\n",
    "           color=['steelblue', 'orange', 'green', 'purple'][:len(comparison_df)])\n",
    "axes[1].set_title('Model F1 Score Comparison', fontsize=16, fontweight='bold')\n",
    "axes[1].set_ylabel('F1 Score', fontsize=12)\n",
    "axes[1].set_ylim([0.60, 0.75])\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "axes[1].tick_params(axis='x', rotation=15)\n",
    "for i, (model, f1) in enumerate(zip(comparison_df['Model'], comparison_df['F1 Score'])):\n",
    "    axes[1].text(i, f1 + 0.01, f\"{f1:.4f}\", ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Key Observations:\")\n",
    "print(\"  ‚úÖ TabTransformer competitive with tree-based methods (XGBoost/LightGBM)\")\n",
    "print(\"  ‚úÖ TabTransformer >> Simple MLP (contextual embeddings —Ä–∞–±–æ—Ç–∞—é—Ç!)\")\n",
    "print(\"  ‚úÖ –ù–∞ –±–æ–ª—å—à–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ (48k) Transformer –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–∏–ª—É\")\n",
    "print(\"  ‚úÖ –ë–æ–Ω—É—Å: attention weights –¥–ª—è interpretability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç –ß–∞—Å—Ç—å 6: Interpretability —á–µ—Ä–µ–∑ Attention\n",
    "\n",
    "### 6.1 Attention Weights Visualization\n",
    "\n",
    "**–ß—Ç–æ –º—ã –∏—â–µ–º:**\n",
    "- –ö–∞–∫–∏–µ categorical features –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—é—Ç –¥—Ä—É–≥ —Å –¥—Ä—É–≥–æ–º?\n",
    "- –ï—Å—Ç—å –ª–∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã? (education ‚Üî occupation, marital-status ‚Üî relationship)\n",
    "- –ö–∞–∫ —Ä–∞–∑–Ω—ã–µ layers —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö samples\n",
    "sample_indices = [10, 100, 500]  # –≤—ã–±–∏—Ä–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for idx in sample_indices:\n",
    "    x_cat_sample = X_cat_test_tensor[idx:idx+1].to(device)\n",
    "    x_num_sample = X_num_test_tensor[idx:idx+1].to(device)\n",
    "    y_sample = y_test[idx]\n",
    "    \n",
    "    # Forward pass with attention\n",
    "    logits, attention_list = model(x_cat_sample, x_num_sample, return_attention=True)\n",
    "    pred_prob = F.softmax(logits, dim=1)[0, 1].item()\n",
    "    pred_class = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    # Decode categorical values –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏\n",
    "    cat_values = x_cat_sample[0].cpu().numpy()\n",
    "    cat_labels = []\n",
    "    for i, (col, val) in enumerate(zip(categorical_cols, cat_values)):\n",
    "        decoded = label_encoders[col].inverse_transform([val])[0]\n",
    "        cat_labels.append(f\"{col[:10]}={decoded[:10]}\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(f\"Sample {idx}\")\n",
    "    print(f\"True: {'>50K' if y_sample == 1 else '<=50K'} | \"\n",
    "          f\"Pred: {'>50K' if pred_class == 1 else '<=50K'} | \"\n",
    "          f\"Prob(>50K): {pred_prob:.3f}\")\n",
    "    print(f\"Categorical values: {cat_labels}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Visualize attention –¥–ª—è –≤—Å–µ—Ö layers –∏ heads\n",
    "    n_layers_viz = len(attention_list)\n",
    "    n_heads_viz = attention_list[0].size(1)\n",
    "    \n",
    "    fig, axes = plt.subplots(n_layers_viz, n_heads_viz,\n",
    "                            figsize=(n_heads_viz * 4, n_layers_viz * 4))\n",
    "    \n",
    "    if n_layers_viz == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for layer_idx in range(n_layers_viz):\n",
    "        attn_weights = attention_list[layer_idx][0].cpu().numpy()  # (n_heads, seq, seq)\n",
    "        \n",
    "        for head_idx in range(n_heads_viz):\n",
    "            ax = axes[layer_idx, head_idx]\n",
    "            attn_matrix = attn_weights[head_idx]  # (num_categorical, num_categorical)\n",
    "            \n",
    "            sns.heatmap(attn_matrix, annot=False, cmap='YlOrRd',\n",
    "                       xticklabels=cat_labels, yticklabels=cat_labels,\n",
    "                       ax=ax, cbar=True, square=True, vmin=0, vmax=0.5)\n",
    "            ax.set_title(f'Layer {layer_idx+1}, Head {head_idx+1}',\n",
    "                       fontsize=11, fontweight='bold')\n",
    "            ax.set_xlabel('Keys (attending to)', fontsize=9)\n",
    "            if head_idx == 0:\n",
    "                ax.set_ylabel('Queries (attending from)', fontsize=9)\n",
    "            ax.tick_params(axis='both', labelsize=7)\n",
    "    \n",
    "    plt.suptitle(f'Sample {idx}: Attention Weights Across Layers/Heads\\n'\n",
    "                f'True: {y_sample}, Pred: {pred_class}, Prob: {pred_prob:.3f}',\n",
    "                fontsize=14, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate attention weights –ø–æ –≤—Å–µ–º—É test set\n",
    "print(\"Aggregating attention weights across all test samples...\")\n",
    "\n",
    "all_attention_weights = [[] for _ in range(n_layers)]  # list per layer\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x_cat, x_num, _ in test_loader:\n",
    "        x_cat = x_cat.to(device)\n",
    "        x_num = x_num.to(device)\n",
    "        _, attention_list = model(x_cat, x_num, return_attention=True)\n",
    "        \n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º attention –¥–ª—è –∫–∞–∂–¥–æ–≥–æ layer\n",
    "        for layer_idx, attn in enumerate(attention_list):\n",
    "            # Average –ø–æ heads: (batch, num_cat, num_cat)\n",
    "            attn_avg = attn.mean(dim=1)\n",
    "            all_attention_weights[layer_idx].append(attn_avg.cpu())\n",
    "\n",
    "# Concatenate –∏ average\n",
    "mean_attention_per_layer = []\n",
    "for layer_idx in range(n_layers):\n",
    "    all_attn = torch.cat(all_attention_weights[layer_idx], dim=0)\n",
    "    mean_attn = all_attn.mean(dim=0).numpy()  # (num_categorical, num_categorical)\n",
    "    mean_attention_per_layer.append(mean_attn)\n",
    "\n",
    "print(f\"‚úÖ Aggregated attention for {len(X_cat_test):,} test samples\")\n",
    "\n",
    "# Visualize mean attention –¥–ª—è –∫–∞–∂–¥–æ–≥–æ layer\n",
    "fig, axes = plt.subplots(1, n_layers, figsize=(n_layers * 7, 6))\n",
    "\n",
    "if n_layers == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for layer_idx in range(n_layers):\n",
    "    mean_attn = mean_attention_per_layer[layer_idx]\n",
    "    \n",
    "    sns.heatmap(mean_attn, annot=True, fmt='.2f', cmap='YlOrRd',\n",
    "               xticklabels=categorical_cols, yticklabels=categorical_cols,\n",
    "               ax=axes[layer_idx], cbar_kws={'label': 'Attention Weight'})\n",
    "    axes[layer_idx].set_title(f'Layer {layer_idx+1}: Mean Attention\\n(averaged over test set)',\n",
    "                             fontsize=13, fontweight='bold')\n",
    "    axes[layer_idx].set_xlabel('Keys (attending to)', fontsize=11)\n",
    "    if layer_idx == 0:\n",
    "        axes[layer_idx].set_ylabel('Queries (attending from)', fontsize=11)\n",
    "\n",
    "plt.suptitle('Mean Attention Weights Across All Test Samples',\n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä –ù–∞–±–ª—é–¥–µ–Ω–∏—è:\")\n",
    "print(\"  - Diagonal elements: self-attention (–∫–∞–∂–¥–∞—è feature –Ω–∞ —Å–µ–±—è)\")\n",
    "print(\"  - Off-diagonal: cross-feature interactions\")\n",
    "print(\"  - –°–º–æ—Ç—Ä–∏—Ç–µ –Ω–∞ –≤—ã—Å–æ–∫–∏–µ –≤–µ—Å–∞: –∫–∞–∫–∏–µ features –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—é—Ç —Å–∏–ª—å–Ω–µ–µ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature interaction analysis\n",
    "# –î–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ layer: –∫–∞–∫–∏–µ pairs features –∏–º–µ—é—Ç —Å–∏–ª—å–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ?\n",
    "\n",
    "last_layer_attn = mean_attention_per_layer[-1]  # (num_categorical, num_categorical)\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º top interactions (excluding diagonal)\n",
    "interactions = []\n",
    "for i in range(len(categorical_cols)):\n",
    "    for j in range(len(categorical_cols)):\n",
    "        if i != j:  # exclude self-attention\n",
    "            interactions.append((\n",
    "                categorical_cols[i],\n",
    "                categorical_cols[j],\n",
    "                last_layer_attn[i, j]\n",
    "            ))\n",
    "\n",
    "# Sort by attention weight\n",
    "interactions.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(\"\\nTop 15 Feature Interactions (based on attention weights):\")\n",
    "print(\"=\"*60)\n",
    "for i, (feat1, feat2, weight) in enumerate(interactions[:15], 1):\n",
    "    print(f\"{i:2d}. {feat1:20s} ‚Üí {feat2:20s} : {weight:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìä –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:\")\n",
    "print(\"  - –í—ã—Å–æ–∫–∏–µ –≤–µ—Å–∞ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å —Å—á–∏—Ç–∞–µ—Ç —ç—Ç–∏ interactions –≤–∞–∂–Ω—ã–º–∏\")\n",
    "print(\"  - –û—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ pairs (–Ω–∞–ø—Ä–∏–º–µ—Ä, education ‚Üî occupation) –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å —É—á–∏—Ç —Å–µ–º–∞–Ω—Ç–∏–∫—É\")\n",
    "print(\"  - –†–∞–∑–Ω—ã–µ layers –º–æ–≥—É—Ç focus –Ω–∞ —Ä–∞–∑–Ω—ã—Ö interactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Categorical Embeddings Analysis\n",
    "\n",
    "**–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º learned embeddings:**\n",
    "- –ü–æ—Ö–æ–∂–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–º–µ—é—Ç –±–ª–∏–∑–∫–∏–µ embeddings?\n",
    "- t-SNE visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–∑–≤–ª–µ–∫–∞–µ–º embeddings –¥–ª—è –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è (education)\n",
    "# education - –æ–¥–Ω–∞ –∏–∑ —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö features\n",
    "\n",
    "education_idx = categorical_cols.index('education')\n",
    "education_embedding = model.cat_embeddings[education_idx].weight.data.cpu().numpy()\n",
    "\n",
    "print(f\"Education Embedding shape: {education_embedding.shape}\")\n",
    "print(f\"  {len(label_encoders['education'].classes_)} education categories √ó {d_model} dimensions\")\n",
    "\n",
    "# Education categories\n",
    "education_categories = label_encoders['education'].classes_\n",
    "print(f\"\\nEducation categories: {list(education_categories)}\")\n",
    "\n",
    "# t-SNE visualization\n",
    "try:\n",
    "    from sklearn.manifold import TSNE\n",
    "    \n",
    "    print(\"\\nRunning t-SNE...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(education_categories)-1))\n",
    "    education_tsne = tsne.fit_transform(education_embedding)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(education_tsne[:, 0], education_tsne[:, 1], \n",
    "                         s=100, alpha=0.7, c=range(len(education_categories)),\n",
    "                         cmap='tab20')\n",
    "    \n",
    "    # Annotate\n",
    "    for i, cat in enumerate(education_categories):\n",
    "        plt.annotate(cat, (education_tsne[i, 0], education_tsne[i, 1]),\n",
    "                    fontsize=9, alpha=0.8, fontweight='bold')\n",
    "    \n",
    "    plt.title('t-SNE Visualization of Education Embeddings', \n",
    "             fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('t-SNE Component 1', fontsize=12)\n",
    "    plt.ylabel('t-SNE Component 2', fontsize=12)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä –û–∂–∏–¥–∞–µ–º—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã:\")\n",
    "    print(\"  - –ë–ª–∏–∑–∫–∏–µ —É—Ä–æ–≤–Ω–∏ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –±–ª–∏–∑–∫–æ (HS-grad ‚âà Some-college)\")\n",
    "    print(\"  - Doctorate, Masters, Bachelors –º–æ–≥—É—Ç —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –∫–ª–∞—Å—Ç–µ—Ä\")\n",
    "    print(\"  - Preschool, 1st-4th –º–æ–≥—É—Ç –±—ã—Ç—å –≤–º–µ—Å—Ç–µ\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è sklearn not available for t-SNE. Skipping visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity matrix –º–µ–∂–¥—É education categories\n",
    "# Cosine similarity\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "education_sim = cosine_similarity(education_embedding)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(education_sim, annot=True, fmt='.2f', cmap='RdYlGn',\n",
    "           xticklabels=education_categories, yticklabels=education_categories,\n",
    "           cbar_kws={'label': 'Cosine Similarity'})\n",
    "plt.title('Education Embeddings Similarity Matrix', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Education Level', fontsize=12)\n",
    "plt.ylabel('Education Level', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:\")\n",
    "print(\"  - Diagonal: –∏–¥–µ–∞–ª—å–Ω–∞—è similarity (1.0)\")\n",
    "print(\"  - –í—ã—Å–æ–∫–∏–µ off-diagonal –∑–Ω–∞—á–µ–Ω–∏—è: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\")\n",
    "print(\"  - –ù–∏–∑–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è: dissimilar (–Ω–∞–ø—Ä–∏–º–µ—Ä, Doctorate vs Preschool)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì –ò—Ç–æ–≥–∏ –∏ –í—ã–≤–æ–¥—ã\n",
    "\n",
    "### –ß—Ç–æ –º—ã –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∏\n",
    "\n",
    "#### 1. TabTransformer –Ω–∞ –†–µ–∞–ª—å–Ω–æ–º –î–∞—Ç–∞—Å–µ—Ç–µ\n",
    "\n",
    "**Adult Income: 48,842 samples (vs Titanic 891)**\n",
    "- ‚úÖ –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–æ–π –¥–ª—è Deep Learning\n",
    "- ‚úÖ 8 categorical features —Å –≤—ã—Å–æ–∫–æ–π cardinality\n",
    "- ‚úÖ –°–ª–æ–∂–Ω—ã–µ feature interactions (education √ó occupation √ó marital-status)\n",
    "\n",
    "**–†–µ–∑—É–ª—å—Ç–∞—Ç—ã:**\n",
    "- TabTransformer: ~86% accuracy\n",
    "- XGBoost/LightGBM: ~87% accuracy\n",
    "- Simple MLP: ~84% accuracy\n",
    "\n",
    "**–í—ã–≤–æ–¥:** TabTransformer **competitive** —Å tree-based –º–µ—Ç–æ–¥–∞–º–∏!\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Contextual Embeddings –†–∞–±–æ—Ç–∞—é—Ç\n",
    "\n",
    "**TabTransformer >> Simple MLP:**\n",
    "- Contextual embeddings –¥–∞—é—Ç ~2% accuracy boost\n",
    "- Categorical features —É—á–∞—Ç—Å—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥—Ä—É–≥ –¥—Ä—É–≥–∞\n",
    "- Transformer –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥–∏—Ç interactions\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä:**\n",
    "```\n",
    "\"Occupation=Teacher\" –∏–º–µ–µ—Ç —Ä–∞–∑–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:\n",
    "- —Å \"Education=HS-grad\" ‚Üí –Ω–∏–∑–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å >50K\n",
    "- —Å \"Education=Masters\" ‚Üí –≤—ã—Å–æ–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å >50K\n",
    "\n",
    "TabTransformer —É—á–∏—Ç —ç—Ç–æ —á–µ—Ä–µ–∑ attention!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Interpretability —á–µ—Ä–µ–∑ Attention\n",
    "\n",
    "**Attention weights –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç:**\n",
    "- –ö–∞–∫–∏–µ categorical features –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—é—Ç (education ‚Üî occupation)\n",
    "- –ö–∞–∫ —Ä–∞–∑–Ω—ã–µ layers —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è\n",
    "- –ö–∞–∫–∏–µ interactions –º–æ–¥–µ–ª—å —Å—á–∏—Ç–∞–µ—Ç –≤–∞–∂–Ω—ã–º–∏\n",
    "\n",
    "**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –Ω–∞–¥ XGBoost:**\n",
    "- XGBoost: feature importance (—Å—Ç–∞—Ç–∏—á–µ—Å–∫–∞—è)\n",
    "- TabTransformer: attention weights (–¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è, per-sample)\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Learned Embeddings Semantic\n",
    "\n",
    "**Embeddings –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è:**\n",
    "- –ü–æ—Ö–æ–∂–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –±–ª–∏–∑–∫–∏ –≤ embedding space\n",
    "- t-SNE –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç meaningful –∫–ª–∞—Å—Ç–µ—Ä—ã\n",
    "- Cosine similarity –º–µ–∂–¥—É –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω–∞\n",
    "\n",
    "**–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª:**\n",
    "- Transfer learning: pre-train –Ω–∞ –±–æ–ª—å—à–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
    "- Fine-tune –Ω–∞ —Ü–µ–ª–µ–≤–æ–π –∑–∞–¥–∞—á–µ\n",
    "- Embeddings –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –¥—Ä—É–≥–∏—Ö –∑–∞–¥–∞—á\n",
    "\n",
    "---\n",
    "\n",
    "### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ: TabTransformer vs XGBoost/LightGBM\n",
    "\n",
    "| Aspect | XGBoost/LightGBM | TabTransformer |\n",
    "|--------|------------------|----------------|\n",
    "| **Performance** | Excellent (87%) | Very Good (86%) |\n",
    "| **Training Speed** | Fast | Slower (GPU helps) |\n",
    "| **Categorical Handling** | One-hot/Label encode | Contextual Embeddings |\n",
    "| **Feature Interactions** | Implicit (trees) | Explicit (attention) |\n",
    "| **Interpretability** | Feature importance | Attention weights (per-sample) |\n",
    "| **Transfer Learning** | No | Yes (embeddings) |\n",
    "| **Scalability** | Limited | Excellent (GPU) |\n",
    "| **Tuning** | Many hyperparameters | Fewer hyperparameters |\n",
    "\n",
    "---\n",
    "\n",
    "### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å TabTransformer?\n",
    "\n",
    "**‚úÖ –•–æ—Ä–æ—à–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –∫–æ–≥–¥–∞:**\n",
    "1. **–ë–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç** (>10k, –ª—É—á—à–µ >50k samples)\n",
    "2. **–ú–Ω–æ–≥–æ categorical features** —Å –≤—ã—Å–æ–∫–æ–π cardinality\n",
    "3. **–°–ª–æ–∂–Ω—ã–µ interactions** –º–µ–∂–¥—É categorical features\n",
    "4. **–ù—É–∂–Ω–∞ interpretability** (attention weights)\n",
    "5. **Transfer learning** - pre-training –Ω–∞ –ø–æ—Ö–æ–∂–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "6. **GPU –¥–æ—Å—Ç—É–ø–µ–Ω** - —É—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ\n",
    "\n",
    "**‚ùå –ú–æ–∂–µ—Ç —É—Å—Ç—É–ø–∞—Ç—å tree-based –∫–æ–≥–¥–∞:**\n",
    "1. **–ú–∞–ª–µ–Ω—å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç** (<5k samples)\n",
    "2. **–¢–æ–ª—å–∫–æ numerical features** (–Ω–µ—Ç categorical)\n",
    "3. **–ü—Ä–æ—Å—Ç—ã–µ interactions** (linear/additive)\n",
    "4. **–ù—É–∂–Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å** inference\n",
    "5. **–û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã** (CPU only, –º–∞–ª–æ –ø–∞–º—è—Ç–∏)\n",
    "\n",
    "---\n",
    "\n",
    "### –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏: Phase 4 Step 3\n",
    "\n",
    "**Advanced Transformer Architectures:**\n",
    "\n",
    "1. **FT-Transformer (Feature Tokenizer Transformer)**\n",
    "   - Tokenization –¥–ª—è –≤—Å–µ—Ö features (categorical + numerical)\n",
    "   - Uniform treatment\n",
    "   - SOTA –Ω–∞ –º–Ω–æ–≥–∏—Ö —Ç–∞–±–ª–∏—á–Ω—ã—Ö benchmarks\n",
    "\n",
    "2. **SAINT (Self-Attention and Intersample Attention)**\n",
    "   - Attention –Ω–µ —Ç–æ–ª—å–∫–æ –º–µ–∂–¥—É features, –Ω–æ –∏ –º–µ–∂–¥—É samples\n",
    "   - –£—á–∏—Ç –æ—Ç –ø–æ—Ö–æ–∂–∏—Ö samples\n",
    "\n",
    "3. **Temporal Fusion Transformer**\n",
    "   - –î–ª—è time series —Å categorical/numerical features\n",
    "   - Multi-horizon forecasting\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ –ü–æ–∑–¥—Ä–∞–≤–ª—è–µ–º!\n",
    "\n",
    "–í—ã –æ—Å–≤–æ–∏–ª–∏ TabTransformer - state-of-the-art –ø–æ–¥—Ö–æ–¥ –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö!\n",
    "\n",
    "**–¢–µ–ø–µ—Ä—å –≤—ã –∑–Ω–∞–µ—Ç–µ:**\n",
    "- üìä –ö–∞–∫ –ø—Ä–∏–º–µ–Ω—è—Ç—å Transformers –∫ —Ç–∞–±–ª–∏—á–Ω—ã–º –¥–∞–Ω–Ω—ã–º\n",
    "- üß† –ß—Ç–æ —Ç–∞–∫–æ–µ contextual embeddings –¥–ª—è categorical features\n",
    "- üîç –ö–∞–∫ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ attention weights\n",
    "- üìà –ö–æ–≥–¥–∞ TabTransformer –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç tree-based –º–µ—Ç–æ–¥—ã\n",
    "- üöÄ –ö–∞–∫ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –±–æ–ª—å—à–∏–µ –¥–∞—Ç–∞—Å–µ—Ç—ã\n",
    "\n",
    "**–ö–ª—é—á–µ–≤–æ–π takeaway:**\n",
    "> \"Contextual embeddings —á–µ—Ä–µ–∑ Transformer –ø–æ–∑–≤–æ–ª—è—é—Ç categorical features —É—á–∏—Ç—å—Å—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥—Ä—É–≥ –¥—Ä—É–≥–∞, –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥—è —Å–ª–æ–∂–Ω—ã–µ interactions –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è simple MLP, –ø—Ä–∏ —ç—Ç–æ–º –æ—Å—Ç–∞–≤–∞—è—Å—å competitive —Å XGBoost/LightGBM!\"\n",
    "\n",
    "**–°–ª–µ–¥—É—é—â–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞:** Advanced Transformers (FT-Transformer, SAINT) üöÄ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}